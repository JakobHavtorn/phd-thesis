{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wav2vec 2.0\n",
    "- `wav2vec2-large` pretrained: https://huggingface.co/facebook/wav2vec2-large-lv60\n",
    "- `wav2vec2-large` fine-tuned: https://huggingface.co/facebook/wav2vec2-large-960h-lv60\n",
    "\n",
    "Whisper\n",
    "- `whisper-large-v2`: https://huggingface.co/openai/whisper-large-v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To precompute transcripts and other expensive things, run the notebook via ipython from the terminal:\n",
    "\n",
    "```bash\n",
    "ipython --to python --convert \"ssl_uncertainty.ipynb\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WAV2VEC_LARGE_PRETRAINED = \"facebook/wav2vec2-large-lv60\"\n",
    "WAV2VEC_LARGE_FINETUNED = \"facebook/wav2vec2-large-960h-lv60\"\n",
    "WHISPER_LARGE_V2 = \"openai/whisper-large-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGINFACE_HOME = \"/m2/research/huggingface\"\n",
    "TEMP_SAVE_DIR = \"/m2/research/jdh/thesis/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_HOME\"] = HUGGINFACE_HOME\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "from typing import List, Dict\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import numba\n",
    "import numpy as np\n",
    "import rich\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import Wav2Vec2Model, Wav2Vec2FeatureExtractor, Wav2Vec2ForCTC, Wav2Vec2FeatureExtractor, Wav2Vec2CTCTokenizer, Wav2Vec2Config\n",
    "from torch.utils.data import DataLoader\n",
    "from jiwer import wer, cer\n",
    "from tqdm import tqdm\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available(), torch.cuda.device_count())\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty in transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librispeech_test_clean = load_dataset(\"librispeech_asr\", \"clean\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v2_model = Wav2Vec2ForCTC.from_pretrained(WAV2VEC_LARGE_FINETUNED).to(device)\n",
    "w2v2_feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(WAV2VEC_LARGE_FINETUNED)\n",
    "w2v2_tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(WAV2VEC_LARGE_FINETUNED)\n",
    "# processor = Wav2Vec2Processor.from_pretrained(WAV2VEC_LARGE_FINETUNED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librispeech_test_clean[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, tokenizer, feature_extractor):\n",
    "    files = [b[\"file\"] for b in batch]\n",
    "    audios = [b[\"audio\"][\"array\"] for b in batch]\n",
    "    texts = [b[\"text\"] for b in batch]\n",
    "    speaker_ids = [b[\"speaker_id\"] for b in batch]\n",
    "    chapter_ids = [b[\"chapter_id\"] for b in batch]\n",
    "    overall_ids = [b[\"id\"] for b in batch]\n",
    "    lengths = [b[\"length\"] for b in batch]\n",
    "    \n",
    "    labels = tokenizer(texts, return_tensors=\"np\", padding=\"longest\", return_attention_mask=False).input_ids\n",
    "    features = feature_extractor(audios, sampling_rate=16_000, padding=\"longest\", return_tensors=\"np\", return_attention_mask=True, )\n",
    "    audios, attention_mask = features.input_values, features.attention_mask\n",
    "    \n",
    "    labels = torch.from_numpy(labels)\n",
    "    audios = torch.from_numpy(audios)\n",
    "    attention_mask = torch.from_numpy(attention_mask)\n",
    "    \n",
    "    batch = {\n",
    "        \"input_values\": audios,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "        \"speaker_ids\": speaker_ids,\n",
    "        \"chapter_ids\": chapter_ids,\n",
    "        \"ids\": overall_ids,\n",
    "        \"files\": files,\n",
    "        \"texts\": texts,\n",
    "        \"lengths\": torch.as_tensor(lengths, device=\"cpu\"),\n",
    "    }    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_length(example):\n",
    "    example[\"length\"] = len(example[\"audio\"][\"array\"])\n",
    "    return example\n",
    "\n",
    "librispeech_test_clean = librispeech_test_clean.map(compute_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn_w2v2 = functools.partial(collate_fn, tokenizer=w2v2_tokenizer, feature_extractor=w2v2_feature_extractor)\n",
    "dataloader = DataLoader(librispeech_test_clean, batch_size=1, collate_fn=collate_fn_w2v2, num_workers=4)\n",
    "iterator = iter(dataloader)\n",
    "next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.autocast(device_type=\"cuda\")\n",
    "@torch.inference_mode()\n",
    "def transcribe_batch(batch, model, tokenizer):\n",
    "    input_values = batch[\"input_values\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    logits = model(input_values, attention_mask=attention_mask).logits\n",
    "    \n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    transcripts = tokenizer.batch_decode(pred_ids)\n",
    "    \n",
    "    # remove padding from logits\n",
    "    lens = batch[\"lengths\"] // 320  # 320 is the model hop length\n",
    "    logits = logits.unbind(0)\n",
    "    logits = [logits[i][:lens[i], :] for i in range(len(lens))]\n",
    "    return batch[\"ids\"], transcripts, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def transcribe_dataset(dataloader, model, tokenizer):\n",
    "    ids = []\n",
    "    transcripts = []\n",
    "    logits = []\n",
    "    for batch in tqdm(dataloader, desc=\"Transcribing\"):\n",
    "        id, transcript, logit = transcribe_batch(batch, model, tokenizer)\n",
    "        transcripts.extend(transcript)\n",
    "        logits.extend(logit)\n",
    "        ids.extend(id)\n",
    "\n",
    "    return ids, transcripts, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe the dataset and save the results (load if exists already)\n",
    "\n",
    "LIBRISPEECH_TEST_CLEAN_WITH_W2V2_TRANSCRIPTS_DIR = os.path.join(TEMP_SAVE_DIR, \"librispeech_test_clean_with_w2v2_transcripts\")\n",
    "\n",
    "try:\n",
    "    librispeech_test_clean = load_from_disk(LIBRISPEECH_TEST_CLEAN_WITH_W2V2_TRANSCRIPTS_DIR)\n",
    "    assert \"transcript\" in librispeech_test_clean.column_names\n",
    "    logits = torch.load(os.path.join(LIBRISPEECH_TEST_CLEAN_WITH_W2V2_TRANSCRIPTS_DIR, \"logits.pt\"))\n",
    "except (AssertionError, FileNotFoundError):\n",
    "    ids, transcripts, logits = transcribe_dataset(dataloader, w2v2_model, w2v2_tokenizer)\n",
    "    librispeech_test_clean = librispeech_test_clean.add_column(\"transcript\", transcripts)\n",
    "    librispeech_test_clean.save_to_disk(LIBRISPEECH_TEST_CLEAN_WITH_W2V2_TRANSCRIPTS_DIR)\n",
    "    torch.save(logits, os.path.join(LIBRISPEECH_TEST_CLEAN_WITH_W2V2_TRANSCRIPTS_DIR, \"logits.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mc_transcripts(dataloader, model, tokenizer, num_samples):\n",
    "    \"\"\"Compute Monte Carlo transcripts for a dataset `num_samples` times.\"\"\"\n",
    "    model.train()  # set to train mode to enable dropout\n",
    "    mc_transcripts = []\n",
    "    mc_logits = []\n",
    "\n",
    "    bar = tqdm(range(num_samples), desc=\"Getting MC Transcripts\")\n",
    "    for i in bar:\n",
    "        out_dir = os.path.join(TEMP_SAVE_DIR, f\"librispeech_test_clean_with_w2v2_transcripts_mc_{i:03d}\")\n",
    "        bar.set_description(f\"Getting MC Transcripts {i:03d}\")\n",
    "        try:\n",
    "            mc_dataset = load_from_disk(out_dir)\n",
    "            transcripts = mc_dataset[\"mc_transcript\"]\n",
    "            logits = torch.load(os.path.join(out_dir, \"mc_logits.pt\"))\n",
    "        except:\n",
    "            transcripts, logits = transcribe_dataset(dataloader, model, tokenizer)\n",
    "            mc_dataset = librispeech_test_clean.add_column(\"mc_transcript\", transcripts)\n",
    "            mc_dataset.save_to_disk(out_dir)\n",
    "            torch.save(logits, os.path.join(out_dir, \"mc_logits.pt\"))\n",
    "\n",
    "        mc_transcripts.append(transcripts)\n",
    "        mc_logits.append(logits)\n",
    "\n",
    "    return mc_transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mc_transcripts(dataloader, model, tokenizer, num_samples):\n",
    "    \"\"\"Compute Monte Carlo transcripts for a dataset `num_samples` times.\"\"\"\n",
    "    model.train()  # set to train mode to enable dropout\n",
    "    \n",
    "    mc_ids = []\n",
    "    mc_transcripts = []\n",
    "    mc_logits = []\n",
    "    \n",
    "    # bar = tqdm(reversed(range(num_samples)), desc=\"Getting MC Transcripts\")\n",
    "    bar = tqdm(range(num_samples), desc=\"Getting MC Transcripts\")\n",
    "    for i in bar:\n",
    "        bar.set_description(f\"Getting MC Transcripts {i:03d}\")\n",
    "\n",
    "        try:\n",
    "            ids = torch.load(os.path.join(LIBRISPEECH_TEST_CLEAN_WITH_W2V2_TRANSCRIPTS_DIR, f\"mc_ids_{i:03d}.pt\"))\n",
    "            transcripts = torch.load(os.path.join(LIBRISPEECH_TEST_CLEAN_WITH_W2V2_TRANSCRIPTS_DIR, f\"mc_transcript_{i:03d}.pt\"))\n",
    "            logits = torch.load(os.path.join(LIBRISPEECH_TEST_CLEAN_WITH_W2V2_TRANSCRIPTS_DIR, f\"mc_logits_{i:03d}.pt\"))\n",
    "        except FileNotFoundError as exc:\n",
    "            print(exc)\n",
    "            ids, transcripts, logits = transcribe_dataset(dataloader, model, tokenizer)\n",
    "            torch.save(ids, os.path.join(LIBRISPEECH_TEST_CLEAN_WITH_W2V2_TRANSCRIPTS_DIR, f\"mc_ids_{i:03d}.pt\"))\n",
    "            torch.save(transcripts, os.path.join(LIBRISPEECH_TEST_CLEAN_WITH_W2V2_TRANSCRIPTS_DIR, f\"mc_transcript_{i:03d}.pt\"))\n",
    "            torch.save(logits, os.path.join(LIBRISPEECH_TEST_CLEAN_WITH_W2V2_TRANSCRIPTS_DIR, f\"mc_logits_{i:03d}.pt\"))\n",
    "\n",
    "        mc_ids.append(ids)\n",
    "        mc_transcripts.append(transcripts)\n",
    "        mc_logits.append(logits)\n",
    "\n",
    "    return mc_ids, mc_transcripts, mc_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and add Monte Carlo Dropout simulations\n",
    "NUM_SAMPLES = 256\n",
    "\n",
    "mc_ids, mc_transcripts, mc_logits = compute_mc_transcripts(dataloader, w2v2_model, w2v2_tokenizer, num_samples=NUM_SAMPLES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise Exception(\"Stop here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlignmentElement:\n",
    "    \"\"\"Class to store an element of an alignment between a reference and a Monte Carlo transcript.\"\"\"\n",
    "    def __init__(self, type_, hyp_word, ref_word):\n",
    "        self.type = type_\n",
    "        self.hyp_word = hyp_word\n",
    "        self.ref_word = ref_word\n",
    "        self.cer = cer(hyp_word, ref_word) if hyp_word != \"\" and ref_word != \"\" else 1.0  \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"AlignmentElement(type_='{self.type}', hyp_word='{self.hyp_word}', ref_word='{self.ref_word}')\"\n",
    "\n",
    "\n",
    "class Alignment:\n",
    "    \"\"\"Class to store an alignment between a reference and a Monte Carlo transcript.\"\"\"\n",
    "    def __init__(self, reference, prediction):\n",
    "        self.reference = reference\n",
    "        self.prediction = prediction\n",
    "        if len(prediction) == 0:\n",
    "            self.alignment = [AlignmentElement(\"OMITTED\", \"\", word) for word in reference.split()]\n",
    "            self.wer = 1.0\n",
    "            self.cer = 1.0\n",
    "        else:    \n",
    "            self.alignment = align_transcripts(reference, prediction)\n",
    "            self.wer = wer(reference, prediction)\n",
    "            self.cer = cer(reference, prediction)\n",
    "\n",
    "    def visualize(self, include_cer=False, other_numeric_scores: Dict[str, List[float]] = None):\n",
    "        visualize_alignment(self.alignment, include_cer=include_cer, other_numeric_scores=other_numeric_scores)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Alignment(\\n\\treference='{self.reference}',\\n\\tprediction='{self.prediction}'\\n)\"\n",
    "\n",
    "\n",
    "def levenstein_distance_nojit(reference: List[str], prediction: List[str]) -> List[List[int]]:\n",
    "    \"\"\"Compute the Levenstein distance between two tokenized strings.\"\"\"\n",
    "    \n",
    "    # Create a matrix to store alignment costs\n",
    "    alignment_matrix = np.zeros((len(reference) + 1, len(prediction) + 1))\n",
    "    \n",
    "    # Initialize the matrix with deletion costs for reference words\n",
    "    for i in range(len(reference) + 1):\n",
    "        alignment_matrix[i][0] = i\n",
    "    \n",
    "    # Initialize the matrix with insertion costs for Monte Carlo words\n",
    "    for j in range(len(prediction) + 1):\n",
    "        alignment_matrix[0][j] = j\n",
    "    \n",
    "    # Fill in the alignment matrix\n",
    "    for i in range(1, len(reference) + 1):\n",
    "        for j in range(1, len(prediction) + 1):\n",
    "            cost = 0 if reference[i - 1] == prediction[j - 1] else 1\n",
    "            alignment_matrix[i][j] = min(\n",
    "                alignment_matrix[i - 1][j] + 1,        # Deletion\n",
    "                alignment_matrix[i][j - 1] + 1,        # Insertion\n",
    "                alignment_matrix[i - 1][j - 1] + cost  # Substitution\n",
    "            )\n",
    "    \n",
    "    return alignment_matrix\n",
    "\n",
    "\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def levenstein_distance(reference: List[str], prediction: List[str]) -> np.ndarray:\n",
    "    \"\"\"Compute the Levenstein distance between two tokenized strings.\"\"\"\n",
    "    \n",
    "    # Create a matrix to store alignment costs\n",
    "    alignment_matrix = np.zeros((len(reference) + 1, len(prediction) + 1))\n",
    "    \n",
    "    # Initialize the matrix with deletion costs for reference words\n",
    "    alignment_matrix[:, 0] = np.arange(len(reference) + 1)\n",
    "    \n",
    "    # Initialize the matrix with insertion costs for Monte Carlo words\n",
    "    alignment_matrix[0, :] = np.arange(len(prediction) + 1)\n",
    "    \n",
    "    # Fill in the alignment matrix\n",
    "    for i in range(1, len(reference) + 1):\n",
    "        for j in range(1, len(prediction) + 1):\n",
    "            cost = 0 if reference[i - 1] == prediction[j - 1] else 1\n",
    "            alignment_matrix[i][j] = min(\n",
    "                alignment_matrix[i - 1][j] + 1,        # Deletion\n",
    "                alignment_matrix[i][j - 1] + 1,        # Insertion\n",
    "                alignment_matrix[i - 1][j - 1] + cost  # Substitution\n",
    "            )\n",
    "    \n",
    "    return alignment_matrix\n",
    "    \n",
    "\n",
    "def align_transcripts(reference: str, prediction: str) -> List[AlignmentElement]:\n",
    "    reference_words = reference.split()\n",
    "    prediction_words = prediction.split()\n",
    "    \n",
    "    alignment_matrix = levenstein_distance(reference.split(), prediction.split())\n",
    "    \n",
    "    # Backtrace to find the alignment\n",
    "    i, j = len(reference_words), len(prediction_words)\n",
    "    alignment = []\n",
    "    \n",
    "    while i > 0 or j > 0:\n",
    "        if i > 0 and alignment_matrix[i][j] == alignment_matrix[i - 1][j] + 1:  # Deletion\n",
    "            alignment.insert(0, AlignmentElement(\"OMITTED\", \"\", reference_words[i - 1]))\n",
    "            i -= 1\n",
    "        elif j > 0 and alignment_matrix[i][j] == alignment_matrix[i][j - 1] + 1:  # Insertion\n",
    "            alignment.insert(0, AlignmentElement(\"EXTRA\", prediction_words[j - 1], \"\"))\n",
    "            j -= 1\n",
    "        else:\n",
    "            if reference_words[i - 1] != prediction_words[j - 1]:  # Substitution\n",
    "                alignment.insert(0, AlignmentElement(\"MISSPELLED\", prediction_words[j - 1], reference_words[i - 1]))\n",
    "            else:  # Match\n",
    "                alignment.insert(0, AlignmentElement(\"MATCHED\", prediction_words[j - 1], reference_words[i - 1]))\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "    \n",
    "    return alignment\n",
    "\n",
    "\n",
    "def visualize_alignment(alignment: List[AlignmentElement], include_cer=False, other_numeric_scores: Dict[str, List[float]] = None):\n",
    "    \"\"\"Visualize the reference transcript and a potentially erroneous transcript given in `alignment`. \n",
    "    \n",
    "    `alignment` is a list that contains tuples of (type, hyp_word, ref_word) where type is one of \"MATCHED\", \n",
    "    \"MISSPELLED\", \"OMITTED\", \"EXTRA\" and hyp_word is the word in the potentially erroneous transcript and ref_word \n",
    "    is the word in the reference transcript.\n",
    "    \n",
    "    The two transcripts are printed on top of each other with the words aligned.\n",
    "    \"\"\"\n",
    "    \n",
    "    row_ref = \"<tr><td>Reference</td>\"\n",
    "    row_pre = \"<tr><td>Prediction</td>\"\n",
    "    row_cer = \"<tr><td>CER</td>\"\n",
    "    \n",
    "    for element in alignment:\n",
    "        if element.type == \"MATCHED\":\n",
    "            row_ref += f\"<td>{element.ref_word}</td>\"\n",
    "            row_pre += f\"<td>{element.hyp_word}</td>\"\n",
    "        elif element.type == \"MISSPELLED\":\n",
    "            row_ref += f\"<td>{element.ref_word}</td>\"\n",
    "            row_pre += f\"<td style='color:red'>{element.hyp_word}</td>\"\n",
    "        elif element.type == \"OMITTED\":\n",
    "            row_ref += f\"<td style='color:green'>{element.ref_word}</td>\"\n",
    "            row_pre += \"<td>-</td>\"\n",
    "        elif element.type == \"EXTRA\":\n",
    "            row_ref += \"<td>-</td>\"\n",
    "            row_pre += f\"<td style='color:orange'>{element.hyp_word}</td>\"\n",
    "\n",
    "        if include_cer:\n",
    "            row_cer += f\"<td>{element.cer:.2f}</td>\"\n",
    "\n",
    "    html = \"<table>\"\n",
    "    html += row_ref + \"</tr>\"\n",
    "    html += row_pre + \"</tr>\"\n",
    "\n",
    "    if include_cer:\n",
    "        html += row_cer + \"</tr>\"\n",
    "\n",
    "    if other_numeric_scores is not None:\n",
    "        for k, v in other_numeric_scores.items():\n",
    "            html += f\"<tr><td>{k}</td>\"\n",
    "            html += \"\".join([f\"<td>{score:.2f}</td>\" for score in v])\n",
    "            html += \"</tr>\"\n",
    "\n",
    "    html += \"</table>\"\n",
    "    \n",
    "    display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "reference_transcript = \"the quick brown fox jumps far\"\n",
    "predicted_transcript = \"and the quck broown fox jumps\"\n",
    "\n",
    "alignment = Alignment(reference_transcript, predicted_transcript)\n",
    "\n",
    "alignment.visualize(include_cer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_id = 15\n",
    "\n",
    "reference = librispeech_test_clean[example_id][\"text\"]\n",
    "prediction = librispeech_test_clean[example_id][\"transcript\"]\n",
    "mc_prediction = mc_transcripts[0][example_id]\n",
    "\n",
    "print(\"Gold VS Prediction\")\n",
    "alignment = Alignment(reference, prediction)\n",
    "alignment.visualize(include_cer=True)\n",
    "\n",
    "print(\"Prediction VS MC Prediction\")\n",
    "alignment = Alignment(prediction, mc_prediction)\n",
    "alignment.visualize(include_cer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the alignment to the prediction for all Monte Carlo transcripts\n",
    "alignments_pred_to_gold = []\n",
    "alignments_mc_to_pred = []\n",
    "\n",
    "mc_transcripts_transposed = list(zip(*mc_transcripts))\n",
    "for i in tqdm(range(len(librispeech_test_clean)), desc=\"Computing alignments\"):\n",
    "    reference = librispeech_test_clean[i][\"text\"]\n",
    "    prediction = librispeech_test_clean[i][\"transcript\"]\n",
    "\n",
    "    alignments_pred_to_gold.append(Alignment(reference, prediction))\n",
    "    alignments_mc_to_pred.append([Alignment(prediction, prediction_mc) for prediction_mc in mc_transcripts_transposed[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the list of CERs over Monte Carlo transcripts for each word in the reference transcript\n",
    "\n",
    "all_errors = [defaultdict(list) for _ in range(len(librispeech_test_clean))]\n",
    "misspellings = [defaultdict(list) for _ in range(len(librispeech_test_clean))]\n",
    "fraction_misspellings = [defaultdict(int) for _ in range(len(librispeech_test_clean))]\n",
    "for i in tqdm(range(len(librispeech_test_clean)), desc=\"Counting misspellings and omissions\"):\n",
    "\n",
    "    # idx2word = {i: e.ref_word for i, e in enumerate(alignments_mc_to_pred_i[0].alignment)}\n",
    "    \n",
    "    # Get the alignments\n",
    "    alignments_mc_to_pred_i = alignments_mc_to_pred[i]  # Get the alignments for all Monte Carlo transcripts\n",
    "    for alignment_mc_to_pred in alignments_mc_to_pred_i:\n",
    "\n",
    "        # Keep track of how many times each reference word has been seen in a single alignment.\n",
    "        # Some words might be used multiple times and we must discern between them.\n",
    "        word_count = defaultdict(int)\n",
    "\n",
    "        # Iterate over the alignment and save the CER for each word\n",
    "        for element in alignment_mc_to_pred.alignment:\n",
    "            word_count[element.ref_word] += 1\n",
    "            \n",
    "            k = element.ref_word + \"_\" * (word_count[element.ref_word] - 1)\n",
    "            all_errors[i][k].append(element.cer)\n",
    "\n",
    "            if element.type != \"EXTRA\":\n",
    "                misspellings[i][k].append(element.cer)\n",
    "                fraction_misspellings[i][k] += 1 / NUM_SAMPLES\n",
    "                \n",
    "print(\"Num CERs in gold: \", len([e.cer for a in alignments_pred_to_gold for e in a.alignment if e.type != \"OMITTED\"]))\n",
    "print(\"Num CERs in pred: \", len([cer for error_cer in misspellings for cer in error_cer.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_misspellings = [{k: np.median(v) for k, v in misspelling.items()} for misspelling in misspellings]\n",
    "mean_misspellings = [{k: np.mean(v) for k, v in misspelling.items()} for misspelling in misspellings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_id = 59\n",
    "alignments_pred_to_gold[example_id].visualize(include_cer=True, other_numeric_scores={\"Median CER\": median_misspellings[example_id].values()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cers_from_prediction = np.array([e.cer for a in alignments_pred_to_gold for e in a.alignment if e.type != \"OMITTED\"])\n",
    "cers_from_montecarlo = np.array([cer for misspelling_cer in median_misspellings for cer in misspelling_cer.values()])\n",
    "# cers_from_montecarlo = np.array([cer for misspelling_cer in mean_misspellings for cer in misspelling_cer.values()])\n",
    "cers_from_prediction.shape, cers_from_montecarlo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6.4, 4.8))\n",
    "ax.scatter(cers_from_prediction, cers_from_montecarlo, alpha=0.1)\n",
    "\n",
    "ax.text(0.76, 0.95, f\"Pearson: {scipy.stats.pearsonr(cers_from_prediction, cers_from_montecarlo)[0]:.3f}\", transform=ax.transAxes)\n",
    "ax.text(0.73, 0.90, f\"Spearman: {scipy.stats.spearmanr(cers_from_prediction, cers_from_montecarlo)[0]:.3f}\", transform=ax.transAxes)\n",
    "\n",
    "ax.set_xlabel(\"CER of standard transcript cf. target\")\n",
    "ax.set_ylabel(\"Median CER of MC transcripts cf. standard transcript\")\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "fig.savefig(\"mc_transcript_cer_scatter.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Above but heatmap\n",
    "from matplotlib.colors import LogNorm\n",
    "import seaborn as sns\n",
    "\n",
    "data = np.stack([cers_from_prediction, cers_from_montecarlo], axis=0)\n",
    "kde = scipy.stats.gaussian_kde(data, bw_method=0.3)\n",
    "\n",
    "xmin, xmax = 0, 1\n",
    "ymin, ymax = 0, 1\n",
    "\n",
    "# log_norm = LogNorm(vmin=data.min().min(), vmax=data.max().max())\n",
    "xx, yy = np.meshgrid(np.linspace(xmin, xmax, 100), np.linspace(ymin, ymax, 100))\n",
    "positions = np.vstack([xx.ravel(), yy.ravel()])\n",
    "# values = np.vstack([x, y])\n",
    "\n",
    "f = np.log(np.reshape(kde(positions).T, xx.shape))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca()\n",
    "\n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_ylim(ymin, ymax)\n",
    "\n",
    "ax.contourf(xx, yy, f, 15, cmap='Blues', label=\"Log-density\")\n",
    "cset = ax.contour(xx, yy, f, 15, colors='k', linewidths=0.5)\n",
    "\n",
    "ax.scatter(cers_from_prediction, cers_from_montecarlo, alpha=0.1, marker=\".\", s=1, color=\"black\", label=\"Data\")\n",
    "\n",
    "ax.clabel(cset, inline=1, fontsize=10)\n",
    "ax.set_xlabel(\"CER from Prediction\")\n",
    "ax.set_ylabel(\"CER from Monte Carlo Dropout\")\n",
    "# ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is there a correlation (per word) between the fraction of MC transcripts that are wrong compared to the standard transcript and whether the standard transcript was correct compared to the target?\n",
    "# Is there a correlation (per word) between the fraction of MC transcripts that are wrong compared to the standard transcript and the CER of the standard transcript compared to the target?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty in representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librispeech_test_clean = load_dataset(\"librispeech_asr\", \"clean\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librispeech_test_clean.with_format(\"torch\", device=device)\n",
    "librispeech_test_clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processor = Wav2Vec2Processor.from_pretrained(WAV2VEC_LARGE_PRETRAINED)\n",
    "wav2vec_config = Wav2Vec2Config.from_pretrained(WAV2VEC_LARGE_PRETRAINED)\n",
    "wav2vec_feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(WAV2VEC_LARGE_PRETRAINED)\n",
    "wav2vec_large_pretrained = Wav2Vec2Model.from_pretrained(WAV2VEC_LARGE_PRETRAINED).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav2vec_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_of_dicts_to_dict_of_lists(list_of_dicts):\n",
    "    return {k: [d[k] for d in list_of_dicts] for k in list_of_dicts[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = librispeech_test_clean[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"file\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = wav2vec_feature_extractor(batch[\"audio\"][\"array\"], sample_rate=16000, return_tensors=\"pt\", padding=\"longest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_values = inputs.input_values.to(\"cuda\")\n",
    "attention_mask = inputs.attention_mask.to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    features = wav2vec_large_pretrained(input_values, attention_mask=attention_mask, output_hidden_states=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_wav2vec2_features(batch):\n",
    "    print(batch)\n",
    "    inputs = wav2vec_feature_extractor(batch[\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\")\n",
    "    input_values = inputs.input_values.to(\"cuda\")\n",
    "    attention_mask = inputs.attention_mask.to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features = wav2vec_large_pretrained(input_values, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        \n",
    "        batch[\"features-15\"] = features.hidden_states[15]\n",
    "\n",
    "    return batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librispeech_test_clean.map(map_to_wav2vec2_features, batched=True, batch_size=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
