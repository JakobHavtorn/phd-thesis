{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wav2vec 2.0\n",
    "- `wav2vec2-large` pretrained: https://huggingface.co/facebook/wav2vec2-large-lv60\n",
    "- `wav2vec2-large` fine-tuned: https://huggingface.co/facebook/wav2vec2-large-960h-lv60\n",
    "\n",
    "Whisper\n",
    "- `whisper-large-v2`: https://huggingface.co/openai/whisper-large-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WAV2VEC_LARGE_PRETRAINED = \"facebook/wav2vec2-large-lv60\"\n",
    "WAV2VEC_LARGE_FINETUNED = \"facebook/wav2vec2-large-960h-lv60\"\n",
    "WHISPER_LARGE_V2 = \"openai/whisper-large-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGINFACE_HOME = \"/m2/research/huggingface\"\n",
    "TEMP_SAVE_DIR = \"/m2/research/jdh/thesis/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_HOME\"] = HUGGINFACE_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import functools\n",
    "\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import Wav2Vec2Model, Wav2Vec2FeatureExtractor, Wav2Vec2ForCTC, Wav2Vec2FeatureExtractor, Wav2Vec2CTCTokenizer, Wav2Vec2Config\n",
    "from torch.utils.data import DataLoader\n",
    "from jiwer import wer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available(), torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "\n",
    "torch.set_default_device(torch.device('cuda:0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty in transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librispeech_test_clean = load_dataset(\"librispeech_asr\", \"clean\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_length(example):\n",
    "    example[\"length\"] = len(example[\"audio\"][\"array\"])\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librispeech_test_clean = librispeech_test_clean.map(compute_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v2_model = Wav2Vec2ForCTC.from_pretrained(WAV2VEC_LARGE_FINETUNED).to(device)\n",
    "w2v2_feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(WAV2VEC_LARGE_FINETUNED)\n",
    "w2v2_tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(WAV2VEC_LARGE_FINETUNED)\n",
    "# processor = Wav2Vec2Processor.from_pretrained(WAV2VEC_LARGE_FINETUNED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librispeech_test_clean[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, tokenizer, feature_extractor):\n",
    "    files = [b[\"file\"] for b in batch]\n",
    "    audios = [b[\"audio\"][\"array\"] for b in batch]\n",
    "    texts = [b[\"text\"] for b in batch]\n",
    "    speaker_ids = [b[\"speaker_id\"] for b in batch]\n",
    "    chapter_ids = [b[\"chapter_id\"] for b in batch]\n",
    "    overall_ids = [b[\"id\"] for b in batch]\n",
    "    \n",
    "    labels = tokenizer(texts, return_tensors=\"np\", padding=\"longest\", return_attention_mask=False).input_ids\n",
    "    features = feature_extractor(audios, sampling_rate=16_000, padding=\"longest\", return_tensors=\"np\", return_attention_mask=True, )\n",
    "    audios, attention_mask = features.input_values, features.attention_mask\n",
    "    \n",
    "    labels = torch.from_numpy(labels)\n",
    "    audios = torch.from_numpy(audios)\n",
    "    attention_mask = torch.from_numpy(attention_mask)\n",
    "    \n",
    "    batch = {\n",
    "        \"input_values\": audios,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "        \"speaker_ids\": speaker_ids,\n",
    "        \"chapter_ids\": chapter_ids,\n",
    "        \"ids\": overall_ids,\n",
    "        \"files\": files,\n",
    "        \"texts\": texts\n",
    "    }    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn_w2v2 = functools.partial(collate_fn, tokenizer=w2v2_tokenizer, feature_extractor=w2v2_feature_extractor)\n",
    "dataloader = DataLoader(librispeech_test_clean, batch_size=8, collate_fn=collate_fn_w2v2, num_workers=4)\n",
    "iterator = iter(dataloader)\n",
    "next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.autocast(device_type=\"cuda\")\n",
    "@torch.inference_mode()\n",
    "def transcribe_batch(batch, model, tokenizer):\n",
    "    input_values = batch[\"input_values\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    logits = model(input_values, attention_mask=attention_mask).logits\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    pred_str = tokenizer.batch_decode(pred_ids)\n",
    "    return pred_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_dataset(dataloader, model, tokenizer):\n",
    "    transcripts = []\n",
    "    for batch in tqdm(dataloader):\n",
    "        pred_str = transcribe_batch(batch, model, tokenizer)\n",
    "        transcripts.extend(pred_str)\n",
    "    return transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIBRISPEECH_TEST_CLEAN_WITH_W2V2_TRANSCRIPTS_DIR = os.path.join(TEMP_SAVE_DIR, \"librispeech_test_clean_with_w2v2_transcripts\")\n",
    "try:\n",
    "    librispeech_test_clean = load_from_disk(LIBRISPEECH_TEST_CLEAN_WITH_W2V2_TRANSCRIPTS_DIR)\n",
    "    assert \"transcript\" in librispeech_test_clean.column_names\n",
    "except (AssertionError, FileNotFoundError):\n",
    "    transcripts = transcribe_dataset(dataloader, w2v2_model, w2v2_tokenizer)\n",
    "    librispeech_test_clean = librispeech_test_clean.add_column(\"transcript\", transcripts)\n",
    "    librispeech_test_clean.save_to_disk(LIBRISPEECH_TEST_CLEAN_WITH_W2V2_TRANSCRIPTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mc_transcripts(dataloader, model, tokenizer, num_samples):\n",
    "    model.train()\n",
    "    mc_transcripts = []\n",
    "    for i in tqdm(range(num_samples)):\n",
    "        out_dir = os.path.join(TEMP_SAVE_DIR, f\"librispeech_test_clean_with_w2v2_transcripts_mc_{i:03d}\")\n",
    "        try:\n",
    "            mc_dataset = load_from_disk(out_dir)\n",
    "            transcripts = mc_dataset[\"mc_transcript\"]\n",
    "        except:\n",
    "            transcripts = transcribe_dataset(dataloader, model, tokenizer)\n",
    "            mc_dataset = librispeech_test_clean.add_column(\"mc_transcript\", transcripts)\n",
    "            mc_dataset.save_to_disk(out_dir)\n",
    "\n",
    "        mc_transcripts.append(transcripts)\n",
    "\n",
    "    return transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and add Monte Carlo Dropout simulations\n",
    "mc_transcripts = compute_mc_transcripts(dataloader, w2v2_model, w2v2_tokenizer, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librispeech_test_clean[\"transcript\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"STOP HERE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_pred(batch):\n",
    "    print(batch)\n",
    "    audios = [a[\"array\"] for a in batch[\"audio\"]]\n",
    "    sampling_rate = batch[\"audio\"][0][\"sampling_rate\"]\n",
    "    inputs = processor(audios, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=\"longest\")\n",
    "    input_values = inputs.input_values.to(device)  # .to(torch.float16)\n",
    "    attention_mask = inputs.attention_mask.to(device)  # .to(torch.float16)\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        logits = model(input_values, attention_mask=attention_mask).logits\n",
    "\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = processor.batch_decode(predicted_ids)\n",
    "    batch[\"transcription\"] = transcription\n",
    "    return batch\n",
    "\n",
    "\n",
    "try:\n",
    "    librispeech_test_clean = load_from_disk(TEMP_SAVE_DIR + \"librispeech_test_clean\")\n",
    "    assert \"transcription\" in librispeech_test_clean\n",
    "except:\n",
    "    result = librispeech_test_clean.map(map_to_pred, batched=True, batch_size=8)#, remove_columns=[\"speech\"])\n",
    "    librispeech_test_clean.save_to_disk(TEMP_SAVE_DIR + \"librispeech_test_clean\")\n",
    "\n",
    "print(\"WER:\", wer(result[\"text\"], result[\"transcription\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(librispeech_test_clean, batch_size=8, shuffle=False, collate_fn=lambda x: x, drop_last=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty in representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librispeech_test_clean = load_dataset(\"librispeech_asr\", \"clean\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librispeech_test_clean.with_format(\"torch\", device=device)\n",
    "librispeech_test_clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processor = Wav2Vec2Processor.from_pretrained(WAV2VEC_LARGE_PRETRAINED)\n",
    "wav2vec_config = Wav2Vec2Config.from_pretrained(WAV2VEC_LARGE_PRETRAINED)\n",
    "wav2vec_feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(WAV2VEC_LARGE_PRETRAINED)\n",
    "wav2vec_large_pretrained = Wav2Vec2Model.from_pretrained(WAV2VEC_LARGE_PRETRAINED).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav2vec_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_of_dicts_to_dict_of_lists(list_of_dicts):\n",
    "    return {k: [d[k] for d in list_of_dicts] for k in list_of_dicts[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = librispeech_test_clean[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"file\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = wav2vec_feature_extractor(batch[\"audio\"][\"array\"], sample_rate=16000, return_tensors=\"pt\", padding=\"longest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_values = inputs.input_values.to(\"cuda\")\n",
    "attention_mask = inputs.attention_mask.to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    features = wav2vec_large_pretrained(input_values, attention_mask=attention_mask, output_hidden_states=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_wav2vec2_features(batch):\n",
    "    print(batch)\n",
    "    inputs = wav2vec_feature_extractor(batch[\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\")\n",
    "    input_values = inputs.input_values.to(\"cuda\")\n",
    "    attention_mask = inputs.attention_mask.to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features = wav2vec_large_pretrained(input_values, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        \n",
    "        batch[\"features-15\"] = features.hidden_states[15]\n",
    "\n",
    "    return batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librispeech_test_clean.map(map_to_wav2vec2_features, batched=True, batch_size=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
