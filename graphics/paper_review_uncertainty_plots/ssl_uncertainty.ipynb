{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wav2vec 2.0\n",
    "- `wav2vec2-large` pretrained: https://huggingface.co/facebook/wav2vec2-large-lv60\n",
    "- `wav2vec2-large` fine-tuned: https://huggingface.co/facebook/wav2vec2-large-960h-lv60\n",
    "\n",
    "Whisper\n",
    "- `whisper-large-v2`: https://huggingface.co/openai/whisper-large-v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To precompute transcripts and other expensive things, run the notebook via ipython from the terminal:\n",
    "\n",
    "```bash\n",
    "ipython --to python --convert \"ssl_uncertainty.ipynb\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WAV2VEC_LARGE_PRETRAINED = \"facebook/wav2vec2-large-lv60\"\n",
    "WAV2VEC_LARGE_FINETUNED = \"facebook/wav2vec2-large-960h-lv60\"\n",
    "WHISPER_LARGE_V2 = \"openai/whisper-large-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGINFACE_HOME = \"/m2/research/huggingface\"\n",
    "TEMP_SAVE_DIR = \"/m2/research/jdh/thesis/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_HOME\"] = HUGGINFACE_HOME\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "from typing import List, Dict\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import numba\n",
    "import numpy as np\n",
    "import rich\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import jiwer\n",
    "\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import Wav2Vec2Model, Wav2Vec2FeatureExtractor, Wav2Vec2ForCTC, Wav2Vec2FeatureExtractor, Wav2Vec2CTCTokenizer, Wav2Vec2Config\n",
    "from torch.utils.data import DataLoader\n",
    "from jiwer import wer, cer\n",
    "from tqdm import tqdm\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available(), torch.cuda.device_count())\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, feature_extractor, tokenizer=None):\n",
    "    files = [b[\"file\"] for b in batch]\n",
    "    audios = [b[\"audio\"][\"array\"] for b in batch]\n",
    "    texts = [b[\"text\"] for b in batch]\n",
    "    speaker_ids = [b[\"speaker_id\"] for b in batch]\n",
    "    chapter_ids = [b[\"chapter_id\"] for b in batch]\n",
    "    overall_ids = [b[\"id\"] for b in batch]\n",
    "    \n",
    "    features = feature_extractor(audios, sampling_rate=16_000, padding=\"longest\", return_tensors=\"np\", return_attention_mask=True, )\n",
    "    audios, attention_mask = features.input_values, features.attention_mask\n",
    "    \n",
    "    audios = torch.from_numpy(audios)\n",
    "    attention_mask = torch.from_numpy(attention_mask)\n",
    "    \n",
    "    out_batch = {\n",
    "        \"input_values\": audios,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"speaker_ids\": speaker_ids,\n",
    "        \"chapter_ids\": chapter_ids,\n",
    "        \"ids\": overall_ids,\n",
    "        \"files\": files,\n",
    "        \"texts\": texts,\n",
    "    }\n",
    "    if all(\"length\" in b for b in batch):\n",
    "        lengths = [b[\"length\"] for b in batch]\n",
    "        out_batch[\"lengths\"] = torch.as_tensor(lengths, device=\"cpu\")\n",
    "    \n",
    "    if tokenizer is not None:\n",
    "        labels = tokenizer(texts, return_tensors=\"np\", padding=\"longest\", return_attention_mask=False).input_ids\n",
    "        labels = torch.from_numpy(labels)\n",
    "        out_batch[\"labels\"] = labels\n",
    "\n",
    "    return out_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.autocast(device_type=\"cuda\")\n",
    "@torch.inference_mode()\n",
    "def transcribe_batch(batch, model, tokenizer):\n",
    "    input_values = batch[\"input_values\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    logits = model(input_values, attention_mask=attention_mask).logits\n",
    "    \n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    transcripts = tokenizer.batch_decode(pred_ids)\n",
    "    \n",
    "    # remove padding from logits\n",
    "    lens = batch[\"lengths\"] // 320  # 320 is the model hop length\n",
    "    logits = logits.cpu().unbind(0)\n",
    "    logits = [logits[i][:lens[i], :] for i in range(len(lens))]\n",
    "    return batch[\"ids\"], transcripts, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty in transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librispeech_test_clean = load_dataset(\"librispeech_asr\", \"clean\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_length(example):\n",
    "    example[\"length\"] = len(example[\"audio\"][\"array\"])\n",
    "    return example\n",
    "\n",
    "librispeech_test_clean = librispeech_test_clean.map(compute_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v2_model = Wav2Vec2ForCTC.from_pretrained(WAV2VEC_LARGE_FINETUNED).to(device)\n",
    "w2v2_feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(WAV2VEC_LARGE_FINETUNED)\n",
    "w2v2_tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(WAV2VEC_LARGE_FINETUNED)\n",
    "# processor = Wav2Vec2Processor.from_pretrained(WAV2VEC_LARGE_FINETUNED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn_w2v2 = functools.partial(collate_fn, tokenizer=w2v2_tokenizer, feature_extractor=w2v2_feature_extractor)\n",
    "dataloader = DataLoader(librispeech_test_clean, batch_size=1, collate_fn=collate_fn_w2v2, num_workers=0)\n",
    "iterator = iter(dataloader)\n",
    "next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librispeech_test_clean[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe the dataset and save the results (load if exists already)\n",
    "\n",
    "LIBRISPEECH_TEST_CLEAN_WITH_W2V2_TRANSCRIPTS_DIR = os.path.join(TEMP_SAVE_DIR, \"librispeech_test_clean_with_w2v2_transcripts\")\n",
    "\n",
    "try:\n",
    "    librispeech_test_clean = load_from_disk(LIBRISPEECH_TEST_CLEAN_WITH_W2V2_TRANSCRIPTS_DIR)\n",
    "    assert \"transcript\" in librispeech_test_clean.column_names\n",
    "    logits = torch.load(os.path.join(LIBRISPEECH_TEST_CLEAN_WITH_W2V2_TRANSCRIPTS_DIR, \"logits.pt\"), map_location=\"cpu\")\n",
    "except (AssertionError, FileNotFoundError):\n",
    "    ids, transcripts, logits = transcribe_dataset(dataloader, w2v2_model, w2v2_tokenizer)\n",
    "    librispeech_test_clean = librispeech_test_clean.add_column(\"transcript\", transcripts)\n",
    "    librispeech_test_clean.save_to_disk(LIBRISPEECH_TEST_CLEAN_WITH_W2V2_TRANSCRIPTS_DIR)\n",
    "    torch.save(logits, os.path.join(LIBRISPEECH_TEST_CLEAN_WITH_W2V2_TRANSCRIPTS_DIR, \"logits.pt\"), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mc_transcripts(dataloader, model, tokenizer, num_samples):\n",
    "    \"\"\"Compute Monte Carlo transcripts for a dataset `num_samples` times.\"\"\"\n",
    "    model.train()  # set to train mode to enable dropout\n",
    "    \n",
    "    mc_ids = []\n",
    "    mc_transcripts = []\n",
    "    mc_logits = []\n",
    "    \n",
    "    # bar = tqdm(reversed(range(num_samples)), desc=\"Getting MC Transcripts\")\n",
    "    bar = tqdm(range(num_samples), desc=\"Getting MC Transcripts\")\n",
    "    for i in bar:\n",
    "        bar.set_description(f\"Getting MC Transcripts {i:03d}\")\n",
    "\n",
    "        try:\n",
    "            ids = torch.load(os.path.join(LIBRISPEECH_TEST_CLEAN_WITH_W2V2_TRANSCRIPTS_DIR, f\"mc_ids_{i:03d}.pt\"), map_location=\"cpu\")\n",
    "            transcripts = torch.load(os.path.join(LIBRISPEECH_TEST_CLEAN_WITH_W2V2_TRANSCRIPTS_DIR, f\"mc_transcript_{i:03d}.pt\"), map_location=\"cpu\")\n",
    "            logits = torch.load(os.path.join(LIBRISPEECH_TEST_CLEAN_WITH_W2V2_TRANSCRIPTS_DIR, f\"mc_logits_{i:03d}.pt\"), map_location=\"cpu\")\n",
    "        except FileNotFoundError as exc:\n",
    "            print(exc)\n",
    "            ids, transcripts, logits = transcribe_dataset(dataloader, model, tokenizer)\n",
    "            torch.save(ids, os.path.join(LIBRISPEECH_TEST_CLEAN_WITH_W2V2_TRANSCRIPTS_DIR, f\"mc_ids_{i:03d}.pt\"))\n",
    "            torch.save(transcripts, os.path.join(LIBRISPEECH_TEST_CLEAN_WITH_W2V2_TRANSCRIPTS_DIR, f\"mc_transcript_{i:03d}.pt\"))\n",
    "            torch.save(logits, os.path.join(LIBRISPEECH_TEST_CLEAN_WITH_W2V2_TRANSCRIPTS_DIR, f\"mc_logits_{i:03d}.pt\"))\n",
    "\n",
    "        mc_ids.append(ids)\n",
    "        mc_transcripts.append(transcripts)\n",
    "        mc_logits.append(logits)\n",
    "\n",
    "    return mc_ids, mc_transcripts, mc_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and add Monte Carlo Dropout simulations\n",
    "NUM_SAMPLES = 256\n",
    "\n",
    "mc_ids, mc_transcripts, mc_logits = compute_mc_transcripts(dataloader, w2v2_model, w2v2_tokenizer, num_samples=NUM_SAMPLES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise Exception(\"Stop here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def levenstein_distance(reference: List[str], prediction: List[str]) -> np.ndarray:\n",
    "    \"\"\"Compute the Levenstein distance between two tokenized strings.\"\"\"\n",
    "    \n",
    "    # Create a matrix to store alignment costs\n",
    "    alignment_matrix = np.zeros((len(reference) + 1, len(prediction) + 1))\n",
    "    \n",
    "    # Initialize the matrix with deletion costs for reference words\n",
    "    alignment_matrix[:, 0] = np.arange(len(reference) + 1)\n",
    "    \n",
    "    # Initialize the matrix with insertion costs for Monte Carlo words\n",
    "    alignment_matrix[0, :] = np.arange(len(prediction) + 1)\n",
    "    \n",
    "    # Fill in the alignment matrix\n",
    "    for i in range(1, len(reference) + 1):\n",
    "        for j in range(1, len(prediction) + 1):\n",
    "            cost = 0 if reference[i - 1] == prediction[j - 1] else 1\n",
    "            alignment_matrix[i][j] = min(\n",
    "                alignment_matrix[i - 1][j] + 1,        # Deletion\n",
    "                alignment_matrix[i][j - 1] + 1,        # Insertion\n",
    "                alignment_matrix[i - 1][j - 1] + cost  # Substitution\n",
    "            )\n",
    "    \n",
    "    return alignment_matrix\n",
    "\n",
    "\n",
    "def align_transcripts_charlevel(reference: str, prediction: str) -> List[CharAlignmentElement]:\n",
    "    reference_chars = list(reference)\n",
    "    prediction_chars = list(prediction)\n",
    "    \n",
    "    alignment_matrix = levenstein_distance(reference_chars, prediction_chars)\n",
    "    \n",
    "    # Backtrace to find the alignment\n",
    "    i, j = len(reference_chars), len(prediction_chars)\n",
    "    alignment = []\n",
    "    \n",
    "    while i > 0 or j > 0:\n",
    "        if i > 0 and alignment_matrix[i][j] == alignment_matrix[i - 1][j] + 1:  # Deletion\n",
    "            alignment.insert(0, CharAlignmentElement(\"OMITTED\", \"\", reference_chars[i - 1]))\n",
    "            i -= 1\n",
    "        elif j > 0 and alignment_matrix[i][j] == alignment_matrix[i][j - 1] + 1:  # Insertion\n",
    "            alignment.insert(0, CharAlignmentElement(\"EXTRA\", prediction_chars[j - 1], \"\"))\n",
    "            j -= 1\n",
    "        else:\n",
    "            if reference_chars[i - 1] != prediction_chars[j - 1]:  # Substitution\n",
    "                alignment.insert(0, CharAlignmentElement(\"MISSPELLED\", prediction_chars[j - 1], reference_chars[i - 1]))\n",
    "            else:  # Match\n",
    "                alignment.insert(0, CharAlignmentElement(\"MATCHED\", prediction_chars[j - 1], reference_chars[i - 1]))\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "    \n",
    "    return alignment\n",
    "\n",
    "\n",
    "MISSING_INDICATOR = \"□\"\n",
    "WHITESPACE = \" \"\n",
    "WHITESPACE_TOKEN = \"·\"\n",
    "PUNCTUATION = [\".\", \",\", \"!\", \"?\", \":\", \";\", \"'\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"-\", \"_\", \"/\", \"\\\\\", \"|\", \"@\", \"#\", \"$\", \"%\", \"^\", \"&\", \"*\", \"~\", \"`\", \"+\", \"=\", \"<\", \">\"]\n",
    "\n",
    "\n",
    "class CharAlignmentElement:\n",
    "    \"\"\"Class to store an element of an alignment between a reference and a Monte Carlo transcript.\"\"\"\n",
    "    def __init__(self, type_, hyp_char, ref_char):\n",
    "        self.type = type_\n",
    "        self.hyp_char = hyp_char if hyp_char != WHITESPACE else WHITESPACE_TOKEN\n",
    "        self.ref_char = ref_char if ref_char != WHITESPACE else WHITESPACE_TOKEN\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"CharAlignmentElement(type_='{self.type}', hyp_char='{self.hyp_char}', ref_char='{self.ref_char}')\"\n",
    "\n",
    "\n",
    "class WordAlignmentElement:\n",
    "    def __init__(self, char_alignment: List[CharAlignmentElement]):\n",
    "        self.char_alignment = char_alignment\n",
    "        \n",
    "        self.hyp_chars = [e.hyp_char for e in char_alignment]\n",
    "        self.ref_chars = [e.ref_char for e in char_alignment]\n",
    "\n",
    "        self.hyp_word = \"\".join(self.hyp_chars).strip(WHITESPACE_TOKEN)  # trailing whitespace is not part of word\n",
    "        self.ref_word = \"\".join(self.ref_chars).strip(WHITESPACE_TOKEN)  # trailing whitespace is not part of word\n",
    "        \n",
    "        if self.hyp_word != \"\" and self.ref_word != \"\":\n",
    "            self.cer = cer(\n",
    "                self.ref_word,\n",
    "                self.hyp_word,\n",
    "                reference_transform=jiwer.transforms.ReduceToListOfListOfChars(),\n",
    "                hypothesis_transform=jiwer.transforms.ReduceToListOfListOfChars()\n",
    "            )\n",
    "        elif self.hyp_word == \"\" and self.ref_word == \"\":\n",
    "            self.cer = 0.0\n",
    "        elif self.ref_word == \"\":\n",
    "            self.cer = np.inf\n",
    "        else:\n",
    "            self.cer = 1.0\n",
    "\n",
    "        if self.hyp_word == self.ref_word:\n",
    "            self.type = \"MATCHED\"\n",
    "        elif self.hyp_word == \"\":\n",
    "            self.type = \"OMITTED\"\n",
    "        elif self.ref_word == \"\":\n",
    "            self.type = \"EXTRA\"\n",
    "        else:\n",
    "            self.type = \"MISSPELLED\"\n",
    "\n",
    "        self.hyp_word_aligned = \"\"\n",
    "        self.ref_word_aligned = \"\"\n",
    "        self.hyp_word_aligned_colored = \"\"\n",
    "        self.ref_word_aligned_colored = \"\"\n",
    "\n",
    "        for element in self.char_alignment:\n",
    "            if element.type == \"MATCHED\":\n",
    "                self.ref_word_aligned += element.ref_char\n",
    "                self.hyp_word_aligned += element.hyp_char\n",
    "                self.ref_word_aligned_colored += element.ref_char\n",
    "                self.hyp_word_aligned_colored += element.hyp_char\n",
    "            elif element.type == \"MISSPELLED\":\n",
    "                self.ref_word_aligned += element.ref_char\n",
    "                self.hyp_word_aligned += element.hyp_char\n",
    "                self.ref_word_aligned_colored += f\"[orange1]{element.ref_char}[/orange1]\"\n",
    "                self.hyp_word_aligned_colored += f\"[orange1]{element.hyp_char}[/orange1]\"\n",
    "            elif element.type == \"OMITTED\":\n",
    "                self.ref_word_aligned += element.ref_char\n",
    "                self.hyp_word_aligned += f\"{MISSING_INDICATOR}\"\n",
    "                self.ref_word_aligned_colored += f\"[red]{element.ref_char}[/red]\"\n",
    "                self.hyp_word_aligned_colored += f\"[red]{MISSING_INDICATOR}[/red]\"\n",
    "            elif element.type == \"EXTRA\":\n",
    "                self.ref_word_aligned += f\"{MISSING_INDICATOR}\"\n",
    "                self.hyp_word_aligned += element.hyp_char\n",
    "                self.ref_word_aligned_colored += f\"[green]{MISSING_INDICATOR}[/green]\"\n",
    "                self.hyp_word_aligned_colored += f\"[green]{element.hyp_char}[/green]\"\n",
    "\n",
    "        self.hyp_word_aligned = self.hyp_word_aligned.strip(WHITESPACE_TOKEN)\n",
    "        self.ref_word_aligned = self.ref_word_aligned.strip(WHITESPACE_TOKEN)\n",
    "        self.hyp_word_aligned_colored = self.hyp_word_aligned_colored.strip(WHITESPACE_TOKEN)\n",
    "        self.ref_word_aligned_colored = self.ref_word_aligned_colored.strip(WHITESPACE_TOKEN)\n",
    "\n",
    "    def print_raw(self):\n",
    "        rich.print(self.ref_word)\n",
    "        rich.print(self.hyp_word)\n",
    "\n",
    "    def print_aligned(self):\n",
    "        rich.print(self.ref_word_aligned)\n",
    "        rich.print(self.hyp_word_aligned)\n",
    "\n",
    "    def print_aligned_colored(self):\n",
    "        rich.print(self.ref_word_aligned_colored)\n",
    "        rich.print(self.hyp_word_aligned_colored)\n",
    "\n",
    "    def __repr__(self):\n",
    "        s = \"WordAlignmentElement(\\n\"\n",
    "        s += f\"    ref_word='{self.ref_word_aligned}',\\n\"\n",
    "        s += f\"    hyp_word='{self.hyp_word_aligned}',\\n\"\n",
    "        s += f\"    type_='{self.type}',\\n\"\n",
    "        s += f\"    cer={self.cer:.2f}\\n\"\n",
    "        s += \")\"\n",
    "        return s\n",
    "\n",
    "\n",
    "class Alignment:\n",
    "    \"\"\"Class to store an alignment between a reference and a Monte Carlo transcript.\"\"\"\n",
    "    def __init__(self, reference: str, prediction: str):\n",
    "        self.reference = reference.replace(WHITESPACE, WHITESPACE_TOKEN)\n",
    "        self.prediction = prediction.replace(WHITESPACE, WHITESPACE_TOKEN)\n",
    "\n",
    "        if len(prediction) == 0:\n",
    "            self.char_alignments = [CharAlignmentElement(\"OMITTED\", \"\", char) for char in reference]\n",
    "            self.wer = 1.0\n",
    "            self.cer = 1.0\n",
    "        elif len(reference) == 0:\n",
    "            self.char_alignments = [CharAlignmentElement(\"EXTRA\", char, \"\") for char in prediction]\n",
    "            self.wer = np.inf\n",
    "            self.cer = np.inf\n",
    "        else:\n",
    "            self.char_alignments = align_transcripts_charlevel(reference, prediction)\n",
    "            self.wer = wer(reference, prediction)\n",
    "            self.cer = cer(reference, prediction)\n",
    "\n",
    "        self.word_alignments = []\n",
    "        \n",
    "        ref_ended = False\n",
    "        hyp_ended = False\n",
    "        i = 0\n",
    "        for j in range(len(self.char_alignments)):\n",
    "            ref_ended = ref_ended or self.char_alignments[j].ref_char == WHITESPACE_TOKEN\n",
    "            hyp_ended = hyp_ended or self.char_alignments[j].hyp_char == WHITESPACE_TOKEN\n",
    "            \n",
    "            if ref_ended and hyp_ended:\n",
    "                self.word_alignments.append(WordAlignmentElement(self.char_alignments[i:j]))\n",
    "                i = j\n",
    "                ref_ended = False\n",
    "                hyp_ended = False\n",
    "\n",
    "        self.word_alignments.append(WordAlignmentElement(self.char_alignments[i:]))\n",
    "\n",
    "        self.reference_aligned = WHITESPACE_TOKEN.join([e.ref_word_aligned for e in self.word_alignments])\n",
    "        self.prediction_aligned = WHITESPACE_TOKEN.join([e.hyp_word_aligned for e in self.word_alignments])\n",
    "\n",
    "        self.reference_aligned_colored = WHITESPACE_TOKEN.join([e.ref_word_aligned_colored for e in self.word_alignments])\n",
    "        self.prediction_aligned_colored = WHITESPACE_TOKEN.join([e.hyp_word_aligned_colored for e in self.word_alignments])\n",
    "\n",
    "        # Compute corrected WER and missing/extra space error\n",
    "        self.num_spaces = sum([1 for e in self.char_alignments if e.ref_char == WHITESPACE_TOKEN])\n",
    "        self.num_space_substitutions = sum([1 for e in self.char_alignments if e.type == \"MISSPELLED\" and e.hyp_char == WHITESPACE_TOKEN])\n",
    "        self.num_space_insertions = sum([1 for e in self.char_alignments if e.type == \"EXTRA\" and e.hyp_char == WHITESPACE_TOKEN])\n",
    "        self.num_space_deletions = sum([1 for e in self.char_alignments if e.type == \"OMITTED\" and e.ref_char == WHITESPACE_TOKEN])\n",
    "        print(self.num_spaces, self.num_space_substitutions, self.num_space_insertions, self.num_space_deletions)\n",
    "        if self.num_spaces == 0:\n",
    "            self.space_error_rate = np.inf\n",
    "        else:\n",
    "            self.space_error_rate = (self.num_space_substitutions + self.num_space_insertions + self.num_space_deletions) / self.num_spaces\n",
    "        \n",
    "        self.corrected_wer = 0.0\n",
    "        self.missing_space_error = 0.0\n",
    "        self.extra_space_error = 0.0\n",
    "        for word_alignment in self.word_alignments:\n",
    "            # if word_alignment.type == \"MATCHED\":\n",
    "            #     self.corrected_wer += word_alignment.cer\n",
    "            if word_alignment.type == \"MISSPELLED\":\n",
    "                self.corrected_wer += 1.0\n",
    "            elif word_alignment.type == \"OMITTED\":\n",
    "                self.missing_space_error += 1.0\n",
    "            elif word_alignment.type == \"EXTRA\":\n",
    "                self.extra_space_error += 1.0\n",
    "                \n",
    "        self.corrected_wer /= len(self.word_alignments)\n",
    "\n",
    "    def print_raw(self):\n",
    "        rich.print(self.reference, self.prediction, sep=\"\\n\", end=\"\\n\")\n",
    "\n",
    "    def print_aligned(self):\n",
    "        rich.print(self.reference_aligned, self.prediction_aligned, sep=\"\\n\", end=\"\\n\")\n",
    "\n",
    "    def print_aligned_colored(self):\n",
    "        rich.print(self.reference_aligned_colored, self.prediction_aligned_colored, sep=\"\\n\", end=\"\\n\")\n",
    "\n",
    "    def report_errors(self):\n",
    "        s = \"\"\n",
    "        s = s + f\"WER  : {self.wer:.2f}\\n\"\n",
    "        s = s + f\"CER  : {self.cer:.2f}\\n\"\n",
    "        s = s + f\"CWER : {self.corrected_wer:.2f}\\n\"\n",
    "        s = s + f\"SER  : {self.space_error_rate:.2f}\\n\"\n",
    "        rich.print(s)\n",
    "        return s\n",
    "    \n",
    "    def __repr__(self):\n",
    "        s = \"Alignment(\\n\"\n",
    "        s += f\"    ref='{self.reference_aligned}',\\n\"\n",
    "        s += f\"    hyp='{self.prediction_aligned}'\\n\"\n",
    "        s += f\"    wer={self.wer:.2f},\\n\"\n",
    "        s += f\"    cer={self.cer:.2f}\\n\"\n",
    "        s += \")\"\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = \"hej digder karl smart\"\n",
    "prediction = \"hejdig der carl smat\"\n",
    "\n",
    "alignment = Alignment(reference, prediction)\n",
    "alignment.print_raw()\n",
    "alignment.print_aligned_colored()\n",
    "alignment.report_errors();\n",
    "\n",
    "for word_alignment in alignment.word_alignments:\n",
    "    print(word_alignment)\n",
    "\n",
    "alignment.num_spaces, alignment.num_space_substitutions, alignment.num_space_insertions, alignment.num_space_deletions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment.word_alignments[1].ref_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = \"hejsa du der\"\n",
    "prediction = \"hej sa duder\"\n",
    "\n",
    "alignment = Alignment(reference, prediction)\n",
    "alignment.print_raw()\n",
    "alignment.print_aligned_colored()\n",
    "alignment.report_errors();\n",
    "\n",
    "for word_alignment in alignment.word_alignments:\n",
    "    print(word_alignment)\n",
    "\n",
    "alignment.num_spaces, alignment.num_space_substitutions, alignment.num_space_insertions, alignment.num_space_deletions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_id = 20\n",
    "\n",
    "reference = librispeech_test_clean[example_id][\"text\"]\n",
    "prediction = librispeech_test_clean[example_id][\"transcript\"]\n",
    "mc_prediction = mc_transcripts[0][example_id]\n",
    "\n",
    "print(\"Gold VS Prediction\")\n",
    "alignment = Alignment(reference, prediction)\n",
    "alignment.print_aligned_colored()\n",
    "\n",
    "print(\"Prediction VS MC Prediction\")\n",
    "alignment = Alignment(prediction, mc_prediction)\n",
    "alignment.print_aligned_colored()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment, alignment.word_alignments[0], alignment.word_alignments[-4], alignment.word_alignments[-7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_id = 20\n",
    "\n",
    "reference = librispeech_test_clean[example_id][\"text\"]\n",
    "prediction = librispeech_test_clean[example_id][\"transcript\"]\n",
    "mc_prediction = mc_transcripts[0][example_id]\n",
    "\n",
    "print(\"Gold VS Prediction\")\n",
    "alignment = CharAlignment(reference, prediction)\n",
    "alignment.visualize(include_cer=True)\n",
    "\n",
    "print(\"Prediction VS MC Prediction\")\n",
    "alignment = CharAlignment(prediction, mc_prediction)\n",
    "alignment.visualize(include_cer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_id = 20\n",
    "\n",
    "reference = librispeech_test_clean[example_id][\"text\"]\n",
    "prediction = librispeech_test_clean[example_id][\"transcript\"]\n",
    "mc_prediction = mc_transcripts[0][example_id]\n",
    "\n",
    "print(\"Gold VS Prediction\")\n",
    "alignment = CharAlignment(reference, prediction)\n",
    "alignment.visualize(include_cer=True)\n",
    "\n",
    "print(\"Prediction VS MC Prediction\")\n",
    "alignment = CharAlignment(prediction, mc_prediction)\n",
    "alignment.visualize(include_cer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_id = 20\n",
    "\n",
    "reference = librispeech_test_clean[example_id][\"text\"]\n",
    "prediction = librispeech_test_clean[example_id][\"transcript\"]\n",
    "mc_prediction = mc_transcripts[0][example_id]\n",
    "\n",
    "print(\"Gold VS Prediction\")\n",
    "alignment = WordAlignment(reference, prediction)\n",
    "alignment.visualize(include_cer=True)\n",
    "alignment = CharAlignment(reference, prediction)\n",
    "alignment.visualize(include_cer=True)\n",
    "\n",
    "print(\"Prediction VS MC Prediction\")\n",
    "alignment = WordAlignment(prediction, mc_prediction)\n",
    "alignment.visualize(include_cer=True)\n",
    "alignment = CharAlignment(prediction, mc_prediction)\n",
    "alignment.visualize(include_cer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the alignment to the prediction for all Monte Carlo transcripts\n",
    "alignments_pred_to_gold = []\n",
    "alignments_mc_to_pred = []\n",
    "\n",
    "mc_transcripts_transposed = list(zip(*mc_transcripts))\n",
    "for i in tqdm(range(len(librispeech_test_clean)), desc=\"Computing alignments\"):\n",
    "    reference = librispeech_test_clean[i][\"text\"]\n",
    "    prediction = librispeech_test_clean[i][\"transcript\"]\n",
    "\n",
    "    alignments_pred_to_gold.append(Alignment(reference, prediction))\n",
    "    alignments_mc_to_pred.append([Alignment(prediction, prediction_mc) for prediction_mc in mc_transcripts_transposed[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the list of CERs over Monte Carlo transcripts for each word in the reference transcript\n",
    "\n",
    "all_errors = [defaultdict(list) for _ in range(len(librispeech_test_clean))]\n",
    "misspellings = [defaultdict(list) for _ in range(len(librispeech_test_clean))]\n",
    "fraction_misspellings = [defaultdict(int) for _ in range(len(librispeech_test_clean))]\n",
    "for i in tqdm(range(len(librispeech_test_clean)), desc=\"Counting misspellings and omissions\"):\n",
    "\n",
    "    # idx2word = {i: e.ref_word for i, e in enumerate(alignments_mc_to_pred_i[0].alignment)}\n",
    "    \n",
    "    # Get the alignments\n",
    "    alignments_mc_to_pred_i = alignments_mc_to_pred[i]  # Get the alignments for all Monte Carlo transcripts\n",
    "    for alignment_mc_to_pred in alignments_mc_to_pred_i:\n",
    "\n",
    "        # Keep track of how many times each reference word has been seen in a single alignment.\n",
    "        # Some words might be used multiple times and we must discern between them.\n",
    "        word_count = defaultdict(int)\n",
    "\n",
    "        # Iterate over the alignment and save the CER for each word\n",
    "        for element in alignment_mc_to_pred.alignment:\n",
    "            word_count[element.ref_word] += 1\n",
    "            \n",
    "            k = element.ref_word + \"_\" * (word_count[element.ref_word] - 1)\n",
    "            all_errors[i][k].append(element.cer)\n",
    "\n",
    "            if element.type != \"EXTRA\":\n",
    "                misspellings[i][k].append(element.cer)\n",
    "                fraction_misspellings[i][k] += 1 / NUM_SAMPLES\n",
    "                \n",
    "print(\"Num CERs in gold: \", len([e.cer for a in alignments_pred_to_gold for e in a.alignment if e.type != \"OMITTED\"]))\n",
    "print(\"Num CERs in pred: \", len([cer for error_cer in misspellings for cer in error_cer.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_misspellings = [{k: np.median(v) for k, v in misspelling.items()} for misspelling in misspellings]\n",
    "mean_misspellings = [{k: np.mean(v) for k, v in misspelling.items()} for misspelling in misspellings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_id = 59\n",
    "alignments_pred_to_gold[example_id].visualize(include_cer=True, other_numeric_scores={\"Median CER\": median_misspellings[example_id].values()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cers_from_prediction = np.array([e.cer for a in alignments_pred_to_gold for e in a.alignment if e.type != \"OMITTED\"])\n",
    "cers_from_montecarlo = np.array([cer for misspelling_cer in median_misspellings for cer in misspelling_cer.values()])\n",
    "# cers_from_montecarlo = np.array([cer for misspelling_cer in mean_misspellings for cer in misspelling_cer.values()])\n",
    "cers_from_prediction.shape, cers_from_montecarlo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6.4, 4.8))\n",
    "ax.scatter(cers_from_prediction, cers_from_montecarlo, alpha=0.1)\n",
    "\n",
    "ax.text(0.76, 0.95, f\"Pearson: {scipy.stats.pearsonr(cers_from_prediction, cers_from_montecarlo)[0]:.3f}\", transform=ax.transAxes)\n",
    "ax.text(0.73, 0.90, f\"Spearman: {scipy.stats.spearmanr(cers_from_prediction, cers_from_montecarlo)[0]:.3f}\", transform=ax.transAxes)\n",
    "\n",
    "ax.set_xlabel(\"CER of standard transcript cf. target\")\n",
    "ax.set_ylabel(\"Median CER of MC transcripts cf. standard transcript\")\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "fig.savefig(\"mc_transcript_cer_scatter.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Above but heatmap\n",
    "from matplotlib.colors import LogNorm\n",
    "import seaborn as sns\n",
    "\n",
    "data = np.stack([cers_from_prediction, cers_from_montecarlo], axis=0)\n",
    "kde = scipy.stats.gaussian_kde(data, bw_method=0.3)\n",
    "\n",
    "xmin, xmax = 0, 1\n",
    "ymin, ymax = 0, 1\n",
    "\n",
    "# log_norm = LogNorm(vmin=data.min().min(), vmax=data.max().max())\n",
    "xx, yy = np.meshgrid(np.linspace(xmin, xmax, 100), np.linspace(ymin, ymax, 100))\n",
    "positions = np.vstack([xx.ravel(), yy.ravel()])\n",
    "# values = np.vstack([x, y])\n",
    "\n",
    "f = np.log(np.reshape(kde(positions).T, xx.shape))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca()\n",
    "\n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_ylim(ymin, ymax)\n",
    "\n",
    "ax.contourf(xx, yy, f, 15, cmap='Blues', label=\"Log-density\")\n",
    "cset = ax.contour(xx, yy, f, 15, colors='k', linewidths=0.5)\n",
    "\n",
    "ax.scatter(cers_from_prediction, cers_from_montecarlo, alpha=0.1, marker=\".\", s=1, color=\"black\", label=\"Data\")\n",
    "\n",
    "ax.clabel(cset, inline=1, fontsize=10)\n",
    "ax.set_xlabel(\"CER from Prediction\")\n",
    "ax.set_ylabel(\"CER from Monte Carlo Dropout\")\n",
    "# ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is there a correlation (per word) between the fraction of MC transcripts that are wrong compared to the standard transcript and whether the standard transcript was correct compared to the target?\n",
    "# Is there a correlation (per word) between the fraction of MC transcripts that are wrong compared to the standard transcript and the CER of the standard transcript compared to the target?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty in representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librispeech_test_clean = load_dataset(\"librispeech_asr\", \"clean\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_length(example):\n",
    "    example[\"length\"] = len(example[\"audio\"][\"array\"])\n",
    "    return example\n",
    "\n",
    "librispeech_test_clean = librispeech_test_clean.map(compute_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processor = Wav2Vec2Processor.from_pretrained(WAV2VEC_LARGE_PRETRAINED)\n",
    "wav2vec_config = Wav2Vec2Config.from_pretrained(WAV2VEC_LARGE_PRETRAINED)\n",
    "w2v2_feature_extractor_pretrained = Wav2Vec2FeatureExtractor.from_pretrained(WAV2VEC_LARGE_PRETRAINED)\n",
    "wav2vec_large_pretrained = Wav2Vec2Model.from_pretrained(WAV2VEC_LARGE_PRETRAINED).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav2vec_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn_w2v2 = functools.partial(collate_fn, feature_extractor=w2v2_feature_extractor_pretrained)\n",
    "dataloader = DataLoader(librispeech_test_clean, batch_size=1, collate_fn=collate_fn_w2v2, num_workers=4)\n",
    "iterator = iter(dataloader)\n",
    "batch = next(iterator)\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_values = batch[\"input_values\"].to(\"cuda\")\n",
    "attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    features = wav2vec_large_pretrained(input_values, attention_mask=attention_mask, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.extract_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features.hidden_states), features.hidden_states[0].shape, features.hidden_states[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.autocast(device_type=\"cuda\")\n",
    "@torch.inference_mode()\n",
    "def extract_features_batch(batch, model):\n",
    "    input_values = batch[\"input_values\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    features = model(input_values, attention_mask=attention_mask, output_hidden_states=True)\n",
    "    \n",
    "    lens = batch[\"lengths\"] // 320  # 320 is the model hop length\n",
    "    \n",
    "    # remove padding from features\n",
    "    features_18 = features.hidden_states[17].cpu().unbind(0)\n",
    "    features_18 = [features_18[i][:lens[i], :] for i in range(len(lens))]\n",
    "    \n",
    "    features_15 = features.hidden_states[14].cpu().unbind(0)\n",
    "    features_15 = [features_15[i][:lens[i], :] for i in range(len(lens))]\n",
    "    \n",
    "    features = {\n",
    "        15: features_15,\n",
    "        18: features_18,\n",
    "    }\n",
    "    \n",
    "    return batch[\"ids\"], features\n",
    "\n",
    "@torch.autocast(device_type=\"cuda\")\n",
    "@torch.inference_mode()\n",
    "def extract_features_dataset(dataloader, model):\n",
    "    ids = []\n",
    "    features = defaultdict(list)\n",
    "    for batch in tqdm(dataloader, desc=\"Extracting features\"):\n",
    "        id, feat = extract_features_batch(batch, model)\n",
    "        ids.extend(id)\n",
    "        for k, v in feat.items():\n",
    "            features[k].extend(v)\n",
    "\n",
    "    return ids, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIBRISPEECH_TEST_CLEAN_WITH_W2V2_FEATURES_DIR = os.path.join(TEMP_SAVE_DIR, \"librispeech_test_clean_with_w2v2_features\")\n",
    "os.makedirs(LIBRISPEECH_TEST_CLEAN_WITH_W2V2_FEATURES_DIR, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    features = torch.load(os.path.join(LIBRISPEECH_TEST_CLEAN_WITH_W2V2_FEATURES_DIR, \"features.pt\"), map_location=\"cpu\")\n",
    "except (AssertionError, FileNotFoundError):\n",
    "    ids, features = extract_features_dataset(dataloader, wav2vec_large_pretrained)\n",
    "    torch.save(features, os.path.join(LIBRISPEECH_TEST_CLEAN_WITH_W2V2_FEATURES_DIR, \"features.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
