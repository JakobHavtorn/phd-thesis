@online{_babylonian_2008,
  title = {Babylonian {{Mathematics}} [{{CDLI Wiki}}]},
  date = {2008-08-14},
  url = {https://cdli.ox.ac.uk/wiki/doku.php?id=babylonian_mathematics},
  urldate = {2023-08-25}
}

@online{_frontiers_,
  title = {Frontiers | {{On Assessing Trustworthy AI}} in {{Healthcare}}. {{Machine Learning}} as a {{Supportive Tool}} to {{Recognize Cardiac Arrest}} in {{Emergency Calls}} | {{Human Dynamics}}},
  url = {https://www.frontiersin.org/articles/10.3389/fhumd.2021.673104/full?utm_source=F-NTF&utm_medium=EMLX&utm_campaign=PRD_FEOPS_20170000_ARTICLE},
  urldate = {2021-09-06}
}

@inproceedings{abati_conditional_2020,
  title = {Conditional {{Channel Gated Networks}} for {{Task-Aware Continual Learning}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Abati, Davide and Tomczak, Jakub and Blankevoort, Tijmen and Calderara, Simone and Cucchiara, Rita and Bejnordi, Babak Ehteshami},
  date = {2020-06},
  pages = {3930--3939},
  publisher = {{IEEE}},
  location = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.00399},
  url = {https://ieeexplore.ieee.org/document/9156310/},
  urldate = {2022-03-30},
  abstract = {Convolutional Neural Networks experience catastrophic forgetting when optimized on a sequence of learning problems: as they meet the objective of the current training examples, their performance on previous tasks drops drastically. In this work, we introduce a novel framework to tackle this problem with conditional computation. We equip each convolutional layer with task-specific gating modules, selecting which filters to apply on the given input. This way, we achieve two appealing properties. Firstly, the execution patterns of the gates allow to identify and protect important filters, ensuring no loss in the performance of the model for previously learned tasks. Secondly, by using a sparsity objective, we can promote the selection of a limited set of kernels, allowing to retain sufficient model capacity to digest new tasks. Existing solutions require, at test time, awareness of the task to which each example belongs to. This knowledge, however, may not be available in many practical scenarios. Therefore, we additionally introduce a task classifier that predicts the task label of each example, to deal with settings in which a task oracle is not available. We validate our proposal on four continual learning datasets. Results show that our model consistently outperforms existing methods both in the presence and the absence of a task oracle. Notably, on Split SVHN and Imagenet-50 datasets, our model yields up to 23.98\% and 17.42\% improvement in accuracy w.r.t. competing methods.},
  eventtitle = {{{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72817-168-5},
  langid = {english}
}

@inproceedings{abdolmaleki_deriving_2017,
  title = {Deriving and Improving {{CMA-ES}} with Information Geometric Trust Regions},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference}} on   - {{GECCO}} '17},
  author = {Abdolmaleki, Abbas and Price, Bob and Lau, Nuno and Reis, Luis Paulo and Neumann, Gerhard},
  date = {2017},
  number = {17},
  eprint = {1602.05561v1},
  eprinttype = {arxiv},
  pages = {657--664},
  location = {{Berlin, Germany}},
  issn = {16130073},
  doi = {10.1145/3071178.3071252},
  url = {http://dl.acm.org/citation.cfm?doid=3071178.3071252},
  urldate = {2017-11-27},
  abstract = {CMA-ES is one of the most popular stochastic search algorithms. It performs favourably in many tasks without the need of extensive parameter tuning. The algorithm has many beneficial properties, including automatic step-size adaptation, efficient covariance up-dates that incorporates the current samples as well as the evolution path and its invariance properties. Its update rules are composed of well established heuristics where the theoretical foundations of some of these rules are also well understood. In this paper we will fully derive all CMA-ES update rules within the framework of expectation-maximisation-based stochastic search algorithms using information-geometric trust regions. We show that the use of the trust region results in similar updates to CMA-ES for the mean and the covariance matrix while it allows for the derivation of an improved update rule for the step-size. Our new algorithm, Trust-Region Co-variance Matrix Adaptation Evolution Strategy (TR-CMA-ES) is fully derived from first order optimization principles and performs favourably in compare to standard CMA-ES algorithm.},
  isbn = {978-1-4503-4920-8}
}

@online{abstreiter_diffusionbased_2022,
  title = {Diffusion-{{Based Representation Learning}}},
  author = {Abstreiter, Korbinian and Mittal, Sarthak and Bauer, Stefan and Schölkopf, Bernhard and Mehrjou, Arash},
  date = {2022-08-01},
  eprint = {2105.14257},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2105.14257},
  urldate = {2022-09-21},
  abstract = {Diffusion-based methods represented as stochastic differential equations on a continuous-time domain have recently proven successful as a non-adversarial generative model. Training such models relies on denoising score matching, which can be seen as multi-scale denoising autoencoders. Here, we augment the denoising score matching framework to enable representation learning without any supervised signal. GANs and VAEs learn representations by directly transforming latent codes to data samples. In contrast, the introduced diffusionbased representation learning relies on a new formulation of the denoising score matching objective and thus encodes the information needed for denoising. We illustrate how this difference allows for manual control of the level of details encoded in the representation. Using the same approach, we propose to learn an infinite-dimensional latent code that achieves improvements of state-of-the-art models on semi-supervised image classification. We also compare the quality of learned representations of diffusion score matching with other methods like autoencoder and contrastively trained systems through their performances on downstream tasks.},
  langid = {english},
  pubstate = {preprint}
}

@incollection{aggarwal_surprising_2001,
  title = {On the {{Surprising Behavior}} of {{Distance Metrics}} in {{High Dimensional Space}}},
  author = {Aggarwal, Charu C. and Hinneburg, Alexander and Keim, Daniel A.},
  date = {2001},
  eprint = {25246403},
  eprinttype = {pmid},
  pages = {420--434},
  issn = {0956-7925},
  doi = {10.1007/3-540-44503-X_27},
  abstract = {In recent years, the effect of the curse of high dimensionality has been studied in great detail on several problems such as clustering, nearest neighbor search, and indexing. In high dimensional space the data becomes sparse, and traditional indexing and algorithmic techniques fail from a effciency and/or effectiveness perspective. Recent research results show that in high dimensional space, the concept of proximity, distance or nearest neighbor may not even be qualitatively meaningful. In this paper, we view the dimensionality curse from the point of view of the distance metrics which are used to measure the similarity between objects. We specifically examine the behavior of the commonly used L k norm and show that the problem of meaningfulness in high dimensionality is sensitive to the value of k. For example, this means that the Manhattan distance metric L(1 norm) is consistently more preferable than the Euclidean distance metric L(2 norm) for high dimensional data mining applications. Using the intuition derived from our analysis, we introduce and examine a natural extension of the L k norm to fractional distance metrics. We show that the fractional distance metric provides more meaningful results both from the theoretical and empirical perspective. The results show that fractional distance metrics can significantly improve the effectiveness of standard clustering algorithms such as the k-means algorithm.},
  isbn = {978-3-540-41456-8},
  keywords = {★}
}

@article{agic_multilingual_2016,
  title = {Multilingual {{Projection}} for {{Parsing Truly Low-Resource Languages}}},
  author = {Agić, Željko and Johannsen, Anders and Plank, Barbara and Alonso, Héctor Martínez and Schluter, Natalie and Søgaard, Anders},
  date = {2016-12},
  journaltitle = {Transactions of the Association for Computational Linguistics (ACL)},
  volume = {4},
  pages = {301--312},
  url = {https://www.mitpressjournals.org/doi/abs/10.1162/tacl_a_00100},
  urldate = {2019-06-11},
  abstract = {We propose a novel approach to cross-lingual part-of-speech tagging and dependency parsing for truly low-resource languages. Our annotation projection-based approach yields tagging and parsing models for over 100 languages. All that is needed are freely available parallel texts, and taggers and parsers for resource-rich languages. The empirical evaluation across 30 test languages shows that our method consistently provides top-level accuracies, close to established upper bounds, and outperforms several competitive baselines.},
  langid = {english}
}

@inproceedings{ahmad_interpretable_2018,
  title = {Interpretable Machine Learning in Healthcare},
  booktitle = {Proceedings of the 2018 {{ACM}} International Conference on Bioinformatics, Computational Biology, and Health Informatics},
  author = {Ahmad, Muhammad Aurangzeb and Eckert, Carly and Teredesai, Ankur},
  date = {2018},
  pages = {559--560}
}

@inproceedings{ahmadian_likelihoodfree_2021,
  title = {Likelihood-Free out-of-Distribution Detection with Invertible Generative Models},
  booktitle = {Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, {{IJCAI-21}}},
  author = {Ahmadian, Amirhossein and Lindsten, Fredrik},
  editor = {Zhou, Zhi-Hua},
  date = {2021-08},
  pages = {2119--2125},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  doi = {10.24963/ijcai.2021/292},
  url = {https://doi.org/10.24963/ijcai.2021/292}
}

@inproceedings{aksan_stcn_2019,
  title = {{{STCN}}: {{Stochastic Temporal Convolutional Networks}}},
  shorttitle = {{{STCN}}},
  booktitle = {Proceedings of the 7th {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Aksan, Emre and Hilliges, Otmar},
  date = {2019-02-18},
  location = {{New Orleans, LA, USA}},
  abstract = {Convolutional architectures have recently been shown to be competitive on many sequence modelling tasks when compared to the de-facto standard of recurrent neural networks (RNNs), while providing computational and modeling advantages due to inherent parallelism. However, currently there remains a performance gap to more expressive stochastic RNN variants, especially those with several layers of dependent random variables. In this work, we propose stochastic temporal convolutional networks (STCNs), a novel architecture that combines the computational advantages of temporal convolutional networks (TCN) with the representational power and robustness of stochastic latent spaces. In particular, we propose a hierarchy of stochastic latent variables that captures temporal dependencies at different time-scales. The architecture is modular and flexible due to decoupling of deterministic and stochastic layers. We show that the proposed architecture achieves state of the art log-likelihoods across several tasks. Finally, the model is capable of predicting high-quality synthetic samples over a long-range temporal horizon in modeling of handwritten text.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english}
}

@online{alabdulmohsin_generalized_2021,
  title = {A {{Generalized Lottery Ticket Hypothesis}}},
  author = {Alabdulmohsin, Ibrahim and Markeeva, Larisa and Keysers, Daniel and Tolstikhin, Ilya},
  date = {2021-07-26},
  eprint = {2107.06825},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2107.06825},
  urldate = {2022-08-15},
  abstract = {We introduce a generalization to the lottery ticket hypothesis in which the notion of "sparsity" is relaxed by choosing an arbitrary basis in the space of parameters. We present evidence that the original results reported for the canonical basis continue to hold in this broader setting. We describe how structured pruning methods, including pruning units or factorizing fully-connected layers into products of low-rank matrices, can be cast as particular instances of this "generalized" lottery ticket hypothesis. The investigations reported here are preliminary and are provided to encourage further research along this direction.},
  pubstate = {preprint}
}

@online{alain_variance_2015b,
  title = {Variance {{Reduction}} in {{SGD}} by {{Distributed Importance Sampling}}},
  author = {Alain, Guillaume and Lamb, Alex and Sankar, Chinnadhurai and Courville, Aaron and Bengio, Yoshua},
  date = {2015-11-19},
  number = {1511.06481},
  eprint = {1511.06481},
  eprinttype = {arxiv},
  abstract = {Humans are able to accelerate their learning by selecting training materials that are the most informative and at the appropriate level of difficulty. We propose a framework for distributing deep learning in which one set of workers search for the most informative examples in parallel while a single worker updates the model on examples selected by importance sampling. This leads the model to update using an unbiased estimate of the gradient which also has minimum variance when the sampling proposal is proportional to the L2-norm of the gradient. We show experimentally that this method reduces gradient variance even in a context where the cost of synchronization across machines cannot be ignored, and where the factors for importance sampling are not updated instantly across the training set.},
  pubstate = {preprint}
}

@online{alamar_illustrated_2018,
  title = {The {{Illustrated Transformer}} – {{Jay Alammar}} – {{Visualizing}} Machine Learning One Concept at a Time.},
  author = {Alamar, Jay},
  date = {2018-06-27},
  url = {https://jalammar.github.io/illustrated-transformer/},
  urldate = {2022-08-13}
}

@online{alamar_illustrated_2022,
  title = {The {{Illustrated Retrieval Transformer}} – {{Jay Alammar}} – {{Visualizing}} Machine Learning One Concept at a Time.},
  author = {Alamar, Jay},
  date = {2022-01-03},
  url = {https://jalammar.github.io/illustrated-retrieval-transformer/},
  urldate = {2022-08-13}
}

@online{alamar_visualizing_2018,
  title = {Visualizing {{A Neural Machine Translation Model}} ({{Mechanics}} of {{Seq2seq Models With Attention}}) – {{Jay Alammar}} – {{Visualizing}} Machine Learning One Concept at a Time.},
  author = {Alamar, Jay},
  date = {2018-05-09},
  url = {https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/},
  urldate = {2022-08-13}
}

@article{aleksic_audiovisual_2006,
  title = {Audio-Visual Biometrics},
  author = {Aleksic, Petar S and Katsaggelos, Aggelos K},
  date = {2006},
  journaltitle = {Proceedings of the IEEE},
  volume = {94},
  number = {11},
  pages = {2025--2044}
}

@inproceedings{alemi_deep_2017,
  title = {Deep {{Variational Information Bottleneck}}},
  booktitle = {Proceedings of the 5th {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Alemi, Alexander A. and Fischer, Ian and Dillon, Joshua V. and Murphy, Kevin},
  date = {2017},
  location = {{Toulon, France}},
  url = {https://openreview.net/forum?id=HyxQzBceg}
}

@inproceedings{alemi_fixing_2018,
  title = {Fixing a {{Broken ELBO}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Alemi, Alexander A. and Poole, Ben and Fischer, Ian and Dillon, Joshua V. and Saurous, Rif A. and Murphy, Kevin},
  date = {2018-02-13},
  eprint = {1711.00464},
  eprinttype = {arxiv},
  pages = {159--168},
  publisher = {{PMLR}},
  location = {{Stockholm, Sweden}},
  url = {http://arxiv.org/abs/1711.00464},
  urldate = {2019-12-23},
  abstract = {Recent work in unsupervised representation learning has focused on learning deep directed latentvariable models. Fitting these models by maximizing the marginal likelihood or evidence is typically intractable, thus a common approximation is to maximize the evidence lower bound (ELBO) instead. However, maximum likelihood training (whether exact or approximate) does not necessarily result in a good latent representation, as we demonstrate both theoretically and empirically. In particular, we derive variational lower and upper bounds on the mutual information between the input and the latent variable, and use these bounds to derive a rate-distortion curve that characterizes the tradeoff between compression and reconstruction accuracy. Using this framework, we demonstrate that there is a family of models with identical ELBO, but different quantitative and qualitative characteristics. Our framework also suggests a simple new method to ensure that latent variable models with powerful stochastic decoders do not ignore their latent code.},
  langid = {english},
  keywords = {Read}
}

@misc{alemi_uncertainty_2018,
  title = {Uncertainty in the {{Variational Information Bottleneck}}},
  author = {Alemi, Alexander A. and Fischer, Ian and Dillon, Joshua V.},
  date = {2018-07-02},
  eprint = {1807.00906},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1807.00906},
  urldate = {2021-01-23},
  abstract = {We present a simple case study, demonstrating that Variational Information Bottleneck (VIB) can improve a network’s classification calibration as well as its ability to detect out-ofdistribution data. Without explicitly being designed to do so, VIB gives two natural metrics for handling and quantifying uncertainty.},
  langid = {english}
}

@thesis{alex_supervised_2008,
  type = {phdthesis},
  title = {Supervised {{Sequence Labelling}} with {{Recurrent Neural Networks}}},
  author = {Alex, Graves},
  date = {2008},
  institution = {{Technical University of Munich}},
  location = {{Munich}},
  abstract = {Recurrent neural networks are powerful sequence learners. They are able to incorporate context information in a flexible way, and are robust to localised distortions of the input data. These properties make them well suited to sequence labelling, where input sequences are transcribed with streams of labels. Long short-term memory is an especially promising recurrent architecture, able to bridge long time delays between relevant input and output events, and thereby access long range context. The aim of this thesis is to advance the state-of-the-art in supervised sequence labelling with recurrent networks in general, and long short-term memory in particular. Its two main contributions are (1) a new type of output layer that allows recurrent networks to be trained directly for sequence labelling tasks where the alignment between the inputs and the labels is unknown, and (2) an extension of long short-term memory to multidimensional data, such as images and video sequences. Experimental results are presented on speech recognition, online and offline handwriting recognition, keyword spotting, image segmentation and image classification, demonstrating the advantages of advanced recurrent networks over other sequential algorithms, such as hidden Markov Models.},
  langid = {english}
}

@inproceedings{ali_automation_2010,
  title = {Automation of {{Question Generation}} from {{Sentences}}},
  booktitle = {Proceedings of {{QG2010}}: {{The Third Workshop}} on {{Question Generation}}},
  author = {Ali, Husam and Chali, Yllias and Hasan, S.},
  date = {2010},
  pages = {58--67},
  abstract = {Question Generation (QG) and Question Answering (QA) are key challenges facing systems that interact with natural languages. The potential benefits of using automated systems to generate questions helps reduce the dependency on humans to generate questions and other needs associated with systems interacting with natural languages. In this paper we consider a system that automates generation of questions from a sentence, given a sentence, the system, will generate all possible questions which this sentence contain these questions answers. Since the given sentence may be a complex sentence, the system will generate elementary sentences, from the input complex sentences, using a syntactic parser. A part of speech tagger and a named entity recogniser are used to encode needed information. Based on the subject, verb, object and preposition the sentence will be classified, in order determine the type of questions that can possibly be generated from this sentence. We use development data provided by the Question Generation Shared Task Evaluation Challenge 2010. Keywords:}
}

@inproceedings{aljundi_continual_2022,
  title = {Continual Novelty Detection},
  booktitle = {Conference on {{Lifelong Learning Agents}}},
  author = {Aljundi, Rahaf and Reino, Daniel Olmeda and Chumerin, Nikolay and Turner, Richard E},
  date = {2022},
  pages = {1004--1025},
  publisher = {{PMLR}}
}

@inproceedings{alpay_learning_2016,
  title = {Learning {{Multiple Timescales}} in {{Recurrent Neural Networks}}},
  booktitle = {Artificial {{Neural Networks}} and {{Machine Learning}} – {{ICANN}} 2016},
  author = {Alpay, Tayfun and Heinrich, Stefan and Wermter, Stefan},
  editor = {Villa, Alessandro E.P. and Masulli, Paolo and Pons Rivero, Antonio Javier},
  date = {2016},
  volume = {9886},
  pages = {132--139},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-44778-0_16},
  url = {http://link.springer.com/10.1007/978-3-319-44778-0_16},
  urldate = {2021-06-16},
  abstract = {Recurrent Neural Networks (RNNs) are powerful architectures for sequence learning. Recent advances on the vanishing gradient problem have led to improved results and an increased research interest. Among recent proposals are architectural innovations that allow the emergence of multiple timescales during training. This paper explores a number of architectures for sequence generation and prediction tasks with long-term relationships. We compare the Simple Recurrent Network (SRN) and Long Short-Term Memory (LSTM) with the recently proposed Clockwork RNN (CWRNN), Structurally Constrained Recurrent Network (SCRN), and Recurrent Plausibility Network (RPN) with regard to their capabilities of learning multiple timescales. Our results show that partitioning hidden layers under distinct temporal constraints enables the learning of multiple timescales, which contributes to the understanding of the fundamental conditions that allow RNNs to self-organize to accurate temporal abstractions.},
  isbn = {978-3-319-44777-3 978-3-319-44778-0},
  langid = {english}
}

@article{alquraishi_end--end_2018,
  title = {End-to-End Differentiable Learning of Protein Structure},
  author = {AlQuraishi, Mohammed},
  date = {2018},
  journaltitle = {bioRxiv},
  doi = {10.1101/265231},
  url = {http://dx.doi.org/10.1101/265231},
  urldate = {2018-04-01},
  abstract = {Accurate prediction of protein structure is one of the central challenges of biochemistry. Despite significant progress made by co-evolution methods to predict protein structure from signatures of residue-residue coupling found in the evolutionary record, a direct and explicit mapping between protein sequence and structure remains elusive, with no substantial recent progress. Meanwhile, rapid developments in deep learning, which have found remarkable success in computer vision, natural language processing, and quantum chemistry raise the question of whether a deep learning based approach to protein structure could yield similar advancements. A key ingredient of the success of deep learning is the reformulation of complex, human-designed, multi-stage pipelines with differentiable models that can be jointly optimized end-to-end. We report the development of such a model, which reformulates the entire structure prediction pipeline using differentiable primitives. Achieving this required combining four technical ideas: (1) the adoption of a recurrent neural architecture to encode the internal representation of protein sequence, (2) the parameterization of (local) protein structure by torsional angles, which provides a way to reason over protein conformations without violating the covalent chemistry of protein chains, (3) the coupling of local protein structure to its global representation via recurrent geometric units, and (4) the use of a differentiable loss function to capture deviations between predicted and experimental structures. To our knowledge this is the first end-to-end differentiable model for learning of protein structure. We test the effectiveness of this approach using two challenging tasks: the prediction of novel protein folds without the use of co-evolutionary information, and the prediction of known protein folds without the use of structural templates. On the first task the model achieves state-of-the-art performance, even when compared to methods that rely on co-evolutionary data. On the second task the model is competitive with methods that use experimental protein structures as templates, achieving 3-7Å accuracy despite being template-free. Beyond protein structure prediction, end-to-end differentiable models of proteins represent a new paradigm for learning and modeling protein structure, with potential applications in docking, molecular dynamics, and protein design.}
}

@article{amari_natural_1998,
  title = {Natural {{Gradient Works Efficiently}} in {{Learning}}},
  author = {Amari, Shun-Ichi},
  date = {1998},
  journaltitle = {Neural Computation},
  volume = {10},
  number = {2},
  pages = {251--276},
  issn = {0899-7667},
  doi = {10.1162/089976698300017746},
  abstract = {When a parameter space has a certain underlying structure, the ordinary gradient of a function does not represent its steepest direction, but the natural gradient does. Information geometry is used for calculating the natural gradients in the parameter space of perceptrons, the space of matrices (for blind source separation), and the space of linear dynamical systems (for blind source deconvolution). The dynamical behavior of natural gradient online learning is analyzed and is proved to be Fisher efficient, implying that it has asymptotically the same performance as the optimal batch estimation of parameters. This suggests that the plateau phenomenon, which appears in the backpropagation learning algorithm of multilayer perceptrons, might disappear or might not be so serious when the natural gradient is used. An adaptive method of updating the learning rate is proposed and analyzed.}
}

@inproceedings{amdahl_validity_1967,
  title = {Validity of the Single Processor Approach to Achieving Large Scale Computing Capabilities},
  booktitle = {Proceedings of the {{Spring Joint Computer Conference}} ({{AFIPS}})},
  author = {Amdahl, Gene M.},
  date = {1967},
  pages = {483},
  doi = {10.1145/1465482.1465560},
  url = {http://portal.acm.org/citation.cfm?doid=1465482.1465560},
  abstract = {For over a decade prophets have voiced the contention that the organization of a single computer has reached its limits and that truly significant advances can be made only by interconnection of a multiplicity of computers in such a manner as to permit cooperative solution. Variously the proper direction has been pointed out as general purpose computers with a generalized interconnection of memories, or as specialized computers with geometrically related memory interconnections and controlled by one or more instruction streams.}
}

@article{amodei_deep_2016,
  title = {Deep {{Speech}} 2: {{End-to-End Speech Recognition}} in {{English}} and {{Mandarin}}},
  author = {Amodei, Dario and Ananthanarayanan, Sundaram and Anubhai, Rishita and Bai, Jingliang and Battenberg, Eric and Case, Carl and Casper, Jared and Catanzaro, Bryan and Cheng, Qiang and Chen, Guoliang and Chen, Jie and Chen, Jingdong and Chen, Zhijie and Chrzanowski, Mike and Coates, Adam and Diamos, Greg and Ding, Ke and Du, Niandong and Elsen, Erich and Engel, Jesse and Fang, Weiwei and Fan, Linxi and Fougner, Christopher and Gao, Liang and Gong, Caixia and Hannun, Awni and Han, Tony and Johannes, Lappi Vaino and Jiang, Bing and Ju, Cai and Jun, Billy and LeGresley, Patrick and Lin, Libby and Liu, Junjie and Liu, Yang and Li, Weigao and Li, Xiangang and Ma, Dongpeng and Narang, Sharan and Ng, Andrew and Ozair, Sherjil and Peng, Yiping and Prenger, Ryan and Qian, Sheng and Quan, Zongfeng and Raiman, Jonathan and Rao, Vinay and Satheesh, Sanjeev and Seetapun, David and Sengupta, Shubho and Srinet, Kavya and Sriram, Anuroop and Tang, Haiyuan and Tang, Liliang and Wang, Chong and Wang, Jidong and Wang, Kaifu and Wang, Yi and Wang, Zhijian and Wang, Zhiqian and Wu, Shuang and Wei, Likai and Xiao, Bo and Xie, Wen and Xie, Yan and Yogatama, Dani and Yuan, Bin and Zhan, Jun and Zhu, Zhenyao},
  date = {2016},
  pages = {10},
  abstract = {We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech–two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, enabling experiments that previously took weeks to now run in days. This allows us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.},
  langid = {english}
}

@misc{ancona_explaining_2019,
  title = {Explaining {{Deep Neural Networks}} with a {{Polynomial Time Algorithm}} for {{Shapley Values Approximation}}},
  author = {Ancona, Marco and Öztireli, Cengiz and Gross, Markus},
  date = {2019-06-21},
  eprint = {1903.10992},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1903.10992},
  urldate = {2020-11-19},
  abstract = {The problem of explaining the behavior of deep neural networks has recently gained a lot of attention. While several attribution methods have been proposed, most come without strong theoretical foundations, which raises questions about their reliability. On the other hand, the literature on cooperative game theory suggests Shapley values as a unique way of assigning relevance scores such that certain desirable properties are satisfied. Unfortunately, the exact evaluation of Shapley values is prohibitively expensive, exponential in the number of input features. In this work, by leveraging recent results on uncertainty propagation, we propose a novel, polynomial-time approximation of Shapley values in deep neural networks. We show that our method produces significantly better approximations of Shapley values than existing state-of-the-art attribution methods.},
  langid = {english}
}

@article{anderson_test_1954,
  title = {A Test of Goodness of Fit},
  author = {Anderson, Theodore W and Darling, Donald A},
  date = {1954},
  journaltitle = {Journal of the American statistical association},
  volume = {49},
  number = {268},
  pages = {765--769},
  publisher = {{Taylor \& Francis}}
}

@misc{andersson_learning_2021,
  title = {Learning Deep Autoregressive Models for Hierarchical Data},
  author = {Andersson, Carl R. and Wahlström, Niklas and Schön, Thomas B.},
  date = {2021-04-28},
  eprint = {2104.13853},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2104.13853},
  urldate = {2021-05-05},
  abstract = {We propose a model for hierarchical structured data as an extension to the stochastic temporal convolutional network (STCN). The proposed model combines an autoregressive model with a hierarchical variational autoencoder and downsampling to achieve superior computational complexity. We evaluate the proposed model on two different types of sequential data: speech and handwritten text. The results are promising with the proposed model achieving state-of-the-art performance.},
  langid = {english}
}

@misc{andreoli_convolution_2019,
  title = {Convolution Is Outer Product},
  author = {Andreoli, Jean-Marc},
  date = {2019-05-03},
  eprint = {1905.01289},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1905.01289},
  urldate = {2019-05-06},
  abstract = {The inner product operation between tensors is the corner stone of deep neural network architectures, directly inherited from linear algebra. There is a striking contrast between the unicity of this basic construct and the extreme diversity of high level constructs which have been invented to address various application domains. This paper is interested in an intermediate construct, convolution, and its corollary, attention, which have become ubiquitous in many applications, but are still presented in an ad-hoc fashion depending on the application context. We first identify the common problem addressed by most existing forms of convolution, and show how the solution to that problem naturally involves another very generic operation of linear algebra, the outer product between tensors. We then proceed to show that attention is a form of convolution, called "content based" convolution, hence amenable to the generic formulation based on the outer product. The reader looking for yet another architecture yielding better performance results on a specific task is in for some disappointment. The reader aiming at a better, more grounded understanding of familiar concepts may find food for thought.}
}

@inproceedings{andrew_deep_2013,
  title = {Deep Canonical Correlation Analysis},
  booktitle = {Proceedings of the 30th International Conference on Machine Learning},
  author = {Andrew, Galen and Arora, Raman and Bilmes, Jeff A and Livescu, Karen},
  date = {2013}
}

@misc{andrychowicz_learning_2016,
  title = {Learning to Learn by Gradient Descent by Gradient Descent},
  author = {Andrychowicz, Marcin and Denil, Misha and Gómez, Sergio and Hoffman, Matthew W and Pfau, David and Schaul, Tom and family=Freitas, given=Nando, prefix=de, useprefix=false},
  date = {2016},
  journaltitle = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
  pages = {3981--3989}
}

@inproceedings{anguera_quesst2014_2015,
  title = {{{QUESST2014}}: {{Evaluating}} Query-by-Example Speech Search in a Zero-Resource Setting with Real-Life Queries},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Anguera, Xavier and Rodriguez-Fuentes, Luis-J and Buzo, Andi and Metze, Florian and Szöke, Igor and Penagarikano, Mikel},
  date = {2015}
}

@article{anonymous_crossentropy_2018,
  title = {Cross-{{Entropy Loss Leads To Poor Margins}}},
  author = {Anonymous, Anonymous},
  date = {2018},
  abstract = {Neural networks could misclassify inputs that are slightly different from their training data, which indicates a small margin between their decision boundaries and the training dataset. In this work, we study the binary classification of linearly separable datasets and show that linear classifiers could also have decision boundaries that lie close to their training dataset if cross-entropy loss is used for training. In particular, we show that if the features of the training dataset lie in a low-dimensional affine subspace and the cross-entropy loss is minimized by using a gradient method, the margin between the training points and the decision boundary could be much smaller than the optimal value. This result is contrary to the conclusions of recent related works such as (Soudry et al., 2018), and we identify the reason for this contradiction. In order to improve the margin, we introduce differential training, which is a novel training paradigm that uses a loss function defined on pairs of points from each class. We show that the decision boundary of a linear classifier trained with differential training indeed achieves the maximum margin. The results reveal the use of cross-entropy loss as one of the hidden culprits of adversarial examples and introduces a new direction to make neural networks robust against them.}
}

@inproceedings{antol_vqa_2015,
  title = {\{\vphantom\}{{VQA}}\vphantom\{\}: {{Visual}} Question Answering},
  booktitle = {{{IEEE International Conference}} on {{Computer Vision}}},
  author = {Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Lawrence Zitnick, C. and Parikh, Devi},
  date = {2015},
  pages = {2425--2433}
}

@inproceedings{ardila_common_2020,
  title = {Common {{Voice}}: {{A}} Massively-Multilingual Speech Corpus},
  booktitle = {Proceedings of the {{International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}})},
  author = {Ardila, Rosana and Branson, Megan and Davis, Kelly and Kohler, Michael and Meyer, Josh and Henretty, Michael and Morais, Reuben and Saunders, Lindsay and Tyers, Francis and Weber, Gregor},
  date = {2020},
  eventtitle = {International {{Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}})}
}

@misc{arik_deep_2017,
  title = {Deep {{Voice}}: {{Real-time Neural Text-to-Speech}}},
  author = {Arik, Sercan O. and Chrzanowski, Mike and Coates, Adam and Diamos, Gregory and Gibiansky, Andrew and Kang, Yongguo and Li, Xian and Miller, John and Ng, Andrew Y. and Raiman, Jonathan and Sengupta, Shubho and Shoeybi, Mohammad},
  date = {2017-02-24},
  eprint = {1702.07825},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1702.07825},
  urldate = {2017-03-09},
  abstract = {We present Deep Voice, a production-quality text-to-speech system constructed entirely from deep neural networks. Deep Voice lays the groundwork for truly end-to-end neural speech synthesis. The system comprises five major building blocks: a segmentation model for locating phoneme boundaries, a grapheme-to-phoneme conversion model, a phoneme duration prediction model, a fundamental frequency prediction model, and an audio synthesis model. For the segmentation model, we propose a novel way of performing phoneme boundary detection with deep neural networks using connectionist temporal classification (CTC) loss. For the audio synthesis model, we implement a variant of WaveNet that requires fewer parameters and trains faster than the original. By using a neural network for each component, our system is simpler and more flexible than traditional text-to-speech systems, where each component requires laborious feature engineering and extensive domain expertise. Finally, we show that inference with our system can be performed faster than real time and describe optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x speedups over existing implementations.}
}

@online{arjovsky_wasserstein_2017,
  title = {Wasserstein {{GAN}}},
  author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
  date = {2017-12-06},
  eprint = {1701.07875},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1701.07875},
  urldate = {2023-04-12},
  abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
  pubstate = {preprint}
}

@article{arora_simple_2017,
  title = {A {{Simple}} but {{Tough-to-Beat Baseline}} for {{Sentence Embeddings}}},
  author = {Arora, Sanjeev and Liang, Yingyu and Ma, Tengyu},
  date = {2017},
  pages = {16},
  abstract = {The success of neural network methods for computing word embeddings has motivated methods for generating semantic embeddings of longer pieces of text, such as sentences and paragraphs. Surprisingly, Wieting et al (ICLR’16) showed that such complicated methods are outperformed, especially in out-of-domain (transfer learning) settings, by simpler methods involving mild retraining of word embeddings and basic linear regression. The method of Wieting et al. requires retraining with a substantial labeled dataset such as Paraphrase Database (Ganitkevitch et al., 2013).},
  langid = {english}
}

@unpublished{arora_theoretical_2019,
  title = {A {{Theoretical Analysis}} of {{Contrastive Unsupervised Representation Learning}}},
  author = {Arora, Sanjeev and Khandeparkar, Hrishikesh and Khodak, Mikhail and Plevrakis, Orestis and Saunshi, Nikunj},
  date = {2019-02-25},
  eprint = {1902.09229},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1902.09229},
  urldate = {2021-11-01},
  abstract = {Recent empirical works have successfully used unlabeled data to learn feature representations that are broadly useful in downstream classification tasks. Several of these methods are reminiscent of the well-known word2vec embedding algorithm: leveraging availability of pairs of semantically "similar" data points and "negative samples," the learner forces the inner product of representations of similar pairs with each other to be higher on average than with negative samples. The current paper uses the term contrastive learning for such algorithms and presents a theoretical framework for analyzing them by introducing latent classes and hypothesizing that semantically similar points are sampled from the same latent class. This framework allows us to show provable guarantees on the performance of the learned representations on the average classification task that is comprised of a subset of the same set of latent classes. Our generalization bound also shows that learned representations can reduce (labeled) sample complexity on downstream tasks. We conduct controlled experiments in both the text and image domains to support the theory.}
}

@article{arpit_closer_2017,
  title = {A {{Closer Look}} at {{Memorization}} in {{Deep Networks}}},
  author = {Arpit, Devansh and Jastrzębski, Stanisław and Ballas, Nicolas and Krueger, David and Bengio, Emmanuel and Kanwal, Maxinder S. and Maharaj, Tegan and Fischer, Asja and Courville, Aaron and Bengio, Yoshua and Lacoste-Julien, Simon},
  date = {2017-06-16},
  url = {https://arxiv.org/abs/1706.05394},
  urldate = {2018-10-12},
  langid = {english}
}

@misc{arras_explaining_2017,
  title = {Explaining {{Recurrent Neural Network Predictions}} in {{Sentiment Analysis}}},
  author = {Arras, Leila and Montavon, Grégoire and Müller, Klaus-Robert and Samek, Wojciech},
  date = {2017-06-22},
  eprint = {1706.07206},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1706.07206},
  urldate = {2019-08-01},
  abstract = {Recently, a technique called Layer-wise Relevance Propagation (LRP) was shown to deliver insightful explanations in the form of input space relevances for understanding feed-forward neural network classification decisions. In the present work, we extend the usage of LRP to recurrent neural networks. We propose a specific propagation rule applicable to multiplicative connections as they arise in recurrent network architectures such as LSTMs and GRUs. We apply our technique to a word-based bi-directional LSTM model on a five-class sentiment prediction task, and evaluate the resulting LRP relevances both qualitatively and quantitatively, obtaining better results than a gradient-based related method which was used in previous work.},
  langid = {english}
}

@inproceedings{artetxe_robust_2018,
  title = {A Robust Self-Learning Method for Fully Unsupervised Cross-Lingual Mappings of Word Embeddings},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{ACL}})},
  author = {Artetxe, Mikel and Labaka, Gorka and Agirre, Eneko},
  date = {2018-07},
  pages = {789--798},
  publisher = {{Association for Computational Linguistics}},
  location = {{Melbourne, Australia}},
  doi = {10.18653/v1/P18-1073},
  url = {https://aclanthology.org/P18-1073}
}

@article{artetxe_unsupervised_2018,
  title = {Unsupervised Neural Machine Translation},
  author = {Artetxe, Mikel and Labaka, Gorka and Agirre, Eneko and Cho, Kyunghyun},
  date = {2018},
  journaltitle = {International Conference on Learning Representations (ICLR)}
}

@inproceedings{arthur_kmeans_2007,
  title = {K-Means++: {{The Advantages}} of {{Careful Seeding}}},
  booktitle = {Proceedings of the Eighteenth Annual {{ACM-SIAM}} Symposium on {{Discrete}} Algorithms},
  author = {Arthur, David and Vassilvitskii, Sergei},
  date = {2007},
  eprint = {1000164511},
  eprinttype = {pmid},
  pages = {1027--1025},
  issn = {0898716241},
  doi = {10.1145/1283383.1283494},
  abstract = {" ... Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, held January 7-9, 2007, in New Orleans, Louisiana ... jointly sponsored by the SIAM Activity Group on Discrete Mathematics and by SIGACT, the ACM Special Interest Group on Algorithms and Computation Theory."--Page xiii. Title from PDF title page (viewed Mar. 26, 2010).},
  isbn = {978-0-89871-624-5}
}

@article{arulkumaran_deep_2017,
  title = {Deep Reinforcement Learning: {{A}} Brief Survey},
  author = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
  date = {2017-08-19},
  journaltitle = {IEEE Signal Processing Magazine},
  volume = {34},
  number = {6},
  eprint = {25719670},
  eprinttype = {pmid},
  pages = {26--38},
  issn = {10535888},
  doi = {10.1109/MSP.2017.2743240},
  url = {http://arxiv.org/abs/1708.05866},
  urldate = {2017-10-23},
  abstract = {Deep reinforcement learning is poised to revolutionise the field of AI and represents a step towards building autonomous systems with a higher level understanding of the visual world. Currently, deep learning is enabling reinforcement learning to scale to problems that were previously intractable, such as learning to play video games directly from pixels. Deep reinforcement learning algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of reinforcement learning, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep reinforcement learning, including the deep \$Q\$-network, trust region policy optimisation, and asynchronous advantage actor-critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via reinforcement learning. To conclude, we describe several current areas of research within the field.},
  isbn = {9781424469178}
}

@inproceedings{arvanitidis_latent_2018,
  title = {Latent {{Space Oddity}}: {{On}} the {{Curvature}} of {{Deep Generative Models}}},
  booktitle = {Proceedings of the 6th {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Arvanitidis, Georgios and Hansen, Lars Kai and Hauberg, Søren},
  date = {2018},
  pages = {15},
  location = {{Vancouver, BC, Canada}},
  abstract = {Deep generative models provide a systematic way to learn nonlinear data distributions through a set of latent variables and a nonlinear “generator” function that maps latent points into the input space. The nonlinearity of the generator implies that the latent space gives a distorted view of the input space. Under mild conditions, we show that this distortion can be characterized by a stochastic Riemannian metric, and we demonstrate that distances and interpolants are significantly improved under this metric. This in turn improves probability distributions, sampling algorithms and clustering in the latent space. Our geometric analysis further reveals that current generators provide poor variance estimates and we propose a new generator architecture with vastly improved variance estimates. Results are demonstrated on convolutional and fully connected variational autoencoders, but the formalism easily generalizes to other deep generative models.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english}
}

@article{astudillo_integration_2013,
  title = {Integration of Beamforming and Uncertainty-of-Observation Techniques for Robust {{ASR}} in Multi-Source Environments},
  author = {Astudillo, Ramón Fernandez and Kolossa, Dorothea and Abad, Alberto and Zeiler, Steffen and Saeidi, Rahim and Mowlaee, Pejman and family=Silva Neto, given=João Paulo, prefix=da, useprefix=true and Martin, Rainer},
  date = {2013},
  journaltitle = {Computer Speech \& Language},
  volume = {27},
  number = {3},
  pages = {837--850},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2012.07.009},
  url = {https://www.sciencedirect.com/science/article/pii/S0885230812000575},
  abstract = {This paper presents a new approach for increasing the robustness of multi-channel automatic speech recognition in noisy and reverberant multi-source environments. The proposed method uses uncertainty propagation techniques to dynamically compensate the speech features and the acoustic models for the observation uncertainty determined at the beamforming stage. We present and analyze two methods that allow integrating classical multi-channel signal processing approaches like delay and sum beamformers or Zelinski-type Wiener filters, with uncertainty-of-observation techniques like uncertainty decoding or modified imputation. An analysis of the results on the PASCAL-CHiME task shows that this approach consistently outperforms conventional beamformers with a minimal increase in computational complexity. The use of dynamic compensation based on observation uncertainty also outperforms conventional static adaptation with no need of adaptation data.},
  keywords = {Beamforming,Robust speech recognition,Uncertainty decoding,Uncertainty propagation}
}

@article{astudillo_uncertainty_2010,
  title = {An Uncertainty Propagation Approach to Robust {{ASR}} Using the {{ETSI}} Advanced Front-End},
  author = {Astudillo, Ramon Fernandez and Kolossa, Dorothea and Mandelartz, Philipp and Orglmeister, Reinhold},
  date = {2010},
  journaltitle = {IEEE Journal of Selected Topics in Signal Processing},
  volume = {4},
  number = {5},
  pages = {824--833},
  publisher = {{IEEE}}
}

@article{atan_deep-treat_nodate,
  title = {Deep-{{Treat}}: {{Learning Optimal Personalized Treatments}} from {{Observational Data}} Using {{Neural Networks}}},
  author = {Atan, Onur and Jordon, James},
  pages = {8},
  abstract = {We propose a novel approach for constructing effective treatment policies when the observed data is biased and lacks counterfactual information. Learning in settings where the observed data does not contain all possible outcomes for all treatments is difficult since the observed data is typically biased due to existing clinical guidelines. This is an important problem in the medical domain as collecting unbiased data is expensive and so learning from the wealth of existing biased data is a worthwhile task. Our approach separates the problem into two stages: first we reduce the bias by learning a representation map using a novel auto-encoder network – this allows us to control the trade-off between the bias-reduction and the information loss – and then we construct effective treatment policies on the transformed data using a novel feedforward network. Separation of the problem into these two stages creates an algorithm that can be adapted to the problem at hand – the bias-reduction step can be performed as a preprocessing step for other algorithms. We compare our algorithm against state-of-art algorithms on two semi-synthetic datasets and demonstrate that our algorithm achieves a significant improvement in performance.},
  langid = {english}
}

@inproceedings{athiwaratkun_probabilistic_2018,
  title = {Probabilistic {{FastText}} for {{Multi-Sense Word Embeddings}}},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Athiwaratkun, Ben and Wilson, Andrew and Anandkumar, Anima},
  date = {2018-07},
  pages = {1--11},
  publisher = {{Association for Computational Linguistics}},
  location = {{Melbourne, Australia}},
  url = {https://www.aclweb.org/anthology/P18-1001},
  urldate = {2019-06-18},
  abstract = {We introduce Probabilistic FastText, a new model for word embeddings that can capture multiple word senses, sub-word structure, and uncertainty information. In particular, we represent each word with a Gaussian mixture density, where the mean of a mixture component is given by the sum of n-grams. This representation allows the model to share the “strength” across sub-word structures (e.g. Latin roots), producing accurate representations of rare, misspelt, or even unseen words. Moreover, each component of the mixture can capture a different word sense. Probabilistic FastText outperforms both FastText, which has no probabilistic model, and dictionary-level probabilistic embeddings, which do not incorporate subword structures, on several word-similarity benchmarks, including English RareWord and foreign language datasets. We also achieve state-of-art performance on benchmarks that measure ability to discern different meanings. Thus, our model is the first to achieve best of both the worlds: multi-sense representations while having enriched semantics on rare words.}
}

@article{athreya_bootstrap_1987,
  title = {Bootstrap of the Mean in the Infinite Variance Case},
  author = {Athreya, K. B.},
  date = {1987},
  journaltitle = {The Annals of Statistics},
  volume = {15},
  number = {2},
  pages = {724--731},
  doi = {10.1214/aos/1176350371},
  url = {https://doi.org/10.1214/aos/1176350371},
  abstract = {Let X₁, X₂, …, Xₙ be independent identically distributed random variables with EX²₁ = ∞ but X₁ belonging to the domain of attraction of a stable law. It is known that the sample mean X̄ₙ appropriately normalized converges to a stable law. It is shown here that the bootstrap version of the normalized mean has a random distribution (given the sample) whose limit is also a random distribution implying that the naive bootstrap could fail in the heavy tailed case.},
  keywords = {60F,62E,62F,bootstrap,Poisson random measure,Stable law}
}

@misc{audhkhasi_direct_2017,
  title = {Direct {{Acoustics-to-Word Models}} for {{English Conversational Speech Recognition}}},
  author = {Audhkhasi, Kartik and Ramabhadran, Bhuvana and Saon, George and Picheny, Michael and Nahamoo, David},
  date = {2017-03-22},
  eprint = {1703.07754},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1703.07754},
  urldate = {2018-10-31},
  abstract = {Recent work on end-to-end automatic speech recognition (ASR) has shown that the connectionist temporal classification (CTC) loss can be used to convert acoustics to phone or character sequences. Such systems are used with a dictionary and separately-trained Language Model (LM) to produce word sequences. However, they are not truly end-to-end in the sense of mapping acoustics directly to words without an intermediate phone representation. In this paper, we present the first results employing direct acoustics-to-word CTC models on two well-known public benchmark tasks: Switchboard and CallHome. These models do not require an LM or even a decoder at run-time and hence recognize speech with minimal complexity. However, due to the large number of word output units, CTC word models require orders of magnitude more data to train reliably compared to traditional systems. We present some techniques to mitigate this issue. Our CTC word model achieves a word error rate of 13.0\%/18.8\% on the Hub5-2000 Switchboard/CallHome test sets without any LM or decoder compared with 9.6\%/16.0\% for phone-based CTC with a 4-gram LM. We also present rescoring results on CTC word model lattices to quantify the performance benefits of a LM, and contrast the performance of word and phone CTC models.}
}

@misc{aytar_playing_2018,
  title = {Playing Hard Exploration Games by Watching {{YouTube}}},
  author = {Aytar, Yusuf and Pfaff, Tobias and Budden, David and Paine, Tom Le and Wang, Ziyu and family=Freitas, given=Nando, prefix=de, useprefix=true},
  date = {2018-05-29},
  eprint = {1805.11592},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1805.11592},
  abstract = {Deep reinforcement learning methods traditionally struggle with tasks where environment rewards are particularly sparse. One successful method of guiding exploration in these domains is to imitate trajectories provided by a human demonstrator. However, these demonstrations are typically collected under artificial conditions, i.e. with access to the agent's exact environment setup and the demonstrator's action and reward trajectories. Here we propose a two-stage method that overcomes these limitations by relying on noisy, unaligned footage without access to such data. First, we learn to map unaligned videos from multiple sources to a common representation using self-supervised objectives constructed over both time and modality (i.e. vision and sound). Second, we embed a single YouTube video in this representation to construct a reward function that encourages an agent to imitate human gameplay. This method of one-shot imitation allows our agent to convincingly exceed human-level performance on the infamously hard exploration games Montezuma's Revenge, Pitfall! and Private Eye for the first time, even if the agent is not presented with any environment rewards.}
}

@misc{ba_layer_2016,
  title = {Layer {{Normalization}}},
  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  date = {2016},
  eprint = {1607.06450},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1607.06450},
  urldate = {2018-05-02},
  abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
  isbn = {978-3-642-04273-7}
}

@online{babu_xlsr_2021,
  title = {{{XLS-R}}: {{Self-supervised}} Cross-Lingual Speech Representation Learning at Scale},
  author = {Babu, Arun and Wang, Changhan and Tjandra, Andros and Lakhotia, Kushal and Xu, Qiantong and Goyal, Naman and Singh, Kritika and family=Platen, given=Patrick, prefix=von, useprefix=true and Saraf, Yatharth and Pino, Juan and Baevski, Alexei and Conneau, Alexis and Auli, Michael},
  date = {2021},
  eprint = {2111.09296},
  eprinttype = {arxiv},
  pubstate = {preprint}
}

@report{bach_probabilistic_2005,
  title = {A Probabilistic Interpretation of Canonical Correlation Analysis},
  author = {Bach, Francis R. and Jordan, Michael I.},
  date = {2005},
  number = {688},
  institution = {{Department of Statistics, University of California, Berkeley}}
}

@inproceedings{badino_autoencoder_2014,
  title = {An Auto-Encoder Based Approach to Unsupervised Learning of Subword Units},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Badino, Leonardo and Canevari, Claudia and Fadiga, Luciano and Metta, Giorgio},
  date = {2014},
  pages = {7634--7638}
}

@inproceedings{badino_discovering_2015,
  title = {Discovering Discrete Subword Units with Binarized Autoencoders and Hidden-{{Markov-model}} Encoders},
  booktitle = {Sixteenth {{Annual Conference}} of the {{International Speech Communication Association}}},
  author = {Badino, Leonardo and Mereta, Alessio and Rosasco, Lorenzo},
  date = {2015}
}

@article{badino_integrating_2016,
  title = {Integrating {{Articulatory Data}} in {{Deep Neural Network-Based Acoustic Modeling}}},
  author = {Badino, L. and Canevari, C. and Fadiga, L. and Metta, G.},
  date = {2016},
  journaltitle = {Comp. Sp. \& Lang.},
  volume = {36},
  pages = {173--195}
}

@misc{baevski_data2vec_2022,
  title = {Data2vec: {{A General Framework}} for {{Self-supervised Learning}} in {{Speech}}, {{Vision}} and {{Language}}},
  author = {Baevski, Alexei and Hsu, Wei-Ning and Xu, Qiantong and Babu, Arun and Gu, Jiatao and Auli, Michael},
  date = {2022-01},
  url = {https://ai.facebook.com/research/data2vec-a-general-framework-for-self-supervised-learning-in-speech-vision-and-language/},
  abstract = {While the general idea of self-supervised learning is identical across modalities, the actual algorithms and objectives differ widely because they were developed with a single modality in mind. To get us closer to general self-supervised learning, we present data2vec, a framework that uses the same learning method for either speech, NLP or computer vision. The core idea is to predict latent representations of the full input data based on a masked view of the input in a selfdistillation setup using a standard Transformer architecture. Instead of predicting modality-specific targets such as words, visual tokens or units of human speech which are local in nature, data2vec predicts contextualized latent representations that contain information from the entire input. Experiments on the major benchmarks of speech recognition, image classification, and natural language understanding demonstrate a new state of the art or competitive performance to predominant approaches. Models and code are available at www.github.com/pytorch/fairseq/ tree/master/examples/data2vec.},
  langid = {english},
  organization = {{Facebook AI Research blog}}
}

@unpublished{baevski_effectiveness_2020,
  title = {Effectiveness of Self-Supervised Pre-Training for Speech Recognition},
  author = {Baevski, Alexei and Auli, Michael and Mohamed, Abdelrahman},
  date = {2020-05-18},
  eprint = {1911.03912},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1911.03912},
  urldate = {2021-10-11},
  abstract = {We compare self-supervised representation learning algorithms which either explicitly quantize the audio data or learn representations without quantization. We find the former to be more accurate since it builds a good vocabulary of the data through vq-wav2vec [1] to enable learning of effective representations in subsequent BERT training. Different to previous work, we directly fine-tune the pre-trained BERT models on transcribed speech using a Connectionist Temporal Classification (CTC) loss instead of feeding the representations into a task-specific model. We also propose a BERT-style model learning directly from the continuous audio data and compare pre-training on raw audio to spectral features. Fine-tuning a BERT model on 10 hour of labeled Librispeech data with a vq-wav2vec vocabulary is almost as good as the best known reported system trained on 100 hours of labeled data on testclean, while achieving a 25\% WER reduction on test-other. When using only 10 minutes of labeled data, WER is 25.2 on test-other and 16.3 on test-clean. This demonstrates that self-supervision can enable speech recognition systems trained on a near-zero amount of transcribed data.}
}

@inproceedings{baevski_unsupervised_2021,
  title = {Unsupervised Speech Recognition},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Baevski, Alexei and Hsu, Wei-Ning and CONNEAU, Alexis and Auli, Michael},
  editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P.S. and Vaughan, J. Wortman},
  date = {2021},
  volume = {34},
  pages = {27826--27839},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/ea159dc9788ffac311592613b7f71fbb-Paper.pdf}
}

@unpublished{baevski_vqwav2vec_2020,
  title = {Vq-Wav2vec: {{Self-Supervised Learning}} of {{Discrete Speech Representations}}},
  shorttitle = {Vq-Wav2vec},
  author = {Baevski, Alexei and Schneider, Steffen and Auli, Michael},
  date = {2020-02-16},
  eprint = {1910.05453},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1910.05453},
  urldate = {2021-10-11},
  abstract = {We propose vq-wav2vec to learn discrete representations of audio segments through a wav2vec-style self-supervised context prediction task. The algorithm uses either a gumbel softmax or online k-means clustering to quantize the dense representations. Discretization enables the direct application of algorithms from the NLP community which require discrete inputs. Experiments show that BERT pre-training achieves a new state of the art on TIMIT phoneme classification and WSJ speech recognition.}
}

@inproceedings{baevski_wav2vec_2020,
  title = {Wav2vec 2.0: {{A Framework}} for {{Self-Supervised Learning}} of {{Speech Representations}}},
  shorttitle = {Wav2vec 2.0},
  booktitle = {Proceedings of the 34th {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Baevski, Alexei and Zhou, Henry and Mohamed, Abdelrahman and Auli, Michael},
  date = {2020-06-19},
  eprint = {2006.11477},
  eprinttype = {arxiv},
  location = {{Virtual}},
  url = {http://arxiv.org/abs/2006.11477},
  urldate = {2020-08-12},
  abstract = {We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. We set a new state of the art on both the 100 hour subset of Librispeech as well as on TIMIT phoneme recognition. When lowering the amount of labeled data to one hour, our model outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 5.7/10.1 WER on the noisy/clean test sets of Librispeech. This demonstrates the feasibility of speech recognition with limited amounts of labeled data. Fine-tuning on all of Librispeech achieves 1.9/3.5 WER using a simple baseline model architecture. We will release code and models.},
  eventtitle = {Conference on {{Neural Information Processing Systems}}},
  langid = {english}
}

@article{bahari_age_,
  title = {Age {{Estimation}} from {{Telephone Speech}} Using I-Vectors},
  author = {Bahari, Mohamad Hasan and McLaren, Mitchell},
  pages = {4},
  abstract = {Motivated by the success of i-vectors in the field of speaker recognition, this paper proposes a new approach for age estimation from telephone speech patterns based on i-vectors. In this method, each utterance is modeled by its corresponding ivector. Then, Support Vector Regression (SVR) is applied to estimate the age of speakers. The proposed method is trained and tested on telephone conversations of the National Institute for Standard in Technology (NIST) 2010 and 2008 Speaker Recognition Evaluations databases. Evaluation results show that the proposed method outperforms different conventional methods in speaker age estimation.},
  langid = {english}
}

@misc{bahdanau_actorcritic_2016,
  title = {An {{Actor-Critic Algorithm}} for {{Sequence Prediciton}}},
  author = {Bahdanau, Dzmitry and Brakel, Philemon and Xu, Kelvin and Goyal, Anirudh and Lowe, Ryan and Pineau, Joelle and Courville, Aaron and Bengio, Yoshua},
  date = {2016-07-24},
  eprint = {1607.07086},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1607.07086},
  urldate = {2018-06-06},
  abstract = {We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a critic network that is trained to predict the value of an output token, given the policy of an actor network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling.}
}

@article{bahdanau_end--end_2015,
  title = {End-to-{{End Attention-based Large Vocabulary Speech Recognition}}},
  author = {Bahdanau, Dzmitry and Chorowski, Jan and Serdyuk, Dmitriy and Brakel, Philemon and Bengio, Yoshua},
  date = {2015-08-18},
  journaltitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  eprint = {1508.04395},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1508.04395},
  abstract = {Many of the current state-of-the-art Large Vocabulary Continuous Speech Recognition Systems (LVCSR) are hybrids of neural networks and Hidden Markov Models (HMMs). Most of these systems contain separate components that deal with the acoustic modelling, language modelling and sequence decoding. We investigate a more direct approach in which the HMM is replaced with a Recurrent Neural Network (RNN) that performs sequence prediction directly at the character level. Alignment between the input features and the desired character sequence is learned automatically by an attention mechanism built into the RNN. For each predicted character, the attention mechanism scans the input sequence and chooses relevant frames. We propose two methods to speed up this operation: limiting the scan to a subset of most promising frames and pooling over time the information contained in neighboring frames, thereby reducing source sequence length. Integrating an n-gram language model into the decoding process yields recognition accuracies similar to other HMM-free RNN-based approaches.}
}

@inproceedings{bahdanau_neural_2014,
  title = {Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  date = {2014-09-01},
  eprint = {1409.0473},
  eprinttype = {arxiv},
  doi = {10.1146/annurev.neuro.26.041002.131047},
  url = {http://arxiv.org/abs/1409.0473},
  urldate = {2018-04-01},
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  isbn = {0147-006X (Print)},
  keywords = {★}
}

@inproceedings{bahl_maximum_1986,
  title = {Maximum Mutual Information Estimation of Hidden {{Markov}} Model Parameters for Speech Recognition},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Bahl, L. and Brown, P. and family=Souza, given=P., prefix=de, useprefix=true and Mercer, R.},
  date = {1986},
  volume = {11},
  pages = {49--52},
  eventtitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})}
}

@inproceedings{bai_contrastively_2021,
  title = {Contrastively {{Disentangled Sequential Variational Autoencoder}}},
  booktitle = {Proceedings of the 35th {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Bai, Junwen and Wang, Weiran and Gomes, Carla},
  date = {2021-12},
  pages = {14},
  abstract = {Self-supervised disentangled representation learning is a critical task in sequence modeling. The learnt representations contribute to better model interpretability as well as the data generation, and improve the sample efficiency for downstream tasks. We propose a novel sequence representation learning method, named Contrastively Disentangled Sequential Variational Autoencoder (C-DSVAE), to extract and separate the static (time-invariant) and dynamic (time-variant) factors in the latent space. Different from previous sequential variational autoencoder methods, we use a novel evidence lower bound which maximizes the mutual information between the input and the latent factors, while penalizes the mutual information between the static and dynamic factors. We leverage contrastive estimations of the mutual information terms in training, together with simple yet effective augmentation techniques, to introduce additional inductive biases. Our experiments show that C-DSVAE significantly outperforms the previous state-of-the-art methods on multiple metrics.},
  eventtitle = {Neural {{Information Processing Systems}}},
  langid = {english}
}

@article{bai_empirical_nodate,
  title = {An {{Empirical Evaluation}} of {{Generic Convolutional}} and {{Recurrent Networks}} for {{Sequence Modeling}}},
  author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
  abstract = {For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional ar-chitectures can outperform recurrent networks on tasks such as audio synthesis and machine trans-lation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convo-lutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our re-sults indicate that a simple convolutional archi-tecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common associ-ation between sequence modeling and recurrent networks should be reconsidered, and convolu-tional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at http://github.com/locuslab/TCN.}
}

@inproceedings{bai_supervised_2009,
  title = {Supervised Semantic Indexing},
  booktitle = {Proceedings of the {{Conference}} on {{Information}} and {{Knowledge Management}} ({{CIKM}})},
  author = {Bai, Bing and Weston, Jason and Collobert, Ronan and Grangier, David},
  date = {2009},
  volume = {5478 LNCS},
  pages = {761--765},
  issn = {03029743},
  url = {http://hunch.net/~vw/},
  urldate = {2018-07-16},
  abstract = {In this article we propose Supervised Semantic Indexing (SSI), an algorithm that is trained on (query, document) pairs of text documents to predict the quality of their match. Like Latent Semantic Indexing (LSI), our models take ac- count of correlations between words (synonymy, polysemy). However, unlike LSI our models are trained with a super- vised signal directly on the ranking task of interest, which we argue is the reason for our superior results. As the query and target texts are modeled separately, our approach is easily generalized to different retrieval tasks, such as online advertising placement. Dealing with models on all pairs of words features is computationally challenging. We propose several improvements to our basic model for addressing this issue, including low rank (but diagonal preserving) represen- tations, and correlated feature hashing (CFH). We provide an empirical study of all these methods on retrieval tasks based onWikipedia documents as well as an Internet adver- tisement task. We obtain state-of-the-art performance while providing realistically scalable methods.},
  isbn = {3-642-00957-3}
}

@inproceedings{baiImprovingMedicalCode2019,
  title = {Improving {{Medical Code Prediction}} from {{Clinical Text}} via {{Incorporating Online Knowledge Sources}}},
  booktitle = {The {{World Wide Web Conference}}},
  author = {Bai, Tian and Vucetic, Slobodan},
  date = {2019-05},
  series = {{{WWW}} '19},
  pages = {72--82},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3308558.3313485},
  abstract = {Clinical notes contain detailed information about health status of patients for each of their encounters with a health system. Developing effective models to automatically assign medical codes to clinical notes has been a long-standing active research area. Despite a great recent progress in medical informatics fueled by deep learning, it is still a challenge to find the specific piece of evidence in a clinical note which justifies a particular medical code out of all possible codes. Considering the large amount of online disease knowledge sources, which contain detailed information about signs and symptoms of different diseases, their risk factors, and epidemiology, there is an opportunity to exploit such sources. In this paper we consider Wikipedia as an external knowledge source and propose Knowledge Source Integration (KSI), a novel end-to-end code assignment framework, which can integrate external knowledge during training of any baseline deep learning model. The main idea of KSI is to calculate matching scores between a clinical note and disease related Wikipedia documents, and combine the scores with output of the baseline model. To evaluate KSI, we experimented with automatic assignment of ICD-9 diagnosis codes to the emergency department clinical notes from MIMIC-III data set, aided by Wikipedia documents corresponding to the ICD-9 codes. We evaluated several baseline models, ranging from logistic regression to recently proposed deep learning models known to achieve the state-of-the-art accuracy on clinical notes. The results show that KSI consistently improves the baseline models and that it is particularly successful in assignment of rare codes. In addition, by analyzing weights of KSI models, we can gain understanding about which words in Wikipedia documents provide useful information for predictions.},
  isbn = {978-1-4503-6674-8}
}

@online{balestriero_cookbook_2023,
  title = {A {{Cookbook}} of {{Self-Supervised Learning}}},
  author = {Balestriero, Randall and Ibrahim, Mark and Sobal, Vlad and Morcos, Ari and Shekhar, Shashank and Goldstein, Tom and Bordes, Florian and Bardes, Adrien and Mialon, Gregoire and Tian, Yuandong and Schwarzschild, Avi and Wilson, Andrew Gordon and Geiping, Jonas and Garrido, Quentin and Fernandez, Pierre and Bar, Amir and Pirsiavash, Hamed and LeCun, Yann and Goldblum, Micah},
  date = {2023-04-24},
  eprint = {2304.12210},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2304.12210},
  urldate = {2023-04-25},
  abstract = {Self-supervised learning, dubbed the dark matter of intelligence, is a promising path to advance machine learning. Yet, much like cooking, training SSL methods is a delicate art with a high barrier to entry. While many components are familiar, successfully training a SSL method involves a dizzying set of choices from the pretext tasks to training hyper-parameters. Our goal is to lower the barrier to entry into SSL research by laying the foundations and latest SSL recipes in the style of a cookbook. We hope to empower the curious researcher to navigate the terrain of methods, understand the role of the various knobs, and gain the know-how required to explore how delicious SSL can be.},
  pubstate = {preprint}
}

@online{balog_deepcoder_2017,
  title = {{{DeepCoder}}: {{Learning}} to {{Write Programs}}},
  shorttitle = {{{DeepCoder}}},
  author = {Balog, Matej and Gaunt, Alexander L. and Brockschmidt, Marc and Nowozin, Sebastian and Tarlow, Daniel},
  date = {2017-03-08},
  eprint = {1611.01989},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1611.01989},
  urldate = {2023-07-02},
  abstract = {We develop a first line of attack for solving programming competition-style problems from input-output examples using deep learning. The approach is to train a neural network to predict properties of the program that generated the outputs from the inputs. We use the neural network's predictions to augment search techniques from the programming languages community, including enumerative search and an SMT-based solver. Empirically, we show that our approach leads to an order of magnitude speedup over the strong non-augmented baselines and a Recurrent Neural Network approach, and that we are able to solve problems of difficulty comparable to the simplest problems on programming competition websites.},
  pubstate = {preprint}
}

@article{balog_lost_,
  title = {Lost {{Relatives}} of the {{Gumbel Trick}}},
  author = {Balog, Matej and Tripuraneni, Nilesh and Ghahramani, Zoubin and Weller, Adrian},
  abstract = {The Gumbel trick is a method to sample from a discrete probability distribution, or to estimate its normalizing partition function. The method relies on repeatedly applying a random perturbation to the distribution in a particular way, each time solving for the most likely configuration. We derive an entire family of related methods, of which the Gumbel trick is one member, and show that the new methods have superior properties in several settings with minimal additional computational cost. In particular, for the Gumbel trick to yield computational benefits for discrete graphical models, Gumbel perturbations on all configurations are typically replaced with socalled low-rank perturbations. We show how a subfamily of our new methods adapts to this setting, proving new upper and lower bounds on the log partition function and deriving a family of sequential samplers for the Gibbs distribution. Finally, we balance the discussion by showing how the simpler analytical form of the Gumbel trick enables additional theoretical results.},
  langid = {english},
  keywords = {Medium,Unread}
}

@misc{baluenfeldt_national_2021,
  title = {National {{Neurologisk Behandlingsvejledning}}: {{Iskæmisk}} Apopleksi - Akut Udredning Og Behandling},
  author = {Baluenfeldt, Rolf and Wienecke, Troels and {Dansk Neurologisk Selskab}},
  date = {2021-10-25},
  url = {https://nnbv.dk/iskaemisk-apopleksi-akut-udredning-og-behandling/},
  urldate = {2023-09-19}
}

@online{bansal_speechtotext_2017,
  title = {Towards Speech-to-Text Translation without Speech Recognition},
  author = {Bansal, Sameer and Kamper, Herman and Lopez, Adam and Goldwater, Sharon},
  date = {2017-02-13},
  eprint = {1702.03856},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1702.03856},
  urldate = {2023-09-23},
  abstract = {We explore the problem of translating speech to text in low-resource scenarios where neither automatic speech recognition (ASR) nor machine translation (MT) are available, but we have training data in the form of audio paired with text translations. We present the first system for this problem applied to a realistic multi-speaker dataset, the CALLHOME Spanish-English speech translation corpus. Our approach uses unsupervised term discovery (UTD) to cluster repeated patterns in the audio, creating a pseudotext, which we pair with translations to create a parallel text and train a simple bag-of-words MT model. We identify the challenges faced by the system, finding that the difficulty of cross-speaker UTD results in low recall, but that our system is still able to correctly translate some content words in test data.},
  pubstate = {preprint}
}

@article{baoMedicalCodePrediction2021,
  title = {Medical Code Prediction via Capsule Networks and {{ICD}} Knowledge},
  author = {Bao, Weidong and Lin, Hongfei and Zhang, Yijia and Wang, Jian and Zhang, Shaowu},
  date = {2021-07},
  journaltitle = {BMC Medical Informatics and Decision Making},
  volume = {21},
  number = {2},
  pages = {55},
  issn = {1472-6947},
  doi = {10.1186/s12911-021-01426-9},
  abstract = {Clinical notes record the health status, clinical manifestations and other detailed information of each patient. The International Classification of Diseases (ICD) codes are important labels for electronic health records. Automatic medical codes assignment to clinical notes through the deep learning model can not only improve work efficiency and accelerate the development of medical informatization but also facilitate the resolution of many issues related to medical insurance. Recently, neural network-based methods have been proposed for the automatic medical code assignment. However, in the medical field, clinical notes are usually long documents and contain many complex sentences, most of the current methods cannot effective in learning the representation of potential features from document text.}
}

@misc{bapna_slam_2021,
  title = {{{SLAM}}: {{A Unified Encoder}} for {{Speech}} and {{Language Modeling}} via {{Speech-Text Joint Pre-Training}}},
  shorttitle = {{{SLAM}}},
  author = {Bapna, Ankur and Chung, Yu-an and Wu, Nan and Gulati, Anmol and Jia, Ye and Clark, Jonathan H. and Johnson, Melvin and Riesa, Jason and Conneau, Alexis and Zhang, Yu},
  date = {2021-10-19},
  eprint = {2110.10329},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2110.10329},
  urldate = {2022-02-02},
  abstract = {Unsupervised pre-training is now the predominant approach for both text and speech understanding. Self-attention models pre-trained on large amounts of unannotated data have been hugely successful when fine-tuned on downstream tasks from a variety of domains and languages. This paper takes the universality of unsupervised language pre-training one step further, by unifying speech and text pre-training within a single model. We build a single encoder with the BERT objective on unlabeled text together with the w2v-BERT objective on unlabeled speech. To further align our model representations across modalities, we leverage alignment losses, specifically Translation Language Modeling (TLM) and Speech Text Matching (STM) that make use of supervised speech-text recognition data. We demonstrate that incorporating both speech and text data during pre-training can significantly improve downstream quality on CoVoST 2 speech translation, by around 1 BLEU compared to single-modality pre-trained models, while retaining close to SotA performance on LibriSpeech and SpeechStew ASR tasks. On four GLUE tasks and text-normalization, we observe evidence of capacity limitations and interference between the two modalities, leading to degraded performance compared to an equivalent text-only model, while still being competitive with BERT. Through extensive empirical analysis we also demonstrate the importance of the choice of objective function for speech pre-training, and the beneficial effect of adding additional supervised signals on the quality of the learned representations.},
  langid = {english}
}

@article{baranski_atomic_2012,
  title = {The Atomic Mass Unit, the {{Avogadro}} Constant, and the Mole: A Way to Understanding},
  author = {Baranski, Andrzej},
  date = {2012},
  journaltitle = {Journal of Chemical Education},
  volume = {89},
  number = {1},
  pages = {97--102},
  publisher = {{ACS Publications}}
}

@inproceedings{barber_ensemble_1998,
  title = {Ensemble Learning in {{Bayesian}} Neural Networks},
  booktitle = {Generalization in {{Neural Networks}} and {{Machine Learning}}},
  author = {Barber, David and Bishop, Christopher M.},
  date = {1998},
  pages = {215--237},
  publisher = {{Springer}},
  url = {https://www.microsoft.com/en-us/research/publication/ensemble-learning-in-bayesian-neural-networks/},
  urldate = {2018-05-06},
  abstract = {Bayesian treatments of learning in neural networks are typically based either on a local Gaussian approximation to a mode of the posterior weight distribution, or on Markov chain Monte Carlo simulations. A third approach, called `ensemble learning', was introduced by Hinton (1993). It aims to approximate the posterior distribution by minimizing the Kullback-Leibler divergence between the true posterior and a parametric approximating distribution. The original derivation of a deterministic algorithm relied on the use of a Gaussian approximating distribution with a diagonal covariance matrix and hence was unable to capture the posterior correlations between parameters. In this chapter we show how the ensemble learning approach can be extended to full-covariance Gaussian distributions while remaining computationally tractable. We also extend the framework to deal with hyperparameters, leading to a simple re-estimation procedure. One of the benefits of our approach is that it yields a strict lower bound on the marginal likelihood, in contrast to other approximate procedures.}
}

@online{barber_evolutionary_2017,
  title = {Evolutionary {{Optimization}} as a {{Variational Method}}},
  author = {Barber, David},
  date = {2017},
  url = {https://davidbarber.github.io/blog/2017/04/03/variational-optimisation/},
  urldate = {2018-03-23},
  abstract = {A simple connection between evolutionary optimisation and variational methods}
}

@unpublished{barham_pathways_2022,
  title = {Pathways: {{Asynchronous Distributed Dataflow}} for {{ML}}},
  shorttitle = {Pathways},
  author = {Barham, Paul and Chowdhery, Aakanksha and Dean, Jeff and Ghemawat, Sanjay and Hand, Steven and Hurt, Dan and Isard, Michael and Lim, Hyeontaek and Pang, Ruoming and Roy, Sudip and Saeta, Brennan and Schuh, Parker and Sepassi, Ryan and Shafey, Laurent El and Thekkath, Chandramohan A. and Wu, Yonghui},
  date = {2022-03-23},
  eprint = {2203.12533},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2203.12533},
  urldate = {2022-04-22},
  abstract = {We present the design of a new large scale orchestration layer for accelerators. Our system, Pathways, is explicitly designed to enable exploration of new systems and ML research ideas, while retaining state of the art performance for current models. Pathways uses a sharded dataflow graph of asynchronous operators that consume and produce futures, and efficiently gang-schedules heterogeneous parallel computations on thousands of accelerators while coordinating data transfers over their dedicated interconnects. Pathways makes use of a novel asynchronous distributed dataflow design that lets the control plane execute in parallel despite dependencies in the data plane. This design, with careful engineering, allows Pathways to adopt a single-controller model that makes it easier to express complex new parallelism patterns. We demonstrate that Pathways can achieve performance parity (\textasciitilde 100\% accelerator utilization) with state-of-the-art systems when running SPMD computations over 2048 TPUs, while also delivering throughput comparable to the SPMD case for Transformer models that are pipelined across 16 stages, or sharded across two islands of accelerators connected over a data center network.}
}

@inproceedings{barker_fifth_2018,
  title = {The Fifth `{{CHiME}}' Speech Separation and Recognition Challenge: {{Dataset}}, Task and Baselines},
  booktitle = {Annual {{Conference}} of the {{International Speech Communication Association}}},
  author = {Barker, Jon and Watanabe, Shinji and Vincent, Emmanuel and Trmal, Jan},
  date = {2018}
}

@book{bartholomew_latent_2011,
  title = {Latent Variable Models and Factor Analysis: A Unified Approach},
  shorttitle = {Latent Variable Models and Factor Analysis},
  author = {Bartholomew, David J. and Knott, M. and Moustaki, Irini},
  date = {2011},
  series = {Wiley Series in Probability and Statistics},
  edition = {3rd ed},
  publisher = {{Wiley}},
  location = {{Chichester, West Sussex}},
  isbn = {978-0-470-97192-5 978-1-119-97059-0 978-1-119-97058-3 978-1-119-97370-6 978-1-119-97371-3},
  pagetotal = {277},
  annotation = {OCLC: ocn710044915}
}

@inproceedings{baskar_semisupervised_2019,
  title = {Semi-Supervised Sequence-to-Sequence {{ASR}} Using Unpaired Speech and Text},
  booktitle = {Proceedings of the {{Conference}} of the {{International Speech Communication Association}} ({{Interspeech}})},
  author = {Baskar, Murali Karthick and Watanabe, Shinji and Astudillo, Ramón Fernandez and Hori, Takaaki and Burget, Lukás and Cernocký, Jan},
  editor = {Kubin, Gernot and Kacic, Zdravko},
  date = {2019},
  pages = {3790--3794},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2019-3167},
  url = {https://doi.org/10.21437/Interspeech.2019-3167}
}

@article{battaglia_relational_2018,
  title = {Relational Inductive Biases, Deep Learning, and Graph Networks},
  author = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
  date = {2018-06-04},
  url = {https://arxiv.org/abs/1806.01261},
  urldate = {2018-10-20},
  langid = {english}
}

@article{batygin_planet_2019,
  title = {The {{Planet Nine Hypothesis}}},
  author = {Batygin, Konstantin and Adams, Fred C. and Brown, Michael E. and Becker, Juliette C.},
  date = {2019-05},
  journaltitle = {Physics Reports},
  shortjournal = {Physics Reports},
  volume = {805},
  eprint = {1902.10103},
  eprinttype = {arxiv},
  pages = {1--53},
  issn = {03701573},
  doi = {10/gf5m44},
  url = {http://arxiv.org/abs/1902.10103},
  urldate = {2019-08-01},
  abstract = {Over the course of the past two decades, observational surveys have unveiled the intricate orbital structure of the Kuiper Belt, a field of icy bodies orbiting the Sun beyond Neptune. In addition to a host of readily-predictable orbital behavior, the emerging census of trans-Neptunian objects displays dynamical phenomena that cannot be accounted for by interactions with the known eight-planet solar system alone. Specifically, explanations for the observed physical clustering of orbits with semi-major axes in excess of ∼ 250 AU, the detachment of perihelia of select Kuiper belt objects from Neptune, as well as the dynamical origin of highly inclined/retrograde long-period orbits remain elusive within the context of the classical view of the solar system. This newly outlined dynamical architecture of the distant solar system points to the existence of a new planet with mass of m9 ∼ 5 − 10 M⊕, residing on a moderately inclined orbit (i9 ∼ 15 − 25 deg) with semi-major axis a9 ∼ 400 − 800 AU and eccentricity between e9 ∼ 0.2 − 0.5. This paper reviews the observational motivation, dynamical constraints, and prospects for detection of this proposed object known as Planet Nine.},
  langid = {english}
}

@inproceedings{bauer_generalized_2021,
  title = {Generalized {{Doubly-Reparameterized Gradient Estimators}}},
  booktitle = {3rd {{Symposium}} on {{Advances}} in {{Approximate Bayesian Inference}}},
  author = {Bauer, Matthias and Mnih, Andriy},
  date = {2021},
  eprint = {2101.11046},
  eprinttype = {arxiv},
  eprintclass = {stat.ML},
  pages = {21},
  url = {https://openreview.net/forum?id=epqs2X317ta},
  eventtitle = {Symposium on {{Advances}} in {{Approximate Bayesian Inference}}}
}

@article{bauer_regularization_2007,
  title = {On Regularization Algorithms in Learning Theory},
  author = {Bauer, Frank and Pereverzev, Sergei and Rosasco, Lorenzo},
  date = {2007},
  journaltitle = {Journal of Complexity},
  volume = {23},
  number = {1},
  pages = {52--72},
  issn = {0885064X},
  doi = {10.1016/j.jco.2006.07.001},
  abstract = {In this paper we discuss a relation between Learning Theory and Regularization of linear ill-posed inverse problems. It is well known that Tikhonov regularization can be profitably used in the context of supervised learning, where it usually goes under the name of regularized least-squares algorithm. Moreover, the gradient descent algorithm was studied recently, which is an analog of Landweber regularization scheme. In this paper we show that a notion of regularization defined according to what is usually done for ill-posed inverse problems allows to derive learning algorithms which are consistent and provide a fast convergence rate. It turns out that for priors expressed in term of variable Hilbert scales in reproducing kernel Hilbert spaces our results for Tikhonov regularization match those in Smale and Zhou [Learning theory estimates via integral operators and their approximations, submitted for publication, retrievable at 〈http://www.tti-c.org/smale.html〉, 2005] and improve the results for Landweber iterations obtained in Yao et al. [On early stopping in gradient descent learning, Constructive Approximation (2005), submitted for publication]. The remarkable fact is that our analysis shows that the same properties are shared by a large class of learning algorithms which are essentially all the linear regularization schemes. The concept of operator monotone functions turns out to be an important tool for the analysis. © 2006 Elsevier Inc. All rights reserved.},
  isbn = {0885-064X}
}

@misc{bayer_learning_2015,
  title = {Learning {{Stochastic Recurrent Networks}}},
  author = {Bayer, Justin and Osendorfer, Christian},
  date = {2015-03-05},
  eprint = {1411.7610},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1411.7610},
  urldate = {2020-05-22},
  abstract = {Leveraging advances in variational inference, we propose to enhance recurrent neural networks with latent variables, resulting in Stochastic Recurrent Networks (STORNs). The model i) can be trained with stochastic gradient methods, ii) allows structured and multi-modal conditionals at each time step, iii) features a reliable estimator of the marginal likelihood and iv) is a generalisation of deterministic recurrent neural networks. We evaluate the method on four polyphonic musical data sets and motion capture data.},
  langid = {english}
}

@inproceedings{bayer_mind_2021,
  title = {Mind the {{Gap}} When {{Conditioning Amortised Inference}} in {{Sequential Latent-Variable Models}}},
  booktitle = {Proceedings of the 9th {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Bayer, Justin and Soelch, Maximilian and Mirchev, Atanas and Kayalibay, Baris and family=Smagt, given=Patrick, prefix=van der, useprefix=true},
  date = {2021-05-03},
  location = {{Virtual}},
  url = {http://arxiv.org/abs/2101.07046},
  urldate = {2021-11-13},
  abstract = {Amortised inference enables scalable learning of sequential latent-variable models (LVMs) with the evidence lower bound (ELBO). In this setting, variational posteriors are often only partially conditioned. While the true posteriors depend, e. g., on the entire sequence of observations, approximate posteriors are only informed by past observations. This mimics the Bayesian filter—a mixture of smoothing posteriors. Yet, we show that the ELBO objective forces partially-conditioned amortised posteriors to approximate products of smoothing posteriors instead. Consequently, the learned generative model is compromised. We demonstrate these theoretical findings in three scenarios: traffic flow, handwritten digits, and aerial vehicle dynamics. Using fully-conditioned approximate posteriors, performance improves in terms of generative modelling and multi-step prediction.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english}
}

@unpublished{beckmann_speechvgg_2020,
  title = {Speech-{{VGG}}: {{A}} Deep Feature Extractor for Speech Processing},
  shorttitle = {Speech-{{VGG}}},
  author = {Beckmann, Pierre and Kegler, Mikolaj and Saltini, Hugues and Cernak, Milos},
  date = {2020-05-16},
  eprint = {1910.09909},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/1910.09909},
  urldate = {2021-10-22},
  abstract = {Recent breakthroughs in deep learning often rely on representation learning and knowledge transfer. In particular, readily available models pre-trained on large datasets are key for the efficient transfer of knowledge. They can be applied as feature extractors for data preprocessing, fine-tuned to perform a variety of tasks, or used for computing feature losses in the training of deep learning systems. While applications of transfer learning are common in the fields of computer vision and natural language processing, audio- and speech processing are surprisingly lacking readily available and transferable models. Here, we introduce speechVGG, a flexible, transferable feature extractor tailored for integration with deep learning frameworks for speech processing. Our transferable model adopts the classic VGG-16 architecture and is trained on a spoken word classification task. We demonstrate the application of the pre-trained model in four speech processing tasks, including speech enhancement, language identification, speech, noise and music classification, and speaker identification. Each time, we compare the performance of our approach to existing baselines. Our results confirm that the representation of natural speech captured using speechVGG is transferable and generalizable across various speech processing problems and datasets. Notably, relatively simple applications of our pre-trained model are capable of achieving competitive results.},
  langid = {english}
}

@misc{beijingdatatangtechnologycoltd_aidatatang_,
  title = {Aidatatang 200zh, a {{Free Chinese Mandarin Speech Corpus}}},
  author = {{Beijing DataTang Technology Co., Ltd.}},
  annotation = {http://www.datatang.com https://openslr.org/62/}
}

@unpublished{bejnordi_batchshaping_2020,
  title = {Batch-{{Shaping}} for {{Learning Conditional Channel Gated Networks}}},
  author = {Bejnordi, Babak Ehteshami and Blankevoort, Tijmen and Welling, Max},
  date = {2020-04-03},
  eprint = {1907.06627},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1907.06627},
  urldate = {2022-03-30},
  abstract = {We present a method that trains large capacity neural networks with significantly improved accuracy and lower dynamic computational cost. We achieve this by gating the deep-learning architecture on a fine-grained-level. Individual convolutional maps are turned on/off conditionally on features in the network. To achieve this, we introduce a new residual block architecture that gates convolutional channels in a fine-grained manner. We also introduce a generally applicable tool \$batch\$-\$shaping\$ that matches the marginal aggregate posteriors of features in a neural network to a pre-specified prior distribution. We use this novel technique to force gates to be more conditional on the data. We present results on CIFAR-10 and ImageNet datasets for image classification, and Cityscapes for semantic segmentation. Our results show that our method can slim down large architectures conditionally, such that the average computational cost on the data is on par with a smaller architecture, but with higher accuracy. In particular, on ImageNet, our ResNet50 and ResNet34 gated networks obtain 74.60\% and 72.55\% top-1 accuracy compared to the 69.76\% accuracy of the baseline ResNet18 model, for similar complexity. We also show that the resulting networks automatically learn to use more features for difficult examples and fewer features for simple examples.}
}

@inproceedings{belabbas_fast_2007,
  title = {Fast Low-Rank Approximation for Covariance Matrices},
  booktitle = {{{IEEE International Workshop}} on {{Computational Advances}} in {{Multi-Sensor Adaptive Processing}} ({{CAMSAP}})},
  author = {Belabbas, Mohamed Ali and Wolfe, Patrick J.},
  date = {2007-12},
  pages = {293--296},
  publisher = {{IEEE}},
  doi = {10.1109/CAMSAP.2007.4498023},
  url = {http://ieeexplore.ieee.org/document/4498023/},
  urldate = {2018-03-25},
  abstract = {Computing an efficient low-rank approximation of a given positive definite matrix is a ubiquitous task in statistical signal processing and numerical linear algebra. The optimal solution is well known and is given by the singular value decomposition; however, its complexity scales as the cube of the matrix dimension. Here we introduce a low-complexity alternative which approximates this optimal low-rank solution, together with a bound on its worst-case error. Our methodology also reveals a connection between the approximation of matrix products and Schur complements. We present simulation results that verify performance improvements relative to contemporary randomized algorithms for low-rank approximation.},
  isbn = {978-1-4244-1714-8}
}

@inproceedings{belghazi_mine_2018,
  title = {{{MINE}}: {{Mutual Information Neural Estimation}}},
  shorttitle = {{{MINE}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Belghazi, Mohamed Ishmael and Baratin, Aristide and Rajeswar, Sai and Ozair, Sherjil and Bengio, Yoshua and Courville, Aaron and Hjelm, R. Devon},
  date = {2018},
  location = {{Stockholm, Sweden}},
  abstract = {We argue that the estimation of mutual information between high dimensional continuous random variables can be achieved by gradient descent over neural networks. We present a Mutual Information Neural Estimator (MINE) that is linearly scalable in dimensionality as well as in sample size, trainable through back-prop, and strongly consistent. We present a handful of applications on which MINE can be used to minimize or maximize mutual information. We apply MINE to improve adversarially trained generative models. We also use MINE to implement Information Bottleneck, applying it to supervised classification; our results demonstrate substantial improvement in flexibility and performance in these settings.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}}
}

@article{bell_adaptation_2021,
  title = {Adaptation Algorithms for Neural Network-Based Speech Recognition: {{An}} Overview},
  author = {Bell, Peter and Fainberg, Joachim and Klejch, Ondrej and Li, Jinyu and Renals, Steve and Swietojanski, Pawel},
  date = {2021},
  journaltitle = {IEEE Open Journal of Signal Processing},
  volume = {2},
  pages = {33--66},
  doi = {10.1109/OJSP.2020.3045349}
}

@inproceedings{bendale_open_2016,
  title = {Towards Open Set Deep Networks},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Bendale, Abhijit and Boult, Terrance E},
  date = {2016},
  pages = {1563--1572}
}

@article{bender_climbing_,
  title = {Climbing towards {{NLU}}: {{On Meaning}}, {{Form}}, and {{Understanding}} in the {{Age}} of {{Data}}},
  author = {Bender, Emily M and Koller, Alexander},
  pages = {14},
  abstract = {The success of the large neural language models on many NLP tasks is exciting. However, we find that these successes sometimes lead to hype in which these models are being described as “understanding” language or capturing “meaning”. In this position paper, we argue that a system trained only on form has a priori no way to learn meaning. In keeping with the ACL 2020 theme of “Taking Stock of Where We’ve Been and Where We’re Going”, we argue that a clear understanding of the distinction between form and meaning will help guide the field towards better science around natural language understanding.},
  langid = {english}
}

@inproceedings{bengio_advances_2013,
  title = {Advances in {{Optimizing Recurrent Networks}}},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Bengio, Yoshua and Boulanger-Lewandowski, Nicolas and Pascanu, Razvan},
  date = {2013},
  eprint = {25246403},
  eprinttype = {pmid},
  pages = {8624--8628},
  issn = {15206149},
  doi = {10.1109/ICASSP.2013.6639349},
  abstract = {After a more than decade-long period of relatively little research activity in the area of recurrent neural networks, several new developments will be reviewed here that have allowed substantial progress both in understanding and in technical solutions towards more efficient training of recurrent networks. These advances have been motivated by and related to the optimization issues surrounding deep learning. Although recurrent networks are extremely powerful in what they can in principle represent in terms of modelling sequences,their training is plagued by two aspects of the same issue regarding the learning of long-term dependencies. Experiments reported here evaluate the use of clipping gradients, spanning longer time ranges with leaky integration, advanced momentum techniques, using more powerful output probability models, and encouraging sparser gradients to help symmetry breaking and credit assignment. The experiments are performed on text and music data and show off the combined effects of these techniques in generally improving both training and test error.},
  isbn = {978-1-4799-0356-6}
}

@unpublished{bengio_conditional_2016,
  title = {Conditional {{Computation}} in {{Neural Networks}} for Faster Models},
  author = {Bengio, Emmanuel and Bacon, Pierre-Luc and Pineau, Joelle and Precup, Doina},
  date = {2016-01-07},
  eprint = {1511.06297},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1511.06297},
  urldate = {2022-03-30},
  abstract = {Deep learning has become the state-of-art tool in many applications, but the evaluation and training of deep models can be time-consuming and computationally expensive. The conditional computation approach has been proposed to tackle this problem (Bengio et al., 2013; Davis \& Arel, 2013). It operates by selectively activating only parts of the network at a time. In this paper, we use reinforcement learning as a tool to optimize conditional computation policies. More specifically, we cast the problem of learning activation-dependent policies for dropping out blocks of units as a reinforcement learning problem. We propose a learning scheme motivated by computation speed, capturing the idea of wanting to have parsimonious activations while maintaining prediction accuracy. We apply a policy gradient algorithm for learning policies that optimize this loss function and propose a regularization mechanism that encourages diversification of the dropout policy. We present encouraging empirical results showing that this approach improves the speed of computation without impacting the quality of the approximation.}
}

@misc{bengio_deep_2013,
  title = {Deep {{Generative Stochastic Networks Trainable}} by {{Backprop}}},
  author = {Bengio, Yoshua and Thibodeau-Laufer, Éric and Alain, Guillaume and Yosinski, Jason},
  date = {2013},
  eprint = {1306.1091},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1306.1091},
  abstract = {We introduce a novel training principle for probabilistic models that is an alternative to maximum likelihood. The proposed Generative Stochastic Networks (GSN) framework is based on learning the transition operator of a Markov chain whose stationary distribution estimates the data distribution. The transition distribution of the Markov chain is conditional on the previous state, generally involving a small move, so this conditional distribution has fewer dominant modes, being unimodal in the limit of small moves. Thus, it is easier to learn because it is easier to approximate its partition function, more like learning to perform supervised function approximation, with gradients that can be obtained by backprop. We provide theorems that generalize recent work on the probabilistic interpretation of denoising autoencoders and obtain along the way an interesting justification for dependency networks and generalized pseudolikelihood, along with a definition of an appropriate joint distribution and sampling mechanism even when the conditionals are not consistent. GSNs can be used with missing inputs and can be used to sample subsets of variables given the rest. We validate these theoretical results with experiments on two image datasets using an architecture that mimics the Deep Boltzmann Machine Gibbs sampler but allows training to proceed with simple backprop, without the need for layerwise pretraining.},
  isbn = {9781634393973}
}

@misc{bengio_estimating_2013,
  title = {Estimating or {{Propagating Gradients Through Stochastic Neurons}} for {{Conditional Computation}}},
  author = {Bengio, Yoshua and Léonard, Nicholas and Courville, Aaron},
  date = {2013-08-15},
  eprint = {1308.3432},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1308.3432},
  urldate = {2021-02-19},
  abstract = {Stochastic neurons and hard non-linearities can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic or non-smooth neurons? I.e., can we “back-propagate” through these stochastic neurons? We examine this question, existing approaches, and compare four families of solutions, applicable in different settings. One of them is the minimum variance unbiased gradient estimator for stochatic binary neurons (a special case of the REINFORCE algorithm). A second approach, introduced here, decomposes the operation of a binary stochastic neuron into a stochastic binary part and a smooth differentiable part, which approximates the expected effect of the pure stochatic binary neuron to first order. A third approach involves the injection of additive or multiplicative noise in a computational graph that is otherwise differentiable. A fourth approach heuristically copies the gradient with respect to the stochastic output directly as an estimator of the gradient with respect to the sigmoid argument (we call this the straight-through estimator). To explore a context where these estimators are useful, we consider a small-scale version of conditional computation, where sparse stochastic units form a distributed representation of gaters that can turn off in combinatorially many ways large chunks of the computation performed in the rest of the neural network. In this case, it is important that the gating units produce an actual 0 most of the time. The resulting sparsity can be potentially be exploited to greatly reduce the computational cost of large deep networks for which conditional computation would be useful.},
  langid = {english}
}

@article{bengio_learning_1994,
  title = {Learning {{Long-Term Dependencies}} with {{Gradient Descent}} Is {{Difficult}}},
  author = {Bengio, Yoshua and Simard, Patrice Y. and Frasconi, Paolo},
  date = {1994},
  journaltitle = {IEEE Transactions on Neural Networks},
  volume = {5},
  number = {2},
  pages = {157--166},
  abstract = {Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered.}
}

@article{bengio_neural_2003,
  title = {A {{Neural Probabilistic Language Model}}},
  author = {Bengio, Yoshua and Ducharme, Réjean and Vincent, Pascal and Janvin, Christian},
  date = {2003},
  journaltitle = {Journal of Machine Learning Research},
  volume = {3},
  eprint = {18244602},
  eprinttype = {pmid},
  pages = {1137--1155},
  issn = {15324435},
  doi = {10.1162/153244303322533223},
  abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
  isbn = {1532-4435}
}

@article{bengio_practical_2012,
  title = {Practical Recommendations for Gradient-Based Training of Deep Architectures},
  author = {Bengio, Yoshua},
  date = {2012},
  journaltitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  volume = {7700 LECTU},
  eprint = {25497547},
  eprinttype = {pmid},
  pages = {437--478},
  issn = {03029743},
  doi = {10.1007/978-3-642-35289-8-26},
  abstract = {Learning algorithms related to artificial neural networks and in particular for Deep Learning may seem to involve many bells and whistles, called hyper-parameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyper-parameters, in particular in the context of learning algorithms based on back-propagated gradient and gradient-based optimization. It also discusses how to deal with the fact that more interesting results can be obtained when allowing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observed with deeper architectures.},
  isbn = {9783642352881}
}

@article{bengio_representation_2013,
  title = {Representation Learning: {{A}} Review and New Perspectives},
  author = {Bengio, Yoshua and Courville, Aaron C. and Vincent, Pascal},
  date = {2013},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {35},
  number = {8},
  pages = {1798--1828},
  doi = {10.1109/TPAMI.2013.50},
  url = {https://doi.org/10.1109/TPAMI.2013.50},
  abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.}
}

@article{bengio_scheduled_2015,
  title = {Scheduled {{Sampling}} for {{Sequence Prediction}} with {{Recurrent Neural Networks}}},
  author = {Bengio, Samy and Vinyals, Oriol and Jaitly, Navdeep and Shazeer, Noam},
  date = {2015-06-09},
  url = {https://arxiv.org/abs/1506.03099},
  urldate = {2018-10-29},
  langid = {english}
}

@inproceedings{bengio_word_2014,
  title = {Word {{Embeddings}} for {{Speech Recognition}}},
  booktitle = {Proceedings of the 15th {{Conference}} of the {{International Speech Communication Association}} ({{Interspeech}})},
  author = {Bengio, Samy and Heigold, Georg},
  date = {2014}
}

@incollection{bengtsson_curse--dimensionality_2008,
  title = {Curse-of-Dimensionality Revisited: {{Collapse}} of the Particle Filter in Very Large Scale Systems},
  booktitle = {Probability and {{Statistics}}: {{Essays}} in {{Honor}} of {{David A}}. {{Freedman}}},
  author = {Bengtsson, Thomas and Bickel, Peter and Li, Bo},
  date = {2008},
  eprint = {0805.3034},
  eprinttype = {arxiv},
  pages = {316--334},
  doi = {10.1214/193940307000000518},
  url = {http://projecteuclid.org/euclid.imsc/1207580091},
  urldate = {2018-04-14},
  abstract = {It has been widely realized that Monte Carlo methods (approximation via a sample ensemble) may fail in large scale systems. This work offers some theoretical insight into this phenomenon in the context of the particle filter. We demonstrate that the maximum of the weights associated with the sample ensemble converges to one as both the sample size and the system dimension tends to infinity. Specifically, under fairly weak assumptions, if the ensemble size grows sub-exponentially in the cube root of the system dimension, the convergence holds for a single update step in state-space models with independent and identically distributed kernels. Further, in an important special case, more refined arguments show (and our simulations suggest) that the convergence to unity occurs unless the ensemble grows super-exponentially in the system dimension. The weight singularity is also established in models with more general multivariate likelihoods, e.g. Gaussian and Cauchy. Although presented in the context of atmospheric data assimilation for numerical weather prediction, our results are generally valid for high-dimensional particle filters.},
  isbn = {0-940600-74-9}
}

@article{benjamini_controlling_1995,
  title = {Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing},
  author = {Benjamini, Yoav and Hochberg, Yosef},
  date = {1995},
  journaltitle = {Journal of the Royal statistical society: series B (Methodological)},
  volume = {57},
  number = {1},
  pages = {289--300},
  publisher = {{Wiley Online Library}}
}

@inproceedings{bennett_using_2003,
  title = {Using Asymmetric Distributions to Improve Text Classifier Probability Estimates},
  booktitle = {Proceedings of the 26th Annual International {{ACM SIGIR}} Conference on {{Research}} and Development in Informaion Retrieval},
  author = {Bennett, Paul N},
  date = {2003},
  pages = {111--118}
}

@misc{berg_idf_2020,
  title = {{{IDF}}++: {{Analyzing}} and {{Improving Integer Discrete Flows}} for {{Lossless Compression}}},
  shorttitle = {{{IDF}}++},
  author = {family=Berg, given=Rianne, prefix=van den, useprefix=false and Gritsenko, Alexey A. and Dehghani, Mostafa and Sønderby, Casper Kaae and Salimans, Tim},
  date = {2020-06-22},
  eprint = {2006.12459},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2006.12459},
  urldate = {2020-07-14},
  abstract = {In this paper we analyse and improve integer discrete flows for lossless compression. Integer discrete flows are a recently proposed class of models that learn invertible transformations for integer-valued random variables. Due to its discrete nature, they can be combined in a straightforward manner with entropy coding schemes for lossless compression without the need for bits-back coding. We discuss the potential difference in flexibility between invertible flows for discrete random variables and flows for continuous random variables and show that (integer) discrete flows are more flexible than previously claimed. We furthermore investigate the influence of quantization operators on optimization and gradient bias in integer discrete flows. Finally, we introduce modifications to the architecture to improve the performance of this model class for lossless compression.},
  langid = {english}
}

@inproceedings{bergamin_modelagnostic_2022,
  title = {Model-{{Agnostic Out-of-Distribution Detection Using Combined Statistical Tests}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}} ({{AISTATS}})},
  author = {Bergamin, Federico and Mattei, Pierre-Alexandre and Havtorn, Jakob D and Senetaire, Hugo and Schmutz, Hugo and Maaløe, Lars and Hauberg, Søren and Frellsen, Jes},
  date = {2022},
  volume = {151},
  eprint = {2203.01097},
  eprinttype = {arxiv},
  publisher = {{PMLR}},
  location = {{Valencia, Span}},
  abstract = {We present simple methods for out-ofdistribution detection using a trained generative model. These techniques, based on classical statistical tests, are model-agnostic in the sense that they can be applied to any differentiable generative model. The idea is to combine a classical parametric test (Rao’s score test) with the recently introduced typicality test. These two test statistics are both theoretically well-founded and exploit different sources of information based on the likelihood for the typicality test and its gradient for the score test. We show that combining them using Fisher’s method overall leads to a more accurate out-of-distribution test. We also discuss the benefits of casting out-ofdistribution detection as a statistical testing problem, noting in particular that false positive rate control can be valuable for practical out-of-distribution detection. Despite their simplicity and generality, these methods can be competitive with model-specific outof-distribution detection algorithms without any assumptions on the out-distribution.},
  langid = {english},
  keywords = {Read}
}

@inproceedings{bergman_classificationbased_2020,
  title = {Classification-Based Anomaly Detection for General Data},
  booktitle = {Proceedings of the {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Bergman, Liron and Hoshen, Yedid},
  date = {2020},
  location = {{Addis Ababa, Ethiopia}},
  eventtitle = {International {{Conference}} on {{Learning Representations}}}
}

@incollection{bergstra_algorithms_2011,
  title = {Algorithms for {{Hyper-Parameter Optimization}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 24},
  author = {Bergstra, James S. and Bardenet, Rémi and Bengio, Yoshua and Kégl, Balázs},
  editor = {Shawe-Taylor, J. and Zemel, R. S. and Bartlett, P. L. and Pereira, F. and Weinberger, K. Q.},
  date = {2011},
  pages = {2546--2554},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf},
  urldate = {2018-12-20}
}

@article{bergstra_making_,
  title = {Making a {{Science}} of {{Model Search}}: {{Hyperparameter Optimization}} in {{Hundreds}} of {{Dimensions}} for {{Vision Architectures}}},
  author = {Bergstra, J and Yamins, D and Cox, D D},
  pages = {9},
  abstract = {Many computer vision algorithms depend on configuration settings that are typically hand-tuned in the course of evaluating the algorithm for a particular data set. While such parameter tuning is often presented as being incidental to the algorithm, correctly setting these parameter choices is frequently critical to realizing a method’s full potential. Compounding matters, these parameters often must be re-tuned when the algorithm is applied to a new problem domain, and the tuning process itself often depends on personal experience and intuition in ways that are hard to quantify or describe. Since the performance of a given technique depends on both the fundamental quality of the algorithm and the details of its tuning, it is sometimes difficult to know whether a given technique is genuinely better, or simply better tuned.},
  langid = {english}
}

@article{bergstra_random_2012,
  title = {Random {{Search}} for {{Hyper-Parameter Optimization}}},
  author = {Bergstra, James and Bengio, Yoshua},
  date = {2012},
  journaltitle = {Journal of Machine Learning Research},
  volume = {13},
  pages = {281--305},
  abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimiza-tion. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a compar-ison with a large previous study that used grid search and manual search to configure neural net-works and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising con-figuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent " High Throughput " methods achieve surprising success—they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural base-line against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.}
}

@misc{berner_modern_2021,
  title = {The {{Modern Mathematics}} of {{Deep Learning}}},
  author = {Berner, Julius and Grohs, Philipp and Kutyniok, Gitta and Petersen, Philipp},
  date = {2021-05-09},
  eprint = {2105.04026},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2105.04026},
  urldate = {2021-06-15},
  abstract = {We describe the new field of mathematical analysis of deep learning. This field emerged around a list of research questions that were not answered within the classical framework of learning theory. These questions concern: the outstanding generalization power of overparametrized neural networks, the role of depth in deep architectures, the apparent absence of the curse of dimensionality, the surprisingly successful optimization performance despite the non-convexity of the problem, understanding what features are learned, why deep architectures perform exceptionally well in physical problems, and which fine aspects of an architecture affect the behavior of a learning task in which way. We present an overview of modern approaches that yield partial answers to these questions. For selected approaches, we describe the main ideas in more detail.},
  langid = {english}
}

@misc{bernstein_learning_2020,
  title = {Learning Compositional Functions via Multiplicative Weight Updates},
  author = {Bernstein, Jeremy and Zhao, Jiawei and Meister, Markus and Liu, Ming-Yu and Anandkumar, Anima and Yue, Yisong},
  date = {2020-06-25},
  eprint = {2006.14560},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2006.14560},
  urldate = {2020-07-13},
  abstract = {Compositionality is a basic structural feature of both biological and artificial neural networks. Learning compositional functions via gradient descent incurs well known problems like vanishing and exploding gradients, making careful learning rate tuning essential for real-world applications. This paper proves that multiplicative weight updates satisfy a descent lemma tailored to compositional functions. Based on this lemma, we derive Madam—a multiplicative version of the Adam optimiser—and show that it can train state of the art neural network architectures without learning rate tuning. We further show that Madam is easily adapted to train natively compressed neural networks by representing their weights in a logarithmic number system. We conclude by drawing connections between multiplicative weight updates and recent findings about synapses in biology.},
  langid = {english}
}

@misc{betancourt_conceptual_2018,
  title = {A {{Conceptual Introduction}} to {{Hamiltonian Monte Carlo}}},
  author = {Betancourt, Michael},
  date = {2018-07-15},
  eprint = {1701.02434},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1701.02434},
  urldate = {2020-10-08},
  abstract = {Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous understanding of why it performs so well on difficult problems and how it is best applied in practice. Unfortunately, that understanding is confined within the mathematics of differential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important.},
  langid = {english}
}

@misc{bhalodia_dpvaes_2019,
  title = {{{dpVAEs}}: {{Fixing Sample Generation}} for {{Regularized VAEs}}},
  shorttitle = {{{dpVAEs}}},
  author = {Bhalodia, Riddhish and Lee, Iain and Elhabian, Shireen},
  date = {2019-11-24},
  eprint = {1911.10506},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1911.10506},
  urldate = {2019-12-20},
  abstract = {Unsupervised representation learning via generative modeling is a staple to many computer vision applications in the absence of labeled data. Variational Autoencoders (VAEs) are powerful generative models that learn representations useful for data generation. However, due to inherent challenges in the training objective, VAEs fail to learn useful representations amenable for downstream tasks. Regularization-based methods that attempt to improve the representation learning aspect of VAEs come at a price: poor sample generation. In this paper, we explore this representation-generation trade-off for regularized VAEs and introduce a new family of priors, namely decoupled priors, or dpVAEs, that decouple the representation space from the generation space. This decoupling enables the use of VAE regularizers on the representation space without impacting the distribution used for sample generation, and thereby reaping the representation learning benefits of the regularizations without sacrificing the sample generation. dpVAE leverages invertible networks to learn a bijective mapping from an arbitrarily complex representation distribution to a simple, tractable, generative distribution. Decoupled priors can be adapted to the state-of-the-art VAE regularizers without additional hyperparameter tuning. We showcase the use of dpVAEs with different regularizers. Experiments on MNIST, SVHN, and CelebA demonstrate, quantitatively and qualitatively, that dpVAE fixes sample generation for regularized VAEs.},
  langid = {english}
}

@online{bhati_segmental_2021,
  title = {Segmental {{Contrastive Predictive Coding}} for {{Unsupervised Word Segmentation}}},
  author = {Bhati, Saurabhchand and Villalba, Jesús and Żelasko, Piotr and Moro-Velazquez, Laureano and Dehak, Najim},
  date = {2021-06-03},
  eprint = {2106.02170},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2106.02170},
  urldate = {2023-09-23},
  abstract = {Automatic detection of phoneme or word-like units is one of the core objectives in zero-resource speech processing. Recent attempts employ self-supervised training methods, such as contrastive predictive coding (CPC), where the next frame is predicted given past context. However, CPC only looks at the audio signal's frame-level structure. We overcome this limitation with a segmental contrastive predictive coding (SCPC) framework that can model the signal structure at a higher level e.g. at the phoneme level. In this framework, a convolutional neural network learns frame-level representation from the raw waveform via noise-contrastive estimation (NCE). A differentiable boundary detector finds variable-length segments, which are then used to optimize a segment encoder via NCE to learn segment representations. The differentiable boundary detector allows us to train frame-level and segment-level encoders jointly. Typically, phoneme and word segmentation are treated as separate tasks. We unify them and experimentally show that our single model outperforms existing phoneme and word segmentation methods on TIMIT and Buckeye datasets. We analyze the impact of boundary threshold and when is the right time to include the segmental loss in the learning process.},
  pubstate = {preprint}
}

@article{bhati_unsupervised_2022,
  title = {Unsupervised Speech Segmentation and Variable Rate Representation Learning Using Segmental Contrastive Predictive Coding},
  author = {Bhati, Saurabhchand and Villalba, Jesús and Żelasko, Piotr and Moro-Velazquez, Laureano and Dehak, Najim},
  date = {2022},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {30},
  pages = {2002--2014},
  publisher = {{IEEE}}
}

@inproceedings{binder_layerwise_2016,
  title = {Layer-Wise {{Relevance Propagation}} for {{Neural Networks}} with {{Local Renormalization Layers}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Artificial Neural Networks}} ({{ICANN}})},
  author = {Binder, Alexander and Montavon, Grégoire and Bach, Sebastian and Müller, Klaus-Robert and Samek, Wojciech},
  date = {2016-04-04},
  volume = {9887},
  eprint = {1604.00825},
  eprinttype = {arxiv},
  pages = {63--71},
  publisher = {{Springer}},
  location = {{Barcelona, Spain}},
  url = {http://arxiv.org/abs/1604.00825},
  urldate = {2023-04-20},
  abstract = {Layer-wise relevance propagation is a framework which allows to decompose the prediction of a deep neural network computed over a sample, e.g. an image, down to relevance scores for the single input dimensions of the sample such as subpixels of an image. While this approach can be applied directly to generalized linear mappings, product type non-linearities are not covered. This paper proposes an approach to extend layer-wise relevance propagation to neural networks with local renormalization layers, which is a very common product-type non-linearity in convolutional neural networks. We evaluate the proposed method for local renormalization layers on the CIFAR-10, Imagenet and MIT Places datasets.}
}

@report{bishop_mixture_1994,
  title = {Mixture {{Density Networks}}},
  author = {Bishop, Christopher M.},
  date = {1994-02},
  number = {NCRG/94/4288},
  institution = {{Neural Computing Research Group - Dept. of Computer Science and Applied Mathematics}},
  location = {{Aston University}},
  abstract = {Minimization of a sum-of-squares or cross-entropy error function leads to network outputs which approximate the conditional averages of the target data, conditioned on the input vector. For classi cations problems, with a suitably chosen target coding scheme, these averages represent the posterior probabilities of class membership, and so can be regarded as optimal. For problems involving the prediction of continuous variables, however, the conditional averages provide only a very limited description of the properties of the target variables. This is particularly true for problems in which the mapping to be learned is multi-valued, as often arises in the solution of inverse problems, since the average of several correct target values is not necessarily itself a correct value. In order to obtain a complete description of the data, for the purposes of predicting the outputs corresponding to new input vectors, we must model the conditional probability distribution of the target data, again conditioned on the input vector. In this paper we introduce a new class of network models obtained by combining a conventional neural network with a mixture density model. The complete system is called a Mixture Density Network, and can in principle represent arbitrary conditional probability distributions in the same way that a conventional neural network can represent arbitrary functions. We demonstrate the e ectiveness of Mixture Density Networks using both a toy problem and a problem involving robot inverse kinematics.}
}

@article{bishop_novelty_1994,
  title = {Novelty {{Detection}} and {{Neural-Network Validation}}},
  author = {Bishop, Christopher M.},
  date = {1994},
  journaltitle = {IEE Proceedings - Vision, Image and Signal Processing},
  volume = {141},
  number = {4},
  pages = {217--222},
  issn = {1350245x, 13597108},
  doi = {10.1049/ip-vis:19941330},
  langid = {english}
}

@book{bishop_pattern_2006,
  title = {Pattern {{Recognition}} and {{Machine Learning}}},
  author = {Bishop, Christopher M.},
  date = {2006},
  journaltitle = {New York: springer.},
  publisher = {{Springer}},
  isbn = {978-0-387-31073-2},
  keywords = {★}
}

@article{blaabjerg_early_2016,
  title = {Early Stage Investigation of Sensor Technologies and Related Bioassay for in Vivo Diagnosis of Colorectal Cancer},
  author = {Blaabjerg, Lasse and Havtorn, Jakob D. and Svendsen, Winnie E. and Abi, Alireza and Kwasny, Dorota},
  date = {2016}
}

@online{blanco_nhtsa_2021,
  title = {{{NHTSA Investigating Tesla Autopilot Crashes}} with {{First Responders}}},
  author = {Blanco, Sebastian},
  date = {2021-08-16},
  url = {https://www.caranddriver.com/news/a37320725/nhtsa-investigating-tesla-autopilot-crashes-fatalities/},
  urldate = {2023-08-25},
  organization = {{Car and Driver}}
}

@misc{blei_distance_2009,
  title = {Distance {{Dependent Chinese Restaurant Processes}}},
  author = {Blei, David M. and Frazier, Peter I.},
  date = {2009-10-06},
  eprint = {0910.1022},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/0910.1022},
  urldate = {2019-03-25},
  abstract = {We develop the distance dependent Chinese restaurant process (CRP), a flexible class of distributions over partitions that allows for non-exchangeability. This class can be used to model many kinds of dependencies between data in infinite clustering models, including dependencies across time or space. We examine the properties of the distance dependent CRP, discuss its connections to Bayesian nonparametric mixture models, and derive a Gibbs sampler for both observed and mixture settings. We study its performance with three text corpora. We show that relaxing the assumption of exchangeability with distance dependent CRPs can provide a better fit to sequential data. We also show its alternative formulation of the traditional CRP leads to a faster-mixing Gibbs sampling algorithm than the one based on the original formulation.}
}

@article{blei_variational_2017,
  title = {Variational {{Inference}}: {{A Review}} for {{Statisticians}}},
  author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
  date = {2017-01-04},
  journaltitle = {Journal of the American Statistical Association},
  volume = {112},
  number = {518},
  eprint = {303902},
  eprinttype = {pmid},
  pages = {859--877},
  issn = {1537274X},
  doi = {10.1080/01621459.2017.1285773},
  url = {http://arxiv.org/abs/1601.00670},
  urldate = {2018-05-26},
  abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
  isbn = {1601.00670}
}

@misc{blier_learning_2018,
  title = {Learning with {{Random Learning Rates}}},
  author = {Blier, Léonard and Wolinski, Pierre and Ollivier, Yann},
  date = {2018-10-02},
  eprint = {1810.01322},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1810.01322},
  urldate = {2018-10-10},
  abstract = {Hyperparameter tuning is a bothersome step in the training of deep learning models. One of the most sensitive hyperparameters is the learning rate of the gradient descent. We present the All Learning Rates At Once (Alrao) optimization method for neural networks: each unit or feature in the network gets its own learning rate sampled from a random distribution spanning several orders of magnitude. This comes at practically no computational cost. Perhaps surprisingly, stochastic gradient descent (SGD) with Alrao performs close to SGD with an optimally tuned learning rate, for various architectures and problems. Alrao could save time when testing deep learning models: a range of models could be quickly assessed with Alrao, and the most promising models could then be trained more extensively. This text comes with a PyTorch implementation of the method, which can be plugged on an existing PyTorch model: https://github.com/leonardblier/alrao.},
  langid = {english}
}

@book{blitzstein_introduction_2019,
  title = {Introduction to {{Probability}}},
  author = {Blitzstein, Joseph K. and Hwang, Jessica},
  date = {2019-02-08},
  series = {Chapman \& {{Hall}}/{{CRC Texts}} in {{Statistical Science}}},
  edition = {2},
  publisher = {{CRC Press}},
  location = {{Boca Raton, London, New York}},
  doi = {10.1201/9780429428357},
  isbn = {978-1-138-36991-7},
  langid = {english},
  pagetotal = {619}
}

@article{block_analysis_1962,
  title = {Analysis of a {{Four-Layer Series-Coupled Perceptron II}}},
  author = {Block, H. D. and Knight, Bruce and Rosenblatt, Frank},
  date = {1962},
  journaltitle = {Reviews of Modern Physics},
  volume = {34},
  number = {1},
  url = {http://digitalcommons.rockefeller.edu/knight_laboratory},
  urldate = {2018-05-22},
  abstract = {Block, H. D., Knight, B. W., Jr., Rosenblatt, F. (1962) Analysis of a four-layer series-coupled perceptron. II. Rev. Mod. Phys. 34: 135 THE PERC EPTION. I 100 C— ISPLAY 75-DISPLAY 50 \textasciitilde{} \textasciitilde\textasciitilde\textasciitilde\textasciitilde{} \textasciitilde{} \textasciitilde{} \textasciitilde{} \textasciitilde{} \textasciitilde{} \textasciitilde{} \textasciitilde{} \textasciitilde\textasciitilde\textasciitilde CHANCE EXPECTAHCY I/2 ASSOCIATIOH 3/4 ASSOCIATIOH \&\& ASSOCIATION UNITS REMOVED UNITS RKMOYED UNITS REIAO+ED IHT ACT PERCEPTROH (240 ASSOCIATIOH IjHITS) I'ro. 13. EGect of association unit removal on trained "E"— "X" discrimination. simple perceptron of Fig. 4 is no longer adequate. We shall find certain temporal e8ects in the paper which follows, but for others it is necessary to introduce time delays into the system. ' A speech recognizing perceptron which utilizes such delays is currently being built at Cornell University. Other activities now in progress' include quantitative studies of cross-coupled and multi-layer systems (by means of analysis and digital simulation), studies of selective attention mechanisms, the effects of geometric constraints on network organization, new types of reinforcement rules, and attempts at relating this research to biological data. Work is also in progress on development of electrolytic and other low-cost inte-grating devices and additional electronic components necessary for the construction of large-scale physical models. It is clear that we are still far from the point of understanding how the brain functions. It is equally clear, we believe, that a promising road is open for further investigation.}
}

@inproceedings{blundell_weight_2015,
  title = {Weight {{Uncertainty}} in {{Neural Networks}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
  date = {2015},
  eprint = {1505.05424},
  eprinttype = {arxiv},
  location = {{Lille, France}},
  issn = {1938-7228},
  url = {http://arxiv.org/abs/1505.05424},
  urldate = {2018-05-06},
  abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
  isbn = {978-1-5108-1058-7}
}

@article{bock_improvement_2018,
  title = {An Improvement of the Convergence Proof of the {{ADAM-Optimizer}}},
  author = {Bock, Sebastian and Goppold, Josef and Weiß, Martin},
  date = {2018},
  abstract = {—A common way to train neural networks is the Backpropagation. This algorithm includes a gradient descent method, which needs an adaptive step size. In the area of neural networks, the ADAM-Optimizer is one of the most popular adaptive step size methods. It was invented in [1] by Kingma and Ba. The 5865 citations in only three years shows additionally the importance of the given paper. We discovered that the given convergence proof of the optimizer contains some mistakes, so that the proof will be wrong. In this paper we give an improvement to the convergence proof of the ADAM-Optimizer.}
}

@inproceedings{bocklet_age_2008,
  title = {Age and Gender Recognition for Telephone Applications Based on {{GMM}} Supervectors and Support Vector Machines},
  booktitle = {2008 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  author = {Bocklet, Tobias and Maier, Andreas and Bauer, Josef G. and Burkhardt, Felix and Noth, Elmar},
  date = {2008-03},
  pages = {1605--1608},
  publisher = {{IEEE}},
  location = {{Las Vegas, NV, USA}},
  doi = {10/cchnnz},
  url = {http://ieeexplore.ieee.org/document/4517932/},
  urldate = {2019-01-07},
  abstract = {This paper compares two approaches of automatic age and gender classification with 7 classes. The first approach are Gaussian Mixture Models (GMMs) with Universal Background Models (UBMs), which is well known for the task of speaker identification/verification. The training is performed by the EM algorithm or MAP adaptation respectively. For the second approach for each speaker of the test and training set a GMM model is trained. The means of each model are extracted and concatenated, which results in a GMM supervector for each speaker. These supervectors are then used in a support vector machine (SVM). Three different kernels were employed for the SVM approach: a polynomial kernel (with different polynomials), an RBF kernel and a linear GMM distance kernel, based on the KL divergence. With the SVM approach we improved the recognition rate to 74\% (p {$<$} 0.001) and are in the same range as humans.},
  eventtitle = {{{ICASSP}} 2008 - 2008 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  isbn = {978-1-4244-1483-3 978-1-4244-1484-0},
  langid = {english}
}

@article{bolukbasi_adaptive_,
  title = {Adaptive {{Neural Networks}} for {{Efficient Inference}}},
  author = {Bolukbasi, Tolga and Wang, Joseph and Dekel, Ofer and Saligrama, Venkatesh},
  pages = {10},
  abstract = {We present an approach to adaptively utilize deep neural networks in order to reduce the evaluation time on new examples without loss of accuracy. Rather than attempting to redesign or approximate existing networks, we propose two schemes that adaptively utilize networks. We first pose an adaptive network evaluation scheme, where we learn a system to adaptively choose the components of a deep network to be evaluated for each example. By allowing examples correctly classified using early layers of the system to exit, we avoid the computational time associated with full evaluation of the network. We extend this to learn a network selection system that adaptively selects the network to be evaluated for each example. We show that computational time can be dramatically reduced by exploiting the fact that many examples can be correctly classified using relatively efficient networks and that complex, computationally costly networks are only necessary for a small fraction of examples. We pose a global objective for learning an adaptive early exit or network selection policy and solve it by reducing the policy learning problem to a layerby-layer weighted binary classification problem. Empirically, these approaches yield dramatic reductions in computational cost, with up to a 2.8x speedup on state-of-the-art networks from the ImageNet image recognition challenge with minimal ({$<$} 1\%) loss of top5 accuracy.},
  langid = {english}
}

@article{bolukbasi_man_2016,
  title = {Man Is to {{Computer Programmer}} as {{Woman}} Is to {{Homemaker}}? {{Debiasing Word Embeddings}}},
  author = {Bolukbasi, Tolga and Chang, Kai-wei and Zou, James and Saligrama, Venkatesh and Kalai, Adam},
  date = {2016-07-21},
  journaltitle = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
  eprint = {1607.06520},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1607.06520},
  abstract = {The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between between the words receptionist and female, while maintaining desired associations such as between the words queen and female. We define metrics to quantify both direct and indirect gender biases in embeddings, and develop algorithms to "debias" the embedding. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.},
  keywords = {★}
}

@online{bommasani_opportunities_2021,
  title = {On the Opportunities and Risks of Foundation Models},
  author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ B. and Arora, Simran and family=Arx, given=Sydney, prefix=von, useprefix=true and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri S. and Chen, Annie S. and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dorottya and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah D. and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark S. and Krishna, Ranjay and Kuditipudi, Rohith and {al.}, et},
  date = {2021},
  eprint = {2108.07258},
  eprinttype = {arxiv},
  url = {https://arxiv.org/abs/2108.07258},
  pubstate = {preprint}
}

@misc{bond-taylor_deep_2021,
  title = {Deep {{Generative Modelling}}: {{A Comparative Review}} of {{VAEs}}, {{GANs}}, {{Normalizing Flows}}, {{Energy-Based}} and {{Autoregressive Models}}},
  shorttitle = {Deep {{Generative Modelling}}},
  author = {Bond-Taylor, Sam and Leach, Adam and Long, Yang and Willcocks, Chris G.},
  date = {2021-03-08},
  eprint = {2103.04922},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2103.04922},
  urldate = {2021-03-10},
  abstract = {Deep generative modelling is a class of techniques that train deep neural networks to model the distribution of training samples. Research has fragmented into various interconnected approaches, each of which making trade-offs including run-time, diversity, and architectural restrictions. In particular, this compendium covers energy-based models, variational autoencoders, generative adversarial networks, autoregressive models, normalizing flows, in addition to numerous hybrid approaches. These techniques are drawn under a single cohesive framework, comparing and contrasting to explain the premises behind each, while reviewing current state-of-the-art advances and implementations.},
  langid = {english}
}

@misc{bonheme_be_2021,
  title = {Be {{More Active}}! {{Understanding}} the {{Differences}} between {{Mean}} and {{Sampled Representations}} of {{Variational Autoencoders}}},
  author = {Bonheme, Lisa and Grzes, Marek},
  date = {2021-09-29},
  eprint = {2109.12679},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2109.12679},
  urldate = {2021-10-22},
  abstract = {The ability of Variational Autoencoders to learn disentangled representations has made them appealing for practical applications. However, their mean representations, which are generally used for downstream tasks, have recently been shown to be more correlated than their sampled counterpart, on which disentanglement is usually measured. In this paper, we refine this observation through the lens of selective posterior collapse, which states that only a subset of the learned representations, the active variables, is encoding useful information while the rest (the passive variables) is discarded. We first extend the existing definition, originally proposed for sampled representations, to mean representations and show that active variables are equally disentangled in both representations. Based on this new definition and the pre-trained models from disentanglement lib, we then isolate the passive variables and show that they are responsible for the discrepancies between mean and sampled representations. Specifically, passive variables exhibit high correlation scores with other variables in mean representations while being fully uncorrelated in sampled ones. We thus conclude that despite what their higher correlation might suggest, mean representations are still good candidates for downstream tasks applications. However, it may be beneficial to remove their passive variables, especially when used with models sensitive to correlated features.},
  langid = {english}
}

@inproceedings{borgholt_brief_2022,
  title = {A {{Brief Overview}} of {{Neural Speech Representation Learning}}},
  booktitle = {Proceedings of the 2nd {{Workshop}} on {{Self-supervised Learning}} for {{Audio}} and {{Speech Processing}} ({{SAS}})},
  author = {Borgholt, Lasse and Havtorn, Jakob D. and Edin, Joakim and Maaløe, Lars and Igel, Christian},
  date = {2022-02-28},
  eprint = {2203.01829},
  eprinttype = {arxiv},
  location = {{Virtual}},
  url = {https://aaai-sas-2022.github.io/static/media/A_Brief_Overview_of_Unsupervised_Neural_Speech_Representation_Learning__AAAI_SAS_.b293cc96.pdf},
  abstract = {Unsupervised representation learning for speech processing has matured greatly in the last few years. Work in computer vision and natural language processing has paved the way, but speech data offers unique challenges. As a result, methods from other domains rarely translate directly. We review the development of unsupervised representation learning for speech over the last decade. We identify two primary model categories: self-supervised methods and probabilistic latent variable models. We describe the models and develop a comprehensive taxonomy. Finally, we discuss and compare models from the two categories.},
  eventtitle = {Thirty-{{Sixth AAAI Conference}} on {{Artificial Intelligence}}}
}

@inproceedings{borgholt_endtoend_2020,
  title = {Do End-to-End Speech Recognition Models Care about Context?},
  booktitle = {Proceedings of the 21st {{Annual Conference}} of the {{International Speech Communication Association}} ({{INTERSPEECH}})},
  author = {Borgholt, Lasse and Havtorn, Jakob D. and Agić, Željko and Søgaard, Anders and Maaløe, Lars and Igel, Christian},
  date = {2020},
  eprint = {2102.09928},
  eprinttype = {arxiv},
  pages = {4352--4356},
  publisher = {{ISCA}},
  location = {{Virtual}},
  doi = {10.21437/Interspeech.2020-1750},
  url = {https://doi.org/10.21437/Interspeech.2020-1750}
}

@inproceedings{borgholt_scaling_2021,
  title = {On {{Scaling Contrastive Representations}} for {{Low-Resource Speech Recognition}}},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Borgholt, Lasse and Tax, Tycho M. S. and Havtorn, Jakob D. and Maaløe, Lars and Igel, Christian},
  date = {2021},
  eprint = {2102.00850},
  eprinttype = {arxiv},
  pages = {3885--3889},
  publisher = {{IEEE}},
  location = {{Virtual}},
  doi = {10.1109/ICASSP39728.2021.9414310},
  url = {https://doi.org/10.1109/ICASSP39728.2021.9414310}
}

@online{borgholt_we_2021,
  title = {Do {{We Still Need Automatic Speech Recognition}} for {{Spoken Language Understanding}}?},
  author = {Borgholt, Lasse and Havtorn, Jakob D. and Abdou, Mostafa and Edin, Joakim and Maaløe, Lars and Søgaard, Anders and Igel, Christian},
  date = {2021-11-29},
  eprint = {2111.14842},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2111.14842},
  abstract = {Spoken language understanding (SLU) tasks are usually solved by first transcribing an utterance with automatic speech recognition (ASR) and then feeding the output to a text-based model. Recent advances in self-supervised representation learning for speech data have focused on improving the ASR component. We investigate whether representation learning for speech has matured enough to replace ASR in SLU. We compare learned speech features from wav2vec 2.0, state-of-the-art ASR transcripts, and the ground truth text as input for a novel speech-based named entity recognition task, a cardiac arrest detection task on real-world emergency calls and two existing SLU benchmarks. We show that learned speech features are superior to ASR transcripts on three classification tasks. For machine translation, ASR transcripts are still the better choice. We highlight the intrinsic robustness of wav2vec 2.0 representations to out-of-vocabulary words as key to better performance.},
  pubstate = {preprint}
}

@inproceedings{bornschein_variational_2017,
  title = {Variational Memory Addressing in Generative Models},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30: {{Annual Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Bornschein, Jörg and Mnih, Andriy and Zoran, Daniel and Rezende, Danilo Jimenez},
  date = {2017},
  pages = {3920--3929},
  location = {{Long Beach, CA, USA}},
  url = {https://proceedings.neurips.cc/paper/2017/hash/3937230de3c8041e4da6ac3246a888e8-Abstract.html}
}

@misc{borsos_online_2018,
  title = {Online {{Variance Reduction}} for {{Stochastic Optimization}}},
  author = {Borsos, Zalán and Krause, Andreas and Levy, Kfir Y.},
  date = {2018},
  eprint = {1802.04715},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1802.04715},
  urldate = {2018-04-23},
  abstract = {Modern stochastic optimization methods often rely on uniform sampling which is agnostic to the underlying characteristics of the data. This might degrade the convergence by yielding estimates that suffer from a high variance. A possible remedy is to employ non-uniform importance sampling techniques, which take the structure of the dataset into account. In this work, we investigate a recently proposed setting which poses variance reduction as an online optimization problem with bandit feedback. We devise a novel and efficient algorithm for this setting that finds a sequence of importance sampling distributions competitive with the best fixed distribution in hindsight, the first result of this kind. While we present our method for sampling datapoints, it naturally extends to selecting coordinates or even blocks of thereof. Empirical validations underline the benefits of our method in several settings.}
}

@misc{botev_practical_2017,
  title = {Practical {{Gauss-Newton Optimisation}} for {{Deep Learning}}},
  author = {Botev, Aleksandar and Ritter, Hippolyt and Barber, David},
  date = {2017},
  eprint = {1706.03662},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1706.03662},
  urldate = {2018-04-23},
  abstract = {We present an efficient block-diagonal ap- proximation to the Gauss-Newton matrix for feedforward neural networks. Our result- ing algorithm is competitive against state- of-the-art first order optimisation methods, with sometimes significant improvement in optimisation performance. Unlike first-order methods, for which hyperparameter tuning of the optimisation parameters is often a labo- rious process, our approach can provide good performance even when used with default set- tings. A side result of our work is that for piecewise linear transfer functions, the net- work objective function can have no differ- entiable local maxima, which may partially explain why such transfer functions facilitate effective optimisation.}
}

@article{bottou_counterfactual_,
  title = {Counterfactual {{Reasoning}} and {{Learning Systems}}: {{The Example}} of {{Computational Advertising}}},
  author = {Bottou, Léon and Peters, Jonas and Quiñonero-Candela, Joaquin and Charles, Denis X and Chickering, D Max and Portugaly, Elon and Ray, Dipankar and Simard, Patrice and Snelson, Ed},
  pages = {54},
  abstract = {This work shows how to leverage causal inference to understand the behavior of complex learning systems interacting with their environment and predict the consequences of changes to the system. Such predictions allow both humans and algorithms to select the changes that would have improved the system performance. This work is illustrated by experiments on the ad placement system associated with the Bing search engine.},
  langid = {english}
}

@inproceedings{boulanger-lewandowski_modeling_2012,
  title = {Modeling {{Temporal Dependencies}} in {{High-Dimensional Sequences}}: {{Application}} to {{Polyphonic Music Generation}} and {{Transcription}}},
  booktitle = {Proceedings of the 29th {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Boulanger-Lewandowski, Nicolas and Bengio, Yoshua and Vincent, Pascal},
  date = {2012},
  pages = {8},
  location = {{Edinburgh, Scotland, UK}},
  abstract = {We investigate the problem of modeling symbolic sequences of polyphonic music in a completely general piano-roll representation. We introduce a probabilistic model based on distribution estimators conditioned on a recurrent neural network that is able to discover temporal dependencies in high-dimensional sequences. Our approach outperforms many traditional models of polyphonic music on a variety of realistic datasets. We show how our musical language model can serve as a symbolic prior to improve the accuracy of polyphonic transcription.},
  langid = {english}
}

@book{bourlard_connectionist_1994,
  title = {Connectionist Speech Recognition: A Hybrid Approach},
  author = {Bourlard, Herve A and Morgan, Nelson},
  date = {1994},
  volume = {247},
  publisher = {{Springer Science \& Business Media}}
}

@inproceedings{bowman_generating_2016,
  title = {Generating {{Sentences}} from a {{Continuous Space}}},
  booktitle = {Proceedings of the 20th {{SIGNLL Conference}} on {{Computational Natural Language Learning}} ({{CoNLL}})},
  author = {Bowman, Samuel R. and Vilnis, Luke and Vinyals, Oriol and Dai, Andrew M. and Józefowicz, Rafal and Bengio, Samy},
  date = {2016},
  pages = {10--21},
  location = {{Berlin, Germany}},
  doi = {10.18653/v1/k16-1002},
  url = {https://doi.org/10.18653/v1/k16-1002},
  eventtitle = {{{SIGNLL Conference}} on {{Computational Natural Language Learning}}}
}

@book{boyd_convex_2004,
  title = {Convex Optimization},
  author = {Boyd, Stephen P. and Vandenberghe, Lieven},
  date = {2004},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge, UK ; New York}},
  isbn = {978-0-521-83378-3},
  langid = {english},
  pagetotal = {716},
  keywords = {★}
}

@inproceedings{bozorgtabar_sood_2021,
  title = {{{SOoD}}: {{Self-Supervised Out-of-Distribution Detection Under Domain Shift}} for {{Multi-Class Colorectal Cancer Tissue Types}}},
  shorttitle = {{{SOoD}}},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision Workshops}} ({{ICCVW}})},
  author = {Bozorgtabar, Behzad and Vray, Guillaume and Mahapatra, Dwarikanath and Thiran, Jean-Philippe},
  date = {2021-10},
  pages = {3317--3326},
  issn = {2473-9944},
  doi = {10.1109/ICCVW54120.2021.00371},
  abstract = {The goal of out-of-distribution (OoD) detection is to identify unseen categories of inputs different from those seen during training, which is an important requirement for the safe deployment of deep neural networks in computational pathology. Additionally, to make OoD detection applicable in clinical applications, one may encounter image data distribution shifts. This paper argues that practical OoD detection should handle both semantic shift and data distribution shift simultaneously. We propose a new self-supervised OoD detector for colorectal cancer tissue types based on a clustering scheme. Our work’s central tenet benefits from multi-view consistency learning with a supplementary view based on style augmentation to mitigate domain shift. The learned representation is then adapted to minimize images’ predictive entropy to segregate indistribution examples from OoDs on the target data domain. We evaluated our method on two public colorectal tissue types datasets. Our method achieved state-of-the-art OoD detection performance over various self-supervised baselines. The code, data, and models are available at https://github.com/BehzadBozorgtabar/SOoD.},
  eventtitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision Workshops}} ({{ICCVW}})}
}

@misc{bradbury_quasirecurrent_2016,
  title = {Quasi-{{Recurrent Neural Networks}}},
  author = {Bradbury, James and Merity, Stephen and Xiong, Caiming and Socher, Richard},
  date = {2016-11-04},
  eprint = {1611.01576},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1611.01576},
  urldate = {2018-09-05},
  abstract = {Recurrent neural networks are a powerful tool for modeling sequential data, but the dependence of each timestep's computation on the previous timestep's output limits parallelism and makes RNNs unwieldy for very long sequences. We introduce quasi-recurrent neural networks (QRNNs), an approach to neural sequence modeling that alternates convolutional layers, which apply in parallel across timesteps, and a minimalist recurrent pooling function that applies in parallel across channels. Despite lacking trainable recurrent layers, stacked QRNNs have better predictive accuracy than stacked LSTMs of the same hidden size. Due to their increased parallelism, they are up to 16 times faster at train and test time. Experiments on language modeling, sentiment classification, and character-level neural machine translation demonstrate these advantages and underline the viability of QRNNs as a basic building block for a variety of sequence tasks.}
}

@software{brandenburg_mpeg-2_1998,
  title = {{{MPEG-2 Audio Layer III}} ({{MP3}})},
  author = {Brandenburg, Karlheinz and Eberlein, Ernst and Gerhäuser, Heinz and Grill, Bernhard and Herre, Jürgen and Popp, Harald},
  date = {1998},
  location = {{Germany}},
  organization = {{Fraunhofer Society}}
}

@misc{braun_curriculum_2016,
  title = {A {{Curriculum Learning Method}} for {{Improved Noise Robustness}} in {{Automatic Speech Recognition}}},
  author = {Braun, Stefan and Neil, Daniel and Liu, Shih-Chii},
  date = {2016-06-22},
  eprint = {1606.06864},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1606.06864},
  urldate = {2018-11-19},
  abstract = {The performance of automatic speech recognition systems under noisy environments still leaves room for improvement. Speech enhancement or feature enhancement techniques for increasing noise robustness of these systems usually add components to the recognition system that need careful optimization. In this work, we propose the use of a relatively simple curriculum training strategy called accordion annealing (ACCAN). It uses a multi-stage training schedule where samples at signal-to-noise ratio (SNR) values as low as 0dB are first added and samples at increasing higher SNR values are gradually added up to an SNR value of 50dB. We also use a method called per-epoch noise mixing (PEM) that generates noisy training samples online during training and thus enables dynamically changing the SNR of our training data. Both the ACCAN and the PEM methods are evaluated on a end-to-end speech recognition pipeline on the Wall Street Journal corpus. ACCAN decreases the average word error rate (WER) on the 20dB to -10dB SNR range by up to 31.4\% when compared to a conventional multi-condition training method.}
}

@online{bredin_endtoend_2021,
  title = {End-to-End Speaker Segmentation for Overlap-Aware Resegmentation},
  author = {Bredin, Hervé and Laurent, Antoine},
  date = {2021-06-10},
  eprint = {2104.04045},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2104.04045},
  urldate = {2023-02-14},
  abstract = {Speaker segmentation consists in partitioning a conversation between one or more speakers into speaker turns. Usually addressed as the late combination of three sub-tasks (voice activity detection, speaker change detection, and overlapped speech detection), we propose to train an end-to-end segmentation model that does it directly. Inspired by the original end-to-end neural speaker diarization approach (EEND), the task is modeled as a multi-label classification problem using permutation-invariant training. The main difference is that our model operates on short audio chunks (5 seconds) but at a much higher temporal resolution (every 16ms). Experiments on multiple speaker diarization datasets conclude that our model can be used with great success on both voice activity detection and overlapped speech detection. Our proposed model can also be used as a post-processing step, to detect and correctly assign overlapped speech regions. Relative diarization error rate improvement over the best considered baseline (VBx) reaches 17\% on AMI, 13\% on DIHARD 3, and 13\% on VoxConverse.},
  pubstate = {preprint}
}

@misc{breen_newton_2019,
  title = {Newton vs the Machine: Solving the Chaotic Three-Body Problem Using Deep Neural Networks},
  shorttitle = {Newton vs the Machine},
  author = {Breen, Philip G. and Foley, Christopher N. and Boekholt, Tjarda and Zwart, Simon Portegies},
  date = {2019-10-16},
  eprint = {1910.07291},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1910.07291},
  urldate = {2019-10-27},
  abstract = {Since its formulation by Sir Isaac Newton, the problem of solving the equations of motion for three bodies under their own gravitational force has remained practically unsolved. Currently, the solution for a given initialization can only be found by performing laborious iterative calculations that have unpredictable and potentially infinite computational cost, due to the system’s chaotic nature. We show that an ensemble of solutions obtained using an arbitrarily precise numerical integrator can be used to train a deep artificial neural network (ANN) that, over a bounded time interval, provides accurate solutions at fixed computational cost and up to 100 million times faster than a state-of-the-art solver. Our results provide evidence that, for computationally challenging regions of phase-space, a trained ANN can replace existing numerical solvers, enabling fast and scalable simulations of many-body systems to shed light on outstanding phenomena such as the formation of black-hole binary systems or the origin of the core collapse in dense star clusters.},
  langid = {english},
  keywords = {★}
}

@online{brehmer_weakly_2022,
  title = {Weakly Supervised Causal Representation Learning},
  author = {Brehmer, Johann and family=Haan, given=Pim, prefix=de, useprefix=true and Lippe, Phillip and Cohen, Taco},
  date = {2022-05-30},
  eprint = {2203.16437},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.16437},
  urldate = {2022-09-07},
  abstract = {Learning high-level causal representations together with a causal model from unstructured low-level data such as pixels is impossible from observational data alone. We prove under mild assumptions that this representation is however identifiable in a weakly supervised setting. This requires a dataset with paired samples before and after random, unknown interventions, but no further labels. We then introduce implicit latent causal models, variational autoencoders that represent causal variables and causal structure without having to optimize an explicit discrete graph structure. On simple image data, including a novel dataset of simulated robotic manipulation, we demonstrate that such models can reliably identify the causal structure and disentangle causal variables.},
  pubstate = {preprint}
}

@article{britton_plimpton_2011,
  title = {Plimpton 322: A Review and a Different Perspective},
  author = {Britton, John P and Proust, Christine and Shnider, Steve},
  date = {2011},
  journaltitle = {Archive for history of exact sciences},
  volume = {65},
  pages = {519--566},
  publisher = {{Springer}}
}

@inproceedings{brock_characterizing_2021,
  title = {Characterizing Signal Propagation to Close the Performance Gap in Unnormalized {{ResNets}}},
  booktitle = {Proceedings of the 9th {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Brock, Andrew and De, Soham and Smith, Samuel L.},
  date = {2021-01-27},
  eprint = {2101.08692},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2101.08692},
  urldate = {2021-02-13},
  abstract = {Batch Normalization is a key component in almost all state-of-the-art image classifiers, but it also introduces practical challenges: it breaks the independence between training examples within a batch, can incur compute and memory overhead, and often results in unexpected bugs. Building on recent theoretical analyses of deep ResNets at initialization, we propose a simple set of analysis tools to characterize signal propagation on the forward pass, and leverage these tools to design highly performant ResNets without activation normalization layers. Crucial to our success is an adapted version of the recently proposed Weight Standardization. Our analysis tools show how this technique preserves the signal in networks with ReLU or Swish activation functions by ensuring that the per-channel activation means do not grow with depth. Across a range of FLOP budgets, our networks attain performance competitive with the state-of-the-art EfficientNets on ImageNet.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english}
}

@misc{brock_highperformance_2021,
  title = {High-{{Performance Large-Scale Image Recognition Without Normalization}}},
  author = {Brock, Andrew and De, Soham and Smith, Samuel L. and Simonyan, Karen},
  date = {2021-02-11},
  eprint = {2102.06171},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2102.06171},
  urldate = {2021-02-13},
  abstract = {Batch normalization is a key component of most image classification models, but it has many undesirable properties stemming from its dependence on the batch size and interactions between examples. Although recent work has succeeded in training deep ResNets without normalization layers, these models do not match the test accuracies of the best batch-normalized networks, and are often unstable for large learning rates or strong data augmentations. In this work, we develop an adaptive gradient clipping technique which overcomes these instabilities, and design a significantly improved class of Normalizer-Free ResNets. Our smaller models match the test accuracy of an EfficientNet-B7 on ImageNet while being up to 8.7x faster to train, and our largest models attain a new state-of-the-art top-1 accuracy of 86.5\%. In addition, Normalizer-Free models attain significantly better performance than their batch-normalized counterparts when finetuning on ImageNet after large-scale pre-training on a dataset of 300 million labeled images, with our best models obtaining an accuracy of 89.2\%. Our code is available at https://github.com/deepmind/ deepmind-research/tree/master/nfnets},
  langid = {english}
}

@online{brock_large_2019,
  title = {Large {{Scale GAN Training}} for {{High Fidelity Natural Image Synthesis}}},
  author = {Brock, Andrew and Donahue, Jeff and Simonyan, Karen},
  date = {2019-02-25},
  eprint = {1809.11096},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1809.11096},
  urldate = {2023-09-22},
  abstract = {Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple "truncation trick," allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.},
  pubstate = {preprint}
}

@misc{brockman_openai_2016,
  title = {{{OpenAI Gym}}},
  author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  date = {2016},
  eprint = {1606.01540},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1606.01540},
  urldate = {2018-04-05},
  abstract = {OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.}
}

@online{bronstein_geometric_2021,
  title = {Geometric {{Deep Learning}}: {{Grids}}, {{Groups}}, {{Graphs}}, {{Geodesics}}, and {{Gauges}}},
  shorttitle = {Geometric {{Deep Learning}}},
  author = {Bronstein, Michael M. and Bruna, Joan and Cohen, Taco and Veličković, Petar},
  date = {2021-04-27},
  eprint = {2104.13478},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2104.13478},
  urldate = {2021-05-01},
  abstract = {The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach -- such as computer vision, playing Go, or protein folding -- are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent type methods, typically implemented as backpropagation. While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world. This text is concerned with exposing these regularities through unified geometric principles that can be applied throughout a wide spectrum of applications. Such a 'geometric unification' endeavour, in the spirit of Felix Klein's Erlangen Program, serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.},
  langid = {english},
  pubstate = {preprint}
}

@article{brown_400_1975,
  title = {400: {{A}} Method for Combining Non-Independent, One-Sided Tests of Significance},
  author = {Brown, Morton B},
  date = {1975},
  journaltitle = {Biometrics. Journal of the International Biometric Society},
  shortjournal = {Biometrics},
  publisher = {{JSTOR}}
}

@inproceedings{brown_language_2020,
  title = {Language Models Are Few-Shot Learners},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M.F. and Lin, H.},
  date = {2020},
  volume = {33},
  pages = {1877--1901},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf}
}

@inproceedings{bu_aishell1_2017,
  title = {{{AISHELL-1}}: {{An}} Open-Source {{Mandarin}} Speech Corpus and a Speech Recognition Baseline},
  booktitle = {Proceedings of the 20th {{Conference}} of the {{Oriental Chapter}} of the {{International Coordinating Committee}} on {{Speech Databases}} and {{Speech I}}/{{O Systems}} and {{Assessment}} ({{O-COCOSDA}})},
  author = {Bu, Hui and Du, Jiayu and Na, Xingyu and Wu, Bengu and Zheng, Hao},
  date = {2017},
  pages = {1--5},
  publisher = {{IEEE}},
  location = {{Seoul, South Korea}},
  doi = {10.1109/ICSDA.2017.8384449},
  url = {https://doi.org/10.1109/ICSDA.2017.8384449}
}

@misc{bubeck_geometric_2015,
  title = {A Geometric Alternative to {{Nesterov}}'s Accelerated Gradient Descent},
  author = {Bubeck, Sébastien and Lee, Yin Tat and Singh, Mohit},
  date = {2015},
  eprint = {1506.08187},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1506.08187},
  urldate = {2018-07-30},
  abstract = {We propose a new method for unconstrained optimization of a smooth and strongly convex function, which attains the optimal rate of convergence of Nesterov's accelerated gradient descent. The new algorithm has a simple geometric interpretation, loosely inspired by the ellipsoid method. We provide some numerical evidence that the new method can be superior to Nesterov's accelerated gradient descent.}
}

@article{buehrer_mathematical_nodate,
  title = {A {{Mathematical Framework}} for {{Superintelligent Machines}}},
  author = {Buehrer, Daniel J.}
}

@online{bulatov_notmnist_2011,
  title = {{{notMNIST}} Dataset},
  author = {Bulatov, Yaroslav},
  date = {2011-09-08},
  url = {http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html},
  organization = {{notMNIST dataset}}
}

@article{buntine_bayesian_1991,
  title = {Bayesian {{Back-Propagation}}},
  author = {Buntine, Wray L.},
  date = {1991},
  journaltitle = {Complex Systems},
  volume = {5},
  eprint = {24439530},
  eprinttype = {pmid},
  pages = {603--643},
  issn = {00308870},
  doi = {10.1016/j.aqpro.2013.07.003},
  abstract = {Connectionist feed-forward networks, t rained with back-prop agat ion, can be used both for nonlinear regression and for (dis-crete one-of-C) classification. T his pap er present s approximate Bayes-ian meth ods to statistical compo nents of back-pro pagat ion: choosing a cost funct ion and penalty term (interpreted as a form of prior prob-ability), pruning insignifican t weights, est imat ing th e uncertainty of weights, predict ing for new pat tern s ("out -of-sample") , est imating t he uncertainty in the choice of t his prediction ("erro r bars"), estima t-ing the generalizat ion erro r, comparing different network st ructures, and handling missing values in t he t raining pattern s. These meth -ods extend some heuristic techniques suggested in the literature, and in most cases require a small additional facto r in comp ut ation during back-propagation, or computation once back-pro pagat ion has finished. 1. I nt r o d u ction Back-propagati on [32] is a p opular sche me for t raining feed-for ward connec -t ionist networks. It can b e applied t o bo t h t he tasks of cla ssification (predic-t ion of discret e variables t aking one of C mutua lly exclusive and exha ust ive va lues) and regr ession (p redi ct ion of rea l va ria bles). Design issu es in t his sche me are prim arily com puta tional-for inst ance, what varia t ions of gradi-ent descen t should b e used-or probabilistic -for inst a nce, what cost func-ti on should b e used a nd how generalizat ion err or ca n b e pred ict ed . Her e we frame t he probabilistic component of back-prop agation in a Bayesian conte xt [6, 3, 7, 27]. In adopt ing a Bayesian justi fica t ion for t he method s present ed , we are not claiming any neurological valid ity for our metho ds. We view},
  isbn = {0030-8870}
}

@inproceedings{burchi_efficient_2021,
  title = {Efficient Conformer: {{Progressive}} Downsampling and Grouped Attention for Automatic Speech Recognition},
  booktitle = {{{IEEE Automatic Speech Recognition}} and {{Understanding Workshop}} ({{ASRU}})},
  author = {Burchi, Maxime and Vielzeuf, Valentin},
  date = {2021-12},
  eprint = {2109.01163},
  eprinttype = {arxiv},
  pages = {8--15},
  publisher = {{IEEE}},
  location = {{Cartagena, Colombia,}},
  doi = {10.1109/ASRU51503.2021.9687874},
  url = {https://doi.org/10.1109/ASRU51503.2021.9687874}
}

@inproceedings{burda_importance_2016,
  title = {Importance {{Weighted Autoencoders}}},
  booktitle = {Proceedings of the 4th {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan R.},
  date = {2016},
  pages = {8},
  location = {{San Juan, Puerto Rico}},
  url = {https://arxiv.org/abs/1509.00519},
  urldate = {2017-10-04},
  abstract = {The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}}
}

@misc{burg_memorization_2021,
  title = {On {{Memorization}} in {{Probabilistic Deep Generative Models}}},
  author = {family=Burg, given=Gerrit J. J., prefix=van den, useprefix=false and Williams, Christopher K. I.},
  date = {2021-06-06},
  eprint = {2106.03216},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2106.03216},
  urldate = {2021-06-15},
  abstract = {Recent advances in deep generative models have led to impressive results in a variety of application domains. Motivated by the possibility that deep learning models might memorize part of the input data, there have been increased efforts to understand how memorization can occur. In this work, we extend a recently proposed measure of memorization for supervised learning (Feldman, 2019) to the unsupervised density estimation problem and simplify the accompanying estimator. Next, we present an exploratory study that demonstrates how memorization can arise in probabilistic deep generative models, such as variational autoencoders. This reveals that the form of memorization to which these models are susceptible differs fundamentally from mode collapse and overfitting. Finally, we discuss several strategies that can be used to limit memorization in practice.},
  langid = {english}
}

@article{burkart_survey_2021,
  title = {A Survey on the Explainability of Supervised Machine Learning},
  author = {Burkart, Nadia and Huber, Marco F},
  date = {2021},
  journaltitle = {Journal of Artificial Intelligence Research},
  volume = {70},
  pages = {245--317}
}

@article{burnsSystematicReviewDischarge2012,
  title = {Systematic Review of Discharge Coding Accuracy},
  author = {Burns, E.M. and Rigby, E. and Mamidanna, R. and Bottle, A. and Aylin, P. and Ziprin, P. and Faiz, O.D.},
  date = {2012-03},
  journaltitle = {Journal of Public Health (Oxford, England)},
  volume = {34},
  number = {1},
  eprint = {21795302},
  eprinttype = {pmid},
  pages = {138--148},
  issn = {1741-3842},
  doi = {10.1093/pubmed/fdr054},
  abstract = {Introduction Routinely collected data sets are increasingly used for research, financial reimbursement and health service planning. High quality data are necessary for reliable analysis. This study aims to assess the published accuracy of routinely collected data sets in Great Britain. Methods Systematic searches of the EMBASE, PUBMED, OVID and Cochrane databases were performed from 1989 to present using defined search terms. Included studies were those that compared routinely collected data sets with case or operative note review and those that compared routinely collected data with clinical registries. Results Thirty-two studies were included. Twenty-five studies compared routinely collected data with case or operation notes. Seven studies compared routinely collected data with clinical registries. The overall median accuracy (routinely collected data sets versus case notes) was 83.2\% (IQR: 67.3–92.1\%). The median diagnostic accuracy was 80.3\% (IQR: 63.3–94.1\%) with a median procedure accuracy of 84.2\% (IQR: 68.7–88.7\%). There was considerable variation in accuracy rates between studies (50.5–97.8\%). Since the 2002 introduction of Payment by Results, accuracy has improved in some respects, for example primary diagnoses accuracy has improved from 73.8\% (IQR: 59.3–92.1\%) to 96.0\% (IQR: 89.3–96.3), P= 0.020. Conclusion Accuracy rates are improving. Current levels of reported accuracy suggest that routinely collected data are sufficiently robust to support their use for research and managerial decision-making.},
  pmcid = {PMC3285117}
}

@article{burton_generalization_1983,
  title = {A Generalization of Isolated Word Recognition Using Vector Quantization},
  author = {Burton, D. K. and Shore, J. E. and Buck, J. T.},
  date = {1983},
  journaltitle = {IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}
}

@article{buse_likelihood_1982,
  title = {The Likelihood Ratio, {{Wald}}, and {{Lagrange}} Multiplier Tests: {{An}} Expository Note},
  author = {Buse, Adolf},
  date = {1982},
  journaltitle = {The American Statistician},
  volume = {36},
  pages = {153--157},
  issue = {3a}
}

@online{butlin_consciousness_2023,
  title = {Consciousness in {{Artificial Intelligence}}: {{Insights}} from the {{Science}} of {{Consciousness}}},
  shorttitle = {Consciousness in {{Artificial Intelligence}}},
  author = {Butlin, Patrick and Long, Robert and Elmoznino, Eric and Bengio, Yoshua and Birch, Jonathan and Constant, Axel and Deane, George and Fleming, Stephen M. and Frith, Chris and Ji, Xu and Kanai, Ryota and Klein, Colin and Lindsay, Grace and Michel, Matthias and Mudrik, Liad and Peters, Megan A. K. and Schwitzgebel, Eric and Simon, Jonathan and VanRullen, Rufin},
  date = {2023-08-22},
  eprint = {2308.08708},
  eprinttype = {arxiv},
  eprintclass = {cs, q-bio},
  url = {http://arxiv.org/abs/2308.08708},
  urldate = {2023-08-24},
  abstract = {Whether current or near-term AI systems could be conscious is a topic of scientific interest and increasing public concern. This report argues for, and exemplifies, a rigorous and empirically grounded approach to AI consciousness: assessing existing AI systems in detail, in light of our best-supported neuroscientific theories of consciousness. We survey several prominent scientific theories of consciousness, including recurrent processing theory, global workspace theory, higher-order theories, predictive processing, and attention schema theory. From these theories we derive "indicator properties" of consciousness, elucidated in computational terms that allow us to assess AI systems for these properties. We use these indicator properties to assess several recent AI systems, and we discuss how future systems might implement them. Our analysis suggests that no current AI systems are conscious, but also suggests that there are no obvious technical barriers to building AI systems which satisfy these indicators.},
  pubstate = {preprint}
}

@article{campbell_deep_2002,
  title = {Deep {{Blue}}},
  author = {Campbell, Murray and Hoane Jr., A. Joseph and Hsu, Feng-hsiung},
  date = {2002},
  journaltitle = {Artificial Intelligence},
  volume = {134},
  number = {1-2},
  pages = {57--83},
  issn = {00043702},
  doi = {10.1016/S0004-3702(01)00129-1},
  url = {http://www.sciencedirect.com/science/article/pii/S0004370201001291},
  urldate = {2017-10-23},
  abstract = {Deep Blue is the chess machine that defeated then-reigning World Chess Champion Garry Kasparov in a six-game match in 1997. There were a number of factors that contributed to this success, including: •a single-chip chess search engine,•a massively parallel system with multiple levels of parallelism,•a strong emphasis on search extensions,•a complex evaluation function, and•effective use of a Grandmaster game database. This paper describes the Deep Blue system, and gives some of the rationale that went into the design decisions behind Deep Blue.},
  isbn = {0004-3702}
}

@misc{candes_robust_2009,
  title = {Robust {{Principal Component Analysis}}?},
  author = {Candes, Emmanuel J. and Li, Xiaodong and Ma, Yi and Wright, John},
  date = {2009-12-18},
  eprint = {0912.3599},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/0912.3599},
  urldate = {2020-06-19},
  abstract = {This paper is about a curious phenomenon. Suppose we have a data matrix, which is the superposition of a low-rank component and a sparse component. Can we recover each component individually? We prove that under some suitable assumptions, it is possible to recover both the low-rank and the sparse components exactly by solving a very convenient convex program called Principal Component Pursuit; among all feasible decompositions, simply minimize a weighted combination of the nuclear norm and of the 1 norm. This suggests the possibility of a principled approach to robust principal component analysis since our methodology and results assert that one can recover the principal components of a data matrix even though a positive fraction of its entries are arbitrarily corrupted. This extends to the situation where a fraction of the entries are missing as well. We discuss an algorithm for solving this optimization problem, and present applications in the area of video surveillance, where our methodology allows for the detection of objects in a cluttered background, and in the area of face recognition, where it offers a principled way of removing shadows and specularities in images of faces.},
  langid = {english}
}

@inproceedings{cao_improving_2021,
  title = {Improving Streaming {{Transformer}} Based {{ASR}} under a Framework of Self-Supervised Learning},
  booktitle = {{{IEEE Annual Conference}} of the {{International Speech Communication Association}} ({{Interspeech}})},
  author = {Cao, Songjun and Kang, Yueteng and Fu, Yanzhe and Xu, Xiaoshuo and Sun, Sining and Zhang, Yike and Ma, Long},
  date = {2021}
}

@inproceedings{caoHyperCoreHyperbolicCograph2020,
  title = {{{HyperCore}}: {{Hyperbolic}} and {{Co-graph Representation}} for {{Automatic ICD Coding}}},
  shorttitle = {{{HyperCore}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Cao, Pengfei and Chen, Yubo and Liu, Kang and Zhao, Jun and Liu, Shengping and Chong, Weifeng},
  date = {2020-07},
  pages = {3105--3114},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.acl-main.282},
  abstract = {The International Classification of Diseases (ICD) provides a standardized way for classifying diseases, which endows each disease with a unique code. ICD coding aims to assign proper ICD codes to a medical record. Since manual coding is very laborious and prone to errors, many methods have been proposed for the automatic ICD coding task. However, most of existing methods independently predict each code, ignoring two important characteristics: Code Hierarchy and Code Co-occurrence. In this paper, we propose a Hyperbolic and Co-graph Representation method (HyperCore) to address the above problem. Specifically, we propose a hyperbolic representation method to leverage the code hierarchy. Moreover, we propose a graph convolutional network to utilize the code co-occurrence. Experimental results on two widely used datasets demonstrate that our proposed model outperforms previous state-of-the-art methods.}
}

@article{cardoso_weightless_2017,
  title = {Weightless Neural Networks for Open Set Recognition},
  author = {Cardoso, Douglas O and Gama, João and França, Felipe MG},
  date = {2017},
  journaltitle = {Machine Learning},
  volume = {106},
  number = {9-10},
  pages = {1547--1567},
  publisher = {{Springer}}
}

@inproceedings{carion_endtoend_2020,
  title = {End-to-End Object Detection with Transformers},
  booktitle = {Proceedings of the 16th {{European Conference}} on {{Computer Vision}} ({{ECCV}})},
  author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  date = {2020},
  series = {Lecture Notes in Computer Science},
  volume = {12346},
  pages = {213--229},
  publisher = {{Springer}},
  location = {{Glassgow, UK}},
  doi = {10.1007/978-3-030-58452-8\_13},
  url = {https://doi.org/10.1007/978-3-030-58452-8_13},
  eventtitle = {European {{Conference}} on {{Computer Vision}} ({{ECCV}})}
}

@inproceedings{carlin_rapid_2011,
  title = {Rapid Evaluation of Speech Representations for Spoken Term Discovery},
  booktitle = {Annual {{Conference}} of the {{International Speech Communication Association}}},
  author = {Carlin, Michael A and Thomas, Samuel and Jansen, Aren and Hermansky, Hynek},
  date = {2011}
}

@online{caron_deep_2018,
  title = {Deep Clustering for Unsupervised Learning of Visual Features},
  author = {Caron, Mathilde and Bojanowski, Piotr and Joulin, Armand and Douze, Matthijs},
  date = {2018},
  eprint = {1807.05520},
  eprinttype = {arxiv},
  pubstate = {preprint}
}

@inproceedings{caron_emerging_2021,
  title = {Emerging Properties in Self-Supervised Vision {{Transformers}}},
  booktitle = {{{IEEE}} International Conference on Computer Vision ({{ICCV}})},
  author = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and Jégou, Hervé and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  date = {2021}
}

@inproceedings{caron_unsupervised_2020,
  title = {Unsupervised Learning of Visual Features by Contrasting Cluster Assignments},
  booktitle = {Conference on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal, Priya and Bojanowski, Piotr and Joulin, Armand},
  date = {2020}
}

@inproceedings{carpentier_adaptive_2012,
  title = {Adaptive {{Stratified Sampling}} for {{Monte-Carlo}} Integration of {{Differentiable}} Functions.},
  booktitle = {Proceedings of the {{Conference}} on {{Neural Information Processing Systems}} ({{NIPS}})},
  author = {Carpentier, Alexandra and Munos, Rémi},
  date = {2012},
  eprint = {1210.5345v1},
  eprinttype = {arxiv},
  pages = {1--23},
  issn = {10495258},
  abstract = {We consider the problem of adaptive stratified sampling for Monte Carlo integra-tion of a differentiable function given a finite number of evaluations to the func-tion. We construct a sampling scheme that samples more often in regions where the function oscillates more, while allocating the samples such that they are well spread on the domain (this notion shares similitude with low discrepancy). We prove that the estimate returned by the algorithm is almost similarly accurate as the estimate that an optimal oracle strategy (that would know the variations of the function everywhere) would return, and provide a finite-sample analysis.},
  isbn = {978-1-62748-003-1}
}

@article{carr_self-supervised_2021,
  title = {Self-{{Supervised Learning}} of {{Audio Representations}} from {{Permutations}} with {{Differentiable Ranking}}},
  author = {Carr, Andrew N. and Berthet, Quentin and Blondel, Mathieu and Teboul, Olivier and Zeghidour, Neil},
  date = {2021},
  journaltitle = {IEEE Signal Processing Letters},
  shortjournal = {IEEE Signal Process. Lett.},
  volume = {28},
  eprint = {2103.09879},
  eprinttype = {arxiv},
  pages = {708--712},
  issn = {1070-9908, 1558-2361},
  doi = {10.1109/LSP.2021.3067635},
  url = {http://arxiv.org/abs/2103.09879},
  urldate = {2021-10-22},
  abstract = {Self-supervised pre-training using so-called "pretext" tasks has recently shown impressive performance across a wide range of modalities. In this work, we advance self-supervised learning from permutations, by pre-training a model to reorder shuffled parts of the spectrogram of an audio signal, to improve downstream classification performance. We make two main contributions. First, we overcome the main challenges of integrating permutation inversions into an end-to-end training scheme, using recent advances in differentiable ranking. This was heretofore sidestepped by casting the reordering task as classification, fundamentally reducing the space of permutations that can be exploited. Our experiments validate that learning from all possible permutations improves the quality of the pre-trained representations over using a limited, fixed set. Second, we show that inverting permutations is a meaningful pretext task for learning audio representations in an unsupervised fashion. In particular, we improve instrument classification and pitch estimation of musical notes by reordering spectrogram patches in the time-frequency space.}
}

@article{caruana_multitask_1997,
  title = {Multitask {{Learning}}},
  author = {Caruana, Rich},
  date = {1997-07},
  journaltitle = {Journal of Machine Learning},
  volume = {28},
  number = {1},
  pages = {41--75},
  doi = {10.1023/A:1007379606734},
  abstract = {Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers task relatedness without the need of supervisory signals, and presents new results for MTL with k-nearest neighbor and kernel regression. In this paper we demonstrate multitask learning in three domains. We explain how multitask learning works, and show that there are many opportunities for multitask learning in real domains. We present an algorithm and results for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Because multitask learning works, can be applied to many different kinds of domains, and can be used with different learning algorithms, we conjecture there will be many opportunities for its use on real-world problems.}
}

@misc{cemgil_autoencoding_2020,
  title = {Autoencoding {{Variational Autoencoder}}},
  author = {Cemgil, A. Taylan and Ghaisas, Sumedh and Dvijotham, Krishnamurthy and Gowal, Sven and Kohli, Pushmeet},
  date = {2020-12-07},
  eprint = {2012.03715},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2012.03715},
  urldate = {2020-12-09},
  abstract = {Does a Variational AutoEncoder (VAE) consistently encode typical samples generated from its decoder? This paper shows that the perhaps surprising answer to this question is `No'; a (nominally trained) VAE does not necessarily amortize inference for typical samples that it is capable of generating. We study the implications of this behaviour on the learned representations and also the consequences of fixing it by introducing a notion of self consistency. Our approach hinges on an alternative construction of the variational approximation distribution to the true posterior of an extended VAE model with a Markov chain alternating between the encoder and the decoder. The method can be used to train a VAE model from scratch or given an already trained VAE, it can be run as a post processing step in an entirely self supervised way without access to the original training data. Our experimental analysis reveals that encoders trained with our self-consistency approach lead to representations that are robust (insensitive) to perturbations in the input introduced by adversarial attacks. We provide experimental results on the ColorMnist and CelebA benchmark datasets that quantify the properties of the learned representations and compare the approach with a baseline that is specifically trained for the desired property.},
  langid = {english}
}

@article{centersformedicaremedicaidservicesus_icd10cm_2023,
  title = {{{ICD-10-CM}} Official Guidelines for Coding and Reporting {{FY}} 2023  -- {{UPDATED April}} 1, 2023 ({{October}} 1, 2022 - {{September}} 30, 2023)},
  editor = {{Centers for Medicare \& Medicaid Services (U.S.)} and {National Center for Health Statistics (U.S.)}},
  date = {2023-04-01},
  location = {{Hyattsville, MD}},
  url = {https://stacks.cdc.gov/view/cdc/126426},
  abstract = {ICD-10-CM-Guidelines-April\%201,\%20FY2023.pdf},
  langid = {english},
  keywords = {Disease/classification,International Classification of Diseases,Terminology as Topic,{Vocabulary, Controlled}}
}

@book{chacon_pro_nodate,
  title = {Pro {{Git}}},
  author = {Chacon, Scott and Straub, Ben},
  edition = {2}
}

@article{chadebec_data_nodate,
  title = {Data {{Augmentation}} in {{High Dimensional Low Sample Size Setting Using}} a {{Geometry-Based Variational Autoencoder}}},
  author = {Chadebec, Clément and Thibeau-Sutre, Elina and Burgos, Ninon and Allassonnière, Stéphanie},
  pages = {26},
  abstract = {In this paper, we propose a new method to perform data augmentation in a reliable way in the High Dimensional Low Sample Size (HDLSS) setting using a geometry-based variational autoencoder. Our approach combines a proper latent space modeling of the VAE seen as a Riemannian manifold with a new generation scheme which produces more meaningful samples especially in the context of small data sets. The proposed method is tested through a wide experimental study where its robustness to data sets, classifiers and training samples size is stressed. It is also validated on a medical imaging classification task on the challenging ADNI database where a small number of 3D brain MRIs are considered and augmented using the proposed VAE framework. In each case, the proposed method allows for a significant and reliable gain in the classification metrics. For instance, balanced accuracy jumps from 66.3\% to 74.3\% for a state-of-the-art CNN classifier trained with 50 MRIs of cognitively normal (CN) and 50 Alzheimer disease (AD) patients and from 77.7\% to 86.3\% when trained with 243 CN and 210 AD while improving greatly sensitivity and specificity metrics.},
  langid = {english}
}

@inproceedings{chan_contentcontext_2022,
  title = {Content-Context Factorized Representations for Automated Speech Recognition},
  booktitle = {{{IEEE Annual Conference}} of the {{International Speech Communication Association}} ({{Interspeech}})},
  author = {Chan, David M. and Ghosh, Shalini},
  date = {2022}
}

@inproceedings{chan_listen_2016,
  title = {Listen, Attend and Spell: A Neural Network for Large Vocabulary Conversational Speech Recognition},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Chan, William and Jaitly, Navdeep and Le, Quoc V. and Vinyals, Oriol},
  date = {2016},
  eprint = {1508.01211},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1508.01211},
  urldate = {2018-10-31},
  abstract = {We present Listen, Attend and Spell (LAS), a neural network that learns to transcribe speech utterances to characters. Unlike traditional DNN-HMM models, this model learns all the components of a speech recognizer jointly. Our system has two components: a listener and a speller. The listener is a pyramidal recurrent network encoder that accepts filter bank spectra as inputs. The speller is an attention-based recurrent network decoder that emits characters as outputs. The network produces character sequences without making any independence assumptions between the characters. This is the key improvement of LAS over previous end-to-end CTC models. On a subset of the Google voice search task, LAS achieves a word error rate (WER) of 14.1\% without a dictionary or a language model, and 10.3\% with language model rescoring over the top 32 beams. By comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0\%.},
  keywords = {★}
}

@inproceedings{chan_multimodal_2022,
  title = {Multi-Modal Pre-Training for Automated Speech Recognition},
  booktitle = {{{IEEE}} International Conference on Acoustics, Speech and Signal Processing ({{ICASSP}})},
  author = {Chan, David M. and Ghosh, Shalini and Chakrabarty, Debmalya and Hoffmeister, Björn},
  date = {2022}
}

@article{chan_updating_1979,
  title = {Updating Formulae and a Pairwise Algorithm for Computing Sample Variances},
  author = {Chan, T. F. and Golub, G. H. and LeVeque, R. J.},
  date = {1979},
  journaltitle = {Annals of Physics},
  volume = {54},
  pages = {258},
  doi = {10.1007/978-3-642-51461-6_3},
  url = {http://scholar.google.com/scholar?hl=en&btnG=Search&q=intitle:Updating+Formulae+and+pairwise+algorithm+for+computing+sample+variances#0},
  urldate = {2018-04-04},
  abstract = {A general formula is presented for computing the sample v;iiiancc for a sample of size m+ n given the means and variances for two subsnn+lcs of sizes m and n. This formula is used in the construction of a pa.irwisc nl\textasciitilde :orithm for computing the variance. Other applications are discussed as v\textasciitilde ll, including the use of updating formulae in a parallel computin g cnviornmcn t. Wc present numerical results and rounding error analyses for several numerical schcr\textasciitilde\textasciitilde s.},
  isbn = {3705100025}
}

@misc{chang_batchnormalized_2015,
  title = {Batch-Normalized {{Maxout Network}} in {{Network}}},
  author = {Chang, Jia-Ren and Chen, Yong-Sheng},
  date = {2015},
  eprint = {1511.02583},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1511.02583},
  urldate = {2018-06-02},
  abstract = {This paper reports a novel deep architecture referred to as Maxout network In Network (MIN), which can enhance model discriminability and facilitate the process of information abstraction within the receptive field. The proposed network adopts the framework of the recently developed Network In Network structure, which slides a universal approximator, multilayer perceptron (MLP) with rectifier units, to exact features. Instead of MLP, we employ maxout MLP to learn a variety of piecewise linear activation functions and to mediate the problem of vanishing gradients that can occur when using rectifier units. Moreover, batch normalization is applied to reduce the saturation of maxout units by pre-conditioning the model and dropout is applied to prevent overfitting. Finally, average pooling is used in all pooling layers to regularize maxout MLP in order to facilitate information abstraction in every receptive field while tolerating the change of object position. Because average pooling preserves all features in the local patch, the proposed MIN model can enforce the suppression of irrelevant information during training. Our experiments demonstrated the state-of-the-art classification performance when the MIN model was applied to MNIST, CIFAR-10, and CIFAR-100 datasets and comparable performance for SVHN dataset.}
}

@unpublished{chang_distilhubert_2021,
  title = {{{DistilHuBERT}}: {{Speech Representation Learning}} by {{Layer-wise Distillation}} of {{Hidden-unit BERT}}},
  shorttitle = {{{DistilHuBERT}}},
  author = {Chang, Heng-Jui and Yang, Shu-wen and Lee, Hung-yi},
  date = {2021-10-06},
  eprint = {2110.01900},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2110.01900},
  urldate = {2021-10-18},
  abstract = {Self-supervised speech representation learning methods like wav2vec 2.0 and Hidden-unit BERT (HuBERT) leverage unlabeled speech data for pre-training and offer good representations for numerous speech processing tasks. Despite the success of these methods, they require large memory and high pre-training costs, making them inaccessible for researchers in academia and small companies. Therefore, this paper introduces DistilHuBERT, a novel multi-task learning framework to distill hidden representations from a HuBERT model directly. This method reduces HuBERT's size by 75\% and 73\% faster while retaining most performance in ten different tasks. Moreover, DistilHuBERT required little training time and data, opening the possibilities of pre-training personal and on-device SSL models for speech.}
}

@unpublished{chang_exploration_2021,
  title = {An {{Exploration}} of {{Self-Supervised Pretrained Representations}} for {{End-to-End Speech Recognition}}},
  author = {Chang, Xuankai and Maekaku, Takashi and Guo, Pengcheng and Shi, Jing and Lu, Yen-Ju and Subramanian, Aswin Shanmugam and Wang, Tianzi and Yang, Shu-wen and Tsao, Yu and Lee, Hung-yi and Watanabe, Shinji},
  date = {2021-10-09},
  eprint = {2110.04590},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2110.04590},
  urldate = {2021-10-18},
  abstract = {Self-supervised pretraining on speech data has achieved a lot of progress. High-fidelity representation of the speech signal is learned from a lot of untranscribed data and shows promising performance. Recently, there are several works focusing on evaluating the quality of self-supervised pretrained representations on various tasks without domain restriction, e.g. SUPERB. However, such evaluations do not provide a comprehensive comparison among many ASR benchmark corpora. In this paper, we focus on the general applications of pretrained speech representations, on advanced end-to-end automatic speech recognition (E2E-ASR) models. We select several pretrained speech representations and present the experimental results on various open-source and publicly available corpora for E2E-ASR. Without any modification of the back-end model architectures or training strategy, some of the experiments with pretrained representations, e.g., WSJ, WSJ0-2mix with HuBERT, reach or outperform current state-of-the-art (SOTA) recognition performance. Moreover, we further explore more scenarios for whether the pretraining representations are effective, such as the cross-language or overlapped speech. The scripts, configuratons and the trained models have been released in ESPnet to let the community reproduce our experiments and improve them.}
}

@article{chang_figure_1993,
  title = {Figure of Merit Training for Detection and Spotting},
  author = {Chang, Eric and Lippmann, Richard P},
  date = {1993},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {6}
}

@misc{chang_neural_2018,
  title = {Neural {{Network Quine}}},
  author = {Chang, Oscar and Lipson, Hod},
  date = {2018},
  eprint = {1803.05859},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1803.05859},
  urldate = {2018-04-01},
  abstract = {Self-replication is a key aspect of biological life that has been largely overlooked in Artificial Intelligence systems. Here we describe how to build and train self-replicating neural networks. The network replicates itself by learning to output its own weights. The network is designed using a loss function that can be optimized with either gradient-based or non-gradient-based methods. We also describe a method we call regeneration to train the network without explicit optimization, by injecting the network with predictions of its own parameters. The best solution for a self-replicating network was found by alternating between regeneration and optimization steps. Finally, we describe a design for a self-replicating neural network that can solve an auxiliary task such as MNIST image classification. We observe that there is a trade-off between the network's ability to classify images and its ability to replicate, but training is biased towards increasing its specialization at image classification at the expense of replication. This is analogous to the trade-off between reproduction and other tasks observed in nature. We suggest that a self-replication mechanism for artificial intelligence is useful because it introduces the possibility of continual improvement through natural selection.}
}

@inproceedings{chang_speechprompt_2022,
  title = {{{SpeechPrompt}}: {{An}} Exploration of Prompt Tuning on Generative Spoken Language Model for Speech Processing Tasks},
  booktitle = {{{IEEE Annual Conference}} of the {{International Speech Communication Association}} ({{Interspeech}})},
  author = {Chang, Kai-Wei and Tseng, Wei-Cheng and Li, Shang-Wen and Lee, Hung-yi},
  date = {2022}
}

@misc{chatfield_return_2014,
  title = {Return of the Devil in the Details: {{Delving}} Deep into Convolutional Nets},
  author = {Chatfield, Ken and Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  date = {2014},
  eprint = {1405.3531},
  eprinttype = {arxiv},
  url = {https://arxiv.org/abs/1405.3531}
}

@misc{chaudhari_stochastic_2018,
  title = {Stochastic Gradient Descent Performs Variational Inference, Converges to Limit Cycles for Deep Networks},
  author = {Chaudhari, Pratik and Soatto, Stefano},
  date = {2018-01-16},
  eprint = {1710.11029},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1710.11029},
  urldate = {2020-09-20},
  abstract = {Stochastic gradient descent (SGD) is widely believed to perform implicit regularization when used to train deep neural networks, but the precise manner in which this occurs has thus far been elusive. We prove that SGD minimizes an average potential over the posterior distribution of weights along with an entropic regularization term. This potential is however not the original loss function in general. So SGD does perform variational inference, but for a different loss than the one used to compute the gradients. Even more surprisingly, SGD does not even converge in the classical sense: we show that the most likely trajectories of SGD for deep networks do not behave like Brownian motion around critical points. Instead, they resemble closed loops with deterministic components. We prove that such “out-of-equilibrium” behavior is a consequence of highly non-isotropic gradient noise in SGD; the covariance matrix of mini-batch gradients for deep networks has a rank as small as 1\% of its dimension. We provide extensive empirical validation of these claims, proven in the appendix.},
  langid = {english}
}

@inproceedings{chellapilla_high_2006,
  title = {High {{Performance Convolutional Neural Networks}} for {{Document Processing}}},
  booktitle = {Proceedings of the 10th {{International Workshop}} on {{Frontiers}} in {{Handwriting Recognition}} ({{IWFHR}})},
  author = {Chellapilla, Kumar and Puri, Sidd and Simard, Patrice},
  date = {2006},
  location = {{La Baule, France}},
  abstract = {Convolutional neural networks (CNNs) are well known for producing state-of-the-art recognizers for document processing [1]. However, they can be difficult to implement and are usually slower than traditional multi-layer perceptrons (MLPs). We present three novel approaches to speeding up CNNs: a) unrolling convolution, b) using BLAS (basic linear algebra subroutines), and c) using GPUs (graphic processing units). Unrolled convolution converts the processing in each convolutional layer (both forward-propagation and back-propagation) into a matrix-matrix product. The matrix-matrix product representation of CNNs makes their implementation as easy as MLPs. BLAS is used to efficiently compute matrix products on the CPU. We also present a pixel shader based GPU implementation of CNNs. Results on character recognition problems indicate that unrolled convolution with BLAS produces a dramatic 2.4X−3.0X speedup. The GPU implementation is even faster and produces a 3.1X−4.1X speedup.},
  langid = {english}
}

@online{chen_cfvit_2022,
  title = {{{CF-ViT}}: {{A General Coarse-to-Fine Method}} for {{Vision Transformer}}},
  shorttitle = {{{CF-ViT}}},
  author = {Chen, Mengzhao and Lin, Mingbao and Li, Ke and Shen, Yunhang and Wu, Yongjian and Chao, Fei and Ji, Rongrong},
  date = {2022-07-19},
  eprint = {2203.03821},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.03821},
  urldate = {2022-10-08},
  abstract = {Vision Transformers (ViT) have made many breakthroughs in computer vision tasks. However, considerable redundancy arises in the spatial dimension of an input image, leading to massive computational costs. Therefore, We propose a coarse-to-fine vision transformer (CF-ViT) to relieve computational burden while retaining performance in this paper. Our proposed CF-ViT is motivated by two important observations in modern ViT models: (1) The coarse-grained patch splitting can locate informative regions of an input image. (2) Most images can be well recognized by a ViT model in a small-length token sequence. Therefore, our CF-ViT implements network inference in a two-stage manner. At coarse inference stage, an input image is split into a small-length patch sequence for a computationally economical classification. If not well recognized, the informative patches are identified and further re-split in a fine-grained granularity. Extensive experiments demonstrate the efficacy of our CF-ViT. For example, without any compromise on performance, CF-ViT reduces 53\% FLOPs of LV-ViT, and also achieves 2.01x throughput.},
  pubstate = {preprint}
}

@unpublished{chen_coarsetofine_2022,
  title = {Coarse-to-{{Fine Vision Transformer}}},
  author = {Chen, Mengzhao and Lin, Mingbao and Li, Ke and Shen, Yunhang and Wu, Yongjian and Chao, Fei and Ji, Rongrong},
  date = {2022-03-07},
  eprint = {2203.03821},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2203.03821},
  urldate = {2022-04-19},
  abstract = {Vision Transformers (ViT) have made many breakthroughs in computer vision tasks. However, considerable redundancy arises in the spatial dimension of an input image, leading to massive computational costs. Therefore, We propose a coarse-to-fine vision transformer (CF-ViT) to relieve computational burden while retaining performance in this paper. Our proposed CF-ViT is motivated by two important observations in modern ViT models: (1) The coarse-grained patch splitting can locate informative regions of an input image. (2) Most images can be well recognized by a ViT model in a small-length token sequence. Therefore, our CF-ViT implements network inference in a two-stage manner. At coarse inference stage, an input image is split into a small-length patch sequence for a computationally economical classification. If not well recognized, the informative patches are identified and further re-split in a fine-grained granularity. Extensive experiments demonstrate the efficacy of our CF-ViT. For example, without any compromise on performance, CF-ViT reduces 53\% FLOPs of LV-ViT, and also achieves 2.01x throughput.},
  keywords = {Read}
}

@inproceedings{chen_completely_2019,
  title = {Completely Unsupervised Speech Recognition by {{A}} Generative Adversarial Network Harmonized with Iteratively Refined Hidden {{Markov}} Models},
  booktitle = {Proceedings of the 20th {{Annual Conference}} of the {{International Speech Communication Association}} ({{Interspeech}})},
  author = {Chen, Kuan-Yu and Tsai, Che-Ping and Liu, Da-Rong and Lee, Hung-Yi and Lee, Lin-shan},
  date = {2019},
  pages = {1856--1860},
  publisher = {{ISCA}}
}

@inproceedings{chen_crossvit_2021a,
  title = {{{CrossViT}}: {{Cross-attention}} Multi-Scale Vision Transformer for Image Classification},
  booktitle = {2021 {{IEEE}}/{{CVF}} International Conference on Computer Vision, {{ICCV}} 2021, Montreal, {{QC}}, Canada, October 10-17, 2021},
  author = {Chen, Chun-Fu (Richard) and Fan, Quanfu and Panda, Rameswar},
  date = {2021},
  pages = {347--356},
  publisher = {{IEEE}},
  doi = {10.1109/ICCV48922.2021.00041},
  url = {https://doi.org/10.1109/ICCV48922.2021.00041},
  abstract = {The recently developed vision transformer (ViT) has achieved promising results on image classification compared to convolutional neural networks. Inspired by this, in this paper, we study how to learn multi-scale feature representations in transformer models for image classification. To this end, we propose a dual-branch transformer to combine image patches (i.e., tokens in a transformer) of different sizes to produce stronger image features. Our approach processes small-patch and large-patch tokens with two separate branches of different computational complexity and these tokens are then fused purely by attention multiple times to complement each other. Furthermore, to reduce computation, we develop a simple yet effective token fusion module based on cross attention, which uses a single token for each branch as a query to exchange information with other branches. Our proposed cross-attention only requires linear time for both computational and memory complexity instead of quadratic time otherwise. Extensive experiments demonstrate that our approach performs better than or on par with several concurrent works on vision transformer, in addition to efficient CNN models. For example, on the ImageNet1K dataset, with some architectural changes, our approach outperforms the recent DeiT by a large margin of 2\textbackslash\% with a small to moderate increase in FLOPs and model parameters. Our source codes and models are available at \textbackslash url\{https://github.com/IBM/CrossViT\}.}
}

@article{chen_ethical_2021,
  title = {Ethical Machine Learning in Healthcare},
  author = {Chen, Irene Y and Pierson, Emma and Rose, Sherri and Joshi, Shalmali and Ferryman, Kadija and Ghassemi, Marzyeh},
  date = {2021},
  journaltitle = {Annual review of biomedical data science},
  volume = {4},
  pages = {123--144},
  publisher = {{Annual Reviews}}
}

@inproceedings{chen_gigaspeech_2021,
  title = {{{GigaSpeech}}: {{An}} Evolving, Multi-Domain {{ASR}} Corpus with 10,000 Hours of Transcribed Audio},
  booktitle = {Annual {{Conference}} of the {{International Speech Communication Association}}},
  author = {Chen, Guoguo and Chai, Shuzhou and Wang, Guan-Bo and Du, Jiayu and Zhang, Wei-Qiang and Weng, Chao and Su, Dan and Povey, Daniel and Trmal, Jan and Zhang, Junbo and Jin, Mingjie and Khudanpur, Sanjeev and Watanabe, Shinji and Zhao, Shuaijiang and Zou, Wei and Li, Xiangang and Yao, Xuchen and Wang, Yongqing and You, Zhao and Yan, Zhiyong},
  date = {2021}
}

@online{chen_improved_2020,
  title = {Improved Baselines with Momentum Contrastive Learning},
  author = {Chen, Xinlei and Fan, Haoqi and Girshick, Ross and He, Kaiming},
  date = {2020},
  eprint = {2003.04297},
  eprinttype = {arxiv},
  pubstate = {preprint}
}

@misc{chen_infogan_2016,
  title = {{{InfoGAN}}: {{Interpretable Representation Learning}} by {{Information Maximizing Generative Adversarial Nets}}},
  author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
  date = {2016},
  eprint = {23459267},
  eprinttype = {pmid},
  url = {http://arxiv.org/abs/1606.03657},
  urldate = {2018-04-19},
  abstract = {This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods.},
  isbn = {978-3-319-16816-6}
}

@inproceedings{chen_isolating_2018,
  title = {Isolating {{Sources}} of {{Disentanglement}} in {{VAEs}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31: {{Annual Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Chen, Tian Qi and Li, Xuechen and Grosse, Roger B. and Duvenaud, David},
  date = {2018},
  pages = {2615--2625},
  location = {{Montreal, Quebec, Canada}},
  url = {https://proceedings.neurips.cc/paper/2018/hash/1ee3dfcd8a0645a25a35977997223d22-Abstract.html}
}

@inproceedings{chen_outlier_2017,
  title = {Outlier Detection with Autoencoder Ensembles},
  booktitle = {Proceedings of the {{SIAM International Conference}} on {{Data Mining}} ({{SDM}})},
  author = {Chen, Jinghui and Sathe, Saket and Aggarwal, Charu and Turaga, Deepak},
  date = {2017},
  pages = {90--98},
  publisher = {{SIAM}}
}

@inproceedings{chen_querybyexample_2015,
  title = {Query-by-Example Keyword Spotting Using Long Short-Term Memory Networks},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Chen, Guoguo and Parada, Carolina and Sainath, Tara N},
  date = {2015}
}

@online{chen_regionvit_2022,
  title = {{{RegionViT}}: {{Regional-to-Local Attention}} for {{Vision Transformers}}},
  shorttitle = {{{RegionViT}}},
  author = {Chen, Chun-Fu and Panda, Rameswar and Fan, Quanfu},
  date = {2022-03-30},
  number = {2106.02689},
  eprint = {2106.02689},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2106.02689},
  urldate = {2022-05-27},
  abstract = {Vision transformer (ViT) has recently shown its strong capability in achieving comparable results to convolutional neural networks (CNNs) on image classification. However, vanilla ViT simply inherits the same architecture from the natural language processing directly, which is often not optimized for vision applications. Motivated by this, in this paper, we propose a new architecture that adopts the pyramid structure and employ a novel regional-to-local attention rather than global self-attention in vision transformers. More specifically, our model first generates regional tokens and local tokens from an image with different patch sizes, where each regional token is associated with a set of local tokens based on the spatial location. The regional-to-local attention includes two steps: first, the regional self-attention extract global information among all regional tokens and then the local self-attention exchanges the information among one regional token and the associated local tokens via self-attention. Therefore, even though local self-attention confines the scope in a local region but it can still receive global information. Extensive experiments on four vision tasks, including image classification, object and keypoint detection, semantics segmentation and action recognition, show that our approach outperforms or is on par with state-of-the-art ViT variants including many concurrent works. Our source codes and models are available at https://github.com/ibm/regionvit.},
  pubstate = {preprint}
}

@article{chen_rise_2018,
  title = {The Rise of Deep Learning in Drug Discovery},
  author = {Chen, Hongming and Engkvist, Ola and Wang, Yinhai and Olivecrona, Marcus and Blaschke, Thomas},
  date = {2018},
  journaltitle = {Drug discovery today},
  volume = {23},
  number = {6},
  pages = {1241--1250},
  publisher = {{Elsevier}}
}

@misc{chen_semantic_2014,
  title = {Semantic {{Image Segmentation}} with {{Deep Convolutional Nets}} and {{Fully Connected CRFs}}},
  author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
  date = {2014},
  eprint = {28463186},
  eprinttype = {pmid},
  url = {http://arxiv.org/abs/1412.7062},
  abstract = {Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called "semantic image segmentation"). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our "DeepLab" system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6\% IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the 'hole' algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU.},
  isbn = {9783901608353}
}

@inproceedings{chen_simple_2020,
  title = {A Simple Framework for Contrastive Learning of Visual Representations},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  date = {2020}
}

@unpublished{chen_speech_2021,
  title = {Speech {{Representation Learning Through Self-supervised Pretraining And Multi-task Finetuning}}},
  author = {Chen, Yi-Chen and Yang, Shu-wen and Lee, Cheng-Kuang and See, Simon and Lee, Hung-yi},
  date = {2021-10-18},
  eprint = {2110.09930},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2110.09930},
  urldate = {2021-10-28},
  abstract = {Speech representation learning plays a vital role in speech processing. Among them, self-supervised learning (SSL) has become an important research direction. It has been shown that an SSL pretraining model can achieve excellent performance in various downstream tasks of speech processing. On the other hand, supervised multi-task learning (MTL) is another representation learning paradigm, which has been proven effective in computer vision (CV) and natural language processing (NLP). However, there is no systematic research on the general representation learning model trained by supervised MTL in speech processing. In this paper, we show that MTL finetuning can further improve SSL pretraining. We analyze the generalizability of supervised MTL finetuning to examine if the speech representation learned by MTL finetuning can generalize to unseen new tasks.}
}

@misc{chen_survey_2017,
  title = {A {{Survey}} on {{Dialogue Systems}}: {{Recent Advances}} and {{New Frontiers}}},
  shorttitle = {A {{Survey}} on {{Dialogue Systems}}},
  author = {Chen, Hongshen and Liu, Xiaorui and Yin, Dawei and Tang, Jiliang},
  date = {2017-11-06},
  eprint = {1711.01731},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1711.01731},
  urldate = {2019-06-11},
  abstract = {Dialogue systems have attracted more and more attention. Recent advances on dialogue systems are overwhelmingly contributed by deep learning techniques, which have been employed to enhance a wide range of big data applications such as computer vision, natural language processing, and recommender systems. For dialogue systems, deep learning can leverage a massive amount of data to learn meaningful feature representations and response generation strategies, while requiring a minimum amount of hand-crafting. In this article, we give an overview to these recent advances on dialogue systems from various perspectives and discuss some possible research directions. In particular, we generally divide existing dialogue systems into task-oriented and nontask-oriented models, then detail how deep learning techniques help them with representative algorithms and finally discuss some appealing research directions that can bring the dialogue system research into a new frontier.},
  langid = {english}
}

@online{chen_unispeechsat_2021,
  title = {{{UniSpeech-SAT}}: {{Universal}} Speech Representation Learning with Speaker Aware Pre-Training},
  author = {Chen, Sanyuan and Wu, Yu and Wang, Chengyi and Chen, Zhengyang and Chen, Zhuo and Liu, Shujie and Wu, Jian and Qian, Yao and Wei, Furu and Li, Jinyu and Yu, Xiangzhan},
  date = {2021},
  eprint = {2110.05752},
  eprinttype = {arxiv},
  pubstate = {preprint}
}

@misc{chen_variational_2017,
  title = {Variational {{Lossy Autoencoder}}},
  author = {Chen, Xi and Kingma, Diederik P. and Salimans, Tim and Duan, Yan and Dhariwal, Prafulla and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
  date = {2017-03-04},
  eprint = {1611.02731},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1611.02731},
  urldate = {2020-09-15},
  abstract = {Representation learning seeks to expose certain aspects of observed data in a learned representation that’s amenable to downstream tasks like classification. For instance, a good representation for 2D images might be one that describes only global structure and discards information about detailed texture. In this paper, we present a simple but principled method to learn such global representations by combining Variational Autoencoder (VAE) with neural autoregressive models such as RNN, MADE and PixelRNN/CNN. Our proposed VAE model allows us to have control over what the global latent code can learn and by designing the architecture accordingly, we can force the global latent code to discard irrelevant information such as texture in 2D images, and hence the VAE only “autoencodes” data in a lossy fashion. In addition, by leveraging autoregressive models as both prior distribution p(z) and decoding distribution p(x|z), we can greatly improve generative modeling performance of VAEs, achieving new state-of-the-art results on MNIST, OMNIGLOT and Caltech-101 Silhouettes density estimation tasks as well as competitive results on CIFAR10.},
  langid = {english}
}

@misc{chen_wavegrad_2020,
  title = {{{WaveGrad}}: {{Estimating Gradients}} for {{Waveform Generation}}},
  shorttitle = {{{WaveGrad}}},
  author = {Chen, Nanxin and Zhang, Yu and Zen, Heiga and Weiss, Ron J. and Norouzi, Mohammad and Chan, William},
  date = {2020-09-02},
  eprint = {2009.00713},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2009.00713},
  urldate = {2020-09-03},
  abstract = {This paper introduces WaveGrad, a conditional model for waveform generation through estimating gradients of the data density. This model is built on the prior work on score matching and diffusion probabilistic models. It starts from Gaussian white noise and iteratively refines the signal via a gradient-based sampler conditioned on the mel-spectrogram. WaveGrad is non-autoregressive, and requires only a constant number of generation steps during inference. It can use as few as 6 iterations to generate high fidelity audio samples. WaveGrad is simple to train, and implicitly optimizes for the weighted variational lower-bound of the log-likelihood. Empirical experiments reveal WaveGrad to generate high fidelity audio samples matching a strong likelihood-based autoregressive baseline with less sequential operations.},
  langid = {english}
}

@unpublished{chen_wavlm_2021,
  title = {{{WavLM}}: {{Large-Scale Self-Supervised Pre-Training}} for {{Full Stack Speech Processing}}},
  shorttitle = {{{WavLM}}},
  author = {Chen, Sanyuan and Wang, Chengyi and Chen, Zhengyang and Wu, Yu and Liu, Shujie and Chen, Zhuo and Li, Jinyu and Kanda, Naoyuki and Yoshioka, Takuya and Xiao, Xiong and Wu, Jian and Zhou, Long and Ren, Shuo and Qian, Yanmin and Qian, Yao and Wu, Jian and Zeng, Micheal and Wei, Furu},
  date = {2021-10-26},
  eprint = {2110.13900},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2110.13900},
  urldate = {2021-10-30},
  abstract = {Self-supervised learning (SSL) achieves great success in speech recognition, while limited exploration has been attempted for other speech processing tasks. As speech signal contains multi-faceted information including speaker identity, paralinguistics, spoken content, etc., learning universal representations for all speech tasks is challenging. In this paper, we propose a new pre-trained model, WavLM, to solve full-stack downstream speech tasks. WavLM is built based on the HuBERT framework, with an emphasis on both spoken content modeling and speaker identity preservation. We first equip the Transformer structure with gated relative position bias to improve its capability on recognition tasks. For better speaker discrimination, we propose an utterance mixing training strategy, where additional overlapped utterances are created unsupervisely and incorporated during model training. Lastly, we scale up the training dataset from 60k hours to 94k hours of public audio data, and optimize its training procedure for better representation extraction. WavLM Large achieves state-of-the-art performance on the SUPERB benchmark, and brings significant improvements for various speech processing tasks on their representative benchmarks.}
}

@article{chi_audio_2020,
  title = {Audio {{ALBERT}}: {{A Lite BERT}} for {{Self-supervised Learning}} of {{Audio Representation}}},
  shorttitle = {Audio {{ALBERT}}},
  author = {Chi, Po-Han and Chung, Pei-Hung and Wu, Tsung-Han and Hsieh, Chun-Cheng and Chen, Yen-Hao and Li, Shang-Wen and Lee, Hung-yi},
  date = {2020-05-18},
  url = {https://arxiv.org/abs/2005.08575v5},
  urldate = {2021-10-12},
  abstract = {For self-supervised speech processing, it is crucial to use pretrained models as speech representation extractors. In recent works, increasing the size of the model has been utilized in acoustic model training in order to achieve better performance. In this paper, we propose Audio ALBERT, a lite version of the self-supervised speech representation model. We use the representations with two downstream tasks, speaker identification, and phoneme classification. We show that Audio ALBERT is capable of achieving competitive performance with those huge models in the downstream tasks while utilizing 91\textbackslash\% fewer parameters. Moreover, we use some simple probing models to measure how much the information of the speaker and phoneme is encoded in latent representations. In probing experiments, we find that the latent representations encode richer information of both phoneme and speaker than that of the last layer.},
  langid = {english}
}

@inproceedings{child_very_2021,
  title = {Very {{Deep VAEs Generalize Autoregressive Models}} and {{Can Outperform Them}} on {{Images}}},
  booktitle = {Proceedings of the 9th {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Child, Rewon},
  date = {2021},
  url = {https://arxiv.org/pdf/2011.10650.pdf},
  abstract = {We present a hierarchical VAE that, for the first time, outperforms the PixelCNN in log-likelihood on all natural image benchmarks. We begin by observing that VAEs can actually implement autoregressive models, and other, more efficient generative models, if made sufficiently deep. Despite this, autoregressive models have traditionally outperformed VAEs. We test if insufficient depth explains why by scaling a VAE to greater stochastic depth than previously explored and evaluating it on CIFAR-10, ImageNet, and FFHQ. We find that, in comparison to the PixelCNN, these very deep VAEs achieve higher likelihoods, use fewer parameters, generate samples thousands of times faster, and are more easily applied to high-resolution images. We visualize the generative process and show the VAEs learn efficient hierarchical visual representations. We release our source code and models at https://github.com/openai/vdvae.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}}
}

@online{chira_image_2022,
  title = {Image {{Super-Resolution With Deep Variational Autoencoders}}},
  author = {Chira, Darius and Haralampiev, Ilian and Winther, Ole and Dittadi, Andrea and Liévin, Valentin},
  date = {2022-10-26},
  eprint = {2203.09445},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2203.09445},
  urldate = {2023-09-25},
  abstract = {Image super-resolution (SR) techniques are used to generate a high-resolution image from a low-resolution image. Until now, deep generative models such as autoregressive models and Generative Adversarial Networks (GANs) have proven to be effective at modelling high-resolution images. VAE-based models have often been criticised for their feeble generative performance, but with new advancements such as VDVAE, there is now strong evidence that deep VAEs have the potential to outperform current state-of-the-art models for high-resolution image generation. In this paper, we introduce VDVAE-SR, a new model that aims to exploit the most recent deep VAE methodologies to improve upon the results of similar models. VDVAE-SR tackles image super-resolution using transfer learning on pretrained VDVAEs. The presented model is competitive with other state-of-the-art models, having comparable results on image quality metrics.},
  pubstate = {preprint}
}

@misc{chiu_selfsupervised_2022,
  title = {Self-Supervised {{Learning}} with {{Random-projection Quantizer}} for {{Speech Recognition}}},
  author = {Chiu, Chung-Cheng and Qin, James and Zhang, Yu and Yu, Jiahui and Wu, Yonghui},
  date = {2022-02-03},
  eprint = {2202.01855},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2202.01855},
  urldate = {2022-02-07},
  abstract = {We present a simple and effective self-supervised learning approach for speech recognition. The approach learns a model to predict the masked speech signals, in the form of discrete labels generated with a random-projection quantizer. In particular the quantizer projects speech inputs with a randomly initialized matrix, and does a nearest-neighbor lookup in a randomly-initialized codebook. Neither the matrix nor the codebook is updated during self-supervised learning. Since the random-projection quantizer is not trained and is separated from the speech recognition model, the design makes the approach flexible and is compatible with universal speech recognition architecture. On LibriSpeech our approach achieves similar word-error-rates as previous work using self-supervised learning with non-streaming models, and provides lower word-error-rates and latency than wav2vec 2.0 and w2v-BERT with streaming models. On multilingual tasks the approach also provides significant improvement over wav2vec 2.0 and w2v-BERT.}
}

@misc{cho_learning_2014,
  title = {Learning {{Phrase Representations}} Using {{RNN Encoder-Decoder}} for {{Statistical Machine Translation}}},
  author = {Cho, Kyunghyun and family=Merrienboer, given=Bart, prefix=van, useprefix=true and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  date = {2014},
  eprint = {2079951},
  eprinttype = {pmid},
  url = {http://arxiv.org/abs/1406.1078},
  urldate = {2018-05-04},
  abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
  isbn = {9781937284961}
}

@misc{cho_properties_2014,
  title = {On the Properties of Neural Machine Translation: {{Encoder-decoder}} Approaches},
  author = {Cho, Kyunghyun and family=Merrienboer, given=Bart, prefix=van, useprefix=true and Bahdanau, Dzmitry and Bengio, Yoshua},
  date = {2014},
  eprint = {1409.1259},
  eprinttype = {arxiv}
}

@article{choi_explaining_2016,
  title = {Explaining {{Deep Convolutional Neural Networks}} on {{Music Classification}}},
  author = {Choi, Keunwoo and Fazekas, George and Sandler, Mark},
  date = {2016-07-08},
  url = {https://arxiv.org/abs/1607.02444},
  urldate = {2018-09-25},
  abstract = {Deep convolutional neural networks (CNNs) have been actively adopted in the field of music information retrieval, e.g. genre classification, mood detection, and chord recognition. However, the process of learning and prediction is little understood, particularly when it is applied to spectrograms. We introduce auralisation of a CNN to understand its underlying mechanism, which is based on a deconvolution procedure introduced in [2]. Auralisation of a CNN is converting the learned convolutional features that are obtained from deconvolution into audio signals. In the experiments and discussions, we explain trained features of a 5-layer CNN based on the deconvolved spectrograms and auralised signals. The pairwise correlations per layers with varying different musical attributes are also investigated to understand the evolution of the learnt features. It is shown that in the deep layers, the features are learnt to capture textures, the patterns of continuous distributions, rather than shapes of lines.},
  langid = {english}
}

@inproceedings{choi_neural_2021,
  title = {Neural Analysis and Synthesis: {{Reconstructing}} Speech from Self-Supervised Representations},
  booktitle = {Conference on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Choi, Hyeong-Seok and Lee, Juheon and Kim, Wansoo and Lee, Jie and Heo, Hoon and Lee, Kyogu},
  editor = {Ranzato, Marc'Aurelio and Beygelzimer, Alina and Dauphin, Yann N. and Liang, Percy and Vaughan, Jennifer Wortman},
  date = {2021},
  pages = {16251--16265},
  location = {{Virtual}}
}

@unpublished{choi_robust_2021,
  title = {Robust Out-of-Distribution Detection on Deep Probabilistic Generative Models},
  author = {Choi, Jaemoo and Yoon, Changyeon and Bae, Jeongwoo and Kang, Myungjoo},
  date = {2021},
  eprint = {2106.07903},
  eprinttype = {arxiv}
}

@misc{choi_waic_2019,
  title = {{{WAIC}}, but {{Why}}? {{Generative Ensembles}} for {{Robust Anomaly Detection}}},
  shorttitle = {{{WAIC}}, but {{Why}}?},
  author = {Choi, Hyunsun and Jang, Eric and Alemi, Alexander A.},
  date = {2019-05-23},
  eprint = {1810.01392},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1810.01392},
  urldate = {2021-01-19},
  abstract = {Machine learning models encounter Out-ofDistribution (OoD) errors when the data seen at test time are generated from a different stochastic generator than the one used to generate the training data. One proposal to scale OoD detection to high-dimensional data is to learn a tractable likelihood approximation of the training distribution, and use it to reject unlikely inputs. However, likelihood models on natural data are themselves susceptible to OoD errors, and even assign large likelihoods to samples from other datasets. To mitigate this problem, we propose Generative Ensembles, which robustify density-based OoD detection by way of estimating epistemic uncertainty of the likelihood model. We present a puzzling observation in need of an explanation – although likelihood measures cannot account for the typical set of a distribution, and therefore should not be suitable on their own for OoD detection, WAIC performs surprisingly well in practice.},
  langid = {english}
}

@inproceedings{choromanska_loss_2014,
  title = {The {{Loss Surfaces}} of {{Multilayer Networks}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}} ({{AISTATS}})},
  author = {Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, Gérard Ben and LeCun, Yann},
  date = {2014},
  volume = {38},
  eprint = {1412.0233},
  eprinttype = {arxiv},
  issn = {15337928},
  abstract = {We study the connection between the highly non-convex loss function of a simple model of the fully-connected feed-forward neural network and the Hamiltonian of the spherical spin-glass model under the assumptions of: i) variable independence, ii) redundancy in network parametrization, and iii) uniformity. These assumptions enable us to explain the complexity of the fully decoupled neural network through the prism of the results from random matrix theory. We show that for large-size decoupled networks the lowest critical values of the random loss function form a layered structure and they are located in a well-defined band lower-bounded by the global minimum. The number of local minima outside that band diminishes exponentially with the size of the network. We empirically verify that the mathematical model exhibits similar behavior as the computer simulations, despite the presence of high dependencies in real networks. We conjecture that both simulated annealing and SGD converge to the band of low critical points, and that all critical points found there are local minima of high quality measured by the test error. This emphasizes a major difference between large- and small-size networks where for the latter poor quality local minima have non-zero probability of being recovered. Finally, we prove that recovering the global minimum becomes harder as the network size increases and that it is in practice irrelevant as global minimum often leads to overfitting.}
}

@misc{choromanski_rethinking_2021,
  title = {Rethinking {{Attention}} with {{Performers}}},
  author = {Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and Belanger, David and Colwell, Lucy and Weller, Adrian},
  date = {2021-03-09},
  eprint = {2009.14794},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2009.14794},
  urldate = {2022-02-03},
  abstract = {We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can be also used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers.}
}

@inproceedings{chorowski_better_2017,
  title = {Towards Better Decoding and Language Model Integration in Sequence to Sequence Models},
  booktitle = {Proceedings of the {{Annual Conference}} of the {{International Speech Communication Association}} ({{Interspeech}})},
  author = {Chorowski, Jan and Jaitly, Navdeep},
  editor = {Lacerda, Francisco},
  date = {2017},
  pages = {523--527},
  publisher = {{ISCA}},
  url = {http://www.isca-speech.org/archive/Interspeech_2017/abstracts/0343.html}
}

@article{chorowski_unsupervised_2019,
  title = {Unsupervised {{Speech Representation Learning Using WaveNet Autoencoders}}},
  author = {Chorowski, Jan and Weiss, Ron J. and Bengio, Samy and family=Oord, given=Aäron, prefix=van den, useprefix=false},
  date = {2019-12},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  shortjournal = {IEEE/ACM Trans. Audio Speech Lang. Process.},
  volume = {27},
  number = {12},
  eprint = {1901.08810},
  eprinttype = {arxiv},
  pages = {2041--2053},
  issn = {2329-9290, 2329-9304},
  doi = {10/ggdbb4},
  url = {http://arxiv.org/abs/1901.08810},
  urldate = {2020-06-19},
  abstract = {We consider the task of unsupervised extraction of meaningful latent representations of speech by applying autoencoding neural networks to speech waveforms. The goal is to learn a representation able to capture high level semantic content from the signal, e.g. phoneme identities, while being invariant to confounding low level details in the signal such as the underlying pitch contour or background noise. Since the learned representation is tuned to contain only phonetic content, we resort to using a high capacity WaveNet decoder to infer information discarded by the encoder from previous samples. Moreover, the behavior of autoencoder models depends on the kind of constraint that is applied to the latent representation. We compare three variants: a simple dimensionality reduction bottleneck, a Gaussian Variational Autoencoder (VAE), and a discrete Vector Quantized VAE (VQ-VAE). We analyze the quality of learned representations in terms of speaker independence, the ability to predict phonetic content, and the ability to accurately reconstruct individual spectrogram frames. Moreover, for discrete encodings extracted using the VQ-VAE, we measure the ease of mapping them to phonemes. We introduce a regularization scheme that forces the representations to focus on the phonetic content of the utterance and report performance comparable with the top entries in the ZeroSpeech 2017 unsupervised acoustic unit discovery task.},
  langid = {english}
}

@inproceedings{chou_capacity_1988,
  title = {The {{Capacity}} of the {{Kanerva Associative Memory}} Is {{Exponential}}},
  booktitle = {Neural {{Information Processing Systems}}},
  author = {Chou, Philip},
  editor = {Anderson, D.},
  date = {1988},
  pages = {184--191},
  publisher = {{American Institute of Physics}},
  url = {https://proceedings.neurips.cc/paper/1987/file/c81e728d9d4c2f636f067f89cc14862c-Paper.pdf},
  urldate = {2021-02-10}
}

@unpublished{chowdhery_palm_2022,
  title = {{{PaLM}}: {{Scaling Language Modeling}} with {{Pathways}}},
  shorttitle = {{{PaLM}}},
  author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
  date = {2022-04-19},
  eprint = {2204.02311},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2204.02311},
  urldate = {2022-04-22},
  abstract = {Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.}
}

@book{christensen_functions_2010,
  title = {Functions, {{Spaces}}, and {{Expansions}}},
  author = {Christensen, Ole},
  date = {2010},
  eprint = {25246403},
  eprinttype = {pmid},
  publisher = {{Birkhäuser}},
  location = {{Boston}},
  issn = {1098-6596},
  doi = {10.1007/978-0-8176-4980-7},
  url = {http://link.springer.com/10.1007/978-0-8176-4980-7},
  urldate = {2018-06-26},
  abstract = {The purpose of this book is to present some mathematical tools that play key roles in mathematics as well as in appliedmathematics, physics, and en- gineering. The treatment is mathematical in nature, and we do not go into concrete applications; but it is important to stress that all the considered topics are selected because they actually play a role outside pure mathe- matics. The hope is that the book will be useful for students in many fields of science and engineering, and professionals who want a deeper insight in some of the topics appearing in the scientific literature.},
  isbn = {978-0-8176-4979-1}
}

@inproceedings{chrupala_representations_2017,
  title = {Representations of Language in a Model of Visually Grounded Speech Signal},
  booktitle = {Proceedings of the 55th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{ACL}})},
  author = {Chrupała, Grzegorz and Gelderloos, Lieke and Alishahi, Afra},
  date = {2017},
  pages = {613--622}
}

@online{chrupala_visually_2021,
  title = {Visually {{Grounded Models}} of {{Spoken Language}}: {{A Survey}} of {{Datasets}}, {{Architectures}} and {{Evaluation Techniques}}},
  author = {Chrupała, Grzegorz},
  date = {2021},
  eprint = {2104.13225},
  eprinttype = {arxiv},
  pubstate = {preprint}
}

@article{chung_audio_2016,
  title = {Audio {{Word2Vec}}: {{Unsupervised Learning}} of {{Audio Segment Representations}} Using {{Sequence-to-sequence Autoencoder}}},
  shorttitle = {Audio {{Word2Vec}}},
  author = {Chung, Yu-An and Wu, Chao-Chung and Shen, Chia-Hao and Lee, Hung-Yi and Lee, Lin-Shan},
  date = {2016-03-03},
  url = {https://arxiv.org/abs/1603.00982v4},
  urldate = {2021-10-12},
  abstract = {The vector representations of fixed dimensionality for words (in text) offered by Word2Vec have been shown to be very useful in many application scenarios, in particular due to the semantic information they carry. This paper proposes a parallel version, the Audio Word2Vec. It offers the vector representations of fixed dimensionality for variable-length audio segments. These vector representations are shown to describe the sequential phonetic structures of the audio segments to a good degree, with very attractive real world applications such as query-by-example Spoken Term Detection (STD). In this STD application, the proposed approach significantly outperformed the conventional Dynamic Time Warping (DTW) based approaches at significantly lower computation requirements. We propose unsupervised learning of Audio Word2Vec from audio data without human annotation using Sequence-to-sequence Audoencoder (SA). SA consists of two RNNs equipped with Long Short-Term Memory (LSTM) units: the first RNN (encoder) maps the input audio sequence into a vector representation of fixed dimensionality, and the second RNN (decoder) maps the representation back to the input audio sequence. The two RNNs are jointly trained by minimizing the reconstruction error. Denoising Sequence-to-sequence Autoencoder (DSA) is furthered proposed offering more robust learning.},
  langid = {english}
}

@inproceedings{chung_generative_2020,
  title = {Generative {{Pre-Training}} for {{Speech}} with {{Autoregressive Predictive Coding}}},
  booktitle = {{{ICASSP}} 2020 - 2020 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Chung, Yu-An and Glass, James},
  date = {2020-05},
  pages = {3497--3501},
  issn = {2379-190X},
  doi = {10.1109/ICASSP40776.2020.9054438},
  abstract = {Learning meaningful and general representations from unannotated speech that are applicable to a wide range of tasks remains challenging. In this paper we propose to use autoregressive predictive coding (APC), a recently proposed self-supervised objective, as a generative pre-training approach for learning meaningful, non-specific, and transferable speech representations. We pre-train APC on large-scale unlabeled data and conduct transfer learning experiments on three speech applications that require different information about speech characteristics to perform well: speech recognition, speech translation, and speaker identification. Extensive experiments show that APC not only outperforms surface features (e.g., log Mel spectrograms) and other popular representation learning methods on all three tasks, but is also effective at reducing downstream labeled data size and model parameters. We also investigate the use of Transformers for modeling APC and find it superior to RNNs.},
  eventtitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})}
}

@misc{chung_hierarchical_2017,
  title = {Hierarchical {{Multiscale Recurrent Neural Networks}}},
  author = {Chung, Junyoung and Ahn, Sungjin and Bengio, Yoshua},
  date = {2017-03-09},
  eprint = {1609.01704},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1609.01704},
  urldate = {2021-04-21},
  abstract = {Learning both hierarchical and temporal representation has been among the longstanding challenges of recurrent neural networks. Multiscale recurrent neural networks have been considered as a promising approach to resolve this issue, yet there has been a lack of empirical evidence showing that this type of models can actually capture the temporal dependencies by discovering the latent hierarchical structure of the sequence. In this paper, we propose a novel multiscale approach, called the hierarchical multiscale recurrent neural network, that can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales using a novel update mechanism. We show some evidence that the proposed model can discover underlying hierarchical structure in the sequences without using explicit boundary information. We evaluate our proposed model on character-level language modelling and handwriting sequence generation.},
  langid = {english}
}

@misc{chung_improved_2020,
  title = {Improved {{Speech Representations}} with {{Multi-Target Autoregressive Predictive Coding}}},
  author = {Chung, Yu-An and Glass, James},
  date = {2020-04-10},
  eprint = {2004.05274},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2004.05274},
  urldate = {2020-06-11},
  abstract = {Training objectives based on predictive coding have recently been shown to be very effective at learning meaningful representations from unlabeled speech. One example is Autoregressive Predictive Coding (Chung et al., 2019), which trains an autoregressive RNN to generate an unseen future frame given a context such as recent past frames. The basic hypothesis of these approaches is that hidden states that can accurately predict future frames are a useful representation for many downstream tasks. In this paper we extend this hypothesis and aim to enrich the information encoded in the hidden states by training the model to make more accurate future predictions. We propose an auxiliary objective that serves as a regularization to improve generalization of the future frame prediction task. Experimental results on phonetic classification, speech recognition, and speech translation not only support the hypothesis, but also demonstrate the effectiveness of our approach in learning representations that contain richer phonetic content.},
  langid = {english}
}

@unpublished{chung_learning_2017,
  title = {Learning {{Word Embeddings}} from {{Speech}}},
  author = {Chung, Yu-An and Glass, James},
  date = {2017-11-04},
  eprint = {1711.01515},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1711.01515},
  urldate = {2021-10-20},
  abstract = {In this paper, we propose a novel deep neural network architecture, Sequence-to-Sequence Audio2Vec, for unsupervised learning of fixed-length vector representations of audio segments excised from a speech corpus, where the vectors contain semantic information pertaining to the segments, and are close to other vectors in the embedding space if their corresponding segments are semantically similar. The design of the proposed model is based on the RNN Encoder-Decoder framework, and borrows the methodology of continuous skip-grams for training. The learned vector representations are evaluated on 13 widely used word similarity benchmarks, and achieved competitive results to that of GloVe. The biggest advantage of the proposed model is its capability of extracting semantic information of audio segments taken directly from raw speech, without relying on any other modalities such as text or images, which are challenging and expensive to collect and annotate.}
}

@inproceedings{chung_lip_2016,
  title = {Lip Reading in the Wild},
  booktitle = {Asian {{Conference}} on {{Computer Vision}}},
  author = {Chung, Joon Son and Zisserman, Andrew},
  date = {2016}
}

@inproceedings{chung_recurrent_2015,
  title = {A {{Recurrent Latent Variable Model}} for {{Sequential Data}}},
  booktitle = {Proceedings of the 29th {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Chung, Junyoung and Kastner, Kyle and Dinh, Laurent and Goel, Kratarth and Courville, Aaron C and Bengio, Yoshua},
  date = {2015},
  pages = {9},
  location = {{Montréal, Quebec, Canada}},
  abstract = {In this paper, we explore the inclusion of latent random variables into the hidden state of a recurrent neural network (RNN) by combining the elements of the variational autoencoder. We argue that through the use of high-level latent random variables, the variational RNN (VRNN)1 can model the kind of variability observed in highly structured sequential data such as natural speech. We empirically evaluate the proposed model against other related sequential models on four speech datasets and one handwriting dataset. Our results show the important roles that latent random variables can play in the RNN dynamics.},
  eventtitle = {Conference on {{Neural Information Processing Systems}}},
  langid = {english}
}

@inproceedings{chung_similarity_2021,
  title = {Similarity {{Analysis}} of {{Self-Supervised Speech Representations}}},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Chung, Yu-An and Belinkov, Yonatan and Glass, James},
  date = {2021-06},
  pages = {3040--3044},
  issn = {2379-190X},
  doi = {10.1109/ICASSP39728.2021.9414321},
  abstract = {Self-supervised speech representation learning has recently been a prosperous research topic. Many algorithms have been proposed for learning useful representations from large-scale unlabeled data, and their applications to a wide range of speech tasks have also been investigated. However, there has been little research focusing on understanding the properties of existing approaches. In this work, we aim to provide a comparative study of some of the most representative self-supervised algorithms. Specifically, we quantify the similarities between different self-supervised representations using existing similarity measures. We also design probing tasks to study the correlation between the models’ pre-training loss and the amount of specific speech information contained in their learned representations. In addition to showing how various self-supervised models behave differently given the same input, our study also finds that the training objective has a higher impact on representation similarity than architectural choices such as building blocks (RNN/Transformer/CNN) and directionality (uni/bidirectional). Our results also suggest that there exists a strong correlation between pre-training loss and downstream performance for some self-supervised algorithms.},
  eventtitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})}
}

@unpublished{chung_speech2vec_2018,
  title = {{{Speech2Vec}}: {{A Sequence-to-Sequence Framework}} for {{Learning Word Embeddings}} from {{Speech}}},
  shorttitle = {{{Speech2Vec}}},
  author = {Chung, Yu-An and Glass, James},
  date = {2018-06-09},
  eprint = {1803.08976},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1803.08976},
  urldate = {2021-10-12},
  abstract = {In this paper, we propose a novel deep neural network architecture, Speech2Vec, for learning fixed-length vector representations of audio segments excised from a speech corpus, where the vectors contain semantic information pertaining to the underlying spoken words, and are close to other vectors in the embedding space if their corresponding underlying spoken words are semantically similar. The proposed model can be viewed as a speech version of Word2Vec. Its design is based on a RNN Encoder-Decoder framework, and borrows the methodology of skipgrams or continuous bag-of-words for training. Learning word embeddings directly from speech enables Speech2Vec to make use of the semantic information carried by speech that does not exist in plain text. The learned word embeddings are evaluated and analyzed on 13 widely used word similarity benchmarks, and outperform word embeddings learned by Word2Vec from the transcriptions.}
}

@misc{chung_splat_2021,
  title = {{{SPLAT}}: {{Speech-Language Joint Pre-Training}} for {{Spoken Language Understanding}}},
  shorttitle = {{{SPLAT}}},
  author = {Chung, Yu-An and Zhu, Chenguang and Zeng, Michael},
  date = {2021-03-14},
  eprint = {2010.02295},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2010.02295},
  urldate = {2022-02-03},
  abstract = {Spoken language understanding (SLU) requires a model to analyze input acoustic signal to understand its linguistic content and make predictions. To boost the models' performance, various pre-training methods have been proposed to learn rich representations from large-scale unannotated speech and text. However, the inherent disparities between the two modalities necessitate a mutual analysis. In this paper, we propose a novel semi-supervised learning framework, SPLAT, to jointly pre-train the speech and language modules. Besides conducting a self-supervised masked language modeling task on the two individual modules using unpaired speech and text, SPLAT aligns representations from the two modules in a shared latent space using a small amount of paired speech and text. Thus, during fine-tuning, the speech module alone can produce representations carrying both acoustic information and contextual semantic knowledge of an input acoustic signal. Experimental results verify the effectiveness of our approach on various SLU tasks. For example, SPLAT improves the previous state-of-the-art performance on the Spoken SQuAD dataset by more than 10\%.}
}

@article{chung_unsupervised_2018,
  title = {Unsupervised Cross-Modal Alignment of Speech and Text Embedding Spaces},
  author = {Chung, Yu-An and Weng, Wei-Hung and Tong, Schrasing and Glass, James},
  date = {2018},
  journaltitle = {Proceedings of the 31st Conference on Neural Information Processing Systems (NeurIPS)},
  pages = {7354--7364}
}

@misc{chung_unsupervised_2019,
  title = {An {{Unsupervised Autoregressive Model}} for {{Speech Representation Learning}}},
  author = {Chung, Yu-An and Hsu, Wei-Ning and Tang, Hao and Glass, James},
  date = {2019-06-18},
  eprint = {1904.03240},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1904.03240},
  urldate = {2020-06-11},
  abstract = {This paper proposes a novel unsupervised autoregressive neural model for learning generic speech representations. In contrast to other speech representation learning methods that aim to remove noise or speaker variabilities, ours is designed to preserve information for a wide range of downstream tasks. In addition, the proposed model does not require any phonetic or word boundary labels, allowing the model to benefit from large quantities of unlabeled data. Speech representations learned by our model significantly improve performance on both phone classification and speaker verification over the surface features and other supervised and unsupervised approaches. Further analysis shows that different levels of speech information are captured by our model at different layers. In particular, the lower layers tend to be more discriminative for speakers, while the upper layers provide more phonetic content.},
  langid = {english}
}

@inproceedings{chung_unsupervised_2019a,
  title = {Towards Unsupervised Speech-to-Text Translation},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Chung, Yu-An and Weng, Wei-Hung and Tong, Schrasing and Glass, James},
  date = {2019},
  pages = {7170--7174}
}

@unpublished{chung_vectorquantized_2020,
  title = {Vector-{{Quantized Autoregressive Predictive Coding}}},
  author = {Chung, Yu-An and Tang, Hao and Glass, James},
  date = {2020-05-17},
  eprint = {2005.08392},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2005.08392},
  urldate = {2021-10-11},
  abstract = {Autoregressive Predictive Coding (APC), as a self-supervised objective, has enjoyed success in learning representations from large amounts of unlabeled data, and the learned representations are rich for many downstream tasks. However, the connection between low self-supervised loss and strong performance in downstream tasks remains unclear. In this work, we propose Vector-Quantized Autoregressive Predictive Coding (VQ-APC), a novel model that produces quantized representations, allowing us to explicitly control the amount of information encoded in the representations. By studying a sequence of increasingly limited models, we reveal the constituents of the learned representations. In particular, we confirm the presence of information with probing tasks, while showing the absence of information with mutual information, uncovering the model's preference in preserving speech information as its capacity becomes constrained. We find that there exists a point where phonetic and speaker information are amplified to maximize a self-supervised objective. As a byproduct, the learned codes for a particular model capacity correspond well to English phones.}
}

@article{chung_w2vbert_2021,
  title = {W2v-{{BERT}}: {{Combining}} Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training},
  author = {Chung, Yu-An and Zhang, Yu and Han, Wei and Chiu, Chung-Cheng and Qin, James and Pang, Ruoming and Wu, Yonghui},
  date = {2021}
}

@inproceedings{cieri_fisher_2004,
  title = {The {{Fisher}} Corpus: {{A}} Resource for the next Generations of Speech-to-Text},
  booktitle = {Proceedings of the {{International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}})},
  author = {Cieri, Christopher and Miller, David and Walker, Kevin},
  date = {2004},
  volume = {4},
  pages = {69--71},
  eventtitle = {International {{Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}})}
}

@article{ciresan_multi-column_2012,
  title = {Multi-Column Deep Neural Network for Traffic Sign Classification},
  author = {Cireşan, Dan and Meier, Ueli and Masci, Jonathan and Schmidhuber, Jürgen},
  date = {2012-08},
  journaltitle = {Proceedings of the International Joint Conference on Neural Networks (IJCNN)},
  volume = {32},
  eprint = {22386783},
  eprinttype = {pmid},
  pages = {333--338},
  issn = {08936080},
  doi = {10.1016/j.neunet.2012.02.023},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S0893608012000524},
  urldate = {2017-03-27},
  abstract = {We describe the approach that won the final phase of the German traffic sign recognition benchmark. Our method is the only one that achieved a better-than-human recognition rate of 99.46\%. We use a fast, fully parameterizable GPU implementation of a Deep Neural Network (DNN) that does not require careful design of pre-wired feature extractors, which are rather learned in a supervised way. Combining various DNNs trained on differently preprocessed data into a Multi-Column DNN (MCDNN) further boosts recognition performance, making the system insensitive also to variations in contrast and illumination. © 2012 Elsevier Ltd.},
  isbn = {0893-6080},
  issue = {Sp. Iss. SI}
}

@article{ciresan_multicolumn_2012,
  title = {Multi-Column {{Deep Neural Networks}} for {{Image Classification}}},
  author = {Cireşan, Dan and Meier, Ueli and Schmidhuber, Jürgen},
  date = {2012-02-13},
  journaltitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  eprint = {22386783},
  eprinttype = {pmid},
  pages = {3642--3649},
  issn = {10636919},
  doi = {10.1109/CVPR.2012.6248110},
  url = {http://arxiv.org/abs/1202.2745},
  abstract = {Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks.},
  isbn = {9781467312264}
}

@article{cite1,
  title = {Global, Regional, and National Burden of Stroke and Its Risk Factors, 1990–2019: A Systematic Analysis for the {{Global Burden}} of {{Disease Study}} 2019},
  author = {{GBD 2019 Stroke Collaborators} and Feigin, Valery L and Stark, Benjamin A and Johnson, Catherine Owens and Roth, Gregory A and Bisignano, Catherine and Abady, Gdiom Gebreheat and Abbasifard, Mitra and Abbasi-Kangevari, Mohsen and Abd-Allah, Foad and Abedi, Vida and Abualhasan, Ahmed and Abu-Rmeileh, Niveen ME and Abushouk, Abdelrahman I and Adebayo, Oladimeji M and Agarwal, Gina and Agasthi, Pradyumna and Ahinkorah, Bright Opoku and Ahmad, Sohail and Ahmadi, Sepideh and Salih, Yusra Ahmed and Aji, Budi and Akbarpour, Samaneh and Akinyemi, Rufus Olusola and Hamad, Hanadi Al and Alahdab, Fares and Alif, Sheikh Mohammad and Alipour, Vahid and Aljunid, Syed Mohamed and Almustanyir, Sami and Al-Raddadi, Rajaa M and Salman, Rustam Al-Shahi and Alvis-Guzman, Nelson and Ancuceanu, Robert and Anderlini, Deanna and Anderson, Jason A and Ansar, Adnan and Antonazzo, Ippazio Cosimo and Arabloo, Jalal and Ärnlöv, Johan and Artanti, Kurnia Dwi and Aryan, Zahra and Asgari, Samaneh and Ashraf, Tahira and Athar, Mohammad and Atreya, Alok and Ausloos, Marcel and Baig, Atif Amin and Baltatu, Ovidiu Constantin and Banach, Maciej and Ghith, Nermin},
  date = {2021},
  journaltitle = {The Lancet Neurology},
  volume = {20},
  number = {10},
  pages = {795--820},
  publisher = {{TheLancet Publishing Group}},
  issn = {1474-4422},
  doi = {10.1016/S1474-4422(21)00252-0},
  abstract = {Background Regularly updated data on stroke and its pathological types, including data on their incidence, prevalence, mortality, disability, risk factors, and epidemiological trends, are important for evidence-based stroke care planning and resource allocation. The Global Burden of Diseases, Injuries, and Risk Factors Study (GBD) aims to provide a standardised and comprehensive measurement of these metrics at global, regional, and national levels. Methods We applied GBD 2019 analytical tools to calculate stroke incidence, prevalence, mortality, disability-adjusted life-years (DALYs), and the population attributable fraction (PAF) of DALYs (with corresponding 95\% uncertainty intervals [UIs]) associated with 19 risk factors, for 204 countries and territories from 1990 to 2019. These estimates were provided for ischaemic stroke, intracerebral haemorrhage, subarachnoid haemorrhage, and all strokes combined, and stratified by sex, age group, and World Bank country income level. Findings In 2019, there were 12·2 million (95\% UI 11·0–13·6) incident cases of stroke, 101 million (93·2–111) prevalent cases of stroke, 143 million (133–153) DALYs due to stroke, and 6·55 million (6·00–7·02) deaths from stroke. Globally, stroke remained the second-leading cause of death (11·6\% [10·8–12·2] of total deaths) and the third-leading cause of death and disability combined (5·7\% [5·1–6·2] of total DALYs) in 2019. From 1990 to 2019, the absolute number of incident strokes increased by 70·0\% (67·0–73·0), prevalent strokes increased by 85·0\% (83·0–88·0), deaths from stroke increased by 43·0\% (31·0–55·0), and DALYs due to stroke increased by 32·0\% (22·0–42·0). During the same period, age-standardised rates of stroke incidence decreased by 17·0\% (15·0–18·0), mortality decreased by 36·0\% (31·0–42·0), prevalence decreased by 6·0\% (5·0–7·0), and DALYs decreased by 36·0\% (31·0–42·0). However, among people younger than 70 years, prevalence rates increased by 22·0\% (21·0–24·0) and incidence rates increased by 15·0\% (12·0–18·0). In 2019, the age-standardised stroke-related mortality rate was 3·6 (3·5–3·8) times higher in the World Bank low-income group than in the World Bank high-income group, and the age-standardised stroke-related DALY rate was 3·7 (3·5–3·9) times higher in the low-income group than the high-income group. Ischaemic stroke constituted 62·4\% of all incident strokes in 2019 (7·63 million [6·57–8·96]), while intracerebral haemorrhage constituted 27·9\% (3·41 million [2·97–3·91]) and subarachnoid haemorrhage constituted 9·7\% (1·18 million [1·01–1·39]). In 2019, the five leading risk factors for stroke were high systolic blood pressure (contributing to 79·6 million [67·7–90·8] DALYs or 55·5\% [48·2–62·0] of total stroke DALYs), high body-mass index (34·9 million [22·3–48·6] DALYs or 24·3\% [15·7–33·2]), high fasting plasma glucose (28·9 million [19·8–41·5] DALYs or 20·2\% [13·8–29·1]), ambient particulate matter pollution (28·7 million [23·4–33·4] DALYs or 20·1\% [16·6–23·0]), and smoking (25·3 million [22·6–28·2] DALYs or 17·6\% [16·4–19·0]). Interpretation The annual number of strokes and deaths due to stroke increased substantially from 1990 to 2019, despite substantial reductions in age-standardised rates, particularly among people older than 70 years. The highest age-standardised stroke-related mortality and DALY rates were in the World Bank low-income group. The fastest-growing risk factor for stroke between 1990 and 2019 was high body-mass index. Without urgent implementation of effective primary prevention strategies, the stroke burden will probably continue to grow across the world, particularly in low-income countries.},
  langid = {english}
}

@article{cite10,
  title = {Dispatcher Stroke Recognition Using a Stroke Screening Tool: A Systematic Review},
  author = {Oostema, John Adam and Carle, Trevor and Talia, Nadine and Reeves, Mathew},
  date = {2016},
  journaltitle = {Cerebrovascular Diseases},
  volume = {42},
  number = {5-6},
  pages = {370--377},
  publisher = {{S. Karger AG}}
}

@article{cite11,
  title = {Medical Dispatchers Recognise Substantial Amount of Acute Stroke during Emergency Calls},
  author = {Viereck, Søren and Møller, Thea Palsgaard and Iversen, Helle Klingenberg and Christensen, Hanne and Lippert, Freddy},
  date = {2016},
  journaltitle = {Scandinavian Journal of Trauma, Resuscitation and Emergency Medicine},
  volume = {24},
  pages = {1--7},
  publisher = {{Springer}}
}

@article{cite12,
  title = {The {{Accuracy}} of {{Medical Dispatch}} - {{A Systematic Review}}},
  author = {Bohm, K and Kurland, Lisa},
  date = {2018},
  journaltitle = {Scandinavian Journal of Trauma, Resuscitation and Emergency Medicine},
  volume = {26},
  pages = {1--10},
  publisher = {{Springer}}
}

@article{cite13,
  title = {Training Emergency Services' Dispatchers to Recognise Stroke: An Interrupted Time-Series Analysis},
  author = {Watkins, Caroline L and Leathley, Michael J and Jones, Stephanie P and Ford, Gary A and Quinn, Tom and Sutton, Chris J},
  date = {2013},
  journaltitle = {BMC Health Services Research},
  volume = {13},
  pages = {1--9},
  publisher = {{Springer}}
}

@article{cite14,
  title = {Machine Learning as a Supportive Tool to Recognize Cardiac Arrest in Emergency Calls},
  author = {Blomberg, Stig Nikolaj and Folke, Fredrik and Ersbøll, Annette Kjær and Christensen, Helle Collatz and Torp-Pedersen, Christian and Sayre, Michael R and Counts, Catherine R and Lippert, Freddy K},
  date = {2019},
  journaltitle = {Resuscitation},
  volume = {138},
  pages = {322--329},
  publisher = {{Elsevier}}
}

@article{cite15,
  title = {Effect of Machine Learning on Dispatcher Recognition of Out-of-Hospital Cardiac Arrest during Calls to Emergency Medical Services: A Randomized Clinical Trial},
  author = {Blomberg, Stig Nikolaj and Christensen, Helle Collatz and Lippert, Freddy and Ersbøll, Annette Kjær and Torp-Petersen, Christian and Sayre, Michael R and Kudenchuk, Peter J and Folke, Fredrik},
  date = {2021},
  journaltitle = {JAMA Network Open},
  volume = {4},
  number = {1},
  pages = {e2032320--e2032320},
  publisher = {{American Medical Association}}
}

@article{cite16,
  title = {The {{Danish Stroke Registry}}},
  author = {Johnsen, Søren Paaske and Ingeman, Annette and Hundborg, Heidi Holmager and Schaarup, Susanne Zielke and Gyllenborg, Jesper},
  date = {2016},
  journaltitle = {Clinical Epidemiology},
  pages = {697--702},
  publisher = {{Taylor \& Francis}}
}

@misc{cite17,
  title = {{{FOLK1}}: {{Population}} Quarterly Database ({{February}} 2023)},
  author = {{Danmarks Statistik (Statistics Denmark)}},
  date = {2023-02},
  url = {https://www.statistikbanken.dk/FOLK1A},
  organization = {{Danmarks Statistik (Statistics Denmark)}}
}

@article{cite18,
  title = {Impact of {{Integrating Out-of-Hours Services}} into {{Emergency Medical Services Copenhagen}}: {{A Descriptive Study}} of {{Transformational Years}}},
  author = {Zinger, Nienke D and Blomberg, Stig Nikolaj and Lippert, Freddy and Krafft, Thomas and Christensen, Helle Collatz},
  date = {2022},
  journaltitle = {International Journal of Emergency Medicine},
  volume = {15},
  number = {1},
  pages = {40},
  publisher = {{Springer}}
}

@article{cite19,
  title = {The {{Danish Civil Registration System}} as a {{Tool}} in {{Epidemiology}}},
  author = {Schmidt, Morten and Pedersen, Lars and Sørensen, Henrik Toft},
  date = {2014},
  journaltitle = {European Journal of Epidemiology},
  volume = {29},
  pages = {541--549},
  publisher = {{Springer}}
}

@article{cite2,
  title = {Global, Regional, and National Disability-Adjusted Life-Years ({{DALYs}}) for 359 Diseases and Injuries and Healthy Life Expectancy ({{HALE}}) for 195 Countries and Territories, 1990–2017: A Systematic Analysis for the {{Global Burden}} of {{Disease Study}} 2017},
  author = {Kyu, Hmwe Hmwe and Abate, Degu and Abate, Kalkidan Hassen and Abay, Solomon M and Abbafati, Cristiana and Abbasi, Nooshin and Abbastabar, Hedayat and Abd-Allah, Foad and Abdela, Jemal and Abdelalim, Ahmed and others},
  date = {2018},
  journaltitle = {The Lancet},
  volume = {392},
  number = {10159},
  pages = {1859--1922},
  publisher = {{Elsevier}}
}

@misc{cite20,
  title = {Dansk {{Indeks}} for {{Akuthjælp}}. {{Landsudgaven}}, Version 1.10 ({{Danish Index}} for {{Emergency Care}}. {{Nation}} Edition, Version 1.10)},
  author = {{Danske Regioner} and {Laerdal}},
  date = {2022-04},
  url = {https://www.ph.rm.dk/siteassets/prahospitalet/fagfolk/dansk-indeks/dansk-indeks-version-1.10---landsudgaven.pdf},
  urldate = {2023-03-27}
}

@article{cite22,
  title = {Convolutional Networks for Images, Speech, and Time Series},
  author = {LeCun, Yann and Bengio, Yoshua and others},
  date = {1995},
  journaltitle = {The handbook of brain theory and neural networks},
  volume = {3361},
  number = {10},
  pages = {1995},
  publisher = {{Citeseer}}
}

@article{cite29,
  title = {Sex {{Matters}} in {{Stroke}}: {{A Review}} of {{Recent Evidence}} on the {{Differences Between Women}} and {{Men}}},
  author = {Carcel, Cheryl and Woodward, Mark and Wang, Xia and Bushnell, Cheryl and Sandset, Else Charlotte},
  date = {2020},
  journaltitle = {Frontiers in Neuroendocrinology},
  volume = {59},
  pages = {100870},
  publisher = {{Elsevier}}
}

@inproceedings{cite3,
  title = {Global Burden of Stroke},
  booktitle = {Seminars in Neurology},
  author = {Katan, Mira and Luft, Andreas},
  date = {2018},
  volume = {38},
  number = {02},
  pages = {208--211},
  publisher = {{Thieme Medical Publishers}}
}

@article{cite30,
  title = {Sex and {{Age Differences}} in {{Patient-Reported Acute Stroke Symptoms}}},
  author = {Eddelien, Heidi S and Butt, Jawad H and Christensen, Thomas and Danielsen, Anne K and Kruuse, Christina},
  date = {2022},
  journaltitle = {Frontiers in Neurology},
  volume = {13},
  pages = {846690},
  publisher = {{Frontiers Media SA}}
}

@article{cite4,
  title = {European {{Stroke Organisation}} ({{ESO}}) Guidelines on Intravenous Thrombolysis for Acute Ischaemic Stroke},
  author = {Berge, Eivind and Whiteley, William and Audebert, Heinrich and De Marchis, Gian Marco and Fonseca, Ana Catarina and Padiglioni, Chiara and Pérez de la Ossa, Natalia and Strbian, Daniel and Tsivgoulis, Georgios and Turc, Guillaume},
  date = {2021},
  journaltitle = {European Stroke Journal},
  volume = {6},
  number = {1},
  pages = {I--LXII},
  publisher = {{SAGE Publications Sage UK: London, England}}
}

@article{cite5,
  title = {European Stroke Organisation ({{ESO}})-{{European}} Society for Minimally Invasive Neurological Therapy ({{ESMINT}}) Guidelines on Mechanical Thrombectomy in Acute Ischemic Stroke},
  author = {Turc, Guillaume and Bhogal, Pervinder and Fischer, Urs and Khatri, Pooja and Lobotesis, Kyriakos and Mazighi, Mikaël and Schellinger, Peter D and Toni, Danilo and De Vries, Joost and White, Philip and others},
  date = {2019},
  journaltitle = {Journal of Neurointerventional Surgery},
  volume = {11},
  number = {8},
  pages = {535--538},
  publisher = {{British Medical Journal Publishing Group}}
}

@article{cite6,
  title = {Mobile Stroke Units: {{Current}} Evidence and Impact},
  author = {Hariharan, Praveen and Tariq, Muhammad Bilal and Grotta, James C and Czap, Alexandra L},
  date = {2022},
  journaltitle = {Current Neurology and Neuroscience Reports},
  volume = {22},
  number = {1},
  pages = {71--81},
  publisher = {{Springer}}
}

@article{cite7,
  title = {Mobile Stroke Units: Evidence, Gaps, and next Steps},
  author = {Navi, Babak B and Audebert, Heinrich J and Alexandrov, Anne W and Cadilhac, Dominique A and Grotta, James C and {PRESTO (Prehospital Stroke Treatment Organization) Writing Group}},
  date = {2022},
  journaltitle = {Stroke},
  shortjournal = {Stroke},
  volume = {53},
  number = {6},
  pages = {2103--2113},
  publisher = {{American Heart Association}}
}

@article{cite8,
  title = {Development and Validation of a Dispatcher Identification Algorithm for Stroke Emergencies},
  author = {Krebes, Sebastian and Ebinger, Martin and Baumann, André M and Kellner, Philipp A and Rozanski, Michal and Doepp, Florian and Sobesky, Jan and Gensecke, Thomas and Leidel, Bernd A and Malzahn, Uwe and others},
  date = {2012},
  journaltitle = {Stroke},
  shortjournal = {Stroke},
  volume = {43},
  number = {3},
  pages = {776--781},
  publisher = {{American Heart Association}}
}

@article{cite9,
  title = {Feasibility Study to Assess the Use of the {{Cincinnati}} Stroke Scale by Emergency Medical Dispatchers: A Pilot Study},
  author = {Govindarajan, Prasanthi and Desouza, Natalie T and Pierog, Jessica and Ghilarducci, David and Johnston, S Claiborne},
  date = {2012},
  journaltitle = {Emergency Medicine Journal},
  volume = {29},
  number = {10},
  pages = {848--850},
  publisher = {{BMJ Publishing Group Ltd and the British Association for Accident …}}
}

@misc{clanuwat_deep_2018,
  title = {Deep {{Learning}} for {{Classical Japanese Literature}}},
  author = {Clanuwat, Tarin and Bober-Irizar, Mikel and Kitamoto, Asanobu and Lamb, Alex and Yamamoto, Kazuaki and Ha, David},
  date = {2018-12-03},
  eprint = {1812.01718},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1812.01718},
  urldate = {2021-02-09},
  abstract = {Much of machine learning research focuses on producing models which perform well on benchmark tasks, in turn improving our understanding of the challenges associated with those tasks. From the perspective of ML researchers, the content of the task itself is largely irrelevant, and thus there have increasingly been calls for benchmark tasks to more heavily focus on problems which are of social or cultural relevance. In this work, we introduce Kuzushiji-MNIST, a dataset which focuses on Kuzushiji (cursive Japanese), as well as two larger, more challenging datasets, Kuzushiji-49 and Kuzushiji-Kanji. Through these datasets, we wish to engage the machine learning community into the world of classical Japanese literature.},
  langid = {english}
}

@online{clark_electra_2020,
  title = {{{ELECTRA}}: {{Pre-training Text Encoders}} as {{Discriminators Rather Than Generators}}},
  shorttitle = {{{ELECTRA}}},
  author = {Clark, Kevin and Luong, Minh-Thang and Le, Quoc V. and Manning, Christopher D.},
  date = {2020-03-23},
  eprint = {2003.10555},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2003.10555},
  urldate = {2023-09-23},
  abstract = {Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute.},
  pubstate = {preprint}
}

@misc{clark_improving_2016,
  title = {Improving {{Coreference Resolution}} by {{Learning Entity-Level Distributed Representations}}},
  author = {Clark, Kevin and Manning, Christopher D.},
  date = {2016},
  eprint = {1606.01323},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1606.01323},
  urldate = {2018-05-02},
  abstract = {A long-standing challenge in coreference resolution has been the incorporation of entity-level information - features defined over clusters of mentions instead of mention pairs. We present a neural network based coreference system that produces high-dimensional vector representations for pairs of coreference clusters. Using these representations, our system learns when combining clusters is desirable. We train the system with a learning-to-search algorithm that teaches it which local decisions (cluster merges) will lead to a high-scoring final coreference partition. The system substantially outperforms the current state-of-the-art on the English and Chinese portions of the CoNLL 2012 Shared Task dataset despite using few hand-engineered features.},
  isbn = {9781510827585}
}

@article{clark_learning_2015,
  title = {Learning {{Knowledge Graphs}} for {{Question Answering}} through {{Conversational Dialog}}},
  author = {Clark, Peter and Hixon, Ben},
  date = {2015},
  journaltitle = {The Annual Conference of the North American Chapter of the ACL},
  pages = {851--861},
  url = {http://www.aclweb.org/anthology/N15-1086},
  urldate = {2018-06-13},
  abstract = {We describe how a question-answering system can learn about its domain from conver-sational dialogs. Our system learns to relate concepts in science questions to propositions in a fact corpus, stores new concepts and re-lations in a knowledge graph (KG), and uses the graph to solve questions. We are the first to acquire knowledge for question-answering from open, natural language dialogs without a fixed ontology or domain model that predeter-mines what users can say. Our relation-based strategies complete more successful dialogs than a query expansion baseline, our task-driven relations are more effective for solving science questions than relations from general knowledge sources, and our method is practical enough to generalize to other domains.},
  isbn = {9781941643495}
}

@misc{clevert_fast_2015,
  title = {Fast and {{Accurate Deep Network Learning}} by {{Exponential Linear Units}} ({{ELUs}})},
  author = {Clevert, Djork-Arné and Unterthiner, Thomas and Hochreiter, Sepp},
  date = {2015},
  eprint = {1511.07289},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1511.07289},
  urldate = {2018-04-19},
  abstract = {We introduce the "exponential linear unit" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PReLUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However, ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore, ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks significantly outperform ReLU networks with batch normalization while batch normalization does not improve ELU networks. ELU networks are among the top 10 reported CIFAR-10 results and yield the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10\% classification error for a single crop, single model network.},
  isbn = {9781614996712}
}

@software{coalson_free_2019,
  title = {Free {{Lossless Audio Encoding}} ({{FLAC}})},
  author = {Coalson, Josh and family=Castro Lopo, given=Erik, prefix=de, useprefix=false},
  date = {2019-08},
  url = {https://xiph.org/flac/},
  abstract = {FLAC stands for Free Lossless Audio Codec, an audio format similar to MP3, but lossless, meaning that audio is compressed in FLAC without any loss in quality. This is similar to how Zip works, except with FLAC you will get much better compression because it is designed specifically for audio, and you can play back compressed FLAC files in your favorite player (or your car or home stereo, see supported devices) just like you would an MP3 file. FLAC stands out as the fastest and most widely supported lossless audio codec, and the only one that at once is non-proprietary, is unencumbered by patents, has an open-source reference implementation, has a well documented format and API, and has several other independent implementations.},
  organization = {{xiph.org}},
  version = {1.3.3}
}

@thesis{cohen_equivariant_2021,
  type = {phdthesis},
  title = {Equivariant {{Convolutional Networks}}},
  author = {Cohen, Taco},
  date = {2021},
  institution = {{University of Amsterdam}},
  location = {{Amsterdam, Netherlands}}
}

@misc{coles_quantum_2018,
  title = {Quantum {{Algorithm Implementations}} for {{Beginners}}},
  author = {Coles, Patrick J. and Eidenbenz, Stephan and Pakin, Scott and Adedoyin, Adetokunbo and Ambrosiano, John and Anisimov, Petr and Casper, William and Chennupati, Gopinath and Coffrin, Carleton and Djidjev, Hristo and Gunter, David and Karra, Satish and Lemons, Nathan and Lin, Shizeng and Lokhov, Andrey and Malyzhenkov, Alexander and Mascarenas, David and Mniszewski, Susan and Nadiga, Balu and O'Malley, Dan and Oyen, Diane and Prasad, Lakshman and Roberts, Randy and Romero, Phil and Santhi, Nandakishore and Sinitsyn, Nikolai and Swart, Pieter and Vuffray, Marc and Wendelberger, Jim and Yoon, Boram and Zamora, Richard and Zhu, Wei},
  date = {2018},
  eprint = {1804.03719},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1804.03719},
  urldate = {2018-05-08},
  abstract = {As quantum computers have become available to the general public, the need has arisen to train a cohort of quantum programmers, many of whom have been developing classic computer programs for most of their career. While currently available quantum computers have less than 100 qubits, quantum computer hardware is widely expected to grow in terms of qubit counts, quality, and connectivity. Our article aims to explain the principles of quantum programming, which are quite different from classical programming, with straight-forward algebra that makes understanding the underlying quantum mechanics optional (but still fascinating). We give an introduction to quantum computing algorithms and their implementation on real quantum hardware. We survey 20 different quantum algorithms, attempting to describe each in a succintc and self-contained fashion; we show how they are implemented on IBM's quantum computer; and in each case we discuss the results of the implementation with respect to differences of the simulator and the actual hardware runs. This article introduces computer scientists and engineers to quantum algorithms and provides a blueprint for their implementations.}
}

@inproceedings{collier_vaes_2021,
  title = {{{VAEs}} in the {{Presence}} of {{Missing Data}}},
  booktitle = {Proceedings of the {{ICML Workshop}} on the {{Art}} of {{Learning}} with {{Missing Values}} ({{Artemiss}})},
  author = {Collier, Mark and Nazabal, Alfredo and Williams, Christopher K. I.},
  date = {2021-03-21},
  eprint = {2006.05301},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2006.05301},
  urldate = {2022-01-31},
  abstract = {Real world datasets often contain entries with missing elements e.g. in a medical dataset, a patient is unlikely to have taken all possible diagnostic tests. Variational Autoencoders (VAEs) are popular generative models often used for unsupervised learning. Despite their widespread use it is unclear how best to apply VAEs to datasets with missing data. We develop a novel latent variable model of a corruption process which generates missing data, and derive a corresponding tractable evidence lower bound (ELBO). Our model is straightforward to implement, can handle both missing completely at random (MCAR) and missing not at random (MNAR) data, scales to high dimensional inputs and gives both the VAE encoder and decoder principled access to indicator variables for whether a data element is missing or not. On the MNIST and SVHN datasets we demonstrate improved marginal log-likelihood of observed data and better missing data imputation, compared to existing approaches.},
  langid = {english}
}

@article{collins_loglinear_,
  title = {Log-{{Linear Models}}, {{MEMMs}}, and {{CRFs}}},
  author = {Collins, Michael},
  pages = {11},
  langid = {english}
}

@article{collobert_fast_nodate,
  title = {Fast {{Semantic Extraction Using}} a {{Novel Neural Network Architecture}}},
  author = {Collobert, Ronan and Weston, Jason},
  url = {http://www.lsi.upc.edu/},
  urldate = {2018-07-16},
  abstract = {We describe a novel neural network architecture for the problem of semantic role labeling. Many current solutions are complicated , consist of several stages and hand-built features, and are too slow to be applied as part of real applications that require such semantic labels, partly because of their use of a syntactic parser (Pradhan et al., 2004; Gildea and Jurafsky, 2002). Our method instead learns a direct mapping from source sentence to semantic tags for a given predicate without the aid of a parser or a chun-ker. Our resulting system obtains accuracies comparable to the current state-of-the-art at a fraction of the computational cost.}
}

@article{collobert_natural_2011,
  title = {Natural {{Language Processing}} (Almost) from {{Scratch}}},
  author = {Collobert, Ronan and Weston, Jason and Bottou, Léon and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel P.},
  date = {2011},
  journaltitle = {Computing Research Repository},
  volume = {abs/1103.0},
  pages = {1--34},
  abstract = {We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.}
}

@inproceedings{collobert_unified_2008,
  title = {A Unified Architecture for Natural Language Processing},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Collobert, Ronan and Weston, Jason},
  date = {2008},
  eprint = {2975184},
  eprinttype = {pmid},
  pages = {160--167},
  issn = {07224028},
  doi = {10.1145/1390156.1390177},
  url = {http://portal.acm.org/citation.cfm?doid=1390156.1390177},
  abstract = {We describe a single convolutional neural net- work architecture that, given a sentence, out- puts a host of language processing predic- tions: part-of-speech tags, chunks, named en- tity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semanti- cally) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data ex- cept the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the general- ization of the shared tasks, resulting in state- of-the-art performance.},
  isbn = {978-1-60558-205-4}
}

@misc{collobert_wav2letter_2016,
  title = {{{Wav2Letter}}: An {{End-to-End ConvNet-based Speech Recognition System}}},
  author = {Collobert, Ronan and Puhrsch, Christian and Synnaeve, Gabriel},
  date = {2016-09-11},
  eprint = {1609.03193},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1609.03193},
  urldate = {2018-05-29},
  abstract = {This paper presents a simple end-to-end model for speech recognition, combining a convolutional network based acoustic model and a graph decoding. It is trained to output letters, with transcribed speech, without the need for force alignment of phonemes. We introduce an automatic segmentation criterion for training from sequence annotation without alignment that is on par with CTC while being simpler. We show competitive results in word error rate on the Librispeech corpus with MFCC features, and promising results from raw waveform.}
}

@article{combes_learning_2018,
  title = {On the {{Learning Dynamics}} of {{Deep Neural Networks}}},
  author = {family=Combes, given=Remi Tachet, prefix=des, useprefix=false and Pezeshki, Mohammad and Shabanian, Samira and Courville, Aaron and Bengio, Yoshua},
  date = {2018-09-18},
  url = {https://arxiv.org/abs/1809.06848v1},
  urldate = {2018-09-25},
  abstract = {While a lot of progress has been made in recent years, the dynamics of learning in deep nonlinear neural networks remain to this day largely misunderstood. In this work, we study the case of binary classification and prove various properties of learning in such networks under strong assumptions such as linear separability of the data. Extending existing results from the linear case, we confirm empirical observations by proving that the classification error also follows a sigmoidal shape in nonlinear architectures. We show that given proper initialization, learning expounds parallel independent modes and that certain regions of parameter space might lead to failed training. We also demonstrate that input norm and features’ frequency in the dataset lead to distinct convergence speeds which might shed some light on the generalization capabilities of deep neural networks. We provide a comparison between the dynamics of learning with cross-entropy and hinge losses, which could prove useful to understand recent progress in the training of generative adversarial networks. Finally, we identify a phenomenon that we baptize gradient starvation where the most frequent features in a dataset prevent the learning of other less frequent but equally informative features.},
  langid = {english}
}

@online{conneau_unsupervised_2020,
  title = {Unsupervised {{Cross-lingual Representation Learning}} for {{Speech Recognition}}},
  author = {Conneau, Alexis and Baevski, Alexei and Collobert, Ronan and Mohamed, Abdelrahman and Auli, Michael},
  date = {2020-12-15},
  eprint = {2006.13979},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2006.13979},
  urldate = {2021-10-29},
  abstract = {This paper presents XLSR which learns cross-lingual speech representations by pretraining a single model from the raw waveform of speech in multiple languages. We build on wav2vec 2.0 which is trained by solving a contrastive task over masked latent speech representations and jointly learns a quantization of the latents shared across languages. The resulting model is fine-tuned on labeled data and experiments show that cross-lingual pretraining significantly outperforms monolingual pretraining. On the CommonVoice benchmark, XLSR shows a relative phoneme error rate reduction of 72\% compared to the best known results. On BABEL, our approach improves word error rate by 16\% relative compared to a comparable system. Our approach enables a single multilingual speech recognition model which is competitive to strong individual models. Analysis shows that the latent discrete speech representations are shared across languages with increased sharing for related languages. We hope to catalyze research in low-resource speech understanding by releasing XLSR-53, a large model pretrained in 53 languages.},
  pubstate = {preprint}
}

@inproceedings{conneau_word_2018,
  title = {Word Translation without Parallel Data},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Conneau, Alexis and Lample, Guillaume and Ranzato, Marc'Aurelio and Denoyer, Ludovic and Jégou, Hervé},
  date = {2018}
}

@misc{conti_improving_2017,
  title = {Improving {{Exploration}} in {{Evolution Strategies}} for {{Deep Reinforcement Learning}} via a {{Population}} of {{Novelty-Seeking Agents}}},
  author = {Conti, Edoardo and Madhavan, Vashisht and Such, Felipe Petroski and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
  date = {2017-12-18},
  eprint = {1712.06560},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1712.06560},
  urldate = {2018-01-03},
  abstract = {Evolution strategies (ES) are a family of black-box optimization algorithms able to train deep neural networks roughly as well as Q-learning and policy gradient methods on challenging deep reinforcement learning (RL) problems, but are much faster (e.g. hours vs. days) because they parallelize better. However, many RL problems require directed exploration because they have reward functions that are sparse or deceptive (i.e. contain local optima), and it is not known how to encourage such exploration with ES. Here we show that algorithms that have been invented to promote directed exploration in small-scale evolved neural networks via populations of exploring agents, specifically novelty search (NS) and quality diversity (QD) algorithms, can be hybridized with ES to improve its performance on sparse or deceptive deep RL tasks, while retaining scalability. Our experiments confirm that the resultant new algorithms, NS-ES and a version of QD we call NSR-ES, avoid local optima encountered by ES to achieve higher performance on tasks ranging from playing Atari to simulated robots learning to walk around a deceptive trap. This paper thus introduces a family of fast, scalable algorithms for reinforcement learning that are capable of directed exploration. It also adds this new family of exploration algorithms to the RL toolbox and raises the interesting possibility that analogous algorithms with multiple simultaneous paths of exploration might also combine well with existing RL algorithms outside ES.}
}

@online{cook_outlier_2020,
  title = {Outlier {{Detection}} through {{Null Space Analysis}} of {{Neural Networks}}},
  author = {Cook, Matthew and Zare, Alina and Gader, Paul},
  date = {2020-07-02},
  eprint = {2007.01263},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2007.01263},
  urldate = {2023-09-17},
  abstract = {Many machine learning classification systems lack competency awareness. Specifically, many systems lack the ability to identify when outliers (e.g., samples that are distinct from and not represented in the training data distribution) are being presented to the system. The ability to detect outliers is of practical significance since it can help the system behave in an reasonable way when encountering unexpected data. In prior work, outlier detection is commonly carried out in a processing pipeline that is distinct from the classification model. Thus, for a complete system that incorporates outlier detection and classification, two models must be trained, increasing the overall complexity of the approach. In this paper we use the concept of the null space to integrate an outlier detection method directly into a neural network used for classification. Our method, called Null Space Analysis (NuSA) of neural networks, works by computing and controlling the magnitude of the null space projection as data is passed through a network. Using these projections, we can then calculate a score that can differentiate between normal and abnormal data. Results are shown that indicate networks trained with NuSA retain their classification performance while also being able to detect outliers at rates similar to commonly used outlier detection algorithms.},
  pubstate = {preprint}
}

@online{cordonnier_relationship_2020,
  title = {On the {{Relationship}} between {{Self-Attention}} and {{Convolutional Layers}}},
  author = {Cordonnier, Jean-Baptiste and Loukas, Andreas and Jaggi, Martin},
  date = {2020-01-10},
  eprint = {1911.03584},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1911.03584},
  urldate = {2022-10-10},
  abstract = {Recent trends of incorporating attention mechanisms in vision have led researchers to reconsider the supremacy of convolutional layers as a primary building block. Beyond helping CNNs to handle long-range dependencies, Ramachandran et al. (2019) showed that attention can completely replace convolution and achieve state-of-the-art performance on vision tasks. This raises the question: do learned attention layers operate similarly to convolutional layers? This work provides evidence that attention layers can perform convolution and, indeed, they often learn to do so in practice. Specifically, we prove that a multi-head self-attention layer with sufficient number of heads is at least as expressive as any convolutional layer. Our numerical experiments then show that self-attention layers attend to pixel-grid patterns similarly to CNN layers, corroborating our analysis. Our code is publicly available.},
  pubstate = {preprint}
}

@article{cortes_supportvector_1995,
  title = {Support-{{Vector Networks}}},
  author = {Cortes, Corinna and Vapnik, Vladimir},
  date = {1995},
  journaltitle = {Journal of Machine Learning},
  volume = {20},
  number = {3},
  pages = {273--297},
  abstract = {Oil/water partition coefficient (log P) is one of the key points for lead compound to be drug. In silico log P models based solely on chemical structures have become an important part of modern drug discovery. Here, we report support vector machines, radial basis function neural networks, and multiple linear regression methods to investigate the correlation between partition coefficient and physico-chemical descriptors for a large data set of compounds. The correlation coefficient r (2) between experimental and predicted log P for training and test sets by support vector machines, radial basis function neural networks, and multiple linear regression is 0.92, 0.90, and 0.88, respectively. The results show that non-linear support vector machines derives statistical models that have better prediction ability than those of radial basis function neural networks and multiple linear regression methods. This indicates that support vector machines can be used as an alternative modeling tool for quantitative structure-property/activity relationships studies.}
}

@misc{coucke_snips_2018,
  title = {Snips {{Voice Platform}}: An Embedded {{Spoken Language Understanding}} System for Private-by-Design Voice Interfaces},
  shorttitle = {Snips {{Voice Platform}}},
  author = {Coucke, Alice and Saade, Alaa and Ball, Adrien and Bluche, Théodore and Caulier, Alexandre and Leroy, David and Doumouro, Clément and Gisselbrecht, Thibault and Caltagirone, Francesco and Lavril, Thibaut and Primet, Maël and Dureau, Joseph},
  date = {2018-05-25},
  eprint = {1805.10190},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1805.10190},
  urldate = {2019-07-22},
  abstract = {This paper presents the machine learning architecture of the Snips Voice Platform, a software solution to perform Spoken Language Understanding on microprocessors typical of IoT devices. The embedded inference is fast and accurate while enforcing privacy by design, as no personal user data is ever collected. Focusing on Automatic Speech Recognition and Natural Language Understanding, we detail our approach to training high-performance Machine Learning models that are small enough to run in real-time on small devices. Additionally, we describe a data generation procedure that provides sufficient, high-quality training data without compromising user privacy.},
  langid = {english}
}

@misc{courbariaux_binarized_2016,
  title = {Binarized {{Neural Networks}}: {{Training Deep Neural Networks}} with {{Weights}} and {{Activations Constrained}} to +1 or -1},
  author = {Courbariaux, Matthieu and Hubara, Itay and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
  date = {2016-02-08},
  eprint = {23554596},
  eprinttype = {pmid},
  url = {http://arxiv.org/abs/1602.02830},
  abstract = {We introduce a method to train Binarized Neural Networks (BNNs) - neural networks with binary weights and activations at run-time. At training-time the binary weights and activations are used for computing the parameters gradients. During the forward pass, BNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations, which is expected to substantially improve power-efficiency. To validate the effectiveness of BNNs we conduct two sets of experiments on the Torch7 and Theano frameworks. On both, BNNs achieved nearly state-of-the-art results over the MNIST, CIFAR-10 and SVHN datasets. Last but not least, we wrote a binary matrix multiplication GPU kernel with which it is possible to run our MNIST BNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The code for training and running our BNNs is available on-line.},
  isbn = {9781510829008}
}

@article{cover_nearest_1967,
  title = {Nearest Neighbor Pattern Classification},
  author = {Cover, Thomas and Hart, Peter},
  date = {1967},
  journaltitle = {IEEE Transactions on Information Theory},
  volume = {13},
  number = {1},
  pages = {21--27},
  publisher = {{IEEE}}
}

@misc{crabbe_explaining_2021,
  title = {Explaining {{Time Series Predictions}} with {{Dynamic Masks}}},
  author = {Crabbé, Jonathan and family=Schaar, given=Mihaela, prefix=van der, useprefix=true},
  date = {2021-06-09},
  eprint = {2106.05303},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2106.05303},
  urldate = {2021-06-15},
  abstract = {How can we explain the predictions of a machine learning model? When the data is structured as a multivariate time series, this question induces additional difficulties such as the necessity for the explanation to embody the time dependency and the large number of inputs. To address these challenges, we propose dynamic masks (Dynamask). This method produces instance-wise importance scores for each feature at each time step by fitting a perturbation mask to the input sequence. In order to incorporate the time dependency of the data, Dynamask studies the effects of dynamic perturbation operators. In order to tackle the large number of inputs, we propose a scheme to make the feature selection parsimonious (to select no more feature than necessary) and legible (a notion that we detail by making a parallel with information theory). With synthetic and real-world data, we demonstrate that the dynamic underpinning of Dynamask, together with its parsimony, offer a neat improvement in the identification of feature importance over time. The modularity of Dynamask makes it ideal as a plug-in to increase the transparency of a wide range of machine learning models in areas such as medicine and finance, where time series are abundant.},
  langid = {english}
}

@inproceedings{cremer_inference_2018,
  title = {Inference {{Suboptimality}} in {{Variational Autoencoders}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Cremer, Chris and Li, Xuechen and Duvenaud, David},
  editor = {Dy, Jennifer and Krause, Andreas},
  date = {2018-07-10/2018-07-15},
  series = {Proceedings of Machine Learning Research},
  volume = {80},
  pages = {1078--1086},
  publisher = {{PMLR}},
  location = {{Stockholmsmässan, Stockholm, Sweden}},
  url = {http://proceedings.mlr.press/v80/cremer18a.html},
  urldate = {2020-01-14},
  abstract = {Amortized inference allows latent-variable models trained via variational learning to scale to large datasets. The quality of approximate inference is determined by two factors: a) the capacity of the variational distribution to match the true posterior and b) the ability of the recognition network to produce good variational parameters for each datapoint. We examine approximate inference in variational autoencoders in terms of these factors. We find that divergence from the true posterior is often due to imperfect recognition networks, rather than the limited complexity of the approximating distribution. We show that this is due partly to the generator learning to accommodate the choice of approximation. Furthermore, we show that the parameters used to increase the expressiveness of the approximation play a role in generalizing inference rather than simply improving the complexity of the approximation.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}}
}

@article{cremer_reinterpreting_2017,
  title = {Reinterpreting {{Importance-Weighted Autoencoders}}},
  author = {Cremer, Chris and Morris, Quaid and Duvenaud, David},
  date = {2017},
  pages = {6},
  abstract = {The standard interpretation of importance-weighted autoencoders is that they maximize a tighter lower bound on the marginal likelihood. We give an alternate interpretation of this procedure: that it optimizes the standard variational lower bound, but using a more complex distribution. We formally derive this result, and visualize the implicit importance-weighted approximate posterior.},
  langid = {english}
}

@misc{cuayahuitl_strategic_2015,
  title = {Strategic {{Dialogue Management}} via {{Deep Reinforcement Learning}}},
  author = {Cuayáhuitl, Heriberto and Keizer, Simon and Lemon, Oliver},
  date = {2015-11-25},
  eprint = {1511.08099},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1511.08099},
  urldate = {2019-06-11},
  abstract = {Artificially intelligent agents equipped with strategic skills that can negotiate during their interactions with other natural or artificial agents are still underdeveloped. This paper describes a successful application of Deep Reinforcement Learning (DRL) for training intelligent agents with strategic conversational skills, in a situated dialogue setting. Previous studies have modelled the behaviour of strategic agents using supervised learning and traditional reinforcement learning techniques, the latter using tabular representations or learning with linear function approximation. In this study, we apply DRL with a high-dimensional state space to the strategic board game of Settlers of Catan—where players can offer resources in exchange for others and they can also reply to offers made by other players. Our experimental results report that the DRL-based learnt policies significantly outperformed several baselines including random, rule-based, and supervised-based behaviours. The DRL-based policy has a 53\% win rate versus 3 automated players (‘bots’), whereas a supervised player trained on a dialogue corpus in this setting achieved only 27\%, versus the same 3 bots. This result supports the claim that DRL is a promising framework for training dialogue systems, and strategic agents with negotiation abilities.},
  langid = {english}
}

@inproceedings{cuervo_contrastive_2022,
  title = {Contrastive Prediction Strategies for Unsupervised Segmentation and Categorization of Phonemes and Words},
  booktitle = {Proceedings of the {{International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Cuervo, Santiago and Grabias, Maciej and Chorowski, Jan and Ciesielski, Grzegorz and Łańcucki, Adrian and Rychlikowski, Paweł and Marxer, Ricard},
  date = {2022},
  pages = {3189--3193},
  publisher = {{IEEE}}
}

@article{cui_multilingual_2015,
  title = {Multilingual Representations for Low Resource Speech Recognition and Keyword Search},
  author = {Cui, Jia and Kingsbury, Brian and Ramabhadran, Bhuvana and Sethy, Abhinav and Audhkhasi, Kartik and Cui, Xiaodong and Kislal, Ellen and Mangu, Lidia and Nußbaum-Thom, Markus and Picheny, Michael and Tüske, Zoltán and Golik, Pavel and Schlüter, Ralf and Ney, Hermann and Gales, Mark John Francis and Knill, Kate and Ragni, Anton and Wang, Haipeng and Woodland, Philip C.},
  date = {2015},
  journaltitle = {2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)}
}

@online{cukier_three_2023,
  title = {Three {{Variations}} on {{Variational Autoencoders}}},
  author = {Cukier, R. I.},
  date = {2023-04-05},
  eprint = {2212.04451},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2212.04451},
  urldate = {2023-10-18},
  abstract = {Variational autoencoders (VAEs) are one class of generative probabilistic latent-variable models designed for inference based on known data. We develop three variations on VAEs by introducing a second parameterized encoder/decoder pair and, for one variation, an additional fixed encoder. The parameters of the encoders/decoders are to be learned with a neural network. The fixed encoder is obtained by probabilistic-PCA. The variations are compared to the Evidence Lower Bound (ELBO) approximation to the original VAE. One variation leads to an Evidence Upper Bound (EUBO) that can be used in conjunction with the original ELBO to interrogate the convergence of the VAE.},
  pubstate = {preprint}
}

@misc{cuneiformdigitallibraryinitiativecdli_mct_2005,
  title = {{{MCT}} 038, {{Plimpton}} 322 Artifact Entry},
  author = {{Cuneiform Digital Library Initiative (CDLI)}},
  date = {2005-02-01},
  url = {https://cdli.ucla.edu/P254790},
  abstract = {Mathematical tablet excavated in Larsa (mod. Tell as-Senkereh), dated to the Old Babylonian (ca. 1900-1600 BC) period and now kept in Rare Book and Manuscript Library, Columbia University, New York, New York, USA}
}

@article{dahl_context-dependent_2012,
  title = {Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition},
  author = {Dahl, George E. and Yu, Dong and Deng, Li and Acero, Alex},
  date = {2012},
  journaltitle = {IEEE Transactions on Audio, Speech, and Language Processing},
  volume = {20},
  number = {1},
  pages = {30--42},
  publisher = {{IEEE}}
}

@misc{dai_semisupervised_2015,
  title = {Semi-Supervised {{Sequence Learning}}},
  author = {Dai, Andrew M. and Le, Quoc V.},
  date = {2015},
  eprint = {414454},
  eprinttype = {pmid},
  url = {http://arxiv.org/abs/1511.01432},
  urldate = {2018-05-02},
  abstract = {We present two approaches that use unlabeled data to improve sequence learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a "pretraining" step for a later supervised sequence learning algorithm. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after being pretrained with the two approaches are more stable and generalize better. With pretraining, we are able to train long short term memory recurrent networks up to a few hundred timesteps, thereby achieving strong performance in many text classification tasks, such as IMDB, DBpedia and 20 Newsgroups.}
}

@misc{dai_transformerxl_2019,
  title = {Transformer-{{XL}}: {{Attentive Language Models Beyond}} a {{Fixed-Length Context}}},
  shorttitle = {Transformer-{{XL}}},
  author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V. and Salakhutdinov, Ruslan},
  date = {2019-06-02},
  eprint = {1901.02860},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1901.02860},
  urldate = {2021-04-28},
  abstract = {Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, TransformerXL learns dependency that is 80\% longer than RNNs and 450\% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-ofthe-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch1.},
  langid = {english}
}

@inproceedings{dai_very_2017,
  title = {Very Deep Convolutional Neural Networks for Raw Waveforms},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Dai, W and Dai, Chia and Qu, Shuhui and Li, Juncheng and Das, Samarjit},
  date = {2017},
  eprint = {1610.00087},
  eprinttype = {arxiv},
  pages = {421--425},
  issn = {19909772},
  doi = {10.1109/ICASSP.2017.7952190},
  abstract = {Learning acoustic models directly from the raw waveform data with minimal processing is challenging. Current waveform-based models have generally used very few (\textasciitilde 2) convolutional layers, which might be insufficient for building high-level discriminative features. In this work, we propose very deep convolutional neural networks (CNNs) that directly use time-domain waveforms as inputs. Our CNNs, with up to 34 weight layers, are efficient to optimize over very long sequences (e.g., vector of size 32000), necessary for processing acoustic waveforms. This is achieved through batch normalization, residual learning, and a careful design of down-sampling in the initial layers. Our networks are fully convolutional, without the use of fully connected layers and dropout, to maximize representation learning. We use a large receptive field in the first convolutional layer to mimic bandpass filters, but very small receptive fields subsequently to control the model capacity. We demonstrate the performance gains with the deeper models. Our evaluation shows that the CNN with 18 weight layers outperforms the CNN with 3 weight layers by over 15\% in absolute accuracy for an environmental sound recognition task and is competitive with the performance of models using log-mel features.},
  isbn = {VO -}
}

@incollection{dalberto_using_2008,
  title = {Using {{Recursion}} to {{Boost ATLAS}}’s {{Performance}}},
  booktitle = {High-{{Performance Computing}}},
  author = {D’Alberto, Paolo and Nicolau, Alexandru},
  editor = {Labarta, Jesús and Joe, Kazuki and Sato, Toshinori},
  date = {2008},
  volume = {4759},
  pages = {142--151},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-77704-5_12},
  url = {http://link.springer.com/10.1007/978-3-540-77704-5_12},
  urldate = {2023-07-17},
  abstract = {We investigate the performance benefits of a novel recursive formulation of Strassen’s algorithm over highly tuned matrix-multiply (MM) routines, such as the widely used ATLAS for high-performance systems.},
  isbn = {978-3-540-77703-8 978-3-540-77704-5},
  langid = {english}
}

@inproceedings{damianou_deep_2013,
  title = {Deep {{Gaussian Processes}}},
  booktitle = {Proceedings of the 16th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}} ({{AISTATS}})},
  author = {Damianou, Andreas and Lawrence, Neil},
  editor = {Carvalho, Carlos M. and Ravikumar, Pradeep},
  date = {2013-04-29/2013-05-01},
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {31},
  pages = {207--215},
  publisher = {{PMLR}},
  location = {{Scottsdale, Arizona, USA}},
  url = {http://proceedings.mlr.press/v31/damianou13a.html},
  abstract = {In this paper we introduce deep Gaussian process (GP) models. Deep GPs are a deep belief network based on Gaussian process mappings. The data is modeled as the output of a multivariate GP. The inputs to that Gaussian process are then governed by another GP. A single layer model is equivalent to a standard GP or the GP latent variable model (GP-LVM). We perform inference in the model by approximate variational marginalization. This results in a strict lower bound on the marginal likelihood of the model which we use for model selection (number of layers and nodes per layer). Deep belief networks are typically applied to relatively large data sets using stochastic gradient descent for optimization. Our fully Bayesian treatment allows for the application of deep models even when data is scarce. Model selection by our variational bound shows that a five layer hierarchy is justified even when modelling a digit data set containing only 150 examples.}
}

@misc{damour_underspecification_2020,
  title = {Underspecification {{Presents Challenges}} for {{Credibility}} in {{Modern Machine Learning}}},
  author = {D'Amour, Alexander and Heller, Katherine and Moldovan, Dan and Adlam, Ben and Alipanahi, Babak and Beutel, Alex and Chen, Christina and Deaton, Jonathan and Eisenstein, Jacob and Hoffman, Matthew D. and Hormozdiari, Farhad and Houlsby, Neil and Hou, Shaobo and Jerfel, Ghassen and Karthikesalingam, Alan and Lucic, Mario and Ma, Yian and McLean, Cory and Mincu, Diana and Mitani, Akinori and Montanari, Andrea and Nado, Zachary and Natarajan, Vivek and Nielson, Christopher and Osborne, Thomas F. and Raman, Rajiv and Ramasamy, Kim and Sayres, Rory and Schrouff, Jessica and Seneviratne, Martin and Sequeira, Shannon and Suresh, Harini and Veitch, Victor and Vladymyrov, Max and Wang, Xuezhi and Webster, Kellie and Yadlowsky, Steve and Yun, Taedong and Zhai, Xiaohua and Sculley, D.},
  date = {2020-11-06},
  eprint = {2011.03395},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2011.03395},
  urldate = {2020-11-19},
  abstract = {ML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification as a key reason for these failures. An ML pipeline is underspecified when it can return many predictors with equivalently strong held-out performance in the training domain. Underspecification is common in modern ML pipelines, such as those based on deep learning. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain.},
  langid = {english}
}

@inproceedings{dangel_backpack_2020,
  title = {{{BackPACK}}: {{Packing}} More into Backprop},
  booktitle = {International Conference on Learning Representations ({{ICLR}})},
  author = {Dangel, Felix and Kunstner, Frederik and Hennig, Philipp},
  date = {2020}
}

@online{dangelo_outofdistribution_2022,
  title = {On Out-of-Distribution Detection with {{Bayesian}} Neural Networks},
  author = {D'Angelo, Francesco and Henning, Christian},
  date = {2022-02-21},
  eprint = {2110.06020},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2110.06020},
  urldate = {2023-09-19},
  abstract = {The question whether inputs are valid for the problem a neural network is trying to solve has sparked interest in out-of-distribution (OOD) detection. It is widely assumed that Bayesian neural networks (BNNs) are well suited for this task, as the endowed epistemic uncertainty should lead to disagreement in predictions on outliers. In this paper, we question this assumption and show that proper Bayesian inference with function space priors induced by neural networks does not necessarily lead to good OOD detection. To circumvent the use of approximate inference, we start by studying the infinite-width case, where Bayesian inference can be exact due to the correspondence with Gaussian processes. Strikingly, the kernels derived from common architectural choices lead to function space priors which induce predictive uncertainties that do not reflect the underlying input data distribution and are therefore unsuited for OOD detection. Importantly, we find the OOD behavior in this limiting case to be consistent with the corresponding finite-width case. To overcome this limitation, useful function space properties can also be encoded in the prior in weight space, however, this can currently only be applied to a specified subset of the domain and thus does not inherently extend to OOD data. Finally, we argue that a trade-off between generalization and OOD capabilities might render the application of BNNs for OOD detection undesirable in practice. Overall, our study discloses fundamental problems when naively using BNNs for OOD detection and opens interesting avenues for future research.},
  pubstate = {preprint}
}

@misc{dauphin_identifying_2014,
  title = {Identifying and Attacking the Saddle Point Problem in High-Dimensional Non-Convex Optimization},
  author = {Dauphin, Yann and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
  date = {2014},
  eprint = {1406.2572},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1406.2572},
  urldate = {2018-06-13},
  abstract = {A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance.}
}

@misc{daxberger_bayesian_2020,
  title = {Bayesian {{Variational Autoencoders}} for {{Unsupervised Out-of-Distribution Detection}}},
  author = {Daxberger, Erik and Hernández-Lobato, José Miguel},
  date = {2020-07-15},
  eprint = {1912.05651},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1912.05651},
  urldate = {2020-11-19},
  abstract = {Despite their successes, deep neural networks may make unreliable predictions when faced with test data drawn from a distribution different to that of the training data, constituting a major problem for AI safety. While this has recently motivated the development of methods to detect such out-of-distribution (OoD) inputs, a robust solution is still lacking. We propose a new probabilistic, unsupervised approach to this problem based on a Bayesian variational autoencoder model, which estimates a full posterior distribution over the decoder parameters using stochastic gradient Markov chain Monte Carlo, instead of fitting a point estimate. We describe how information-theoretic measures based on this posterior can then be used to detect OoD inputs both in input space and in the model’s latent space. We empirically demonstrate the effectiveness of our proposed approach.},
  langid = {english}
}

@article{dayan_helmholtz_1995,
  title = {The {{Helmholtz Machine}}},
  author = {Dayan, Peter and Hinton, Geoffrey E. and Neal, Radford M. and Zemel, Richard S.},
  date = {1995-09},
  journaltitle = {Neural Comput.},
  volume = {7},
  number = {5},
  pages = {889--904},
  publisher = {{MIT Press}},
  location = {{Cambridge, MA, USA}},
  issn = {0899-7667},
  doi = {10.1162/neco.1995.7.5.889},
  url = {https://doi.org/10.1162/neco.1995.7.5.889}
}

@article{DBLP:journals/corr/abs-1910-02760,
  title = {Negative Sampling in Variational Autoencoders},
  author = {Csiszárik, Adrián and Benko, Beatrix and Varga, Dániel},
  date = {2019},
  journaltitle = {CoRR},
  volume = {abs/1910.02760},
  eprint = {1910.02760},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1910.02760},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/journals/corr/abs-1910-02760.bib},
  timestamp = {Wed, 09 Oct 2019 14:07:58 +0200}
}

@article{de_jong_praat_2009,
  title = {Praat Script to Detect Syllable Nuclei and Measure Speech Rate Automatically},
  author = {family=Jong, given=Nivja H., prefix=de, useprefix=true and Wempe, Ton},
  date = {2009-05},
  journaltitle = {Behavior Research Methods},
  volume = {41},
  number = {2},
  pages = {385--390},
  issn = {1554-351X, 1554-3528},
  doi = {10/dwtv2n},
  url = {http://link.springer.com/10.3758/BRM.41.2.385},
  urldate = {2018-11-23},
  abstract = {In this article, we describe a method for automatically detecting syllable nuclei in order to measure speech rate without the need for a transcription. A script written in the software program Praat (Boersma \& Weenink, 2007) detects syllables in running speech. Peaks in intensity (dB) that are preceded and followed by dips in intensity are considered to be potential syllable nuclei. The script subsequently discards peaks that are not voiced. Testing the resulting syllable counts of this script on two corpora of spoken Dutch, we obtained high correlations between speech rate calculated from human syllable counts and speech rate calculated from automatically determined syllable counts. We conclude that a syllable count measured in this automatic fashion suffices to reliably assess and compare speech rates between participants and tasks.},
  langid = {english}
}

@article{debievre_atomic_1992,
  title = {'{{Atomic}} Weight': {{The}} Name, Its History, Definition, and Units},
  author = {family=Bièvre, given=Paul, prefix=de, useprefix=true and Peiser, H Steffen},
  date = {1992},
  journaltitle = {Pure and applied chemistry},
  volume = {64},
  number = {10},
  pages = {1535--1543},
  publisher = {{De Gruyter}}
}

@article{dehak_frontend_2011,
  title = {Front-{{End Factor Analysis}} for {{Speaker Verification}}},
  author = {Dehak, Najim and Kenny, Patrick J. and Dehak, Réda and Dumouchel, Pierre and Ouellet, Pierre},
  date = {2011-05},
  journaltitle = {IEEE Transactions on Audio, Speech, and Language Processing},
  volume = {19},
  number = {4},
  pages = {788--798},
  issn = {1558-7924},
  doi = {10.1109/TASL.2010.2064307},
  abstract = {This paper presents an extension of our previous work which proposes a new speaker representation for speaker verification. In this modeling, a new low-dimensional speaker- and channel-dependent space is defined using a simple factor analysis. This space is named the total variability space because it models both speaker and channel variabilities. Two speaker verification systems are proposed which use this new representation. The first system is a support vector machine-based system that uses the cosine kernel to estimate the similarity between the input data. The second system directly uses the cosine similarity as the final decision score. We tested three channel compensation techniques in the total variability space, which are within-class covariance normalization (WCCN), linear discriminate analysis (LDA), and nuisance attribute projection (NAP). We found that the best results are obtained when LDA is followed by WCCN. We achieved an equal error rate (EER) of 1.12\% and MinDCF of 0.0094 using the cosine distance scoring on the male English trials of the core condition of the NIST 2008 Speaker Recognition Evaluation dataset. We also obtained 4\% absolute EER improvement for both-gender trials on the 10 s-10 s condition compared to the classical joint factor analysis scoring.}
}

@inproceedings{dehak_language_2011,
  title = {Language Recognition via I-Vectors and Dimensionality Reduction},
  booktitle = {Annual {{Conference}} of the {{International Speech Communication Association}}},
  author = {Dehak, N. and Torres-Carrasquillo, P. and Reynolds, D. and Dehak, R.},
  date = {2011}
}

@article{dehak_support_nodate,
  title = {Support {{Vector Machines}} versus {{Fast Scoring}} in the {{Low-Dimensional Total Variability Space}} for {{Speaker Verification}}},
  author = {Dehak, Najim and Dehak, Reda and Kenny, Patrick and Brummer, Niko and Ouellet, Pierre and Dumouchel, Pierre},
  pages = {4},
  abstract = {This paper presents a new speaker verification system architecture based on Joint Factor Analysis (JFA) as feature extractor. In this modeling, the JFA is used to define a new low-dimensional space named the total variability factor space, instead of both channel and speaker variability spaces for the classical JFA. The main contribution in this approach, is the use of the cosine kernel in the new total factor space to design two different systems: the first system is Support Vector Machines based, and the second one uses directly this kernel as a decision score. This last scoring method makes the process faster and less computation complex compared to others classical methods. We tested several intersession compensation methods in total factors, and we found that the combination of Linear Discriminate Analysis and Within Class Covariance Normalization achieved the best performance. We achieved a remarkable results using fast scoring method based only on cosine kernel especially for male trials, we yield an EER of 1.12\% and MinDCF of 0.0094 on the English trials of the NIST 2008 SRE dataset.},
  langid = {english}
}

@book{deisenroth_mathematics_,
  title = {Mathematics for {{Machine Learning}}},
  author = {Deisenroth, Marc Peter and Faisal, A Aldo and Ong, Cheng Soon},
  publisher = {{Cambridge University Press}},
  url = {https://mml-book.github.io/},
  urldate = {2018-10-11},
  keywords = {★}
}

@article{dekens_comparative_2007,
  title = {A {{Comparative Study}} of {{Speech Rate Estimation Techniques}}},
  author = {Dekens, Tomas and Demol, Mike and Verhelst, Werner and Verhoeve, Piet},
  date = {2007},
  journaltitle = {Interspeech},
  pages = {4},
  abstract = {In this paper we evaluate the performance of 8 different speech rate estimators [1, 2, 3, 4, 5] previously described in the literature by applying them on a multilingual test database [6]. All the estimators show an underestimation at high speech rates and some also suffer from an overestimation at low speech rates. Overall the tested methods obtain high correlation coefficients with the reference speech rate. The Temporal Correlation and Selected Sub-band Correlation method (tcssbc), which uses sub-band and time domain correlation for detecting the number of vowels or diphthongs present in the speech signal, shows little errors and appears to be the most appropriate overall technique for speech rate estimation.},
  langid = {english}
}

@article{dempster_maximum_1977,
  title = {Maximum {{Likelihood}} from {{Incomplete Data Via}} the {{Em Algorithm}}},
  author = {Dempster, Arthur P and Laird, Nan M and Rubin, Donald B},
  date = {1977},
  journaltitle = {Journal of the Royal Statistical Society: Series B (methodological)},
  volume = {39},
  number = {1},
  pages = {1--22},
  publisher = {{Wiley Online Library}}
}

@misc{deng_binary_2010,
  title = {Binary {{Coding}} of {{Speech Spectrograms Using}} a {{Deep Auto-encoder}}},
  author = {Deng, L. and Seltzer, M. and Yu, D. and Acero, A. and Mohamed, A. and Hinton, G.},
  date = {2010},
  abstract = {This paper reports our recent exploration of the layer-by-layer learning strategy for training a multi-layer generative model of patches of speech spectrograms. The top layer of the generative model learns binary codes that can be used for efficient compression of speech and could also be used for scalable speech recognition or rapid speech content retrieval. Each layer of the generative model is fully connected to the layer below and the weights on these connections are pretrained efficiently by using the contrastive divergence approximation to the log likelihood gradient. After layer-bylayer pre-training we “unroll ” the generative model to form a deep auto-encoder, whose parameters are then fine-tuned using back-propagation. To reconstruct the full-length speech spectrogram, individual spectrogram segments predicted by their respective binary codes are combined using an overlapand-add method. Experimental results on speech spectrogram coding demonstrate that the binary codes produce a logspectral distortion that is approximately 2 dB lower than a subband vector quantization technique over the entire frequency range of wide-band speech.}
}

@inproceedings{deng_imagenet_2009,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  date = {2009},
  eprint = {21914436},
  eprinttype = {pmid},
  pages = {248--255},
  issn = {1063-6919},
  doi = {10.1109/CVPRW.2009.5206848},
  url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5206848},
  urldate = {2018-04-21},
  abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called \&\#x201C;ImageNet\&\#x201D;, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  isbn = {978-1-4244-3992-8}
}

@article{denker_transforming_1991,
  title = {Transforming {{Neural-Net Output Levels}} to {{Probability Distributions}}},
  author = {Denker, John S. and LeCun, Yann A.},
  date = {1991},
  journaltitle = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
  eprint = {11359-901120-05},
  eprinttype = {arxiv},
  pages = {853--859},
  doi = {10.1.1.32.7096},
  abstract = {(1) The outputs of a typical multi-output classification network do not satisfy the axioms of probability; probabilities should be positive and sum to one. This problem can be solved by treating the trained network as a preprocessor that produces a feature vector that can be further processed, for instance by classical statistical estimation techniques. (2) We present a method for computing the first two moments ofthe probability distribution indicating the range of outputs that are consistent with the input and the training data. It is particularly useful to combine these two ideas: we implement the ideas of section 1 using Parzen windows, where the shape and relative size of each window is computed using the ideas of section 2. This allows us to make contact between important theoretical ideas (e.g. the ensemble formalism) and practical techniques (e.g. back-prop). Our results also shed new light on and generalize the well-known "soft max" scheme.}
}

@online{denouden_improving_2018,
  title = {Improving {{Reconstruction Autoencoder Out-of-distribution Detection}} with {{Mahalanobis Distance}}},
  author = {Denouden, Taylor and Salay, Rick and Czarnecki, Krzysztof and Abdelzad, Vahdat and Phan, Buu and Vernekar, Sachin},
  date = {2018-12-06},
  eprint = {1812.02765},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1812.02765},
  urldate = {2023-09-10},
  abstract = {There is an increasingly apparent need for validating the classifications made by deep learning systems in safety-critical applications like autonomous vehicle systems. A number of recent papers have proposed methods for detecting anomalous image data that appear different from known inlier data samples, including reconstruction-based autoencoders. Autoencoders optimize the compression of input data to a latent space of a dimensionality smaller than the original input and attempt to accurately reconstruct the input using that compressed representation. Since the latent vector is optimized to capture the salient features from the inlier class only, it is commonly assumed that images of objects from outside of the training class cannot effectively be compressed and reconstructed. Some thus consider reconstruction error as a kind of novelty measure. Here we suggest that reconstruction-based approaches fail to capture particular anomalies that lie far from known inlier samples in latent space but near the latent dimension manifold defined by the parameters of the model. We propose incorporating the Mahalanobis distance in latent space to better capture these out-of-distribution samples and our results show that this method often improves performance over the baseline approach.},
  pubstate = {preprint}
}

@misc{desa_representation_2018,
  title = {Representation {{Tradeoffs}} for {{Hyperbolic Embeddings}}},
  author = {De Sa, Christopher and Gu, Albert and Ré, Christopher and Sala, Frederic},
  date = {2018-04-24},
  eprint = {1804.03329},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1804.03329},
  urldate = {2020-07-20},
  abstract = {Hyperbolic embeddings offer excellent quality with few dimensions when embedding hierarchical data structures like synonym or type hierarchies. Given a tree, we give a combinatorial construction that embeds the tree in hyperbolic space with arbitrarily low distortion without using optimization. On WordNet, our combinatorial embedding obtains a mean-average-precision of 0.989 with only two dimensions, while Nickel et al.’s recent construction obtains 0.87 using 200 dimensions. We provide upper and lower bounds that allow us to characterize the precision-dimensionality tradeoff inherent in any hyperbolic embedding. To embed general metric spaces, we propose a hyperbolic generalization of multidimensional scaling (h-MDS). We show how to perform exact recovery of hyperbolic points from distances, provide a perturbation analysis, and give a recovery result that allows us to reduce dimensionality. The h-MDS approach offers consistently low distortion even with few dimensions across several datasets. Finally, we extract lessons from the algorithms and theory above to design a PyTorch-based implementation that can handle incomplete information and is scalable.},
  langid = {english}
}

@inproceedings{desjardins_natural_2015,
  title = {Natural {{Neural Networks}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Neural Information Processing Systems}} ({{NIPS}})},
  author = {Desjardins, Guillaume and Simonyan, Karen and Pascanu, Razvan and Kavukcuoglu, Koray},
  date = {2015},
  eprint = {1507.00210},
  eprinttype = {arxiv},
  location = {{Montreal, Canada}},
  issn = {10495258},
  abstract = {We introduce Natural Neural Networks, a novel family of algorithms that speed up convergence by adapting their internal representation during training to improve conditioning of the Fisher matrix. In particular, we show a specific example that employs a simple and efficient reparametrization of the neural network weights by implicitly whitening the representation obtained at each layer, while preserving the feed-forward computation of the network. Such networks can be trained efficiently via the proposed Projected Natural Gradient Descent algorithm (PRONG), which amortizes the cost of these reparametrizations over many parameter updates and is closely related to the Mirror Descent online learning algorithm. We highlight the benefits of our method on both unsupervised and supervised learning tasks, and showcase its scalability by training on the large-scale ImageNet Challenge dataset.}
}

@misc{detlefsen_reliable_2019,
  title = {Reliable Training and Estimation of Variance Networks},
  author = {Detlefsen, Nicki S. and Jørgensen, Martin and Hauberg, Søren},
  date = {2019-11-04},
  eprint = {1906.03260},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1906.03260},
  urldate = {2020-02-07},
  abstract = {We propose and investigate new complementary methodologies for estimating predictive variance networks in regression neural networks. We derive a locally aware mini-batching scheme that results in sparse robust gradients, and we show how to make unbiased weight updates to a variance network. Further, we formulate a heuristic for robustly fitting both the mean and variance networks post hoc. Finally, we take inspiration from posterior Gaussian processes and propose a network architecture with similar extrapolation properties to Gaussian processes. The proposed methodologies are complementary, and improve upon baseline methods individually. Experimentally, we investigate the impact of predictive uncertainty on multiple datasets and tasks ranging from regression, active learning and generative modeling. Experiments consistently show significant improvements in predictive uncertainty estimation over state-of-the-art methods across tasks and datasets.},
  langid = {english}
}

@online{devlin_bert_2018,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  date = {2018-10-10},
  number = {1810.04805},
  eprint = {1810.04805},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1810.04805},
  urldate = {2019-02-11},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT representations can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE benchmark to 80.4\% (7.6\% absolute improvement), MultiNLI accuracy to 86.7 (5.6\% absolute improvement) and the SQuAD v1.1 question answering Test F1 to 93.2 (1.5\% absolute improvement), outperforming human performance by 2.0\%.},
  pubstate = {preprint}
}

@misc{devries_learning_2018,
  title = {Learning {{Confidence}} for {{Out-of-Distribution Detection}} in {{Neural Networks}}},
  author = {DeVries, Terrance and Taylor, Graham W.},
  date = {2018-02-13},
  eprint = {1802.04865},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1802.04865},
  urldate = {2018-08-27},
  abstract = {Modern neural networks are very powerful predictive models, but they are often incapable of recognizing when their predictions may be wrong. Closely related to this is the task of out-of-distribution detection, where a network must determine whether or not an input is outside of the set on which it is expected to safely perform. To jointly address these issues, we propose a method of learning confidence estimates for neural networks that is simple to implement and produces intuitively interpretable outputs. We demonstrate that on the task of out-of-distribution detection, our technique surpasses recently proposed techniques which construct confidence based on the network's output distribution, without requiring any additional labels or access to out-of-distribution examples. Additionally, we address the problem of calibrating out-of-distribution detectors, where we demonstrate that misclassified in-distribution examples can be used as a proxy for out-of-distribution examples.}
}

@book{dewancker_bayesian_2015,
  title = {Bayesian Optimization Primer},
  author = {Dewancker, Ian and McCourt, Michael and Clark, Scott},
  date = {2015},
  abstract = {SigOpt employs Bayesian optimization to help experts tune machine learning models and simulations. Instead of resorting to standard techniques like grid search, random search, or manual tuning, Bayesian optimization efficiently trades off exploration and exploitation of the parameter space to quickly guide the user into the configuration that best optimizes some overall evaluation criterion (OEC) like accuracy, AUC, or likelihood. In this short introduction we introduce Bayesian optimization and several techniques that SigOpt uses to optimize users models and simulations. For applications and examples of SigOpt using Bayesian optimization in real world problems please visit https://sigopt.com/research.}
}

@article{dhamija_reducing_2018,
  title = {Reducing Network Agnostophobia},
  author = {Dhamija, Akshay Raj and Günther, Manuel and Boult, Terrance},
  date = {2018},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {31}
}

@misc{dhingra_gatedattention_2016,
  title = {Gated-{{Attention Readers}} for {{Text Comprehension}}},
  author = {Dhingra, Bhuwan and Liu, Hanxiao and Yang, Zhilin and Cohen, William W and Salakhutdinov, Ruslan},
  date = {2016},
  eprint = {1606.01549},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1606.01549},
  urldate = {2018-07-29},
  abstract = {In this paper we study the problem of answering cloze-style questions over documents. Our model, the Gated-Attention (GA) Reader, integrates a multi-hop architecture with a novel attention mechanism, which is based on multiplicative interactions between the query embedding and the intermediate states of a recurrent neural network document reader. This enables the reader to build query-specific representations of tokens in the document for accurate answer selection. The GA Reader obtains state-of-the-art results on three benchmarks for this task--the CNN \textbackslash\& Daily Mail news stories and the Who Did What dataset. The effectiveness of multiplicative interaction is demonstrated by an ablation study, and by comparing to alternative compositional operators for implementing the gated-attention. The code is available at https://github.com/bdhingra/ga-reader.},
  isbn = {9781945626753}
}

@misc{dieleman_variablerate_2021,
  title = {Variable-Rate Discrete Representation Learning},
  author = {Dieleman, Sander and Nash, Charlie and Engel, Jesse and Simonyan, Karen},
  date = {2021-03-10},
  eprint = {2103.06089},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2103.06089},
  urldate = {2021-11-22},
  abstract = {Semantically meaningful information content in perceptual signals is usually unevenly distributed. In speech signals for example, there are often many silences, and the speed of pronunciation can vary considerably. In this work, we propose slow autoencoders (SlowAEs) for unsupervised learning of high-level variable-rate discrete representations of sequences, and apply them to speech. We show that the resulting event-based representations automatically grow or shrink depending on the density of salient information in the input signals, while still allowing for faithful signal reconstruction. We develop run-length Transformers (RLTs) for event-based representation modelling and use them to construct language models in the speech domain, which are able to generate grammatical and semantically coherent utterances and continuations. Samples can be found at https://vdrl.github.io/.},
  langid = {english}
}

@inproceedings{dieng_avoiding_2019,
  title = {Avoiding Latent Variable Collapse with Generative Skip Models},
  booktitle = {Proceedings of the 22nd {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}} ({{AISTATS}})},
  author = {Dieng, Adji B. and Kim, Yoon and Rush, Alexander M. and Blei, David M.},
  editor = {Chaudhuri, Kamalika and Sugiyama, Masashi},
  date = {2019},
  volume = {89},
  pages = {2397--2405},
  publisher = {{PMLR}},
  location = {{Naha, Okinawa, Japan}},
  url = {http://proceedings.mlr.press/v89/dieng19a.html},
  abstract = {Variational autoencoders (VAEs) learn distributions of high-dimensional data. They model data with a deep latent-variable model and then fit the model by maximizing a lower bound of the log marginal likelihood. VAEs can capture complex distributions, but they can also suffer from an issue known as "latent variable collapse," especially if the likelihood model is powerful. Specifically, the lower bound involves an approximate posterior of the latent variables; this posterior "collapses" when it is set equal to the prior, i.e., when the approximate posterior is independent of the data. While VAEs learn good generative models, latent variable collapse prevents them from learning useful representations. In this paper, we propose a simple new way to avoid latent variable collapse by including skip connections in our generative model; these connections enforce strong links between the latent variables and the likelihood function. We study generative skip models both theoretically and empirically. Theoretically, we prove that skip models increase the mutual information between the observations and the inferred latent variables. Empirically, we study images (MNIST and Omniglot) and text (Yahoo). Compared to existing VAE architectures, we show that generative skip models maintain similar predictive performance but lead to less collapse and provide more meaningful representations of the data.},
  eventtitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}}
}

@article{dietterich_approximate_1998,
  title = {Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms},
  author = {Dietterich, Thomas G.},
  date = {1998},
  journaltitle = {Neural Computation},
  volume = {10},
  number = {7},
  pages = {1895--1923},
  doi = {10.1162/089976698300017197}
}

@article{ding_flowprior_nodate,
  title = {{{FlowPrior}}: {{Learning Expressive Priors}} for {{Latent Variable Sentence Models}}},
  author = {Ding, Xiaoan and Gimpel, Kevin},
  pages = {17},
  abstract = {Variational autoencoders (VAEs) are widely used for latent variable modeling of text. We focus on variations that learn expressive prior distributions over the latent variable. We find that existing training strategies are not effective for learning rich priors, so we add the importance-sampled log marginal likelihood as a second term to the standard VAE objective to help when learning the prior. Doing so improves results for all priors evaluated, including a novel choice for sentence VAEs based on normalizing flows (NF). Priors parameterized with NF are no longer constrained to a specific distribution family, allowing a more flexible way to encode the data distribution. Our model, which we call FlowPrior, shows a substantial improvement in language modeling tasks compared to strong baselines. We demonstrate that FlowPrior learns an expressive prior with analysis and several forms of evaluation involving generation.},
  langid = {english}
}

@inproceedings{dinh_density_2017,
  title = {Density Estimation Using {{Real NVP}}},
  booktitle = {5th {{International Conference}} on {{Learning Representations}}},
  author = {Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
  date = {2017-04},
  eprint = {1605.08803},
  eprinttype = {arxiv},
  location = {{Palais des Congrès Neptune, Toulon, France}},
  url = {http://arxiv.org/abs/1605.08803},
  urldate = {2020-02-09},
  abstract = {Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful, stably invertible, and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact and efficient sampling, exact and efficient inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation, and latent variable manipulations.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english}
}

@inproceedings{dinh_nice_2015,
  title = {{{NICE}}: {{Non-linear Independent Components Estimation}}},
  shorttitle = {{{NICE}}},
  booktitle = {3rd {{International Conference}} on {{Learning Representations Workshop}}},
  author = {Dinh, Laurent and Krueger, David and Bengio, Yoshua},
  date = {2015-05},
  eprint = {1410.8516},
  eprinttype = {arxiv},
  location = {{San Diego, CA, USA}},
  url = {http://arxiv.org/abs/1410.8516},
  urldate = {2020-02-09},
  abstract = {We propose a deep learning framework for modeling complex high-dimensional densities called Non-linear Independent Component Estimation (NICE). It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the determinant of the Jacobian and inverse Jacobian is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable. Unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english}
}

@misc{dodge_show_2019,
  title = {Show {{Your Work}}: {{Improved Reporting}} of {{Experimental Results}}},
  shorttitle = {Show {{Your Work}}},
  author = {Dodge, Jesse and Gururangan, Suchin and Card, Dallas and Schwartz, Roy and Smith, Noah A.},
  date = {2019-09-06},
  eprint = {1909.03004},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1909.03004},
  urldate = {2019-09-11},
  abstract = {Research in natural language processing proceeds, in part, by demonstrating that new models achieve superior performance (e.g., accuracy) on held-out test data, compared to previous results. In this paper, we demonstrate that test-set performance scores alone are insufficient for drawing accurate conclusions about which model performs best. We argue for reporting additional details, especially performance on validation data obtained during model development. We present a novel technique for doing so: expected validation performance of the best-found model as a function of computation budget (i.e., the number of hyperparameter search trials or the overall training time). Using our approach, we find multiple recent model comparisons where authors would have reached a different conclusion if they had used more (or less) computation. Our approach also allows us to estimate the amount of computation required to obtain a given accuracy; applying it to several recently published results yields massive variation across papers, from hours to weeks. We conclude with a set of best practices for reporting experimental results which allow for robust future comparisons, and provide code to allow researchers to use our technique.},
  langid = {english}
}

@misc{doersch_tutorial_2016,
  title = {Tutorial on {{Variational Autoencoders}}},
  author = {Doersch, Carl},
  date = {2016},
  eprint = {27148061},
  eprinttype = {pmid},
  url = {http://arxiv.org/abs/1606.05908},
  abstract = {In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
  isbn = {1532-4435}
}

@inproceedings{doersch_unsupervised_2015,
  title = {Unsupervised Visual Representation Learning by Context Prediction},
  booktitle = {Proceedings of the {{IEEE}} International Conference on Computer Vision},
  author = {Doersch, Carl and Gupta, Abhinav and Efros, Alexei A},
  date = {2015},
  eprint = {1505.05192},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {1422--1430}
}

@article{domingos_few_2012,
  title = {A Few Useful Things to Know about Machine Learning},
  author = {Domingos, Pedro},
  date = {2012},
  journaltitle = {Communications of the ACM},
  volume = {55},
  number = {10},
  eprint = {1000183096},
  eprinttype = {pmid},
  pages = {78},
  issn = {00010782},
  doi = {10.1145/2347736.2347755},
  url = {http://dl.acm.org/citation.cfm?doid=2347736.2347755},
  urldate = {2018-04-18},
  abstract = {MACHINE LEARNING SYSTEMS automatically learn programs from data. This is often a very attractive alternative to manually constructing them, and in the last decade the use of machine learning has spread rapidly throughout computer science and beyond. Machine learning is used in Web search, spam filters, recommender systems, ad placement, credit scoring, fraud detection, stock trading, drug design, and many other applications. A recent report from the McKinsey Global Institute asserts that machine learning (a.k.a. data mining or predictive analytics) will be the driver of the next big wave of innovation. Several fine textbooks are available to interested practitioners and researchers (for example, Mitchell and Witten et al.). However, much of the “folk knowledge” that is needed to successfully develop machine learning applications is not readily available in them. As a result, many machine learning projects take much longer than necessary or wind up producing less-than-ideal results. Yet much of this folk knowledge is fairly easy to communicate. This is the purpose of this article.},
  isbn = {0001-0782},
  keywords = {★}
}

@inproceedings{domke_importance_2018,
  title = {Importance {{Weighting}} and {{Variational Inference}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Domke, Justin and Sheldon, Daniel R},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
  date = {2018},
  volume = {31},
  pages = {4470--4479},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2018/file/25db67c5657914454081c6a18e93d6dd-Paper.pdf},
  abstract = {Recent work used importance sampling ideas for better variational bounds on likelihoods. We clarify the applicability of these ideas to pure probabilistic inference, by showing the resulting Importance Weighted Variational Inference (IWVI) technique is an instance of augmented variational inference, thus identifying the looseness in previous work. Experiments confirm IWVI’s practicality for probabilistic inference. As a second contribution, we investigate inference with elliptical distributions, which improves accuracy in low dimensions, and convergence in high dimensions}
}

@inproceedings{donahue_decaf_2014,
  title = {{{DeCAF}}: {{A Deep Convolutional Activation Feature}} for {{Generic Visual Recognition}}.},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Donahue, Jeff and Jia, Yangqing and Vinyals, Oriol and Hoffman, Judy and Zhang, Ning and Tzeng, Eric and Darrell, Trevor},
  date = {2014},
  volume = {32},
  pages = {647--655}
}

@inproceedings{donahue_longterm_2015,
  title = {Long-Term Recurrent Convolutional Networks for Visual Recognition and Description},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Donahue, Jeffrey and Anne Hendricks, Lisa and Guadarrama, Sergio and Rohrbach, Marcus and Venugopalan, Subhashini and Saenko, Kate and Darrell, Trevor},
  date = {2015},
  pages = {2625--2634}
}

@article{dong_image_2016,
  title = {Image Super-Resolution Using Deep Convolutional Networks},
  author = {Dong, Chao and Loy, Chen Change and He, Kaiming and Tang, Xiaoou},
  date = {2016},
  journaltitle = {IEEE transactions on pattern analysis and machine intelligence},
  volume = {38},
  number = {2},
  pages = {295--307},
  publisher = {{IEEE}}
}

@article{dong_speech-transformer:_2018,
  title = {Speech-{{Transformer}}: {{A No-Recurrence Sequence-To-Sequence Model}} for {{Speech Recognition}}},
  author = {Dong, Linhao and Xu, Shuang and Xu, Bo},
  date = {2018},
  journaltitle = {International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
  pages = {5884--5888},
  abstract = {Recurrent sequence-to-sequence models using encoder-decoder ar- chitecture have made great progress in speech recognition task. However, they suffer from the drawback of slow training speed because the internal recurrence limits the training parallelization. In this paper, we present the Speech-Transformer, a no-recurrence sequence-to-sequence model entirely relies on attention mechanism- s to learn the positional dependencies, which can be trained faster with more efficiency. We also propose a 2D-Attention mechanis- m, which can jointly attend to the time and frequency axes of the 2-dimensional speech inputs, thus providing more expressive repre- sentations for the Speech-Transformer. Evaluated on the Wall Street Journal (WSJ) speech recognition dataset, our best model achieves competitive word error rate (WER) of 10.9\%, while the whole train- ing process only takes 1.2 days on 1 GPU, significantly faster than the published results of recurrent sequence-to-sequence models.}
}

@article{dongAutomatedClinicalCoding2022,
  title = {Automated Clinical Coding: {{What}}, Why, and Where We Are?},
  shorttitle = {Automated Clinical Coding},
  author = {Dong, Hang and Falis, Matúš and Whiteley, William and Alex, Beatrice and Matterson, Joshua and Ji, Shaoxiong and Chen, Jiaoyan and Wu, Honghan},
  date = {2022-10},
  journaltitle = {npj Digital Medicine},
  volume = {5},
  number = {1},
  pages = {1--8},
  publisher = {{Nature Publishing Group}},
  issn = {2398-6352},
  doi = {10.1038/s41746-022-00705-7},
  abstract = {Clinical coding is the task of transforming medical information in a patient's health records into structured codes so that they can be used for statistical analysis. This is a cognitive and time-consuming task that follows a standard process in order to achieve a high level of consistency. Clinical coding could potentially be supported by an automated system to improve the efficiency and accuracy of the process. We introduce the idea of automated clinical coding and summarise its challenges from the perspective of Artificial Intelligence (AI) and Natural Language Processing (NLP), based on the literature, our project experience over the past two and half years (late 2019–early 2022), and discussions with clinical coding experts in Scotland and the UK. Our research reveals the gaps between the current deep learning-based approach applied to clinical coding and the need for explainability and consistency in real-world practice. Knowledge-based methods that represent and reason the standard, explainable process of a task may need to be incorporated into deep learning-based methods for clinical coding. Automated clinical coding is a promising task for AI, despite the technical and organisational challenges. Coders are needed to be involved in the development process. There is much to achieve to develop and deploy an AI-based automated system to support coding in the next five years and beyond.},
  copyright = {2022 The Author(s)},
  langid = {english}
}

@article{dongExplainableAutomatedCoding2021,
  title = {Explainable Automated Coding of Clinical Notes Using Hierarchical Label-Wise Attention Networks and Label Embedding Initialisation},
  author = {Dong, Hang and Suárez-Paniagua, Víctor and Whiteley, William and Wu, Honghan},
  date = {2021-04},
  journaltitle = {Journal of Biomedical Informatics},
  volume = {116},
  eprint = {33711543},
  eprinttype = {pmid},
  pages = {103728},
  issn = {1532-0480},
  doi = {10.1016/j.jbi.2021.103728},
  abstract = {BACKGROUND: Diagnostic or procedural coding of clinical notes aims to derive a coded summary of disease-related information about patients. Such coding is usually done manually in hospitals but could potentially be automated to improve the efficiency and accuracy of medical coding. Recent studies on deep learning for automated medical coding achieved promising performances. However, the explainability of these models is usually poor, preventing them to be used confidently in supporting clinical practice. Another limitation is that these models mostly assume independence among labels, ignoring the complex correlations among medical codes which can potentially be exploited to improve the performance. METHODS: To address the issues of model explainability and label correlations, we propose a Hierarchical Label-wise Attention Network (HLAN), which aimed to interpret the model by quantifying importance (as attention weights) of words and sentences related to each of the labels. Secondly, we propose to enhance the major deep learning models with a label embedding (LE) initialisation approach, which learns a dense, continuous vector representation and then injects the representation into the final layers and the label-wise attention layers in the models. We evaluated the methods using three settings on the MIMIC-III discharge summaries: full codes, top-50 codes, and the UK NHS (National Health Service) COVID-19 (Coronavirus disease 2019) shielding codes. Experiments were conducted to compare the HLAN model and label embedding initialisation to the state-of-the-art neural network based methods, including variants of Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). RESULTS: HLAN achieved the best Micro-level AUC and F1 on the top-50 code prediction, 91.9\% and 64.1\%, respectively; and comparable results on the NHS COVID-19 shielding code prediction to other models: around 97\% Micro-level AUC. More importantly, in the analysis of model explanations, by highlighting the most salient words and sentences for each label, HLAN showed more meaningful and comprehensive model interpretation compared to the CNN-based models and its downgraded baselines, HAN and HA-GRU. Label embedding (LE) initialisation significantly boosted the previous state-of-the-art model, CNN with attention mechanisms, on the full code prediction to 52.5\% Micro-level F1. The analysis of the layers initialised with label embeddings further explains the effect of this initialisation approach. The source code of the implementation and the results are openly available at https://github.com/acadTags/Explainable-Automated-Medical-Coding. CONCLUSION: We draw the conclusion from the evaluation results and analyses. First, with hierarchical label-wise attention mechanisms, HLAN can provide better or comparable results for automated coding to the state-of-the-art, CNN-based models. Second, HLAN can provide more comprehensive explanations for each label by highlighting key words and sentences in the discharge summaries, compared to the n-grams in the CNN-based models and the downgraded baselines, HAN and HA-GRU. Third, the performance of deep learning based multi-label classification for automated coding can be consistently boosted by initialising label embeddings that captures the correlations among labels. We further discuss the advantages and drawbacks of the overall method regarding its potential to be deployed to a hospital and suggest areas for future studies.},
  langid = {english}
}

@inproceedings{dosovitskiy_image_2021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  date = {2021-06-03},
  eprint = {2010.11929},
  eprinttype = {arxiv},
  location = {{Virtual}},
  url = {http://arxiv.org/abs/2010.11929},
  urldate = {2022-02-02},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  keywords = {Read}
}

@incollection{doucet_introduction_2001,
  title = {An {{Introduction}} to {{Sequential Monte Carlo Methods}}},
  booktitle = {Sequential {{Monte Carlo Methods}} in {{Practice}}},
  author = {Doucet, Arnaud and family=Freitas, given=Nando, prefix=de, useprefix=true and Gordon, Neil},
  editor = {Doucet, Arnaud and family=Freitas, given=Nando, prefix=de, useprefix=true and Gordon, Neil},
  date = {2001},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  url = {https://doi.org/10.1007/978-1-4757-3437-9₁},
  abstract = {Many real-world data analysis tasks involve estimating unknown quantities from some given observations. In most of these applications, prior knowledge about the phenomenon being modelled is available. This knowledge allows us to formulate Bayesian models, that is prior distributions for the unknown quantities and likelihood functions relating these quantities to the observations. Within this setting, all inference on the unknown quantities is based on the posterior distribution obtained from Bayes' theorem. Often, the observations arrive sequentially in time and one is interested in performing inference on-line. It is therefore necessary to update the posterior distribution as data become available. Examples include tracking an aircraft using radar measurements, estimating a digital communications signal using noisy measurements, or estimating the volatility of financial instruments using stock market data. Computational simplicity in the form of not having to store all the data might also be an additional motivating factor for sequential methods.},
  isbn = {978-1-4757-3437-9}
}

@book{downey_think_2012,
  title = {Think {{Bayes}} - {{Bayesian Statistics Made Simple}}},
  author = {Downey, Alan B.},
  date = {2012},
  publisher = {{Green Tea Press}}
}

@inproceedings{dozat_incorporating_2016,
  title = {Incorporating {{Nesterov Momentum}} into {{Adam}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}}) {{Workshop}}},
  author = {Dozat, Timothy},
  date = {2016},
  pages = {2013--2016},
  abstract = {This work aims to improve upon the recently proposed and rapidly popular-ized optimization algorithm Adam (Kingma \& Ba, 2014). Adam has two main components—a momentum component and an adaptive learning rate component. However, regular momentum can be shown conceptually and empirically to be in-ferior to a similar algorithm known as Nesterov's accelerated gradient (NAG). We show how to modify Adam's momentum component to take advantage of insights from NAG, and then we present preliminary evidence suggesting that making this substitution improves the speed of convergence and the quality of the learned mod-els.}
}

@inproceedings{dror_hitchhiker_2018,
  title = {The {{Hitchhiker}}’s {{Guide}} to {{Testing Statistical Significance}} in {{Natural Language Processing}}},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Dror, Rotem and Baumer, Gili and Shlomov, Segev and Reichart, Roi},
  date = {2018},
  pages = {1383--1392},
  publisher = {{Association for Computational Linguistics}},
  location = {{Melbourne, Australia}},
  doi = {10/gf8x76},
  url = {http://aclweb.org/anthology/P18-1128},
  urldate = {2020-01-05},
  abstract = {Statistical significance testing is a standard statistical tool designed to ensure that experimental results are not coincidental. In this opinion/theoretical paper we discuss the role of statistical significance testing in Natural Language Processing (NLP) research. We establish the fundamental concepts of significance testing and discuss the specific aspects of NLP tasks, experimental setups and evaluation measures that affect the choice of significance tests in NLP research. Based on this discussion, we propose a simple practical protocol for statistical significance test selection in NLP setups and accompany this protocol with a brief survey of the most relevant tests. We then survey recent empirical papers published in ACL and TACL during 2017 and show that while our community assigns great value to experimental results, statistical significance testing is often ignored or misused. We conclude with a brief discussion of open issues that should be properly addressed so that this important tool can be applied in NLP research in a statistically sound manner1.},
  eventtitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  langid = {english}
}

@article{drucker_improving_1992,
  title = {Improving Generalization Performance Using Double Backpropagation},
  author = {Drucker, H. and Le Cun, Y.},
  date = {1992},
  journaltitle = {IEEE Transactions on Neural Networks},
  volume = {3},
  number = {6},
  pages = {991--997},
  doi = {10.1109/72.165600}
}

@misc{drusvyatskiy_optimal_2016,
  title = {An Optimal First Order Method Based on Optimal Quadratic Averaging},
  author = {Drusvyatskiy, Dmitriy and Fazel, Maryam and Roy, Scott},
  date = {2016},
  eprint = {1604.06543},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1604.06543},
  urldate = {2018-07-30},
  abstract = {In a recent paper, Bubeck, Lee, and Singh introduced a new first order method for minimizing smooth strongly convex functions. Their geometric descent algorithm, largely inspired by the ellipsoid method, enjoys the optimal linear rate of convergence. We show that the same iterate sequence is generated by a scheme that in each iteration computes an optimal average of quadratic lower-models of the function. Indeed, the minimum of the averaged quadratic approaches the true minimum at an optimal rate. This intuitive viewpoint reveals clear connections to the original fast-gradient methods and cutting plane ideas, and leads to limited-memory extensions with improved performance.}
}

@article{du_gradient_2018,
  title = {Gradient {{Descent Finds Global Minima}} of {{Deep Neural Networks}}},
  author = {Du, Simon S. and Lee, Jason D. and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  date = {2018-11-09},
  url = {https://arxiv.org/abs/1811.03804},
  urldate = {2018-11-14},
  langid = {english}
}

@article{du_implicit_2019,
  title = {Implicit Generation and Modeling with Energy Based Models},
  author = {Du, Yilun and Mordatch, Igor},
  date = {2019},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {32}
}

@online{duan_faster_2023,
  title = {Faster {{Matrix Multiplication}} via {{Asymmetric Hashing}}},
  author = {Duan, Ran and Wu, Hongxun and Zhou, Renfei},
  date = {2023-04-05},
  eprint = {2210.10173},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2210.10173},
  urldate = {2023-07-11},
  abstract = {Fast matrix multiplication is one of the most fundamental problems in algorithm research. The exponent of the optimal time complexity of matrix multiplication is usually denoted by \$\textbackslash omega\$. This paper discusses new ideas for improving the laser method for fast matrix multiplication. We observe that the analysis of higher powers of the Coppersmith-Winograd tensor [Coppersmith \& Winograd 1990] incurs a "combination loss", and we partially compensate for it using an asymmetric version of CW's hashing method. By analyzing the eighth power of the CW tensor, we give a new bound of \$\textbackslash omega{$<$}2.371866\$, which improves the previous best bound of \$\textbackslash omega{$<$}2.372860\$ [Alman \& Vassilevska Williams 2020]. Our result breaks the lower bound of \$2.3725\$ in [Ambainis, Filmus \& Le Gall 2015] because of the new method for analyzing component (constituent) tensors.},
  pubstate = {preprint}
}

@article{duan_low_2014,
  title = {Low Rank Approximation of the Symmetric Positive Semidefinite Matrix},
  author = {Duan, Xuefeng and Li, Jiaofen and Wang, Qingwen and Zhang, Xinjun},
  date = {2014},
  journaltitle = {Journal of Computational and Applied Mathematics},
  volume = {260},
  pages = {236--243},
  issn = {03770427},
  doi = {10.1016/j.cam.2013.09.080},
  url = {www.elsevier.com/locate/cam},
  urldate = {2018-03-25},
  abstract = {In this paper, we consider the low rank approximation of the symmetric positive semidefinite matrix, which arises in machine learning, quantum chemistry and inverse problem. We first characterize the feasible set by X=YYT,Yâ̂̂Rn×k, and then transform low rank approximation into an unconstrained optimization problem. Finally, we use the nonlinear conjugate gradient method with exact line search to compute the optimal low rank symmetric positive semidefinite approximation of the given matrix. Numerical examples show that the new method is feasible and effective. © 2013 Elsevier Inc.}
}

@misc{duan_rl_2016,
  title = {{{RL}}\$\^2\$: {{Fast Reinforcement Learning}} via {{Slow Reinforcement Learning}}},
  author = {Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter L. and Sutskever, Ilya and Abbeel, Pieter},
  date = {2016-11-08},
  eprint = {1611.02779},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1611.02779},
  urldate = {2017-10-23},
  abstract = {Deep reinforcement learning (deep RL) has been successful in learning sophisticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, benefiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a "fast" reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, RL\$\^2\$, the algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose ("slow") RL algorithm. The RNN receives all information a typical RL algorithm would receive, including observations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the "fast" RL algorithm on the current (previously unseen) MDP. We evaluate RL\$\^2\$ experimentally on both small-scale and large-scale problems. On the small-scale side, we train it to solve randomly generated multi-arm bandit problems and finite MDPs. After RL\$\^2\$ is trained, its performance on new MDPs is close to human-designed algorithms with optimality guarantees. On the large-scale side, we test RL\$\^2\$ on a vision-based navigation task and show that it scales up to high-dimensional problems.}
}

@article{duchi_adaptive_2011,
  title = {Adaptive {{Subgradient Methods}} for {{Online Learning}} and {{Stochastic Optimization}}},
  author = {Duchi, John and Hazan, Elad and Singer, Yoram},
  date = {2011},
  journaltitle = {Journal of Machine Learning Research},
  volume = {12},
  pages = {2121--2159},
  abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.}
}

@inproceedings{duke_sstvos_2021,
  title = {{{SSTVOS}}: {{Sparse Spatiotemporal Transformers}} for {{Video Object Segmentation}}},
  shorttitle = {{{SSTVOS}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Duke, Brendan and Ahmed, Abdalla and Wolf, Christian and Aarabi, Parham and Taylor, Graham W.},
  date = {2021-06},
  pages = {5908--5917},
  publisher = {{IEEE}},
  location = {{Nashville, TN, USA}},
  doi = {10.1109/CVPR46437.2021.00585},
  url = {https://ieeexplore.ieee.org/document/9578213/},
  urldate = {2022-10-16},
  abstract = {In this paper we introduce a Transformer-based approach to video object segmentation (VOS). To address compounding error and scalability issues of prior work, we propose a scalable, end-to-end method for VOS called Sparse Spatiotemporal Transformers (SST). SST extracts per-pixel representations for each object in a video using sparse attention over spatiotemporal features. Our attention-based formulation for VOS allows a model to learn to attend over a history of multiple frames and provides suitable inductive bias for performing correspondence-like computations necessary for solving motion segmentation. We demonstrate the effectiveness of attention-based over recurrent networks in the spatiotemporal domain. Our method achieves competitive results on YouTube-VOS and DAVIS 2017 with improved scalability and robustness to occlusions compared with the state of the art. Code is available at https: //github.com/dukebw/SSTVOS.},
  eventtitle = {{{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-66544-509-2},
  langid = {english}
}

@misc{dumoulin_guide_2016,
  title = {A {{Guide}} to {{Convolution Arithmetic}} for {{Deep Learning}}},
  author = {Dumoulin, Vincent and Visin, Francesco},
  date = {2016-03-23},
  eprint = {1603.07285},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1603.07285},
  urldate = {2018-04-18},
  abstract = {We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.}
}

@inproceedings{dunbar_zero_2017,
  title = {The {{Zero Resource Speech Challenge}} 2017},
  booktitle = {{{IEEE Workshop}} on {{Automatic Speech Recognition}} and {{Understanding}} ({{ASRU}})},
  author = {Dunbar, Ewan and Cao, Xuan Nga and Benjumea, Juan and Karadayi, Julien and Bernard, Mathieu and Besacier, Laurent and Anguera, Xavier and Dupoux, Emmanuel},
  date = {2017},
  pages = {323--330}
}

@online{dunbar_zero_2019,
  title = {The {{Zero Resource Speech Challenge}} 2019: {{TTS Without T}}},
  author = {Dunbar, Ewan and Algayres, Robin and Karadayi, Julien and Bernard, Mathieu and Benjumea, Juan and Cao, Xuan-Nga and Miskic, Lucie and Dugrain, Charlotte and Ondel, Lucas and Black, Alan W and others},
  date = {2019},
  eprint = {1904.11469},
  eprinttype = {arxiv},
  pubstate = {preprint}
}

@inproceedings{dunbar_zero_2020,
  title = {The {{Zero Resource Speech Challenge}} 2020: {{Discovering}} Discrete Subword and Word Units},
  booktitle = {Annual {{Conference}} of the {{International Speech Communication Association}}},
  author = {Dunbar, Ewan and Karadayi, Julien and Bernard, Mathieu and Cao, Xuan-Nga and Algayres, Robin and Ondel, Lucas and Besacier, Laurent and Sakti, Sakriani and Dupoux, Emmanuel},
  date = {2020}
}

@article{dunbar_zero_2021,
  title = {The Zero Resource Speech Challenge 2021: Spoken Language Modelling},
  author = {Dunbar, Ewan and Bernard, Mathieu and Hamilakis, Nicolas and Nguyen, Tu and family=Seyssel, given=Maureen, prefix=de, useprefix=true and Rozé, Patricia and Rivière, Morgane and Kharitonov, Eugene and Dupoux, Emmanuel},
  date = {2021},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence}
}

@inproceedings{durall_watch_2020,
  title = {Watch {{Your Up-Convolution}}: {{CNN Based Generative Deep Neural Networks Are Failing}} to {{Reproduce Spectral Distributions}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Durall, R. and Keuper, M. and Keuper, J.},
  date = {2020-06},
  pages = {7887--7896},
  issn = {2575-7075},
  doi = {10.1109/CVPR42600.2020.00791},
  abstract = {Generative convolutional deep neural networks, e.g. popular GAN architectures, are relying on convolution based up-sampling methods to produce non-scalar outputs like images or video sequences. In this paper, we show that common up-sampling methods, i.e. known as up-convolution or transposed convolution, are causing the inability of such models to reproduce spectral distributions of natural training data correctly. This effect is independent of the underlying architecture and we show that it can be used to easily detect generated data like deepfakes with up to 100\% accuracy on public benchmarks. To overcome this drawback of current generative models, we propose to add a novel spectral regularization term to the training optimization objective. We show that this approach not only allows to train spectral consistent GANs that are avoiding high frequency errors. Also, we show that a correct approximation of the frequency spectrum has positive effects on the training stability and output quality of generative networks.}
}

@article{duvenaud_automatic_nodate,
  title = {Automatic {{Model Construction}}  with {{Gaussian Processes}}},
  author = {Duvenaud, David Kristjanson},
  pages = {157},
  langid = {english}
}

@article{dwass_modified_1957,
  title = {Modified Randomization Tests for Nonparametric Hypotheses},
  author = {Dwass, Meyer},
  date = {1957},
  journaltitle = {The Annals of Mathematical Statistics},
  volume = {28},
  number = {1},
  pages = {181--187},
  publisher = {{Institute of Mathematical Statistics}},
  doi = {10.1214/aoms/1177707045},
  url = {https://doi.org/10.1214/aoms/1177707045}
}

@inproceedings{ebbers_hidden_2017,
  title = {Hidden {{Markov Model Variational Autoencoder}} for {{Acoustic Unit Discovery}}},
  booktitle = {Proceedings of the 18th {{Annual Conference}} of the {{International Speech Communication Association}} ({{Interspeech}})},
  author = {Ebbers, Janek and Heymann, Jahn and Drude, Lukas and Glarner, Thomas and Haeb-Umbach, Reinhold and Raj, Bhiksha},
  date = {2017-08-20},
  pages = {488--492},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2017-1160},
  url = {https://www.isca-speech.org/archive/interspeech_2017/ebbers17_interspeech.html},
  urldate = {2021-10-30},
  langid = {english}
}

@article{eden_validity_1933,
  title = {On the Validity of {{Fisher}}'s z Test When Applied to an Actual Example of Non-Normal Data. ({{With}} Five Text-Figures.)},
  author = {Eden, T. and Yates, F.},
  date = {1933},
  journaltitle = {The Journal of Agricultural Science},
  volume = {23},
  number = {1},
  pages = {6--17},
  doi = {10.1017/S0021859600052862}
}

@inproceedings{edin_automated_2023,
  title = {Automated {{Medical Coding}} on {{MIMIC-III}} and {{MIMIC-IV}}: {{A Critical Review}} and {{Replicability Study}}},
  shorttitle = {Automated {{Medical Coding}} on {{MIMIC-III}} and {{MIMIC-IV}}},
  booktitle = {Proceedings of the 46th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}} ({{SIGIR}})},
  author = {Edin, Joakim and Junge, Alexander and Havtorn, Jakob D. and Borgholt, Lasse and Maistro, Maria and Ruotsalo, Tuukka and Maaløe, Lars},
  date = {2023-07},
  eprint = {2304.10909},
  eprinttype = {arxiv},
  publisher = {{ACM}},
  location = {{Taipei, Taiwan}},
  doi = {10.1145/3539618.3591918},
  url = {http://arxiv.org/abs/2304.10909},
  abstract = {Medical coding is the task of assigning medical codes to clinical free-text documentation. Healthcare professionals manually assign such codes to track patient diagnoses and treatments. Automated medical coding can considerably alleviate this administrative burden. In this paper, we reproduce, compare, and analyze state-of-the-art automated medical coding machine learning models. We show that several models underperform due to weak configurations, poorly sampled train-test splits, and insufficient evaluation. In previous work, the macro F1 score has been calculated sub-optimally, and our correction doubles it. We contribute a revised model comparison using stratified sampling and identical experimental setups, including hyperparameters and decision boundary tuning. We analyze prediction errors to validate and falsify assumptions of previous works. The analysis confirms that all models struggle with rare codes, while long documents only have a negligible impact. Finally, we present the first comprehensive results on the newly released MIMIC-IV dataset using the reproduced models. We release our code, model parameters, and new MIMIC-III and MIMIC-IV training and evaluation pipelines to accommodate fair future comparisons.}
}

@misc{edizel_misspelling_2019,
  title = {Misspelling {{Oblivious Word Embeddings}}},
  author = {Edizel, Bora and Piktus, Aleksandra and Bojanowski, Piotr and Ferreira, Rui and Grave, Edouard and Silvestri, Fabrizio},
  date = {2019-05-23},
  eprint = {1905.09755},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1905.09755},
  urldate = {2019-06-04},
  abstract = {In this paper we present a method to learn word embeddings that are resilient to misspellings. Existing word embeddings have limited applicability to malformed texts, which contain a non-negligible amount of outof-vocabulary words. We propose a method combining FastText with subwords and a supervised task of learning misspelling patterns. In our method, misspellings of each word are embedded close to their correct variants. We train these embeddings on a new dataset we are releasing publicly. Finally, we experimentally show the advantages of this approach on both intrinsic and extrinsic NLP tasks using public test sets.},
  langid = {english}
}

@inproceedings{edwards_towards_2017,
  title = {Towards a {{Neural Statistician}}},
  booktitle = {Proceedings of the 5th {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Edwards, Harrison and Storkey, Amos J.},
  date = {2017},
  location = {{Toulon, France}},
  url = {https://openreview.net/forum?id=HJDBUF5le}
}

@article{efron_better_1987,
  title = {Better Bootstrap Confidence Intervals},
  author = {Efron, Bradley},
  date = {1987},
  journaltitle = {Journal of the American Statistical Association},
  volume = {82},
  number = {397},
  eprint = {2289144},
  eprinttype = {jstor},
  pages = {171--185},
  issn = {01621459},
  url = {http://www.jstor.org/stable/2289144},
  urldate = {2023-04-26},
  abstract = {We consider the problem of setting approximate confidence intervals for a single parameter θ in a multiparameter family. The standard approximate intervals based on maximum likelihood theory, θ̂ ± σ̂z(α), can be quite misleading. In practice, tricks based on transformations, bias corrections, and so forth, are often used to improve their accuracy. The bootstrap confidence intervals discussed in this article automatically incorporate such tricks without requiring the statistician to think them through for each new application, at the price of a considerable increase in computational effort. The new intervals incorporate an improvement over previously suggested methods, which results in second-order correctness in a wide variety of problems. In addition to parametric families, bootstrap intervals are also developed for nonparametric situations.}
}

@article{efron_bootstrap_1979,
  title = {Bootstrap Methods: {{Another}} Look at the Jackknife},
  author = {Efron, B.},
  date = {1979},
  journaltitle = {The Annals of Statistics},
  volume = {7},
  number = {1},
  pages = {1--26},
  doi = {10.1214/aos/1176344552},
  url = {https://doi.org/10.1214/aos/1176344552},
  keywords = {bootstrap,discriminant analysis,error rate estimation,jackknife,Nonlinear regression,nonparametric variance estimation,Resampling,subsample values}
}

@article{ehrlinger_definition_,
  title = {Towards a {{Deﬁnition}} of {{Knowledge Graphs}}},
  author = {Ehrlinger, Lisa and Wöß, Wolfram},
  pages = {4},
  abstract = {Recently, the term knowledge graph has been used frequently in research and business, usually in close association with Semantic Web technologies, linked data, large-scale data analytics and cloud computing. Its popularity is clearly influenced by the introduction of Google’s Knowledge Graph in 2012, and since then the term has been widely used without a definition. A large variety of interpretations has hampered the evolution of a common understanding of knowledge graphs. Numerous research papers refer to Google’s Knowledge Graph, although no official documentation about the used methods exists. The prerequisite for widespread academic and commercial adoption of a concept or technology is a common understanding, based ideally on a definition that is free from ambiguity. We tackle this issue by discussing and defining the term knowledge graph, considering its history and diversity in interpretations and use. Our goal is to propose a definition of knowledge graphs that serves as basis for discussions on this topic and contributes to a common vision.},
  langid = {english}
}

@inproceedings{el_hihi_hierarchical_1995,
  title = {Hierarchical Recurrent Neural Networks for Long-Term Dependencies},
  booktitle = {Proceedings of the 8th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {El Hihi, Salah and Bengio, Yoshua},
  date = {1995},
  series = {{{NIPS}}'95},
  pages = {493--499},
  publisher = {{MIT Press}},
  location = {{Denver, CO, USA}},
  abstract = {We have already shown that extracting long-term dependencies from sequential data is difficult, both for determimstic dynamical systems such as recurrent networks, and probabilistic models such as hidden Markov models (HMMs) or input/output hidden Markov models (IOHMMs). In practice, to avoid this problem, researchers have used domain specific a-priori knowledge to give meaning to the hidden or state variables representing past context. In this paper, we propose to use a more general type of a-priori knowledge, namely that the temporal dependencies are structured hierarchically. This implies that long-term dependencies are represented by variables with a long time scale. This principle is applied to a recurrent network which includes delays and multiple time scales. Experiments confirm the advantages of such structures. A similar approach is proposed for HMMs and IOHMMs.}
}

@unpublished{eloff_unsupervised_2019,
  title = {Unsupervised Acoustic Unit Discovery for Speech Synthesis Using Discrete Latent-Variable Neural Networks},
  author = {Eloff, Ryan and Nortje, André and family=Niekerk, given=Benjamin, prefix=van, useprefix=true and Govender, Avashna and Nortje, Leanne and Pretorius, Arnu and family=Biljon, given=Elan, prefix=van, useprefix=true and family=Westhuizen, given=Ewald, prefix=van der, useprefix=true and family=Staden, given=Lisa, prefix=van, useprefix=true and Kamper, Herman},
  date = {2019-06-28},
  eprint = {1904.07556},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/1904.07556},
  urldate = {2021-11-05},
  abstract = {For our submission to the ZeroSpeech 2019 challenge, we apply discrete latent-variable neural networks to unlabelled speech and use the discovered units for speech synthesis. Unsupervised discrete subword modelling could be useful for studies of phonetic category learning in infants or in low-resource speech technology requiring symbolic input. We use an autoencoder (AE) architecture with intermediate discretisation. We decouple acoustic unit discovery from speaker modelling by conditioning the AE's decoder on the training speaker identity. At test time, unit discovery is performed on speech from an unseen speaker, followed by unit decoding conditioned on a known target speaker to obtain reconstructed filterbanks. This output is fed to a neural vocoder to synthesise speech in the target speaker's voice. For discretisation, categorical variational autoencoders (CatVAEs), vector-quantised VAEs (VQ-VAEs) and straight-through estimation are compared at different compression levels on two languages. Our final model uses convolutional encoding, VQ-VAE discretisation, deconvolutional decoding and an FFTNet vocoder. We show that decoupled speaker conditioning intrinsically improves discrete acoustic representations, yielding competitive synthesis quality compared to the challenge baseline.}
}

@article{embi_evaluating_2012,
  title = {Evaluating Alert Fatigue over Time to {{EHR-based}} Clinical Trial Alerts: Findings from a Randomized Controlled Study},
  shorttitle = {Evaluating Alert Fatigue over Time to {{EHR-based}} Clinical Trial Alerts},
  author = {Embi, Peter J and Leonard, Anthony C},
  date = {2012-06-01},
  journaltitle = {Journal of the American Medical Informatics Association},
  shortjournal = {Journal of the American Medical Informatics Association},
  volume = {19},
  number = {e1},
  pages = {e145-e148},
  issn = {1067-5027},
  doi = {10.1136/amiajnl-2011-000743},
  url = {https://doi.org/10.1136/amiajnl-2011-000743},
  urldate = {2023-10-10},
  abstract = {Objective Inadequate participant recruitment is a major problem facing clinical research. Recent studies have demonstrated that electronic health record (EHR)-based, point-of-care, clinical trial alerts (CTA) can improve participant recruitment to certain clinical research studies. Despite their promise, much remains to be learned about the use of CTAs. Our objective was to study whether repeated exposure to such alerts leads to declining user responsiveness and to characterize its extent if present to better inform future CTA deployments.Methods During a 36-week study period, we systematically documented the response patterns of 178 physician users randomized to receive CTAs for an ongoing clinical trial. Data were collected on: (1) response rates to the CTA; and (2) referral rates per physician, per time unit. Variables of interest were offset by the log of the total number of alerts received by that physician during that time period, in a Poisson regression.Results Response rates demonstrated a significant downward trend across time, with response rates decreasing by 2.7\% for each advancing time period, significantly different from zero (flat) (p\&lt;0.0001). Even after 36 weeks, response rates remained in the 30\%–40\% range. Subgroup analyses revealed differences between community-based versus university-based physicians (p=0.0489).Discussion CTA responsiveness declined gradually over prolonged exposure, although it remained reasonably high even after 36 weeks of exposure. There were also notable differences between community-based versus university-based users.Conclusions These findings add to the limited literature on this form of EHR-based alert fatigue and should help inform future tailoring, deployment, and further study of CTAs.}
}

@inproceedings{engel_ddsp_2020,
  title = {{{DDSP}}: {{Differentiable}} Digital Signal Processing},
  booktitle = {Proceedings of the 8th {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Engel, Jesse H. and Hantrakul, Lamtharn and Gu, Chenjie and Roberts, Adam},
  date = {2020},
  eprint = {2001.04643},
  eprinttype = {arxiv},
  publisher = {{OpenReview.net}},
  location = {{Addis Ababa, Ethiopia}},
  url = {https://openreview.net/forum?id=B1x1ma4tDr},
  abstract = {Most generative models of audio directly generate samples in one of two domains: time or frequency. While sufficient to express any signal, these representations are inefficient, as they do not utilize existing knowledge of how sound is generated and perceived. A third approach (vocoders/synthesizers) successfully incorporates strong domain knowledge of signal processing and perception, but has been less actively researched due to limited expressivity and difficulty integrating with modern auto-differentiation-based machine learning methods. In this paper, we introduce the Differentiable Digital Signal Processing (DDSP) library, which enables direct integration of classic signal processing elements with deep learning methods. Focusing on audio synthesis, we achieve high-fidelity generation without the need for large autoregressive models or adversarial losses, demonstrating that DDSP enables utilizing strong inductive biases without losing the expressive power of neural networks. Further, we show that combining interpretable modules permits manipulation of each separate model component, with applications such as independent control of pitch and loudness, realistic extrapolation to pitches not seen during training, blind dereverberation of room acoustics, transfer of extracted room acoustics to new environments, and transformation of timbre between disparate sources. In short, DDSP enables an interpretable and modular approach to generative modeling, without sacrificing the benefits of deep learning. The library is publicly available at https://github.com/magenta/ddsp and we welcome further contributions from the community and domain experts.}
}

@inproceedings{engel_neural_2017,
  title = {Neural Audio Synthesis of Musical Notes with {{WaveNet}} Autoencoders},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Engel, Jesse and Resnick, Cinjon and Roberts, Adam and Dieleman, Sander and Norouzi, Mohammad and Eck, Douglas and Simonyan, Karen},
  date = {2017},
  pages = {1068--1077}
}

@article{ephrat_looking_2018,
  title = {Looking to Listen at the Cocktail Party: {{A}} Speaker-Independent Audio-Visual Model for Speech Separation},
  author = {Ephrat, Ariel and Mosseri, Inbar and Lang, Oran and Dekel, Tali and Wilson, Kevin and Hassidim, Avinatan and Freeman, William T and Rubinstein, Michael},
  date = {2018},
  journaltitle = {ACM Transactions on Graphics (TOG)},
  volume = {37},
  number = {4},
  pages = {1--11},
  publisher = {{ACM New York, NY, USA}}
}

@article{ericsson_selfsupervised_2022,
  title = {Self-Supervised Representation Learning: {{Introduction}}, Advances, and Challenges},
  author = {Ericsson, Linus and Gouk, Henry and Loy, Chen Change and Hospedales, Timothy M},
  date = {2022},
  journaltitle = {IEEE Signal Processing Magazine},
  volume = {39},
  number = {3},
  pages = {42--62}
}

@article{errattahi_automatic_2018,
  title = {Automatic {{Speech Recognition Errors Detection}} and {{Correction}}: {{A Review}}},
  shorttitle = {Automatic {{Speech Recognition Errors Detection}} and {{Correction}}},
  author = {Errattahi, Rahhal and El Hannani, Asmaa and Ouahmane, Hassan},
  date = {2018-01-01},
  journaltitle = {Procedia Computer Science},
  shortjournal = {Procedia Computer Science},
  series = {1st {{International Conference}} on {{Natural Language}} and {{Speech Processing}}},
  volume = {128},
  pages = {32--37},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2018.03.005},
  url = {https://www.sciencedirect.com/science/article/pii/S1877050918302187},
  urldate = {2023-10-19},
  abstract = {Even though Automatic Speech Recognition (ASR) has matured to the point of commercial applications, high error rate in some speech recognition domains remain as one of the main impediment factors to the wide adoption of speech technology, and especially for continuous large vocabulary speech recognition applications. The persistent presence of ASR errors have intensified the need to find alternative techniques to automatically detect and correct such errors. The correction of the transcription errors is very crucial not only to improve the speech recognition accuracy, but also to avoid the propagation of the errors to the subsequent language processing modules such as machine translation. In this paper, basic principles of ASR evaluation are first summarized, and then the state of the current ASR errors detection and correction research is reviewed. We focus on emerging techniques using word error rate metric.}
}

@article{eslami_neural_2018,
  title = {Neural Scene Representation and Rendering},
  author = {Eslami, S M Ali and Rezende, Danilo Jimenez and Besse, Frederic and Viola, Fabio and Morcos, Ari S and Garnelo, Marta and Ruderman, Avraham and Rusu, Andrei A and Danihelka, Ivo and Gregor, Karol and Reichert, David P and Buesing, Lars and Weber, Theophane and Vinyals, Oriol and Rosenbaum, Dan and Rabinowitz, Neil and King, Helen and Hillier, Chloe and Botvinick, Matt},
  date = {2018},
  journaltitle = {Science},
  volume = {360},
  number = {6394},
  eprint = {29903970},
  eprinttype = {pmid},
  pages = {1204--1210},
  issn = {0036-8075},
  doi = {10.1126/science.aar6170},
  abstract = {Scene representation—the process of converting visual sensory data into concise descriptions—is a requirement for intelligent behavior. Recent work has shown that neural networks excel at this task when provided with large, labeled datasets. However, removing the reliance on human labeling remains an important open problem. To this end, we introduce the Generative Query Network (GQN), a framework within which machines learn to represent scenes using only their own sensors. The GQN takes as input images of a scene taken from different viewpoints, constructs an internal representation, and uses this representation to predict the appearance of that scene from previously unobserved viewpoints. The GQN demonstrates representation learning without human labels or domain knowledge, paving the way toward machines that autonomously learn to understand the world around them.},
  isbn = {1312.6114v10}
}

@misc{esser_taming_2021,
  title = {Taming {{Transformers}} for {{High-Resolution Image Synthesis}}},
  author = {Esser, Patrick and Rombach, Robin and Ommer, Björn},
  date = {2021-02-11},
  eprint = {2012.09841},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2012.09841},
  urldate = {2021-03-05},
  abstract = {Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a contextrich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semanticallyguided synthesis of megapixel images with transformers. Project page at https://git.io/JLlvY.},
  langid = {english}
}

@misc{europeancommission_briefing_2021,
  title = {Briefing on the {{Artificial Intelligence Act}}},
  author = {{European Commission}},
  date = {2021-06},
  url = {https://www.europarl.europa.eu/RegData/etudes/BRIE/2021/698792/EPRS_BRI(2021)698792_EN.pdf},
  abstract = {The European Commission tabled a proposal for an EU regulatory framework on artificial intelligence (AI) in April 2021. The draft AI act is the first ever attempt to enact a horizontal regulation for AI. The proposed legal framework focuses on the specific utilisation of AI systems and associated risks. The Commission proposes to establish a technology-neutral definition of AI systems in EU law and to lay down a classification for AI systems with different requirements and obligations tailored on a 'risk-based approach'. Some AI systems presenting 'unacceptable' risks would be prohibited. A wide range of 'high-risk' AI systems would be authorised, but subject to a set of requirements and obligations to gain access to the EU market. Those AI systems presenting only 'limited risk' would be subject to very light transparency obligations. The Council agreed the EU Member States' general position in December 2021. Parliament voted on its position in June 2023. EU lawmakers are now starting negotiations to finalise the new legislation, with substantial amendments to the Commission's proposal including revising the definition of AI systems, broadening the list of prohibited AI systems, and imposing obligations on general purpose AI and generative AI models such as ChatGPT.},
  langid = {english}
}

@book{europeanparliament_artificial_2022,
  title = {Artificial {{Intelligence}} in {{Healthcare}} – {{Applications}}, {{Risks}}, and {{Ethical}} and {{Societal Impacts}}},
  author = {{European Parliament} and {Directorate-General for Parliamentary Research Services} and Lekadir, K and Quaglio, G and Tselioudis Garmendia, A and Gallin, C},
  date = {2022},
  publisher = {{European Parliament}},
  doi = {doi/10.2861/568473}
}

@book{europeanparliamentdirectorategeneralforparliamentaryresearchservices_artificial_2022,
  title = {Artificial {{Intelligence}} in {{Healthcare}}: {{Applications}}, {{Risks}}, and {{Ethical}} and {{Societal Impacts}}.},
  shorttitle = {Artificial {{Intelligence}} in {{Healthcare}}},
  author = {{European Parliament. Directorate General for Parliamentary Research Services}},
  date = {2022},
  publisher = {{Publications Office}},
  location = {{LU}},
  url = {https://data.europa.eu/doi/10.2861/568473},
  urldate = {2023-08-26},
  langid = {english}
}

@inproceedings{evain_lebenchmark_2021,
  title = {{{LeBenchmark}}: {{A}} Reproducible Framework for Assessing Self-Supervised Representation Learning from Speech},
  booktitle = {Annual {{Conference}} of the {{International Speech Communication Association}}},
  author = {Evain, Solène and Nguyen, Ha and Le, Hang and Boito, Marcely Zanon and Mdhaffar, Salima and Alisamir, Sina and Tong, Ziyi and Tomashenko, Natalia and Dinarelli, Marco and Parcollet, Titouan and Allauzen, Alexandre and Estève, Yannick and Lecouteux, Benjamin and Portet, François and Rossato, Solange and Ringeval, Fabien and Schwab, Didier and Besacier, Laurent},
  date = {2021}
}

@misc{everitt_reward_2019,
  title = {Reward {{Tampering Problems}} and {{Solutions}} in {{Reinforcement Learning}}},
  author = {Everitt, Tom and Hutter, Marcus},
  date = {2019},
  eprint = {1908.04734v2},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1908.04734v2},
  urldate = {2019-08-30},
  abstract = {Can an arbitrarily intelligent reinforcement learning agent be kept under control by a human user? Or do agents with sufficient intelligence inevitably find ways to shortcut their reward signal? This question impacts how far reinforcement learning can be scaled, and whether alternative paradigms must be developed in order to build safe artificial general intelligence. In this paper, we use an intuitive yet precise graphical model called causal influence diagrams to formalize reward tampering problems. We also describe a number of tweaks to the reinforcement learning objective that prevent incentives for reward tampering. We verify the solutions using recently developed graphical criteria for inferring agent incentives from causal influence diagrams.}
}

@misc{fabius_variational_2015,
  title = {Variational {{Recurrent Auto-Encoders}}},
  author = {Fabius, Otto and family=Amersfoort, given=Joost R., prefix=van, useprefix=true},
  date = {2015-06-15},
  eprint = {1412.6581},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1412.6581},
  urldate = {2020-05-22},
  abstract = {In this paper we propose a model that combines the strengths of RNNs and SGVB: the Variational Recurrent Auto-Encoder (VRAE). Such a model can be used for efficient, large scale unsupervised learning on time series data, mapping the time series data to a latent vector representation. The model is generative, such that data can be generated from samples of the latent space. An important contribution of this work is that the model can make use of unlabeled data in order to facilitate supervised training of RNNs by initialising the weights and network state.},
  langid = {english}
}

@unpublished{fan_multiscale_2021,
  title = {Multiscale {{Vision Transformers}}},
  author = {Fan, Haoqi and Xiong, Bo and Mangalam, Karttikeya and Li, Yanghao and Yan, Zhicheng and Malik, Jitendra and Feichtenhofer, Christoph},
  date = {2021-04-22},
  eprint = {2104.11227},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2104.11227},
  urldate = {2022-05-03},
  abstract = {We present Multiscale Vision Transformers (MViT) for video and image recognition, by connecting the seminal idea of multiscale feature hierarchies with transformer models. Multiscale Transformers have several channel-resolution scale stages. Starting from the input resolution and a small channel dimension, the stages hierarchically expand the channel capacity while reducing the spatial resolution. This creates a multiscale pyramid of features with early layers operating at high spatial resolution to model simple low-level visual information, and deeper layers at spatially coarse, but complex, high-dimensional features. We evaluate this fundamental architectural prior for modeling the dense nature of visual signals for a variety of video recognition tasks where it outperforms concurrent vision transformers that rely on large scale external pre-training and are 5-10x more costly in computation and parameters. We further remove the temporal dimension and apply our model for image classification where it outperforms prior work on vision transformers. Code is available at: https://github.com/facebookresearch/SlowFast},
  keywords = {Unread}
}

@misc{fan_training_2020,
  title = {Training with {{Quantization Noise}} for {{Extreme Model Compression}}},
  author = {Fan, Angela and Stock, Pierre and Graham, Benjamin and Grave, Edouard and Gribonval, Remi and Jegou, Herve and Joulin, Armand},
  date = {2020-04-17},
  eprint = {2004.07320},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2004.07320},
  urldate = {2020-10-21},
  abstract = {We tackle the problem of producing compact models, maximizing their accuracy for a given model size. A standard solution is to train networks with Quantization Aware Training [1], where the weights are quantized during training and the gradients approximated with the Straight-Through Estimator [2]. In this paper, we extend this approach to work with extreme compression methods where the approximations introduced by STE are severe. Our proposal is to only quantize a different random subset of weights during each forward, allowing for unbiased gradients to flow through the other weights. Controlling the amount of noise and its form allows for extreme compression rates while maintaining the performance of the original model. As a result we establish new state-of-the-art compromises between accuracy and model size both in natural language processing and image classification. For example, applying our method to state-of-the-art Transformer and ConvNet architectures, we can achieve 82.5\% accuracy on MNLI by compressing RoBERTa to 14 MB and 80.0\% top-1 accuracy on ImageNet by compressing an EfficientNet-B3 to 3.3 MB.},
  langid = {english}
}

@misc{fang_discrete_2021,
  title = {Discrete {{Auto-regressive Variational Attention Models}} for {{Text Modeling}}},
  author = {Fang, Xianghong and Bai, Haoli and Li, Jian and Xu, Zenglin and Lyu, Michael and King, Irwin},
  date = {2021-06-16},
  eprint = {2106.08571},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2106.08571},
  urldate = {2021-06-27},
  abstract = {Variational autoencoders (VAEs) have been widely applied for text modeling. In practice, however, they are troubled by two challenges: information underrepresentation and posterior collapse. The former arises as only the last hidden state of LSTM encoder is transformed into the latent space, which is generally insufficient to summarize the data. The latter is a longstanding problem during the training of VAEs as the optimization is trapped to a disastrous local optimum. In this paper, we propose Discrete Auto-regressive Variational Attention Model (DAVAM) to address the challenges. Specifically, we introduce an auto-regressive variational attention approach to enrich the latent space by effectively capturing the semantic dependency from the input. We further design discrete latent space for the variational attention and mathematically show that our model is free from posterior collapse. Extensive experiments on language modeling tasks demonstrate the superiority of DAVAM against several VAE counterparts. Code will be released.},
  langid = {english}
}

@inproceedings{fang_implicit_2019,
  title = {Implicit {{Deep Latent Variable Models}} for {{Text Generation}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  author = {Fang, Le and Li, Chunyuan and Gao, Jianfeng and Dong, Wen and Chen, Changyou},
  date = {2019},
  pages = {3944--3954},
  publisher = {{Association for Computational Linguistics}},
  location = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1407},
  url = {https://www.aclweb.org/anthology/D19-1407},
  urldate = {2021-07-20},
  abstract = {Deep latent variable models (LVM) such as variational auto-encoder (VAE) have recently played an important role in text generation. One key factor is the exploitation of smooth latent structures to guide the generation. However, the representation power of VAEs is limited due to two reasons: (1) the Gaussian assumption is often made on the variational posteriors; and meanwhile (2) a notorious “posterior collapse” issue occurs. In this paper, we advocate sample-based representations of variational distributions for natural language, leading to implicit latent features, which can provide flexible representation power compared with Gaussian-based posteriors. We further develop an LVM to directly match the aggregated posterior to the prior. It can be viewed as a natural extension of VAEs with a regularization of maximizing mutual information, mitigating the “posterior collapse” issue. We demonstrate the effectiveness and versatility of our models in various text generation scenarios, including language modeling, unaligned style transfer, and dialog response generation. The source code to reproduce our experimental results is available on GitHub1.},
  eventtitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  langid = {english}
}

@inproceedings{fang_outofdistribution_,
  title = {Is {{Out-of-Distribution Detection Learnable}}?},
  booktitle = {Proceedings of the 36th {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Fang, Zhen and Li, Yixuan and Lu, Jie and Dong, Jiahua and Han, Bo and Liu, Feng},
  pages = {52},
  abstract = {Supervised learning aims to train a classifier under the assumption that training and test data are from the same distribution. To ease the above assumption, researchers have studied a more realistic setting: out-of-distribution (OOD) detection, where test data may come from classes that are unknown during training (i.e., OOD data). Due to the unavailability and diversity of OOD data, good generalization ability is crucial for effective OOD detection algorithms. To study the generalization of OOD detection, in this paper, we investigate the probably approximately correct (PAC) learning theory of OOD detection, which is proposed by researchers as an open problem. First, we find a necessary condition for the learnability of OOD detection. Then, using this condition, we prove several impossibility theorems for the learnability of OOD detection under some scenarios. Although the impossibility theorems are frustrating, we find that some conditions of these impossibility theorems may not hold in some practical scenarios. Based on this observation, we next give several necessary and sufficient conditions to characterize the learnability of OOD detection in some practical scenarios. Lastly, we also offer theoretical supports for several representative OOD detection works based on our OOD theory.},
  eventtitle = {Conference on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  langid = {english}
}

@article{farabet_learning_2013,
  title = {Learning Hierarchical Features for Scene Labeling},
  author = {Farabet, Clement and Couprie, Camille and Najman, Laurent and LeCun, Yann A.},
  date = {2013},
  journaltitle = {IEEE transactions on pattern analysis and machine intelligence},
  volume = {35},
  number = {8},
  pages = {1915--1929},
  publisher = {{IEEE}}
}

@inproceedings{faruqi_community_2014,
  title = {Community Evaluation and Exchange of Word Vectors and Wordvectors.Org},
  booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics ({{ACL}})},
  author = {Faruqi, Manaal and Dyer, Chris},
  date = {2014}
}

@article{fawzi_discovering_2022,
  title = {Discovering Faster Matrix Multiplication Algorithms with Reinforcement Learning},
  author = {Fawzi, Alhussein and Balog, Matej and Huang, Aja and Hubert, Thomas and Romera-Paredes, Bernardino and Barekatain, Mohammadamin and Novikov, Alexander and R Ruiz, Francisco J and Schrittwieser, Julian and Swirszcz, Grzegorz and others},
  date = {2022},
  journaltitle = {Nature},
  volume = {610},
  number = {7930},
  pages = {47--53}
}

@online{fawzi_discovering_2022a,
  title = {Discovering Novel Algorithms with {{AlphaTensor}}},
  author = {Fawzi, Alhussein and Balog, Matej and Romera-Paredes, Bernardino and Hassabis, Demis and Kohli, Pushmeet},
  date = {2022-10-05},
  url = {https://www.deepmind.com/blog/discovering-novel-algorithms-with-alphatensor},
  urldate = {2023-07-10}
}

@article{fedus_switch_2022,
  title = {Switch Transformers: {{Scaling}} to Trillion Parameter Models with Simple and Efficient Sparsity},
  author = {Fedus, William and Zoph, Barret and Shazeer, Noam},
  date = {2022},
  journaltitle = {Journal of Machine Learning Research},
  volume = {23},
  number = {120},
  pages = {1--39},
  url = {http://jmlr.org/papers/v23/21-0998.html}
}

@article{ferrucci_building_2010,
  title = {Building {{Watson}}: {{An Overview}} of the {{DeepQA Project}}},
  author = {Ferrucci, David},
  date = {2010},
  journaltitle = {Ai Magazine},
  volume = {31},
  number = {3},
  issn = {07384602},
  url = {https://findit.dtu.dk/en/catalog?utf8=%E2%9C%93&locale=en&search_field=all_fields&q=building+watson+an+overview+of+deepqa},
  urldate = {2017-10-23}
}

@inproceedings{feuchtDescriptionbasedLabelAttention2021,
  title = {Description-Based {{Label Attention Classifier}} for {{Explainable ICD-9 Classification}}},
  booktitle = {Proceedings of the {{Seventh Workshop}} on {{Noisy User-generated Text}} ({{W-NUT}} 2021)},
  author = {Feucht, Malte and Wu, Zhiliang and Althammer, Sophia and Tresp, Volker},
  date = {2021-11},
  pages = {62--66},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2021.wnut-1.8},
  abstract = {ICD-9 coding is a relevant clinical billing task, where unstructured texts with information about a patient's diagnosis and treatments are annotated with multiple ICD-9 codes. Automated ICD-9 coding is an active research field, where CNN- and RNN-based model architectures represent the state-of-the-art approaches. In this work, we propose a description-based label attention classifier to improve the model explainability when dealing with noisy texts like clinical notes.}
}

@misc{finn_modelagnostic_2017,
  title = {Model-{{Agnostic Meta-Learning}} for {{Fast Adaptation}} of {{Deep Networks}}},
  author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  date = {2017-07-18},
  eprint = {1703.03400},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1703.03400},
  urldate = {2019-10-30},
  abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two fewshot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
  langid = {english}
}

@article{fischer_bounding_2011,
  title = {Bounding the Bias of Contrastive Divergence Learning},
  author = {Fischer, A. and Igel, C.},
  date = {2011},
  journaltitle = {Neural Computation},
  volume = {23},
  pages = {664--673}
}

@article{fischer_training_2014,
  title = {Training Restricted {{Boltzmann}} Machines: {{An}} Introduction},
  author = {Fischer, A. and Igel, C.},
  date = {2014},
  journaltitle = {Pattern Recognition},
  volume = {47},
  pages = {25--39}
}

@book{fisher_statistical_1925,
  title = {Statistical Methods for Research Workers},
  author = {Fisher, R.A.},
  date = {1925},
  publisher = {{Edinburgh Oliver \& Boyd}}
}

@article{fisher_use_1936,
  title = {The {{Use}} of {{Multiple Measurements}} in {{Taxonomic Problems}}},
  author = {Fisher, Ronald A},
  date = {1936},
  journaltitle = {Annals of eugenics},
  volume = {7},
  number = {2},
  pages = {179--188}
}

@misc{floto_exponentially_2022,
  title = {The {{Exponentially Tilted Gaussian Prior}} for {{Variational Autoencoders}}},
  author = {Floto, Griffin and Kremer, Stefan and Nica, Mihai},
  date = {2022-02-01},
  eprint = {2111.15646},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2111.15646},
  urldate = {2022-02-08},
  abstract = {An important property for deep neural networks is the ability to perform robust out-of-distribution detection on previously unseen data. This property is essential for safety purposes when deploying models for real world applications. Recent studies show that probabilistic generative models can perform poorly on this task, which is surprising given that they seek to estimate the likelihood of training data. To alleviate this issue, we propose the exponentially tilted Gaussian prior distribution for the Variational Autoencoder (VAE) which pulls points onto the surface of a hyper-sphere in latent space. This achieves state-of-the art results on the area under the curve-receiver operator characteristics metric using just the negative log-likelihood that the VAE naturally assigns. Because this prior is a simple modification of the traditional VAE prior, it is faster and easier to implement than competitive methods.}
}

@article{folks_asymptotic_1971,
  title = {Asymptotic Optimality of Fisher’s Method of Combining Independent Tests},
  author = {Folks, Jl and Little, R},
  date = {1971},
  journaltitle = {Journal of the American Statistical Association}
}

@misc{fortuin_priors_2021,
  title = {Priors in {{Bayesian Deep Learning}}: {{A Review}}},
  shorttitle = {Priors in {{Bayesian Deep Learning}}},
  author = {Fortuin, Vincent},
  date = {2021-05-14},
  eprint = {2105.06868},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2105.06868},
  urldate = {2021-05-19},
  abstract = {While the choice of prior is one of the most critical parts of the Bayesian inference workflow, recent Bayesian deep learning models have often fallen back on uninformative priors, such as standard Gaussians. In this review, we highlight the importance of prior choices for Bayesian deep learning and present an overview of different priors that have been proposed for (deep) Gaussian processes, variational autoencoders, and Bayesian neural networks. We also outline different methods of learning priors for these models from data. We hope to motivate practitioners in Bayesian deep learning to think more carefully about the prior specification for their models and to provide them with some inspiration in this regard.},
  langid = {english}
}

@article{fowler_square_1998,
  title = {Square Root Approximations in {{Old Babylonian}} Mathematics: {{YBC}} 7289 in Context},
  author = {Fowler, David and Robson, Eleanor},
  date = {1998},
  journaltitle = {Historia mathematica},
  volume = {25},
  number = {4},
  pages = {366--378},
  publisher = {{Elsevier}}
}

@thesis{fraccaro_deep_2018,
  type = {phdthesis},
  title = {Deep {{Latent Variable Models}} for {{Sequential Data}}},
  author = {Fraccaro, Marco},
  date = {2018},
  institution = {{Technical University of Denmark}},
  location = {{Kongens Lyngby}},
  abstract = {Over the last few decades an ever-increasing amount of data is being collected in a wide range of applications. This has boosted the development of mathematical models that are able to analyze it and discover its underlying structure, and use the extracted information to solve a multitude of different tasks, such as for predictive modelling or pattern recognition. The available data is however often complex and high-dimensional, making traditional data analysis methods ineffective in many applications. In the recent years there has then been a big focus on the development of more powerful models, that need to be general enough to be able to handle many diverse applications and kinds of data. Some of the most interesting advancements in this research direction have been recently obtained combining ideas from probabilistic modelling and deep learning. Variational auto-encoders (VAEs), that belong to the broader family of deep latent variable models, are powerful and scalable models that can be used for unsupervised learning of complex high-dimensional data distributions. They achieve this by parameterizing expressive probability distributions over the latent variables of the model using deep neural networks. VAEs can be used in applications with static data, for example as a generative model of images, but they are not suitable to model temporal data such as the sequences of images that form a video. However, a major part of the data that is being collected has a sequential nature, and finding powerful architectures that are able to model it is therefore fundamental. In the first part of the thesis we will introduce a broad class of deep latent variable models for sequential data, that can be used for unsupervised learning of complex and high-dimensional sequential data distributions. We obtain these models by extending VAEs to the temporal setting, and further combining ideas from deep learning (e.g. deep and recurrent neural networks) and probabilistic modelling (e.g. state-space models) to define generative models for the data that use deep neural networks to parameterize very flexible probability distributions. This results in a family of powerful architectures that can model a wide range of complex temporal data, and can be trained in a scalable way using large unlabelled datasets. In the second part of the thesis we will then present in detail three architectures belonging to this family of models. First, we will introduce stochastic recurrent neural networks (Fraccaro et al., 2016c), that combine the expressiveness of recurrent neural networks and the ability of state-space models to model the uncertainty in the learned latent representation. We will then present Kalman variational auto-encoders (Fraccaro et al., 2017), that can learn from data disentangled and more interpretable visual and dynamic representations. Finally, we will show that to deal with temporal applications that require a high memory capacity we can combine deep latent variable models with external memory architectures, as in the generative temporal model with spatial memory of Fraccaro et al., (2018).},
  langid = {english},
  pagetotal = {146}
}

@inproceedings{fraccaro_sequential_2016,
  title = {Sequential {{Neural Models}} with {{Stochastic Layers}}},
  booktitle = {Proceedings of the 30th {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Fraccaro, Marco and Sønderby, Søren Kaae and Paquet, Ulrich and Winther, Ole},
  date = {2016},
  location = {{Barcelona, Spain}},
  abstract = {How can we efficiently propagate uncertainty in a latent state representation with recurrent neural networks? This paper introduces stochastic recurrent neural networks which glue a deterministic recurrent neural network and a state space model together to form a stochastic and sequential neural generative model. The clear separation of deterministic and stochastic layers allows a structured variational inference network to track the factorization of the model’s posterior distribution. By retaining both the nonlinear recursive structure of a recurrent neural network and averaging over the uncertainty in a latent path, like a state space model, we improve the state of the art results on the Blizzard and TIMIT speech modeling data sets by a large margin, while achieving comparable performances to competing methods on polyphonic music modeling.},
  eventtitle = {Conference on {{Neural Information Processing Systems}}},
  langid = {english}
}

@article{fukushima_neocognitron_1980,
  title = {Neocognitron: {{A Self-organizing Neural Network Model}} for a {{Mechanism}} of {{Pattern Recognition Unaffected}} by {{Shift}} in {{Position}}},
  author = {Fukushima, Kunihiko},
  date = {1980},
  journaltitle = {Biological Cybernetics},
  volume = {36},
  number = {4},
  pages = {193--202},
  abstract = {A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by “learning without a teacher”, and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname “neocognitron”. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of “S-cells”, which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of “C-cells” similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any “teacher” during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.}
}

@article{gaetz_massive_2018,
  title = {Massive Cortical Reorganization Is Reversible Following Bilateral Transplants of the Hands: Evidence from the First Successful Bilateral Pediatric Hand Transplant Patient},
  author = {Gaetz, William and Kessler, Sudha K. and Roberts, Tim P. L. and Berman, Jeffrey I. and Levy, Todd J. and Hsia, Michelle and Humpl, Deborah and Schwartz, Erin S. and Amaral, Sandra and Chang, Ben and Levin, Lawrence Scott},
  date = {2018-12-05},
  journaltitle = {Annals of Clinical and Translational Neurology},
  volume = {5},
  number = {1},
  eprint = {29376095},
  eprinttype = {pmid},
  pages = {92--97},
  issn = {23289503},
  doi = {10.1002/acn3.501},
  url = {http://arxiv.org/abs/1712.01815},
  urldate = {2017-12-06},
  abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
  isbn = {3013372370}
}

@misc{gal_dropout_2016,
  title = {Dropout as a {{Bayesian Approximation}}: {{Representing Model Uncertainty}} in {{Deep Learning}}},
  shorttitle = {Dropout as a {{Bayesian Approximation}}},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  date = {2016-10-04},
  eprint = {1506.02142},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1506.02142},
  urldate = {2019-11-11},
  abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs –extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout’s uncertainty. Various network architectures and nonlinearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout’s uncertainty in deep reinforcement learning.},
  langid = {english}
}

@misc{gal_theoretically_2015,
  title = {A {{Theoretically Grounded Application}} of {{Dropout}} in {{Recurrent Neural Networks}}},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  date = {2015},
  eprint = {21803542},
  eprinttype = {pmid},
  url = {http://arxiv.org/abs/1512.05287},
  urldate = {2018-05-06},
  abstract = {Recurrent neural networks (RNNs) stand at the forefront of many recent developments in deep learning. Yet a major difficulty with these models is their tendency to overfit, with dropout shown to fail when applied to recurrent layers. Recent results at the intersection of Bayesian modelling and deep learning offer a Bayesian interpretation of common deep learning techniques such as dropout. This grounding of dropout in approximate Bayesian inference suggests an extension of the theoretical results, offering insights into the use of dropout with RNN models. We apply this new variational inference based dropout technique in LSTM and GRU models, assessing it on language modelling and sentiment analysis tasks. The new approach outperforms existing techniques, and to the best of our knowledge improves on the single model state-of-the-art in language modelling with the Penn Treebank (73.4 test perplexity). This extends our arsenal of variational tools in deep learning.},
  isbn = {9789537619084}
}

@thesis{gal_uncertainty_2016,
  type = {phdthesis},
  title = {Uncertainty in {{Deep Learning}}},
  author = {Gal, Yarin},
  date = {2016},
  eprint = {27178640},
  eprinttype = {pmid},
  institution = {{University of Cambridge}},
  issn = {18736246},
  doi = {10.1371/journal.pcbi.1005062},
  abstract = {Deep learning has attracted tremendous attention from researchers in various fields of information engineering such as AI, computer vision, and language processing [Kalchbrenner and Blunsom, 2013; Krizhevsky et al., 2012; Mnih et al., 2013], but also from more traditional sciences such as physics, biology, and manufacturing [Anjos et al., 2015; Baldi et al., 2014; Bergmann et al., 2014]. Neural networks, image processing tools such as convolutional neural networks, sequence processing models such as recurrent neural networks, and regularisation tools such as dropout, are used extensively. However, fields such as physics, biology, and manufacturing are ones in which representing model uncertainty is of crucial importance [Ghahramani, 2015; Krzywinski and Altman, 2013]. With the recent shift in many of these fields towards the use of Bayesian uncertainty [Herzog and Ostwald, 2013; Nuzzo, 2014; Trafimow and Marks, 2015], new needs arise from deep learning. In this work we develop tools to obtain practical uncertainty estimates in deep learning, casting recent deep learning tools as Bayesian models without changing either the models or the optimisation. In the first part of this thesis we develop the theory for such tools, providing applications and illustrative examples. We tie approximate inference in Bayesian models to dropout and other stochastic regularisation techniques, and assess the approximations empirically. We give example applications arising from this connection between modern deep learning and Bayesian modelling such as active learning of image data and data efficient deep reinforcement learning. We further demonstrate the method’s practicality through a survey of recent applications making use of the suggested tools in language applications, medical diagnostics, bioinformatics, image processing, and autonomous driving. In the second part of the thesis we explore its theoretical implications, and the insights stemming from the link between Bayesian modelling and deep learning. We discuss what determines model uncertainty properties, analyse the approximate inference analytically in the linear case, and theoretically examine various priors such as spike and slab priors.},
  issue = {October},
  pagetotal = {174},
  keywords = {★}
}

@online{gal_what_2015,
  title = {What My Deep Model Doesn't Know...},
  author = {Gal, Yarin},
  date = {2015},
  url = {http://www.cs.ox.ac.uk/people/yarin.gal/website/blog_3d801aa532c1ce.html},
  urldate = {2021-12-16},
  annotation = {Computer Science Department, University of Oxford}
}

@inproceedings{gales_speech_2014,
  title = {Speech Recognition and Keyword Spotting for Low-Resource Languages: {{BABEL}} Project Research at {{CUED}}},
  booktitle = {Fourth {{International Workshop}} on {{Spoken Language Technologies}} for {{Under-Resourced Languages}} ({{SLTU}})},
  author = {Gales, Mark JF and Knill, Kate M and Ragni, Anton and Rath, Shakti P},
  date = {2014}
}

@inproceedings{galibert_methodologies_2013,
  title = {Methodologies for the Evaluation of Speaker Diarization and Automatic Speech Recognition in the Presence of Overlapping Speech},
  booktitle = {Proceedings of the {{Annual Conference}} of the {{International Speech Communication Association}} ({{Interspeech}})},
  author = {Galibert, Olivier},
  date = {2013-08-25},
  pages = {1131--1134},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2013-303},
  url = {https://www.isca-speech.org/archive/interspeech_2013/galibert13_interspeech.html},
  urldate = {2023-04-14},
  abstract = {Speaker Diarization and Automatic Speech Recognition have been a topic of research for decades. Evaluating the developed systems has been required for almost as long. Following the NIST initiatives a number of metrics have become standard to handle these evaluations, namely the Diarization Error Rate and the Word Error Rate.},
  langid = {english}
}

@online{galke_bagofwords_2022,
  title = {Bag-of-{{Words}} vs. {{Graph}} vs. {{Sequence}} in {{Text Classification}}: {{Questioning}} the {{Necessity}} of {{Text-Graphs}} and the {{Surprising Strength}} of a {{Wide MLP}}},
  shorttitle = {Bag-of-{{Words}} vs. {{Graph}} vs. {{Sequence}} in {{Text Classification}}},
  author = {Galke, Lukas and Scherp, Ansgar},
  date = {2022-04-12},
  eprint = {2109.03777},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2109.03777},
  urldate = {2023-09-22},
  abstract = {Graph neural networks have triggered a resurgence of graph-based text classification methods, defining today's state of the art. We show that a wide multi-layer perceptron (MLP) using a Bag-of-Words (BoW) outperforms the recent graph-based models TextGCN and HeteGCN in an inductive text classification setting and is comparable with HyperGAT. Moreover, we fine-tune a sequence-based BERT and a lightweight DistilBERT model, which both outperform all state-of-the-art models. These results question the importance of synthetic graphs used in modern text classifiers. In terms of efficiency, DistilBERT is still twice as large as our BoW-based wide MLP, while graph-based models like TextGCN require setting up an \$\textbackslash mathcal\{O\}(N\^2)\$ graph, where \$N\$ is the vocabulary plus corpus size. Finally, since Transformers need to compute \$\textbackslash mathcal\{O\}(L\^2)\$ attention weights with sequence length \$L\$, the MLP models show higher training and inference speeds on datasets with long sequences.},
  pubstate = {preprint}
}

@inproceedings{gall_faster_2012,
  title = {Faster {{Algorithms}} for {{Rectangular Matrix Multiplication}}},
  booktitle = {2012 {{IEEE}} 53rd {{Annual Symposium}} on {{Foundations}} of {{Computer Science}}},
  author = {Gall, François Le},
  date = {2012-10},
  eprint = {1204.1111},
  eprinttype = {arxiv},
  pages = {514--523},
  doi = {10.1109/FOCS.2012.80},
  url = {http://arxiv.org/abs/1204.1111},
  urldate = {2023-07-13},
  abstract = {Let \{\textbackslash alpha\} be the maximal value such that the product of an n x n\^\{\textbackslash alpha\} matrix by an n\^\{\textbackslash alpha\} x n matrix can be computed with n\^\{2+o(1)\} arithmetic operations. In this paper we show that \textbackslash alpha{$>$}0.30298, which improves the previous record \textbackslash alpha{$>$}0.29462 by Coppersmith (Journal of Complexity, 1997). More generally, we construct a new algorithm for multiplying an n x n\^k matrix by an n\^k x n matrix, for any value k\textbackslash neq 1. The complexity of this algorithm is better than all known algorithms for rectangular matrix multiplication. In the case of square matrix multiplication (i.e., for k=1), we recover exactly the complexity of the algorithm by Coppersmith and Winograd (Journal of Symbolic Computation, 1990). These new upper bounds can be used to improve the time complexity of several known algorithms that rely on rectangular matrix multiplication. For example, we directly obtain a O(n\^\{2.5302\})-time algorithm for the all-pairs shortest paths problem over directed graphs with small integer weights, improving over the O(n\^\{2.575\})-time algorithm by Zwick (JACM 2002), and also improve the time complexity of sparse square matrix multiplication.}
}

@misc{gan_deep_2015,
  title = {Deep {{Temporal Sigmoid Belief Networks}} for {{Sequence Modeling}}},
  author = {Gan, Zhe and Li, Chunyuan and Henao, Ricardo and Carlson, David and Carin, Lawrence},
  date = {2015-09-23},
  eprint = {1509.07087},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1509.07087},
  urldate = {2020-05-22},
  abstract = {Deep dynamic generative models are developed to learn sequential dependencies in time-series data. The multi-layered model is designed by constructing a hierarchy of temporal sigmoid belief networks (TSBNs), defined as a sequential stack of sigmoid belief networks (SBNs). Each SBN has a contextual hidden state, inherited from the previous SBNs in the sequence, and is used to regulate its hidden bias. Scalable learning and inference algorithms are derived by introducing a recognition model that yields fast sampling from the variational posterior. This recognition model is trained jointly with the generative model, by maximizing its variational lower bound on the log-likelihood. Experimental results on bouncing balls, polyphonic music, motion capture, and text streams show that the proposed approach achieves state-of-the-art predictive performance, and has the capacity to synthesize various sequences.},
  langid = {english}
}

@article{gandy_sequential_2009,
  title = {Sequential {{Implementation}} of {{Monte Carlo Tests}} with {{Uniformly Bounded Resampling Risk}}},
  author = {Gandy, Axel},
  date = {2009-12},
  journaltitle = {Journal of the American Statistical Association},
  shortjournal = {Journal of the American Statistical Association},
  volume = {104},
  number = {488},
  pages = {1504--1511},
  issn = {0162-1459, 1537-274X},
  doi = {10.1198/jasa.2009.tm08368},
  url = {https://arxiv.org/abs/math/0612488#},
  urldate = {2023-04-04},
  abstract = {This paper introduces an open-ended sequential algorithm for computing the p-value of a test using Monte Carlo simulation. It guarantees that the resampling risk, the probability of a different decision than the one based on the theoretical p-value, is uniformly bounded by an arbitrarily small constant. Previously suggested sequential or non-sequential algorithms, using a bounded sample size, do not have this property. Although the algorithm is open-ended, the expected number of steps is finite, except when the p-value is on the threshold between rejecting and not rejecting. The algorithm is suitable as standard for implementing tests that require (re-)sampling. It can also be used in other situations: to check whether a test is conservative, iteratively to implement double bootstrap tests, and to determine the sample size required for a certain power.}
}

@online{ganguly_amortized_2022,
  title = {Amortized {{Variational Inference}}: {{Towards}} the {{Mathematical Foundation}} and {{Review}}},
  shorttitle = {Amortized {{Variational Inference}}},
  author = {Ganguly, Ankush and Jain, Sanjana and Watchareeruetai, Ukrit},
  date = {2022-09-22},
  eprint = {2209.10888},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2209.10888},
  urldate = {2022-09-27},
  abstract = {The core principle of Variational Inference (VI) is to convert the statistical inference problem of computing complex posterior probability densities into a tractable optimization problem. This property enables VI to be faster than several sampling-based techniques. However, the traditional VI algorithm is not scalable to large data sets and is unable to readily infer out-of-bounds data points without re-running the optimization process. Recent developments in the field, like stochastic-, black box- and amortized-VI, have helped address these issues. Generative modeling tasks nowadays widely make use of amortized VI for its efficiency and scalability, as it utilizes a parameterized function to learn the approximate posterior density parameters. With this paper, we review the mathematical foundations of various VI techniques to form the basis for understanding amortized VI. Additionally, we provide an overview of the recent trends that address several issues of amortized VI, such as the amortization gap, generalization issues, inconsistent representation learning, and posterior collapse. Finally, we analyze alternate divergence measures that improve VI optimization.},
  langid = {english},
  pubstate = {preprint}
}

@article{ganin_domain-adversarial_2016,
  title = {Domain-Adversarial Training of Neural Networks},
  author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, François and Marchand, Mario and Lempitsky, Victor},
  date = {2016},
  journaltitle = {Journal of Machine Learning Research},
  volume = {17},
  number = {59},
  pages = {1--35}
}

@article{ganin_unsupervised_2014,
  title = {Unsupervised {{Domain Adaptation}} by {{Backpropagation}}},
  author = {Ganin, Yaroslav and Lempitsky, Victor},
  date = {2014-09-26},
  url = {https://arxiv.org/abs/1409.7495},
  urldate = {2018-09-25},
  abstract = {Top-performing deep architectures are trained on massive amounts of labeled data. In the absence of labeled data for a certain task, domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain (e.g. synthetic images) are available. Here, we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain (no labeled target-domain data is necessary).  As the training progresses, the approach promotes the emergence of "deep" features that are (i) discriminative for the main learning task on the source domain and (ii) invariant with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a simple new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation.  Overall, the approach can be implemented with little effort using any of the deep-learning packages. The method performs very well in a series of image classification experiments, achieving adaptation effect in the presence of big domain shifts and outperforming previous state-of-the-art on Office datasets.},
  langid = {english}
}

@misc{gao_neural_2018,
  title = {Neural {{Approaches}} to {{Conversational AI}}},
  author = {Gao, Jianfeng and Galley, Michel and Li, Lihong},
  date = {2018-09-21},
  eprint = {1809.08267},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1809.08267},
  urldate = {2019-08-02},
  abstract = {The present paper surveys neural approaches to conversational AI that have been developed in the last few years. We group conversational systems into three categories: (1) question answering agents, (2) task-oriented dialogue agents, and (3) chatbots. For each category, we present a review of state-of-the-art neural approaches, draw the connection between them and traditional approaches, and discuss the progress that has been made and challenges still being faced, using specific systems and models as case studies.},
  langid = {english}
}

@article{gaoLimitationsTransformersClinical2021,
  title = {Limitations of {{Transformers}} on {{Clinical Text Classification}}},
  author = {Gao, Shang and Alawad, Mohammed and Young, M. Todd and Gounley, John and Schaefferkoetter, Noah and Yoon, Hong Jun and Wu, Xiao-Cheng and Durbin, Eric B. and Doherty, Jennifer and Stroup, Antoinette and Coyle, Linda and Tourassi, Georgia},
  date = {2021-09},
  journaltitle = {IEEE journal of biomedical and health informatics},
  volume = {25},
  number = {9},
  eprint = {33635801},
  eprinttype = {pmid},
  pages = {3596--3607},
  issn = {2168-2208},
  doi = {10.1109/JBHI.2021.3062322},
  abstract = {Bidirectional Encoder Representations from Transformers (BERT) and BERT-based approaches are the current state-of-the-art in many natural language processing (NLP) tasks; however, their application to document classification on long clinical texts is limited. In this work, we introduce four methods to scale BERT, which by default can only handle input sequences up to approximately 400 words long, to perform document classification on clinical texts several thousand words long. We compare these methods against two much simpler architectures - a word-level convolutional neural network and a hierarchical self-attention network - and show that BERT often cannot beat these simpler baselines when classifying MIMIC-III discharge summaries and SEER cancer pathology reports. In our analysis, we show that two key components of BERT - pretraining and WordPiece tokenization - may actually be inhibiting BERT's performance on clinical text classification tasks where the input document is several thousand words long and where correctly identifying labels may depend more on identifying a few key words or phrases rather than understanding the contextual meaning of sequences of text.},
  langid = {english},
  pmcid = {PMC8387496}
}

@article{garczarek_classification_2002,
  title = {Classification Rules in Standardized Partition Spaces},
  author = {Garczarek, Ursula},
  date = {2002},
  publisher = {{Universität Dortmund}}
}

@misc{garipov_loss_2018,
  title = {Loss {{Surfaces}}, {{Mode Connectivity}}, and {{Fast Ensembling}} of {{DNNs}}},
  author = {Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry and Wilson, Andrew Gordon},
  date = {2018},
  eprint = {1802.10026},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1802.10026},
  urldate = {2018-05-01},
  abstract = {The loss functions of deep neural networks are complex and their geometric properties are not well understood. We show that the optima of these complex loss functions are in fact connected by simple curves, such as a polygonal chain with only one bend, over which training and test accuracy are nearly constant. We introduce a training procedure to discover these high-accuracy pathways between modes. Inspired by this new geometric insight, we also propose a new ensembling method entitled Fast Geometric Ensembling (FGE). Using FGE we can train high-performing ensembles in the time required to train a single model. We achieve improved performance compared to the recent state-of-the-art Snapshot Ensembles, on CIFAR-10 and CIFAR-100, using state-of-the-art deep residual networks. On ImageNet we improve the top-1 error-rate of a pre-trained ResNet by 0.56\% by running FGE for just 5 epochs.}
}

@misc{garofolo_timit_1993,
  title = {{{TIMIT Acoustic-Phonetic Continuous Speech Corpus LDC93S1}}},
  author = {Garofolo, John S.},
  date = {1993},
  url = {https://catalog.ldc.upenn.edu/LDC93S1},
  organization = {{Linguistic Data Consortium}},
  annotation = {Web Download}
}

@misc{gatys_neural_2015,
  title = {A {{Neural Algorithm}} of {{Artistic Style}}},
  author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
  date = {2015},
  eprint = {1000200972},
  eprinttype = {pmid},
  url = {http://arxiv.org/abs/1508.06576},
  abstract = {In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks. Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.},
  isbn = {2200000006}
}

@inproceedings{gauthier_collecting_2016,
  title = {Collecting Resources in Sub-{{Saharan African}} Languages for Automatic Speech Recognition: {{A}} Case Study of {{Wolof}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}})},
  author = {Gauthier, Elodie and Besacier, Laurent and Voisin, Sylvie and Melese, Michael and Elingui, Uriel Pascal},
  date = {2016}
}

@article{gauvain_maximum_1994,
  title = {Maximum a Posteriori Estimation for Multivariate {{Gaussian}} Mixture Observations of {{Markov}} Chains},
  author = {Gauvain, J.-L. and Lee, Chin-Hui},
  date = {1994},
  journaltitle = {IEEE Transactions on Speech and Audio Processing},
  volume = {2},
  number = {2},
  pages = {291--298}
}

@article{gawlikowski_survey_2023,
  title = {A Survey of Uncertainty in Deep Neural Networks},
  author = {Gawlikowski, Jakob and Tassi, Cedrique Rovile Njieutcheu and Ali, Mohsin and Lee, Jongseok and Humt, Matthias and Feng, Jianxiang and Kruspe, Anna and Triebel, Rudolph and Jung, Peter and Roscher, Ribana and others},
  date = {2023},
  journaltitle = {Artificial Intelligence Review},
  pages = {1--77},
  publisher = {{Springer}}
}

@misc{ge_speaker_2017,
  title = {Speaker {{Change Detection Using Features}} through {{A Neural Network Speaker Classifier}}},
  author = {Ge, Zhenhao and Iyer, Ananth N. and Cheluvaraja, Srinath and Ganapathiraju, Aravind},
  date = {2017-02-07},
  eprint = {1702.02285},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1702.02285},
  urldate = {2019-02-04},
  abstract = {The mechanism proposed here is for real-time speaker change detection in conversations, which firstly trains a neural network text-independent speaker classifier using in-domain speaker data. Through the network, features of conversational speech from out-of-domain speakers are then converted into likelihood vectors, i.e. similarity scores comparing to the in-domain speakers. These transformed features demonstrate very distinctive patterns, which facilitates differentiating speakers and enable speaker change detection with some straight-forward distance metrics. The speaker classifier and the speaker change detector are trained/tested using speech of the first 200 (in-domain) and the remaining 126 (out-of-domain) male speakers in TIMIT respectively. For the speaker classification, 100\% accuracy at a 200 speaker size is achieved on any testing file, given the speech duration is at least 0.97 seconds. For the speaker change detection using speaker classification outputs, performance based on 0.5, 1, and 2 seconds of inspection intervals were evaluated in terms of error rate and F1 score, using synthesized data by concatenating speech from various speakers. It captures close to 97\% of the changes by comparing the current second of speech with the previous second, which is very competitive among literature using other methods.}
}

@article{gehring_convolutional_nodate,
  title = {A {{Convolutional Encoder Model}} for {{Neural Machine Translation}}},
  author = {Gehring, Jonas and Auli, Michael and Grangier, David and Dauphin, Yann N},
  url = {http://data.statmt.org/},
  urldate = {2018-07-16},
  abstract = {The prevalent approach to neural machine translation relies on bi-directional LSTMs to encode the source sentence. We present a faster and simpler architecture based on a succession of convolutional layers. This allows to encode the source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies. On WMT'16 English-Romanian translation we achieve competitive accuracy to the state-of-the-art and on WMT'15 English-German we outper-form several recently published results. Our models obtain almost the same accuracy as a very deep LSTM setup on WMT'14 English-French translation. We speed up CPU decoding by more than two times at the same or higher accuracy as a strong bi-directional LSTM. 1}
}

@article{geifman_selective_2017,
  title = {Selective Classification for Deep Neural Networks},
  author = {Geifman, Yonatan and El-Yaniv, Ran},
  date = {2017},
  journaltitle = {Advances in neural information processing systems},
  volume = {30}
}

@inproceedings{gelas_developments_2012,
  title = {Developments of {{Swahili}} Resources for an Automatic Speech Recognition System},
  booktitle = {Spoken {{Language Technologies}} for {{Under-Resourced Languages}}},
  author = {Gelas, Hadrien and Besacier, Laurent and Pellegrino, François},
  date = {2012}
}

@book{gelman_bayesian_2013,
  title = {Bayesian Data Analysis, Third Edition},
  author = {Gelman, A. and Carlin, J.B. and Stern, H.S. and Dunson, D.B. and Vehtari, A. and Rubin, D.B.},
  date = {2013-11-01},
  series = {Chapman \& {{Hall}}/{{CRC Texts}} in {{Statistical Science}}},
  publisher = {{Chapman and Hall/CRC}},
  isbn = {978-1-4398-4095-5},
  langid = {english},
  pagetotal = {675}
}

@misc{gelman_what_2021,
  title = {What Are the Most Important Statistical Ideas of the Past 50 Years?},
  author = {Gelman, Andrew and Vehtari, Aki},
  date = {2021-01-18},
  eprint = {2012.00174},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2012.00174},
  urldate = {2021-05-07},
  abstract = {We argue that the most important statistical ideas of the past half century are: counterfactual causal inference, bootstrapping and simulation-based inference, overparameterized models and regularization, multilevel models, generic computation algorithms, adaptive decision analysis, robust inference, and exploratory data analysis. We discuss common features of these ideas, how they relate to modern computing and big data, and how they might be developed and extended in future decades. The goal of this article is to provoke thought and discussion regarding the larger themes of research in statistics and data science.},
  langid = {english}
}

@inproceedings{gemmeke_audio_2017,
  title = {Audio {{Set}}: {{An}} Ontology and Human-Labeled Dataset for Audio Events},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Gemmeke, Jort F and Ellis, Daniel PW and Freedman, Dylan and Jansen, Aren and Lawrence, Wade and Moore, R Channing and Plakal, Manoj and Ritter, Marvin},
  date = {2017}
}

@book{georges_universal_2001,
  title = {The {{Universal History}} of {{Computing}}: {{From}} the {{Abacus}} to the {{Quantum Computer}}},
  author = {Georges, Ifrah},
  date = {2001},
  publisher = {{Wiley, New York}},
  isbn = {978-0-471-39671-0}
}

@inproceedings{germain_made_2015,
  title = {{{MADE}}: {{Masked Autoencoder}} for {{Distribution Estimation}}},
  shorttitle = {{{MADE}}},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Machine Learning}}},
  author = {Germain, Mathieu and Gregor, Karol and Murray, Iain and Larochelle, Hugo},
  date = {2015},
  eprint = {1502.03509},
  eprinttype = {arxiv},
  location = {{Lille, France}},
  url = {http://arxiv.org/abs/1502.03509},
  urldate = {2020-02-11},
  abstract = {There has been a lot of recent interest in designing neural network models to estimate a distribution from a set of examples. We introduce a simple modification for autoencoder neural networks that yields powerful generative models. Our method masks the autoencoder’s parameters to respect autoregressive constraints: each input is reconstructed only from previous inputs in a given ordering. Constrained this way, the autoencoder outputs can be interpreted as a set of conditional probabilities, and their product, the full joint probability. We can also train a single network that can decompose the joint probability in multiple different orderings. Our simple framework can be applied to multiple architectures, including deep ones. Vectorized implementations, such as on GPUs, are simple and fast. Experiments demonstrate that this approach is competitive with stateof-the-art tractable distribution estimators. At test time, the method is significantly faster and scales better than other autoregressive estimators.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english}
}

@inproceedings{gers_recurrent_2000,
  title = {Recurrent Nets That Time and Count},
  booktitle = {Proceedings of the {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Gers, F.A. and Schmidhuber, Jürgen},
  date = {2000},
  pages = {189-194 vol.3},
  url = {http://ieeexplore.ieee.org/document/861302/},
  urldate = {2018-05-04},
  abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-α-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 Å for the interface backbone atoms) increased from 21\% with default Glide SP settings to 58\% with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63\% success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40\% of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.}
}

@article{geweke_antithetic_1988,
  title = {Antithetic Acceleration of {{Monte Carlo}} Integration in {{Bayesian}} Inference},
  author = {Geweke, John},
  date = {1988},
  journaltitle = {Journal of Econometrics},
  volume = {38},
  number = {1-2},
  pages = {73--89},
  publisher = {{North-Holland}},
  issn = {03044076},
  doi = {10.1016/0304-4076(88)90027-9},
  url = {http://production.datastore.cvt.dk/filestore?oid=5331c75e94c29b733479a4f5&targetid=5331c75e94c29b733479a4f9},
  urldate = {2018-01-03},
  abstract = {It is proposed to sample antithetically rather than randomly from the posterior density in Bayesian inference using Monte Carlo integration. Conditions are established under which the number of replications required with antithetic sampling relative to the number required with random sampling is inversely proportional to sample size, as sample size increases. The result is illustrated in an experiment using a bivariate vector autoregression. © 1988.}
}

@article{geyer_practical_1992,
  title = {Practical {{Markov Chain Monte Carlo}}},
  author = {Geyer, Charles J.},
  date = {1992-11},
  journaltitle = {Statistical Science},
  shortjournal = {Statist. Sci.},
  volume = {7},
  number = {4},
  pages = {473--483},
  issn = {0883-4237},
  doi = {10.1214/ss/1177011137},
  url = {http://projecteuclid.org/euclid.ss/1177011137},
  urldate = {2020-10-24},
  abstract = {Markov chain Monte Carlo using the Metropolis-Hastings algorithm is a general method for the simulation of stochastic processes having probability densities known up to a constant of proportionality. Despite recent advances in its theory, the practice has remained controversial. This article makes the case for basing all inference on one long run of the Markov chain and estimating the Monte Carlo error by standard nonparametric methods well-known in the time-series and operations research literature. In passing it touches on the Kipnis-Varadhan central limit theorem for reversible Markov chains, on some new variance estimators, on judging the relative efficiency of competing Monte Carlo schemes, on methods for constructing more rapidly mixing Markov chains and on diagnostics for Markov chain Monte Carlo.},
  langid = {english}
}

@article{ghahramani_probabilistic_2015,
  title = {Probabilistic Machine Learning and Artificial Intelligence},
  author = {Ghahramani, Zoubin},
  date = {2015},
  journaltitle = {Nature},
  volume = {521},
  number = {7553},
  eprint = {26017444},
  eprinttype = {pmid},
  issn = {0028-0836},
  doi = {10.1038/nature14541},
  abstract = {How can a machine learn from experience? Probabilistic modelling provides a framework for understanding what learning is, and has therefore emerged as one of the principal theoretical and practical approaches for designing machines that learn from data acquired through experience. The probabilistic framework, which describes how to represent and manipulate uncertainty about models and predictions, has a central role in scientific data analysis, machine learning, robotics, cognitive science and artificial intelligence. This Review provides an introduction to this framework, and discusses some of the state-of-the-art advances in the field, namely, probabilistic programming, Bayesian optimization, data compression and automatic model discovery.},
  isbn = {0028-0836}
}

@online{gholami_survey_2021,
  title = {A {{Survey}} of {{Quantization Methods}} for {{Efficient Neural Network Inference}}},
  author = {Gholami, Amir and Kim, Sehoon and Dong, Zhen and Yao, Zhewei and Mahoney, Michael W. and Keutzer, Kurt},
  date = {2021-06-21},
  eprint = {2103.13630},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2103.13630},
  urldate = {2022-07-31},
  abstract = {As soon as abstract mathematical computations were adapted to computation on digital computers, the problem of efficient representation, manipulation, and communication of the numerical values in those computations arose. Strongly related to the problem of numerical representation is the problem of quantization: in what manner should a set of continuous real-valued numbers be distributed over a fixed discrete set of numbers to minimize the number of bits required and also to maximize the accuracy of the attendant computations? This perennial problem of quantization is particularly relevant whenever memory and/or computational resources are severely restricted, and it has come to the forefront in recent years due to the remarkable performance of Neural Network models in computer vision, natural language processing, and related areas. Moving from floating-point representations to low-precision fixed integer values represented in four bits or less holds the potential to reduce the memory footprint and latency by a factor of 16x; and, in fact, reductions of 4x to 8x are often realized in practice in these applications. Thus, it is not surprising that quantization has emerged recently as an important and very active sub-area of research in the efficient implementation of computations associated with Neural Networks. In this article, we survey approaches to the problem of quantizing the numerical values in deep Neural Network computations, covering the advantages/disadvantages of current methods. With this survey and its organization, we hope to have presented a useful snapshot of the current research in quantization for Neural Networks and to have given an intelligent organization to ease the evaluation of future research in this area.},
  pubstate = {preprint}
}

@unpublished{ghosh_deep_2021,
  title = {Deep {{Clustering For General-Purpose Audio Representations}}},
  author = {Ghosh, Sreyan and Katta, Sandesh V. and Seth, Ashish and Umesh, S.},
  date = {2021-10-17},
  eprint = {2110.08895},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2110.08895},
  urldate = {2021-10-22},
  abstract = {We introduce DECAR, a self-supervised pre-training approach for learning general-purpose audio representations. Our system is based on clustering: it utilizes an offline clustering step to provide target labels that act as pseudo-labels for solving a prediction task. We develop on top of recent advances in self-supervised learning for computer vision and design a lightweight, easy-to-use self-supervised pre-training scheme. We pre-train DECAR embeddings on a balanced subset of the large-scale Audioset dataset and transfer those representations to 9 downstream classification tasks, including speech, music, animal sounds, and acoustic scenes. Furthermore, we conduct ablation studies identifying key design choices and also make all our code and pre-trained models publicly available.}
}

@misc{gidney_how_2019,
  title = {How to Factor 2048 Bit {{RSA}} Integers in 8 Hours Using 20 Million Noisy Qubits},
  author = {Gidney, Craig and Ekerå, Martin},
  date = {2019-05-23},
  eprint = {1905.09749},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1905.09749},
  urldate = {2019-08-01},
  abstract = {We significantly reduce the cost of factoring integers and computing discrete logarithms over finite fields on a quantum computer by combining techniques from Griffiths-Niu 1996, Zalka 2006, Fowler 2012, Eker\{\textbackslash aa\}-H\{\textbackslash aa\}stad 2017, Eker\{\textbackslash aa\} 2017, Eker\{\textbackslash aa\} 2018, Gidney-Fowler 2019, Gidney 2019. We estimate the approximate cost of our construction using plausible physical assumptions for large-scale superconducting qubit platforms: a planar grid of qubits with nearest-neighbor connectivity, a characteristic physical gate error rate of \$10\^\{-3\}\$, a surface code cycle time of 1 microsecond, and a reaction time of 10 micro-seconds. We account for factors that are normally ignored such as noise, the need to make repeated attempts, and the spacetime layout of the computation. When factoring 2048 bit RSA integers, our construction's spacetime volume is a hundredfold less than comparable estimates from earlier works (Fowler et al. 2012, Gheorghiu et al. 2019). In the abstract circuit model (which ignores overheads from distillation, routing, and error correction) our construction uses \$3 n + 0.002 n \textbackslash lg n\$ logical qubits, \$0.3 n\^3 + 0.0005 n\^3 \textbackslash lg n\$ Toffolis, and \$500 n\^2 + n\^2 \textbackslash lg n\$ measurement depth to factor \$n\$-bit RSA integers. We quantify the cryptographic implications of our work, both for RSA and for schemes based on the DLP in finite fields.},
  langid = {english}
}

@article{gillman_car_1992,
  title = {The Car and the Goats},
  author = {Gillman, Leonard},
  date = {1992},
  journaltitle = {The American Mathematical Monthly},
  volume = {99},
  number = {1},
  eprint = {2324540},
  eprinttype = {jstor},
  pages = {3--7},
  publisher = {{Mathematical Association of America}},
  issn = {00029890, 19300972},
  url = {http://www.jstor.org/stable/2324540}
}

@article{gilmartin_just_2018,
  title = {Just {{Talking}} - {{Modelling Casual Conversation}}},
  author = {Gilmartin, Emer and Saam, Christian and Vogel, Carl and Campbell, Nick and Wade, Vincent},
  date = {2018},
  pages = {9},
  url = {https://www.aclweb.org/anthology/W18-5006},
  abstract = {Casual conversation has become a focus for dialogue applications. Such talk is ubiquitous and its structure differs from that found in the task-based interactions that have been the focus of dialogue system design for many years. It is unlikely that such conversations can be modelled as an extension of task-based talk. We review theories of casual conversation, report on our studies of the structure of casual dialogue, and outline challenges we see for the development of spoken dialog systems capable of carrying on casual friendly conversation in addition to performing welldefined tasks.},
  langid = {english}
}

@article{girin_dynamical_2021,
  title = {Dynamical Variational Autoencoders: {{A}} Comprehensive Review},
  author = {Girin, Laurent and Leglaive, Simon and Bie, Xiaoyu and Diard, Julien and Hueber, Thomas and Alameda-Pineda, Xavier},
  date = {2021-12},
  journaltitle = {Foundations and Trends in Machine Learning},
  volume = {15},
  pages = {1--175}
}

@inproceedings{girshick_fast_2015,
  title = {Fast R-Cnn},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Girshick, Ross},
  date = {2015},
  pages = {1440--1448}
}

@article{girshick_region-based_2016,
  title = {Region-Based Convolutional Networks for Accurate Object Detection and Segmentation},
  author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  date = {2016},
  journaltitle = {IEEE transactions on pattern analysis and machine intelligence},
  volume = {38},
  number = {1},
  pages = {142--158},
  publisher = {{IEEE}}
}

@inproceedings{girshick_rich_2014,
  title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  date = {2014},
  pages = {580--587}
}

@misc{giryes_deep_2015,
  title = {Deep {{Neural Networks}} with {{Random Gaussian Weights}}: {{A Universal Classification Strategy}}?},
  author = {Giryes, Raja and Sapiro, Guillermo and Bronstein, Alex M.},
  date = {2015-04-30},
  eprint = {1504.08291},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1504.08291},
  urldate = {2017-11-27},
  abstract = {Three important properties of a classification machinery are: (i) the system preserves the core information of the input data; (ii) the training examples convey information about unseen data; and (iii) the system is able to treat differently points from different classes. In this work we show that these fundamental properties are satisfied by the architecture of deep neural networks. We formally prove that these networks with random Gaussian weights perform a distance-preserving embedding of the data, with a special treatment for in-class and out-of-class data. Similar points at the input of the network are likely to have a similar output. The theoretical analysis of deep networks here presented exploits tools used in the compressed sensing and dictionary learning literature, thereby making a formal connection between these important topics. The derived results allow drawing conclusions on the metric learning properties of the network and their relation to its structure, as well as providing bounds on the required size of the training set such that the training examples would represent faithfully the unseen data. The results are validated with state-of-the-art trained networks.}
}

@inproceedings{glarner_full_2018,
  title = {Full {{Bayesian Hidden Markov Model Variational Autoencoder}} for {{Acoustic Unit Discovery}}},
  booktitle = {Proceedings of the 19th {{Annual Conference}} of the {{International Speech Communication Association}} ({{Interspeech}})},
  author = {Glarner, Thomas and Hanebrink, Patrick and Ebbers, Janek and Haeb-Umbach, Reinhold},
  date = {2018-09-02},
  pages = {2688--2692},
  publisher = {{ISCA}},
  location = {{Hyderabad, India}},
  doi = {10.21437/Interspeech.2018-2148},
  url = {https://www.isca-speech.org/archive/interspeech_2018/glarner18_interspeech.html},
  urldate = {2021-12-21},
  abstract = {The invention of the Variational Autoencoder enables the application of Neural Networks to a wide range of tasks in unsupervised learning, including the field of Acoustic Unit Discovery (AUD). The recently proposed Hidden Markov Model Variational Autoencoder (HMMVAE) allows a joint training of a neural network based feature extractor and a structured prior for the latent space given by a Hidden Markov Model. It has been shown that the HMMVAE significantly outperforms pure GMM-HMM based systems on the AUD task. However, the HMMVAE cannot autonomously infer the number of acoustic units and thus relies on the GMM-HMM system for initialization. This paper introduces the Bayesian Hidden Markov Model Variational Autoencoder (BHMMVAE) which solves these issues by embedding the HMMVAE in a Bayesian framework with a Dirichlet Process Prior for the distribution of the acoustic units, and diagonal or full-covariance Gaussians as emission distributions. Experiments on TIMIT and Xitsonga show that the BHMMVAE is able to autonomously infer a reasonable number of acoustic units, can be initialized without supervision by a GMM-HMM system, achieves computationally efficient stochastic variational inference by using natural gradient descent, and, additionally, improves the AUD performance over the HMMVAE.},
  eventtitle = {Interspeech 2018},
  langid = {english}
}

@book{glasserman_monte_2003,
  title = {Monte {{Carlo Methods}} in {{Financial Engineering}}},
  author = {Glasserman, Paul},
  date = {2003},
  volume = {53},
  eprint = {25246403},
  eprinttype = {pmid},
  issn = {14697688},
  doi = {10.1007/978-0-387-21617-1},
  url = {http://link.springer.com/10.1007/978-0-387-21617-1},
  abstract = {This book develops the use of Monte Carlo methods in finance and it also uses simulation as a vehicle for presenting models and ideas from financial engineering. It divides roughly into three parts.},
  isbn = {978-1-4419-1822-2}
}

@article{glorot_deep_2011,
  title = {Deep Sparse Rectifier Neural Networks},
  author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  date = {2011},
  journaltitle = {Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)},
  volume = {15},
  eprint = {1502.03167},
  eprinttype = {arxiv},
  pages = {315--323},
  abstract = {While logistic sigmoid neurons are more biologically plausable that hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros, which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabelled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labelled data sets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised nueral networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training}
}

@inproceedings{glorot_understanding_2010,
  title = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  booktitle = {Proceedings of the {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}} ({{AISTATS}})},
  author = {Glorot, Xavier and Bengio, Yoshua},
  date = {2010},
  volume = {9},
  pages = {249--256},
  abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
}

@article{glynn_likelihood_1990,
  title = {Likelihood Ratio Gradient Estimation for Stochastic Systems},
  author = {Glynn, Peter W.},
  date = {1990-10},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {33},
  number = {10},
  pages = {75--84},
  issn = {0001-0782},
  doi = {10.1145/84537.84552},
  url = {https://doi.org/10.1145/84537.84552},
  abstract = {Consider a computer system having a CPU that feeds jobs to two input/output (I/O) devices having different speeds. Let θ be the fraction of jobs routed to the first I/O device, so that 1 - θ is the fraction routed to the second. Suppose that α = α(θ) is the steady-sate amount of time that a job spends in the system. Given that θ is a decision variable, a designer might wish to minimize α(θ) over θ. Since α(·) is typically difficult to evaluate analytically, Monte Carlo optimization is an attractive methodology. By analogy with deterministic mathematical programming, efficient Monte Carlo gradient estimation is an important ingredient of simulation-based optimization algorithms. As a consequence, gradient estimation has recently attracted considerable attention in the simulation community. It is our goal, in this article, to describe one efficient method for estimating gradients in the Monte Carlo setting, namely the likelihood ratio method (also known as the efficient score method). This technique has been previously described (in less general settings than those developed in this article) in [6, 16, 18, 21]. An alternative gradient estimation procedure is infinitesimal perturbation analysis; see [11, 12] for an introduction. While it is typically more difficult to apply to a given application than the likelihood ratio technique of interest here, it often turns out to be statistically more accurate.In this article, we first describe two important problems which motivate our study of efficient gradient estimation algorithms. Next, we will present the likelihood ratio gradient estimator in a general setting in which the essential idea is most transparent. The section that follows then specializes the estimator to discrete-time stochastic processes. We derive likelihood-ratio-gradient estimators for both time-homogeneous and non-time homogeneous discrete-time Markov chains. Later, we discuss likelihood ratio gradient estimation in continuous time. As examples of our analysis, we present the gradient estimators for time-homogeneous continuous-time Markov chains; non-time homogeneous continuous-time Markov chains; semi-Markov processes; and generalized semi-Markov processes. (The analysis throughout these sections assumes the performance measure that defines α(θ) corresponds to a terminating simulation.) Finally, we conclude the article with a brief discussion of the basic issues that arise in extending the likelihood ratio gradient estimator to steady-state performance measures.}
}

@inproceedings{godfrey_switchboard_1992,
  title = {{{SWITCHBOARD}}: {{Telephone}} Speech Corpus for Research and Development},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Godfrey, John J and Holliman, Edward C and McDaniel, Jane},
  date = {1992}
}

@online{goel_it_2022,
  title = {It's {{Raw}}! {{Audio Generation}} with {{State-Space Models}}},
  author = {Goel, Karan and Gu, Albert and Donahue, Chris and Ré, Christopher},
  date = {2022-02-19},
  eprint = {2202.09729},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2202.09729},
  urldate = {2022-12-12},
  abstract = {Developing architectures suitable for modeling raw audio is a challenging problem due to the high sampling rates of audio waveforms. Standard sequence modeling approaches like RNNs and CNNs have previously been tailored to fit the demands of audio, but the resultant architectures make undesirable computational tradeoffs and struggle to model waveforms effectively. We propose SaShiMi, a new multi-scale architecture for waveform modeling built around the recently introduced S4 model for long sequence modeling. We identify that S4 can be unstable during autoregressive generation, and provide a simple improvement to its parameterization by drawing connections to Hurwitz matrices. SaShiMi yields state-of-the-art performance for unconditional waveform generation in the autoregressive setting. Additionally, SaShiMi improves non-autoregressive generation performance when used as the backbone architecture for a diffusion model. Compared to prior architectures in the autoregressive generation setting, SaShiMi generates piano and speech waveforms which humans find more musical and coherent respectively, e.g. 2x better mean opinion scores than WaveNet on an unconditional speech generation task. On a music generation task, SaShiMi outperforms WaveNet on density estimation and speed at both training and inference even when using 3x fewer parameters. Code can be found at https://github.com/HazyResearch/state-spaces and samples at https://hazyresearch.stanford.edu/sashimi-examples.},
  pubstate = {preprint}
}

@article{goldberg_what_1991,
  title = {What Every Computer Scientist Should Know about Floating-Point Arithmetic},
  author = {Goldberg, David},
  date = {1991-03},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {23},
  number = {1},
  pages = {5--48},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/103162.103163},
  url = {https://dl.acm.org/doi/10.1145/103162.103163},
  urldate = {2022-03-17},
  abstract = {Floating-point arithmetic is considered as esoteric subject by many people. This is rather surprising, because floating-point is ubiquitous in computer systems: Almost every language has a floating-point datatype; computers from PCs to supercomputers have floating-point accelerators; most compilers will be called upon to compile floating-point algorithms from time to time; and virtually every operating system must respond to floating-point exceptions such as overflow. This paper presents a tutorial on the aspects of floating-point that have a direct impact on designers of computer systems. It begins with background on floating-point representation and rounding error, continues with a discussion of the IEEE floating point standard, and concludes with examples of how computer system builders can better support floating point.},
  langid = {english}
}

@article{goldbergerPhysioBankPhysioToolkitPhysioNet2000,
  title = {{{PhysioBank}}, {{PhysioToolkit}}, and {{PhysioNet}}: {{Components}} of a New Research Resource for Complex Physiologic Signals},
  shorttitle = {{{PhysioBank}}, {{PhysioToolkit}}, and {{PhysioNet}}},
  author = {Goldberger, A. L. and Amaral, L. A. and Glass, L. and Hausdorff, J. M. and Ivanov, P. C. and Mark, R. G. and Mietus, J. E. and Moody, G. B. and Peng, C. K. and Stanley, H. E.},
  date = {2000-06},
  journaltitle = {Circulation},
  volume = {101},
  number = {23},
  eprint = {10851218},
  eprinttype = {pmid},
  pages = {E215-220},
  issn = {1524-4539},
  doi = {10.1161/01.cir.101.23.e215},
  abstract = {The newly inaugurated Research Resource for Complex Physiologic Signals, which was created under the auspices of the National Center for Research Resources of the National Institutes of Health, is intended to stimulate current research and new investigations in the study of cardiovascular and other complex biomedical signals. The resource has 3 interdependent components. PhysioBank is a large and growing archive of well-characterized digital recordings of physiological signals and related data for use by the biomedical research community. It currently includes databases of multiparameter cardiopulmonary, neural, and other biomedical signals from healthy subjects and from patients with a variety of conditions with major public health implications, including life-threatening arrhythmias, congestive heart failure, sleep apnea, neurological disorders, and aging. PhysioToolkit is a library of open-source software for physiological signal processing and analysis, the detection of physiologically significant events using both classic techniques and novel methods based on statistical physics and nonlinear dynamics, the interactive display and characterization of signals, the creation of new databases, the simulation of physiological and other signals, the quantitative evaluation and comparison of analysis methods, and the analysis of nonstationary processes. PhysioNet is an on-line forum for the dissemination and exchange of recorded biomedical signals and open-source software for analyzing them. It provides facilities for the cooperative analysis of data and the evaluation of proposed new algorithms. In addition to providing free electronic access to PhysioBank data and PhysioToolkit software via the World Wide Web (http://www.physionet. org), PhysioNet offers services and training via on-line tutorials to assist users with varying levels of expertise.},
  langid = {english}
}

@unpublished{gong_ssast_2021,
  title = {{{SSAST}}: {{Self-Supervised Audio Spectrogram Transformer}}},
  shorttitle = {{{SSAST}}},
  author = {Gong, Yuan and Lai, Cheng-I. Jeff and Chung, Yu-An and Glass, James},
  date = {2021-10-19},
  eprint = {2110.09784},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2110.09784},
  urldate = {2021-10-25},
  abstract = {Recently, neural networks based purely on self-attention, such as the Vision Transformer (ViT), have been shown to outperform deep learning models constructed with convolutional neural networks (CNNs) on various vision tasks, thus extending the success of Transformers, which were originally developed for language processing, to the vision domain. A recent study showed that a similar methodology can also be applied to the audio domain. Specifically, the Audio Spectrogram Transformer (AST) achieves state-of-the-art results on various audio classification benchmarks. However, pure Transformer models tend to require more training data compared to CNNs, and the success of the AST relies on supervised pretraining that requires a large amount of labeled data and a complex training pipeline, thus limiting the practical usage of AST. This paper focuses on audio and speech classification, and aims to alleviate the data requirement issues with the AST by leveraging self-supervised learning using unlabeled data. Specifically, we propose to pretrain the AST model with joint discriminative and generative masked spectrogram patch modeling (MSPM) using unlabeled audio from AudioSet and Librispeech. We evaluate our pretrained models on both audio and speech classification tasks including audio event classification, keyword spotting, emotion recognition, and speaker identification. The proposed self-supervised framework significantly boosts AST performance on all tasks, with an average improvement of 60.9\%, leading to similar or even better results than a supervised pretrained AST. To the best of our knowledge, it is the first patch-based self-supervised learning framework in the audio and speech domain, and also the first self-supervised learning framework for AST.}
}

@inproceedings{gonzalez_selfsupervised_2022,
  title = {Self-Supervised {{Out-of-distribution Detection}} for {{Cardiac CMR Segmentation}}},
  author = {Gonzalez, Camila and Mukhopadhyay, Anirban},
  date = {2022-07-22},
  url = {https://openreview.net/forum?id=E5CpgfwHBoC},
  urldate = {2022-09-04},
  abstract = {Self-supervised losses indicate during inference when U-Net models for Cardiac CMR segmentation will fail silently.},
  eventtitle = {Medical {{Imaging}} with {{Deep Learning}}},
  langid = {english}
}

@book{goodfellow_deep_2016,
  title = {Deep {{Learning}}},
  author = {Goodfellow, Ian J. and Bengio, Yoshua and Courville, Aaron},
  date = {2016},
  journaltitle = {MIT Press},
  edition = {1},
  publisher = {{MIT Press}},
  location = {{Cambridge, Massachusetts}},
  url = {http://www.deeplearningbook.org/},
  abstract = {Deep learning draws upon many modeling formalisms that researchers can use to guide their design efforts and describe their algorithms. One of these formalisms is the idea of structured probabilistic models. We have already discussed structured probabilistic models briefly in Chapter 3.14. That brief presentation was sufficient to understand how to use structured probabilistic models as a language to describe some of the algorithms in part II of this book. Now, in part III, structured probabilistic models are a key ingredient of many of the most important research topics in deep learning. In order to prepare to discuss these research ideas, this chapter describes structured probabilistic models in much greater detail. This chapter is intended to be self-contained; the reader does not need to review the earlier introduction before continuing with this chapter. A structured probabilistic model is a way of describing a probability distribu-tion, using a graph to describe which random variables in the probability distri-bution interact with each other directly. Here we use " graph " in the graph theory sense–a set of vertices connected to one another by a set of edges. Because the structure of the model is defined by a graph, these models are often also referred to as graphical models. The graphical models research community is large and has developed many different models, training algorithms, and inference algorithms. In this chap-ter, we provide basic background on some of the most central ideas of graphical models, with an emphasis on the concepts that have proven most useful to the deep learning research community. If you already have a strong background in graphical models, you may wish to skip most of this chapter. However, even a graphical model expert may benefit from reading the final section of this chap-ter, section 13.6, in which we highlight some of the unique ways that graphical 412 CHAPTER 13. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING models are used for deep learning algorithms. Deep learning practitioners tend to use very different model structures, learning algorithms, and inference procedures than are commonly used by the rest of the graphical models research community. In this chapter, we identify these differences in preferences and explain the reasons for them. In this chapter we first describe the challenges of building large-scale proba-bilistic models in section 13.1. Next, we describe how to use a graph to describe the structure of a probability distribution in section 13.2. We then revisit the challenges we described in section 13.1 and show how the structured approach to probabilistic modeling can overcome these challenges in section 13.3. One of the major difficulties in graphical modeling is understanding which variables need to be able to interact directly, i.e., which graph structures are most suitable for a given problem. We outline two approaches to resolving this difficulty by learning about the dependencies in section 13.4. Finally, we close with a discussion of the unique emphasis that deep learning practitioners place on specific approaches to graphical modeling in section 13.6. 13.1 The Challenge of Unstructured Modeling},
  isbn = {978-0-262-03561-3},
  keywords = {★}
}

@unpublished{goodfellow_efficient_2015,
  title = {Efficient Per-Example Gradient Computations},
  author = {Goodfellow, I.},
  date = {2015},
  eprint = {1510.01799},
  eprinttype = {arxiv}
}

@inproceedings{goodfellow_explaining_2015,
  title = {Explaining and Harnessing Adversarial Examples},
  booktitle = {Proceedings of the 3rd {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
  editor = {Bengio, Yoshua and LeCun, Yann},
  date = {2015},
  location = {{San Diego, CA, USA}},
  url = {http://arxiv.org/abs/1412.6572},
  eventtitle = {International {{Conference}} on {{Learning Representations}}}
}

@article{goodfellow_generative_2014,
  title = {Generative {{Adversarial Networks}}},
  author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  date = {2014-06-10},
  journaltitle = {Proceedings of the Conference on Neural Information Processing Systems (NeurIPS)},
  eprint = {1000183096},
  eprinttype = {pmid},
  pages = {2672--2680},
  issn = {10495258},
  doi = {10.1017/CBO9781139058452},
  url = {http://arxiv.org/abs/1406.2661},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  isbn = {1406.2661}
}

@misc{goodfellow_maxout_2013,
  title = {Maxout {{Networks}}},
  author = {Goodfellow, Ian J. and Warde-Farley, David and Mirza, Mehdi and Courville, Aaron and Bengio, Yoshua},
  date = {2013-02-18},
  eprint = {1302.4389},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1302.4389},
  abstract = {We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.}
}

@inproceedings{goodfellow_qualitatively_2015,
  title = {Qualitatively Characterizing Neural Network Optimization Problems},
  booktitle = {Proceedings of the {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Goodfellow, Ian J and Vinyals, Oriol and Saxe, Andrew M},
  date = {2015},
  eprint = {1412.6544v5},
  eprinttype = {arxiv},
  pages = {1--11},
  url = {http://arxiv.org/abs/1412.6544},
  urldate = {2018-06-13},
  abstract = {Training neural networks involves solving large-scale non-convex optimization problems. This task has long been believed to be extremely difficult, with fear of local minima and other obstacles motivating a variety of schemes to improve opti-mization, such as unsupervised pretraining. However, modern neural networks are able to achieve negligible training error on complex tasks, using only direct train-ing with stochastic gradient descent. We introduce a simple analysis technique to look for evidence that such networks are overcoming local optima. We find that, in fact, on a straight path from initialization to solution, a variety of state of the art neural networks never encounter any significant obstacles.}
}

@inproceedings{gorman_we_2019,
  title = {We Need to Talk about Standard Splits},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Gorman, Kyle and Bedrick, Steven},
  date = {2019-08},
  location = {{Florence, Italy}},
  abstract = {It is standard practice in speech \& language technology to rank systems according to per- formance on a test set held out for evalua- tion. However, few researchers apply statis- tical tests to determine whether differences in performance are likely to arise by chance, and few examine the stability of system ranking across multiple training-testing splits. We con- duct replication and reproduction experiments with nine part-of-speech taggers published be- tween 2000 and 2018, each of which reports state-of-the-art performance on a widely-used “standard split”. We fail to reliably repro- duce some rankings using randomly generated splits. We suggest that randomly generated splits should be used in system comparison.},
  eventtitle = {Association for {{Computational Linguistics}}}
}

@inproceedings{goyal_zforcing_2017,
  title = {Z-{{Forcing}}: {{Training Stochastic Recurrent Networks}}},
  shorttitle = {Z-{{Forcing}}},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Goyal, Anirudh and Sordoni, Alessandro and Côté, Marc-Alexandre and Ke, Nan Rosemary and Bengio, Yoshua},
  date = {2017-11-16},
  eprint = {1711.05411},
  eprinttype = {arxiv},
  location = {{Long Beach, CA, USA}},
  url = {http://arxiv.org/abs/1711.05411},
  urldate = {2022-01-07},
  abstract = {Many efforts have been devoted to training generative latent variable models with autoregressive decoders, such as recurrent neural networks (RNN). Stochastic recurrent models have been successful in capturing the variability observed in natural sequential data such as speech. We unify successful ideas from recently proposed architectures into a stochastic recurrent model: each step in the sequence is associated with a latent variable that is used to condition the recurrent dynamics for future steps. Training is performed with amortized variational inference where the approximate posterior is augmented with a RNN that runs backward through the sequence. In addition to maximizing the variational lower bound, we ease training of the latent variables by adding an auxiliary cost which forces them to reconstruct the state of the backward recurrent network. This provides the latent variables with a task-independent objective that enhances the performance of the overall model. We found this strategy to perform better than alternative approaches such as KL annealing. Although being conceptually simple, our model achieves state-of-the-art results on standard speech benchmarks such as TIMIT and Blizzard and competitive performance on sequential MNIST. Finally, we apply our model to language modeling on the IMDB dataset where the auxiliary cost helps in learning interpretable latent variables.},
  eventtitle = {Neural {{Information Processing Systems}}},
  langid = {english}
}

@misc{grace_when_2017,
  title = {When {{Will AI Exceed Human Performance}}? {{Evidence}} from {{AI Experts}}},
  author = {Grace, Katja and Salvatier, John and Dafoe, Allan and Zhang, Baobao and Evans, Owain},
  date = {2017-05-24},
  eprint = {1705.08807},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1705.08807},
  urldate = {2017-09-19},
  abstract = {Advances in artificial intelligence (AI) will transform modern life by reshaping transportation, health, science, finance, and the military. To adapt public policy, we need to better anticipate these advances. Here we report the results from a large survey of machine learning researchers on their beliefs about progress in AI. Researchers predict AI will outperform humans in many activities in the next ten years, such as translating languages (by 2024), writing high-school essays (by 2026), driving a truck (by 2027), working in retail (by 2031), writing a bestselling book (by 2049), and working as a surgeon (by 2053). Researchers believe there is a 50\% chance of AI outperforming humans in all tasks in 45 years and of automating all human jobs in 120 years, with Asian respondents expecting these dates much sooner than North Americans. These results will inform discussion amongst researchers and policymakers about anticipating and managing trends in AI.}
}

@inproceedings{graham_denoising_2023,
  title = {Denoising Diffusion Models for Out-of-Distribution Detection},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition},
  author = {Graham, Mark S and Pinaya, Walter HL and Tudosiu, Petru-Daniel and Nachev, Parashkev and Ourselin, Sebastien and Cardoso, Jorge},
  date = {2023},
  pages = {2947--2956}
}

@misc{grathwohl_ffjord_2018,
  title = {{{FFJORD}}: {{Free-form Continuous Dynamics}} for {{Scalable Reversible Generative Models}}},
  shorttitle = {{{FFJORD}}},
  author = {Grathwohl, Will and Chen, Ricky T. Q. and Bettencourt, Jesse and Sutskever, Ilya and Duvenaud, David},
  date = {2018-10-22},
  eprint = {1810.01367},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1810.01367},
  urldate = {2020-10-08},
  abstract = {A promising class of generative models maps points from a simple distribution to a complex distribution through an invertible neural network. Likelihood-based training of these models requires restricting their architectures to allow cheap computation of Jacobian determinants. Alternatively, the Jacobian trace can be used if the transformation is specified by an ordinary differential equation. In this paper, we use Hutchinson’s trace estimator to give a scalable unbiased estimate of the log-density. The result is a continuous-time invertible generative model with unbiased density estimation and one-pass sampling, while allowing unrestricted neural network architectures. We demonstrate our approach on high-dimensional density estimation, image generation, and variational inference, achieving the state-of-the-art among exact likelihood methods with efficient sampling.},
  langid = {english}
}

@misc{grathwohl_oops_2021,
  title = {Oops {{I Took A Gradient}}: {{Scalable Sampling}} for {{Discrete Distributions}}},
  shorttitle = {Oops {{I Took A Gradient}}},
  author = {Grathwohl, Will and Swersky, Kevin and Hashemi, Milad and Duvenaud, David and Maddison, Chris J.},
  date = {2021-02-08},
  eprint = {2102.04509},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2102.04509},
  urldate = {2021-05-12},
  abstract = {We propose a general and scalable approximate sampling strategy for probabilistic models with discrete variables. Our approach uses gradients of the likelihood function with respect to its discrete inputs to propose updates in a MetropolisHastings sampler. We show empirically that this approach outperforms generic samplers in a number of difficult settings including Ising models, Potts models, restricted Boltzmann machines, and factorial hidden Markov models. We also demonstrate the use of our improved sampler for training deep energy-based models on high dimensional discrete data. This approach outperforms variational auto-encoders and existing energy-based models. Finally, we give bounds showing that our approach is near-optimal in the class of samplers which propose local updates.},
  langid = {english}
}

@inproceedings{grathwohl_your_2020,
  title = {Your Classifier Is Secretly an Energy Based Model and You Should Treat It like One},
  booktitle = {Proceedings of the {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Grathwohl, Will and Wang, Kuan-Chieh and Jacobsen, Jörn-Henrik and Duvenaud, David and Norouzi, Mohammad and Swersky, Kevin},
  date = {2020},
  location = {{Addis Ababa, Ethiopia}},
  url = {https://openreview.net/forum?id=Hkxzx0NtDB}
}

@misc{graves_adaptive_2017,
  title = {Adaptive {{Computation Time}} for {{Recurrent Neural Networks}}},
  author = {Graves, Alex},
  date = {2017-02-21},
  eprint = {1603.08983},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1603.08983},
  urldate = {2022-04-26},
  abstract = {This paper introduces Adaptive Computation Time (ACT), an algorithm that allows recurrent neural networks to learn how many computational steps to take between receiving an input and emitting an output. ACT requires minimal changes to the network architecture, is deterministic and differentiable, and does not add any noise to the parameter gradients. Experimental results are provided for four synthetic problems: determining the parity of binary vectors, applying binary logic operations, adding integers, and sorting real numbers. Overall, performance is dramatically improved by the use of ACT, which successfully adapts the number of computational steps to the requirements of the problem. We also present character-level language modelling results on the Hutter prize Wikipedia dataset. In this case ACT does not yield large gains in performance; however it does provide intriguing insight into the structure of the data, with more computation allocated to harder-to-predict transitions, such as spaces between words and ends of sentences. This suggests that ACT or other adaptive computation methods could provide a generic method for inferring segment boundaries in sequence data.}
}

@online{graves_associative_2018,
  title = {Associative {{Compression Networks}} for {{Representation Learning}}},
  author = {Graves, Alex and Menick, Jacob and family=Oord, given=Aaron, prefix=van den, useprefix=false},
  date = {2018-04-26},
  eprint = {1804.02476},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1804.02476},
  urldate = {2023-01-24},
  abstract = {This paper introduces Associative Compression Networks (ACNs), a new framework for variational autoencoding with neural networks. The system differs from existing variational autoencoders (VAEs) in that the prior distribution used to model each code is conditioned on a similar code from the dataset. In compression terms this equates to sequentially transmitting the dataset using an ordering determined by proximity in latent space. Since the prior need only account for local, rather than global variations in the latent space, the coding cost is greatly reduced, leading to rich, informative codes. Crucially, the codes remain informative when powerful, autoregressive decoders are used, which we argue is fundamentally difficult with normal VAEs. Experimental results on MNIST, CIFAR-10, ImageNet and CelebA show that ACNs discover high-level latent features such as object class, writing style, pose and facial expression, which can be used to cluster and classify the data, as well as to generate diverse and convincing samples. We conclude that ACNs are a promising new direction for representation learning: one that steps away from IID modelling, and towards learning a structured description of the dataset as a whole.},
  pubstate = {preprint}
}

@inproceedings{graves_connectionist_2006,
  title = {Connectionist {{Temporal Classification}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Graves, Alex and Fernández, Santiago and Gomez, Faustino and Schmidhuber, Jürgen},
  date = {2006},
  eprint = {1000285842},
  eprinttype = {pmid},
  pages = {369--376},
  location = {{Pittsburgh, Pennsylvania, USA}},
  issn = {10987576},
  doi = {10.1145/1143844.1143891},
  url = {ftp://ftp.idsia.ch/pub/juergen/icml2006.pdf},
  abstract = {Many real-world sequence learning tasks re- quire the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their out- puts into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label un- segmented sequences directly, thereby solv- ing both problems. An experiment on the TIMIT speech corpus demonstrates its ad- vantages over both a baseline HMM and a hybrid HMM-RNN.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  isbn = {1-59593-383-2},
  keywords = {★}
}

@misc{graves_generating_2013,
  title = {Generating Sequences with Recurrent Neural Networks},
  author = {Graves, Alex},
  date = {2013},
  eprint = {1308.0850},
  eprinttype = {arxiv},
  url = {https://arxiv.org/abs/1308.0850}
}

@article{graves_hybrid_2016,
  title = {Hybrid Computing Using a Neural Network with Dynamic External Memory},
  author = {Graves, Alex and Wayne, Greg and Reynolds, Malcolm and Harley, Tim and Danihelka, Ivo and Grabska-Barwińska, Agnieszka and Colmenarejo, Sergio Gómez and Grefenstette, Edward and Ramalho, Tiago and Agapiou, John and Badia, Adrià Puigdomènech and Hermann, Karl Moritz and Zwols, Yori and Ostrovski, Georg and Cain, Adam and King, Helen and Summerfield, Christopher and Blunsom, Phil and Kavukcuoglu, Koray and Hassabis, Demis},
  date = {2016-10-01},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {538},
  number = {7626},
  pages = {471--476},
  issn = {1476-4687},
  doi = {10.1038/nature20101},
  url = {https://doi.org/10.1038/nature20101},
  abstract = {Artificial neural networks are remarkably adept at sensory processing, sequence learning and reinforcement learning, but are limited in their ability to represent variables and data structures and to store data over long timescales, owing to the lack of an external memory. Here we introduce a machine learning model called a differentiable neural computer (DNC), which consists of a neural network that can read from and write to an external memory matrix, analogous to the random-access memory in a conventional computer. Like a conventional computer, it can use its memory to represent and manipulate complex data structures, but, like a neural network, it can learn to do so from data. When trained with supervised learning, we demonstrate that a DNC can successfully answer synthetic questions designed to emulate reasoning and inference problems in natural language. We show that it can learn tasks such as finding the shortest path between specified points and inferring the missing links in randomly generated graphs, and then generalize these tasks to specific graphs such as transport networks and family trees. When trained with reinforcement learning, a DNC can complete a moving blocks puzzle in which changing goals are specified by sequences of symbols. Taken together, our results demonstrate that DNCs have the capacity to solve complex, structured tasks that are inaccessible to neural networks without external read–write memory.}
}

@misc{graves_neural_2014,
  title = {Neural {{Turing Machines}}},
  author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  date = {2014-10-20},
  eprint = {18958277},
  eprinttype = {pmid},
  url = {http://arxiv.org/abs/1410.5401},
  urldate = {2018-05-22},
  abstract = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
  isbn = {0028-0836}
}

@inproceedings{graves_practical_2011,
  title = {Practical {{Variational Inference}} for {{Neural Networks}}.},
  booktitle = {Proceedings of the {{International Conference}} on {{Neural Information Processing Systems}} ({{NIPS}})},
  author = {Graves, Alex},
  date = {2011},
  pages = {1--9},
  url = {https://papers.nips.cc/paper/4329-practical-variational-inference-for-neural-networks.pdf},
  urldate = {2018-06-22},
  abstract = {Variational methods have been previously explored as a tractable approximation to Bayesian inference for neural networks. However the approaches proposed so far have only been applicable to a few simple network architectures. This paper introduces an easy-to-implement stochastic variational method (or equivalently, minimum description length loss function) that can be applied to most neural networks. Along the way it revisits several common regularisers from a variational perspective. It also provides a simple pruning heuristic that can both drastically reduce the number of network weights and lead to improved generalisation. Experimental results are provided for a hierarchical multidimensional recurrent neural network applied to the TIMIT speech corpus.},
  isbn = {978-1-61839-599-3}
}

@misc{graves_sequence_2012,
  title = {Sequence {{Transduction}} with {{Recurrent Neural Networks}}},
  author = {Graves, Alex},
  date = {2012-11-14},
  eprint = {1211.3711},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1211.3711},
  urldate = {2019-10-30},
  abstract = {Many machine learning tasks can be expressed as the transformation—or transduction—of input sequences into output sequences: speech recognition, machine translation, protein secondary structure prediction and text-to-speech to name but a few. One of the key challenges in sequence transduction is learning to represent both the input and output sequences in a way that is invariant to sequential distortions such as shrinking, stretching and translating. Recurrent neural networks (RNNs) are a powerful sequence learning architecture that has proven capable of learning such representations. However RNNs traditionally require a pre-defined alignment between the input and output sequences to perform transduction. This is a severe limitation since finding the alignment is the most difficult aspect of many sequence transduction problems. Indeed, even determining the length of the output sequence is often challenging. This paper introduces an end-to-end, probabilistic sequence transduction system, based entirely on RNNs, that is in principle able to transform any input sequence into any finite, discrete output sequence. Experimental results for phoneme recognition are provided on the TIMIT speech corpus.},
  langid = {english}
}

@inproceedings{graves_speech_2013,
  title = {Speech Recognition with Deep Recurrent Neural Networks},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey E.},
  date = {2013},
  pages = {6645--6649}
}

@book{graves_supervised_2012,
  title = {Supervised {{Sequence Labelling}} with {{Recurrent Neural Networks}}},
  author = {Graves, Alex},
  date = {2012-02-06}
}

@inproceedings{graves_towards_2014,
  title = {Towards {{End-To-End Speech Recognition}} with {{Recurrent Neural Networks}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Graves, Alex and Jaitly, Navdeep},
  date = {2014-01-27},
  pages = {1764--1772},
  url = {http://proceedings.mlr.press/v32/graves14.html},
  urldate = {2018-10-31},
  abstract = {This paper presents a speech recognition system that directly transcribes audio data with text, without requiring an intermediate phonetic representation. The system is based on a combination of th...},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english}
}

@article{gray_vector_1984,
  title = {Vector Quantization},
  author = {Gray, Robert},
  date = {1984},
  journaltitle = {IEEE ASSP Magazine},
  volume = {1},
  number = {2},
  pages = {4--29}
}

@article{greff_lstm_2017,
  title = {{{LSTM}}: {{A Search Space Odyssey}}},
  author = {Greff, Klaus and Srivastava, Rupesh K. and Koutník, Jan and Steunebrink, Bas R. and Schmidhuber, Jürgen},
  date = {2017},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {28},
  number = {10},
  eprint = {25246403},
  eprinttype = {pmid},
  pages = {2222--2232},
  issn = {21622388},
  doi = {10.1109/TNNLS.2016.2582924},
  abstract = {Several variants of the Long Short-Term Memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search, and their importance was assessed using the powerful fANOVA framework. In total, we summarize the results of 5400 experimental runs (\$\textbackslash approx 15\$ years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.},
  isbn = {9788578110796}
}

@misc{gregor_deep_2014,
  title = {Deep {{AutoRegressive Networks}}},
  author = {Gregor, Karol and Danihelka, Ivo and Mnih, Andriy and Blundell, Charles and Wierstra, Daan},
  date = {2014-05-20},
  eprint = {1310.8499},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1310.8499},
  urldate = {2021-03-04},
  abstract = {We introduce a deep, generative autoencoder capable of learning hierarchies of distributed representations from data. Successive deep stochastic hidden layers are equipped with autoregressive connections, which enable the model to be sampled from quickly and exactly via ancestral sampling. We derive an efficient approximate parameter estimation method based on the minimum description length (MDL) principle, which can be seen as maximising a variational lower bound on the log-likelihood, with a feedforward neural network implementing approximate inference. We demonstrate state-of-the-art generative performance on a number of classic data sets: several UCI data sets, MNIST and Atari 2600 games.},
  langid = {english}
}

@misc{gregor_draw_2015,
  title = {{{DRAW}}: {{A}} Recurrent Neural Network for Image Generation},
  author = {Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo Jimenez and Wierstra, Daan},
  date = {2015},
  eprint = {1502.04623},
  eprinttype = {arxiv},
  url = {https://arxiv.org/abs/1502.04623}
}

@misc{gregor_temporal_2019,
  title = {Temporal {{Difference Variational Auto-Encoder}}},
  author = {Gregor, Karol and Papamakarios, George and Besse, Frederic and Buesing, Lars and Weber, Theophane},
  date = {2019-01-02},
  eprint = {1806.03107},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1806.03107},
  urldate = {2021-06-02},
  abstract = {To act and plan in complex environments, we posit that agents should have a mental simulator of the world with three characteristics: (a) it should build an abstract state representing the condition of the world; (b) it should form a belief which represents uncertainty on the world; (c) it should go beyond simple step-by-step simulation, and exhibit temporal abstraction. Motivated by the absence of a model satisfying all these requirements, we propose TD-VAE, a generative sequence model that learns representations containing explicit beliefs about states several steps into the future, and that can be rolled out directly without single-step transitions. TD-VAE is trained on pairs of temporally separated time points, using an analogue of temporal difference learning used in reinforcement learning.},
  langid = {english}
}

@article{gretton_kernel_2012,
  title = {A Kernel Two-Sample Test},
  author = {Gretton, Arthur and Borgwardt, Karsten M and Rasch, Malte J and Schölkopf, Bernhard and Smola, Alexander},
  date = {2012},
  journaltitle = {The Journal of Machine Learning Research},
  volume = {13},
  number = {1},
  pages = {723--773},
  publisher = {{JMLR}}
}

@misc{grigg_selfsupervised_2021,
  title = {Do Self-Supervised and Supervised Methods Learn Similar Visual Representations?},
  author = {Grigg*, Tom George and Busbridge*, Dan and Ramapuram, Jason and Webb, Russ},
  date = {2021},
  url = {https://arxiv.org/pdf/2110.00528.pdf}
}

@online{grill_bootstrap_2020,
  title = {Bootstrap {{Your Own Latent}}: {{A}} New Approach to Self-Supervised Learning},
  author = {Grill, Jean-Bastien and Strub, Florian and Altché, Florent and Tallec, Corentin and Richemond, Pierre H. and Buchatskaya, Elena and Doersch, Carl and Pires, Bernardo Ávila and Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi and Piot, Bilal and Kavukcuoglu, Koray and Munos, Rémi and Valko, Michal},
  date = {2020},
  eprint = {2006.07733},
  eprinttype = {arxiv},
  pubstate = {preprint}
}

@online{gu_annotated_2022,
  title = {The {{Annotated S4}}},
  author = {Gu, Albert and Goel, Karan and Ré, Christopher},
  date = {2022},
  url = {https://srush.github.io/annotated-s4/},
  urldate = {2022-12-12},
  abstract = {The Structured State Space for Sequence Modeling (S4) architecture is a new approach to very long-range sequence modeling tasks for vision, language, and audio, showing a capacity to capture dependencies over tens of thousands of steps. Especially impressive are the model’s results on the challenging Long Range Arena benchmark, showing an ability to reason over sequences of up to 16,000+ elements with high accuracy.},
  organization = {{The Annotated S4}}
}

@inproceedings{gu_combining_2021,
  title = {Combining {{Recurrent}}, {{Convolutional}}, and {{Continuous-Time Models}} with {{Linear State Space Layers}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Gu, Albert and Johnson, Isys and Goel, Karan and Saab, Khaled and Dao, Tri and Rudra, Atri and Ré, Christopher},
  editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P.S. and Vaughan, J. Wortman},
  date = {2021},
  volume = {34},
  pages = {572--585},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2021/file/05546b0e38ab9175cd905eebcc6ebb76-Paper.pdf}
}

@online{gu_efficiently_2022,
  title = {Efficiently {{Modeling Long Sequences}} with {{Structured State Spaces}}},
  author = {Gu, Albert and Goel, Karan and Ré, Christopher},
  date = {2022-08-05},
  eprint = {2111.00396},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2111.00396},
  urldate = {2022-12-12},
  abstract = {A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of \$10000\$ or more steps. A promising recent approach proposed modeling sequences by simulating the fundamental state space model (SSM) \textbackslash ( x'(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t) \textbackslash ), and showed that for appropriate choices of the state matrix \textbackslash ( A \textbackslash ), this system could handle long-range dependencies mathematically and empirically. However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution. We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths. Our technique involves conditioning \textbackslash ( A \textbackslash ) with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel. S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91\textbackslash\% accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation \$60\textbackslash times\$ faster (iii) SoTA on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16k that all prior work fails on, while being as efficient as all competitors.},
  pubstate = {preprint}
}

@online{gu_hippo_2020,
  title = {{{HiPPO}}: {{Recurrent Memory}} with {{Optimal Polynomial Projections}}},
  shorttitle = {{{HiPPO}}},
  author = {Gu, Albert and Dao, Tri and Ermon, Stefano and Rudra, Atri and Re, Christopher},
  date = {2020-10-22},
  eprint = {2008.07669},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2008.07669},
  urldate = {2022-12-12},
  abstract = {A central problem in learning from sequential data is representing cumulative history in an incremental fashion as more data is processed. We introduce a general framework (HiPPO) for the online compression of continuous signals and discrete time series by projection onto polynomial bases. Given a measure that specifies the importance of each time step in the past, HiPPO produces an optimal solution to a natural online function approximation problem. As special cases, our framework yields a short derivation of the recent Legendre Memory Unit (LMU) from first principles, and generalizes the ubiquitous gating mechanism of recurrent neural networks such as GRUs. This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale. HiPPO-LegS enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients. By incorporating the memory dynamics into recurrent neural networks, HiPPO RNNs can empirically capture complex temporal dependencies. On the benchmark permuted MNIST dataset, HiPPO-LegS sets a new state-of-the-art accuracy of 98.3\%. Finally, on a novel trajectory classification task testing robustness to out-of-distribution timescales and missing data, HiPPO-LegS outperforms RNN and neural ODE baselines by 25-40\% accuracy.},
  pubstate = {preprint}
}

@misc{gu_muprop_2016,
  title = {{{MuProp}}: {{Unbiased Backpropagation}} for {{Stochastic Neural Networks}}},
  shorttitle = {{{MuProp}}},
  author = {Gu, Shixiang and Levine, Sergey and Sutskever, Ilya and Mnih, Andriy},
  date = {2016-02-25},
  eprint = {1511.05176},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1511.05176},
  urldate = {2021-03-04},
  abstract = {Deep neural networks are powerful parametric models that can be trained efficiently using the backpropagation algorithm. Stochastic neural networks combine the power of large parametric functions with that of graphical models, which makes it possible to learn very complex distributions. However, as backpropagation is not directly applicable to stochastic networks that include discrete sampling operations within their computational graph, training such networks remains difficult. We present MuProp, an unbiased gradient estimator for stochastic networks, designed to make this task easier. MuProp improves on the likelihood-ratio estimator by reducing its variance using a control variate based on the first-order Taylor expansion of a mean-field network. Crucially, unlike prior attempts at using backpropagation for training stochastic networks, the resulting estimator is unbiased and well behaved. Our experiments on structured output prediction and discrete latent variable modeling demonstrate that MuProp yields consistently good performance across a range of difficult tasks.},
  langid = {english}
}

@misc{gu_neural_2015,
  title = {Neural {{Adaptive Sequential Monte Carlo}}},
  author = {Gu, Shixiang and Ghahramani, Zoubin and Turner, Richard E.},
  date = {2015-11-16},
  eprint = {1506.03338},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1506.03338},
  urldate = {2020-05-22},
  abstract = {Sequential Monte Carlo (SMC), or particle filtering, is a popular class of methods for sampling from an intractable target distribution using a sequence of simpler intermediate distributions. Like other importance sampling-based methods, performance is critically dependent on the proposal distribution: a bad proposal can lead to arbitrarily inaccurate estimates of the target distribution. This paper presents a new method for automatically adapting the proposal using an approximation of the Kullback-Leibler divergence between the true posterior and the proposal distribution. The method is very flexible, applicable to any parameterized proposal distribution and it supports online and batch variants. We use the new framework to adapt powerful proposal distributions with rich parameterizations based upon neural networks leading to Neural Adaptive Sequential Monte Carlo (NASMC). Experiments indicate that NASMC significantly improves inference in a non-linear state space model outperforming adaptive proposal methods including the Extended Kalman and Unscented Particle Filters. Experiments also indicate that improved inference translates into improved parameter learning when NASMC is used as a subroutine of Particle Marginal Metropolis Hastings. Finally we show that NASMC is able to train a latent variable recurrent neural network (LV-RNN) achieving results that compete with the state-of-the-art for polymorphic music modelling. NASMC can be seen as bridging the gap between adaptive SMC methods and the recent work in scalable, black-box variational inference.},
  langid = {english}
}

@article{guidi_clinician_2015,
  title = {Clinician {{Perception}} of the {{Effectiveness}} of an {{Automated Early Warning}} and {{Response System}} for {{Sepsis}} in an {{Academic Medical Center}}},
  author = {Guidi, Jessica L. and Clark, Katherine and Upton, Mark T. and Faust, Hilary and Umscheid, Craig A. and Lane-Fall, Meghan B. and Mikkelsen, Mark E. and Schweickert, William D. and Vanzandbergen, Christine A. and Betesh, Joel and Tait, Gordon and Hanish, Asaf and Smith, Kirsten and Feeley, Denise and Fuchs, Barry D.},
  date = {2015-10},
  journaltitle = {Annals of the American Thoracic Society},
  shortjournal = {Ann Am Thorac Soc},
  volume = {12},
  number = {10},
  eprint = {26288388},
  eprinttype = {pmid},
  pages = {1514--1519},
  issn = {2325-6621},
  doi = {10.1513/AnnalsATS.201503-129OC},
  abstract = {RATIONALE: We implemented an electronic early warning and response system (EWRS) to improve detection of and response to severe sepsis. Sustainability of such a system requires stakeholder acceptance. We hypothesized that clinicians receiving such alerts perceive them to be useful and effective. OBJECTIVES: To survey clinicians after EWRS notification about perceptions of the system. METHODS: For a 6-week study period 1 month after EWRS implementation in a large tertiary referral medical center, bedside clinicians, including providers (physicians, advanced practice providers) and registered nurses (RNs), were surveyed confidentially within 2 hours of an alert. MEASUREMENTS AND MAIN RESULTS: For the 247 alerts that triggered, 127 providers (51\%) and 105 RNs (43\%) completed the survey. Clinicians perceived most patients as stable before and after the alert. Approximately half (39\% providers, 48\% RNs) felt the alert provided new information, and about half (44\% providers, 56\% RNs) reported changes in management as a result of the alert, including closer monitoring and additional interventions. Over half (54\% providers, 65\% RNs) felt the alert was appropriately timed. Approximately one-third found the alert helpful (33\% providers, 40\% RNs) and fewer felt it improved patient care (24\% providers, 35\% RNs). CONCLUSIONS: A minority of responders perceived the EWRS to be useful, likely related to the perception that most patients identified were stable. However, management was altered half the time after an alert. These results suggest further improvements to the system are needed to enhance clinician perception of the system's utility.},
  langid = {english}
}

@online{gulati_conformer_2020,
  title = {Conformer: {{Convolution-augmented Transformer}} for {{Speech Recognition}}},
  shorttitle = {Conformer},
  author = {Gulati, Anmol and Qin, James and Chiu, Chung-Cheng and Parmar, Niki and Zhang, Yu and Yu, Jiahui and Han, Wei and Wang, Shibo and Zhang, Zhengdong and Wu, Yonghui and Pang, Ruoming},
  date = {2020-05-16},
  eprint = {2005.08100},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2005.08100},
  urldate = {2023-03-17},
  abstract = {Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent neural networks (RNNs). Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively. In this work, we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient way. To this regard, we propose the convolution-augmented transformer for speech recognition, named Conformer. Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. On the widely used LibriSpeech benchmark, our model achieves WER of 2.1\%/4.3\% without using a language model and 1.9\%/3.9\% with an external language model on test/testother. We also observe competitive performance of 2.7\%/6.3\% with a small model of only 10M parameters.},
  pubstate = {preprint}
}

@online{gulcehre_using_2015,
  title = {On {{Using Monolingual Corpora}} in {{Neural Machine Translation}}},
  author = {Gulcehre, Caglar and Firat, Orhan and Xu, Kelvin and Cho, Kyunghyun and Barrault, Loic and Lin, Huei-Chi and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  date = {2015-06-12},
  eprint = {1503.03535},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1503.03535},
  urldate = {2023-04-12},
  abstract = {Recent work on end-to-end neural network-based architectures for machine translation has shown promising results for En-Fr and En-De translation. Arguably, one of the major factors behind this success has been the availability of high quality parallel corpora. In this work, we investigate how to leverage abundant monolingual corpora for neural machine translation. Compared to a phrase-based and hierarchical baseline, we obtain up to \$1.96\$ BLEU improvement on the low-resource language pair Turkish-English, and \$1.59\$ BLEU on the focused domain task of Chinese-English chat messages. While our method was initially targeted toward such tasks with less parallel data, we show that it also extends to high resource languages such as Cs-En and De-En where we obtain an improvement of \$0.39\$ and \$0.47\$ BLEU scores over the neural machine translation baselines, respectively.},
  pubstate = {preprint}
}

@inproceedings{gulrajani_improved_2017,
  title = {Improved Training of {{Wasserstein GANs}}},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron},
  date = {2017},
  pages = {5769--5779}
}

@book{gumbel_statistical_1954,
  title = {Statistical {{Theory}} of {{Extreme Values}} and {{Some Practical Applications}}. {{A Series}} of {{Lectures}}},
  author = {Gumbel, Emil Julius},
  date = {1954},
  volume = {33},
  publisher = {{National Bureau of Standards}},
  abstract = {This monograph is based on four lectures given at the National Bureau of Standards under the sponsorship of the Applied Mathematics Division. The aim of this publication is to make the statistical theory and techniques of extreme values readily available to scientists and engineers. This seems necessary because the original papers, partly written in foreign languages and published in remote journals, are not easy to find. The first lecture outlines some of the practical problems to which the theory pertains. The second lecture introduces certain new statistical tools necessary for the theory, which is developed in the third lecture, first in exact, then in asymptotic form. The fourth lecture shows a series of practical applications and gives all numerical details for enabling interested readers to apply the method to their own problems.}
}

@online{gunasekar_textbooks_2023,
  title = {Textbooks {{Are All You Need}}},
  author = {Gunasekar, Suriya and Zhang, Yi and Aneja, Jyoti and Mendes, Caio César Teodoro and Del Giorno, Allie and Gopi, Sivakanth and Javaheripi, Mojan and Kauffmann, Piero and family=Rosa, given=Gustavo, prefix=de, useprefix=true and Saarikivi, Olli and Salim, Adil and Shah, Shital and Behl, Harkirat Singh and Wang, Xin and Bubeck, Sébastien and Eldan, Ronen and Kalai, Adam Tauman and Lee, Yin Tat and Li, Yuanzhi},
  date = {2023-10-02},
  eprint = {2306.11644},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2306.11644},
  urldate = {2023-10-18},
  abstract = {We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook quality" data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6\% on HumanEval and 55.5\% on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45\% on HumanEval.},
  pubstate = {preprint}
}

@misc{guo_calibration_2017,
  title = {On {{Calibration}} of {{Modern Neural Networks}}},
  author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q.},
  date = {2017-06-14},
  eprint = {1706.04599},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1706.04599},
  urldate = {2018-10-31},
  abstract = {Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions.}
}

@inproceedings{guo_parameterefficient_2021,
  title = {Parameter-Efficient Transfer Learning with Diff Pruning},
  booktitle = {Proceedings of the {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{ACL}})},
  author = {Guo, Demi and Rush, Alexander and Kim, Yoon},
  date = {2021-08},
  pages = {4884--4896},
  publisher = {{Association for Computational Linguistics}},
  location = {{Virtual}}
}

@unpublished{gururangan_don_2020,
  title = {Don't Stop Pretraining: {{Adapt}} Language Models to Domains and Tasks},
  author = {Gururangan, Suchin and Marasović, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A},
  date = {2020},
  eprint = {2004.10964},
  eprinttype = {arxiv}
}

@article{gutmann_noisecontrastive_2010,
  title = {Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics.},
  author = {Gutmann, Michael U and Hyvärinen, Aapo},
  date = {2010},
  journaltitle = {Journal of Machine Learning Research},
  volume = {13},
  number = {2}
}

@online{gutmann_pen_2022,
  title = {Pen and {{Paper Exercises}} in {{Machine Learning}}},
  author = {Gutmann, Michael U.},
  year = {arXiv:2022-06-27},
  number = {2206.13446},
  eprint = {2206.13446},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2206.13446},
  urldate = {2022-07-02},
  abstract = {This is a collection of (mostly) pen-and-paper exercises in machine learning. The exercises are on the following topics: linear algebra, optimisation, directed graphical models, undirected graphical models, expressive power of graphical models, factor graphs and message passing, inference for hidden Markov models, model-based learning (including ICA and unnormalised models), sampling and Monte-Carlo integration, and variational inference.},
  pubstate = {preprint}
}

@inproceedings{hafner_learning_2019,
  title = {Learning Latent Dynamics for Planning from Pixels},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Hafner, Danijar and Lillicrap, Timothy and Fischer, Ian and Villegas, Ruben and Ha, David and Lee, Honglak and Davidson, James},
  date = {2019-06-09},
  series = {Proceedings of Machine Learning Research},
  volume = {97},
  pages = {2555--2565},
  publisher = {{PMLR}},
  url = {http://proceedings.mlr.press/v97/hafner19a.html},
  abstract = {Planning has been very successful for control tasks with known environment dynamics. To leverage planning in unknown environments, the agent needs to learn the dynamics from interactions with the world. However, learning dynamics models that are accurate enough for planning has been a long-standing challenge, especially in image-based domains. We propose the Deep Planning Network (PlaNet), a purely model-based agent that learns the environment dynamics from images and chooses actions through fast online planning in latent space. To achieve high performance, the dynamics model must accurately predict the rewards ahead for multiple time steps. We approach this using a latent dynamics model with both deterministic and stochastic transition components. Moreover, we propose a multi-step variational inference objective that we name latent overshooting. Using only pixel observations, our agent solves continuous control tasks with contact dynamics, partial observability, and sparse rewards, which exceed the difficulty of tasks that were previously solved by planning with learned models. PlaNet uses substantially fewer episodes and reaches final performance close to and sometimes higher than strong model-free algorithms.}
}

@inproceedings{hajavi_siamese_2021,
  title = {Siamese Capsule Network for End-to-End Speaker Recognition in the Wild},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Hajavi, Amirhossein and Etemad, Ali},
  date = {2021}
}

@online{hajibabaei_unified_2018,
  title = {Unified Hypersphere Embedding for Speaker Recognition},
  author = {Hajibabaei, Mahdi and Dai, Dengxin},
  date = {2018},
  eprint = {1807.08312},
  eprinttype = {arxiv},
  pubstate = {preprint}
}

@article{hamilton_graph_2020,
  title = {Graph {{Representation Learning}}},
  author = {Hamilton, William L.},
  date = {2020-09-15},
  journaltitle = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
  shortjournal = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
  volume = {14},
  number = {3},
  pages = {1--159},
  issn = {1939-4608, 1939-4616},
  doi = {10.2200/S01045ED1V01Y202009AIM046},
  url = {https://www.morganclaypool.com/doi/10.2200/S01045ED1V01Y202009AIM046},
  urldate = {2021-10-30},
  langid = {english}
}

@inproceedings{han_deep_2017,
  title = {Deep Learning-Based Telephony Speech Recognition in the Wild.},
  booktitle = {Proceedings of the {{Annual Conference}} of the {{International Speech Communication Association}} ({{Interspeech}}},
  author = {Han, Kyu J and Hahm, Seongjun and Kim, Byung-Hak and Kim, Jungsuk and Lane, Ian R},
  date = {2017},
  pages = {1323--1327}
}

@inproceedings{han_eie_2016,
  title = {{{EIE}}: Efficient Inference Engine on Compressed Deep Neural Network},
  booktitle = {Proceedings of the 43rd {{International Symposium}} on {{Computer Architecture}}},
  author = {Han, Song and Liu, Xingyu and Mao, Huizi and Pu, Jing and Pedram, Ardavan and Horowitz, Mark A. and Dally, William J.},
  date = {2016},
  pages = {243--254}
}

@misc{hannun_deep_2014,
  title = {Deep {{Speech}}: {{Scaling}} up End-to-End Speech Recognition},
  shorttitle = {Deep {{Speech}}},
  author = {Hannun, Awni and Case, Carl and Casper, Jared and Catanzaro, Bryan and Diamos, Greg and Elsen, Erich and Prenger, Ryan and Satheesh, Sanjeev and Sengupta, Shubho and Coates, Adam and Ng, Andrew Y.},
  date = {2014-12-17},
  eprint = {1412.5567},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1412.5567},
  urldate = {2018-09-06},
  abstract = {We present a state-of-the-art speech recognition system developed using end-to-end deep learning. Our architecture is significantly simpler than traditional speech systems, which rely on laboriously engineered processing pipelines; these traditional systems also tend to perform poorly when used in noisy environments. In contrast, our system does not need hand-designed components to model background noise, reverberation, or speaker variation, but instead directly learns a function that is robust to such effects. We do not need a phoneme dictionary, nor even the concept of a "phoneme." Key to our approach is a well-optimized RNN training system that uses multiple GPUs, as well as a set of novel data synthesis techniques that allow us to efficiently obtain a large amount of varied data for training. Our system, called Deep Speech, outperforms previously published results on the widely studied Switchboard Hub5'00, achieving 16.0\% error on the full test set. Deep Speech also handles challenging noisy environments better than widely used, state-of-the-art commercial speech systems.}
}

@misc{hannun_firstpass_2014,
  title = {First-{{Pass Large Vocabulary Continuous Speech Recognition}} Using {{Bi-Directional Recurrent DNNs}}},
  author = {Hannun, Awni Y. and Maas, Andrew L. and Jurafsky, Daniel and Ng, Andrew Y.},
  date = {2014-08-12},
  eprint = {1408.2873},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1408.2873},
  urldate = {2018-05-29},
  abstract = {We present a method to perform first-pass large vocabulary continuous speech recognition using only a neural network and language model. Deep neural network acoustic models are now commonplace in HMM-based speech recognition systems, but building such systems is a complex, domain-specific task. Recent work demonstrated the feasibility of discarding the HMM sequence modeling framework by directly predicting transcript text from audio. This paper extends this approach in two ways. First, we demonstrate that a straightforward recurrent neural network architecture can achieve a high level of accuracy. Second, we propose and evaluate a modified prefix-search decoding algorithm. This approach to decoding enables first-pass speech recognition with a language model, completely unaided by the cumbersome infrastructure of HMM-based systems. Experiments on the Wall Street Journal corpus demonstrate fairly competitive word error rates, and the importance of bi-directional network recurrence.}
}

@misc{hansen_cma_2016,
  title = {The {{CMA Evolution Strategy}}: {{A Tutorial}}},
  author = {Hansen, Nikolaus},
  date = {2016},
  eprint = {15003161},
  eprinttype = {pmid},
  url = {http://arxiv.org/abs/1604.00772},
  urldate = {2018-03-26},
  abstract = {This tutorial introduces the CMA Evolution Strategy (ES), where CMA stands for Covariance Matrix Adaptation. The CMA-ES is a stochastic, or randomized, method for real-parameter (continuous domain) optimization of non-linear, non-convex functions. We try to motivate and derive the algorithm from intuitive concepts and from requirements of non-linear, non-convex search in continuous domain.},
  isbn = {3540290060}
}

@article{hansen_completely_2001,
  title = {Completely {{Derandomized Self-Adaptation}} in {{Evolution Strategies}}},
  author = {Hansen, Nikolaus and Ostermeier, Andreas},
  date = {2001},
  journaltitle = {Evolutionary Computation},
  volume = {9},
  number = {2},
  eprint = {11382355},
  eprinttype = {pmid},
  pages = {159--195},
  issn = {1063-6560},
  doi = {10.1162/106365601750190398},
  url = {http://www.mitpressjournals.org/doi/10.1162/106365601750190398},
  urldate = {2018-04-19},
  abstract = {This paper puts forward two useful methods for self-adaptation of the mutation distribution - the concepts of derandomization and cumulation. Principle shortcomings of the concept of mutative strategy parameter control and two levels of derandomization are reviewed. Basic demands on the self-adaptation of arbitrary (normal) mutation distributions are developed. Applying arbitrary, normal mutation distributions is equivalent to applying a general, linear problem encoding. The underlying objective of mutative strategy parameter control is roughly to favor previously selected mutation steps in the future. If this objective is pursued rigorously, a completely derandomized self-adaptation scheme results, which adapts arbitrary normal mutation distributions. This scheme, called covariance matrix adaptation (CMA), meets the previously stated demands. It can still be considerably improved by cumulation - utilizing an evolution path rather than single search steps. Simulations on various test functions reveal local and global search properties of the evolution strategy with and without covariance matrix adaptation. Their performances are comparable only on perfectly scaled functions. On badly scaled, non-separable functions usually a speed up factor of several orders of magnitude is observed. On moderately mis-scaled functions a speed up factor of three to ten can be expected.}
}

@article{hansen_neural_1990,
  title = {Neural Network Ensembles},
  author = {Hansen, Lars Kai and Salamon, Peter},
  date = {1990},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {12},
  number = {10},
  pages = {993--1001},
  publisher = {{IEEE}}
}

@article{hanson_comparing_1989,
  title = {Comparing {{Biases}} for {{Minimal Network Construction}} with {{Back-Propagation}}},
  author = {Hanson, Stephen José and Pratt, Lorien},
  date = {1989},
  journaltitle = {Advances in Neural Information Processing Systems},
  pages = {177--185},
  url = {http://portal.acm.org/citation.cfm?id=89851.89872},
  urldate = {2018-06-04},
  abstract = {Rumelhart (1987). has proposed a method for choosing minimal or "simple" representations during learning in Back-propagation networks. This approach can be used to (a) dynamically select the number of hidden units. (b) construct a representation that is appropriate for the problem and (c) thus improve the generalization ability of Back-propagation networks. The method Rumelhart suggests involves adding penalty terms to the usual error function. In this paper we introduce Rumelhart·s minimal networks idea and compare two possible biases on the weight search space. These biases are compared in both simple counting problems and a speech recognition problem. In general. the constrained search does seem to minimize the number of hidden units required with an expected increase in local minima.},
  isbn = {1-558-60015-9}
}

@unpublished{haque_audiolinguistic_2019,
  title = {Audio-{{Linguistic Embeddings}} for {{Spoken Sentences}}},
  author = {Haque, Albert and Guo, Michelle and Verma, Prateek and Fei-Fei, Li},
  date = {2019-02-20},
  eprint = {1902.07817},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/1902.07817},
  urldate = {2021-10-21},
  abstract = {We propose spoken sentence embeddings which capture both acoustic and linguistic content. While existing works operate at the character, phoneme, or word level, our method learns long-term dependencies by modeling speech at the sentence level. Formulated as an audio-linguistic multitask learning problem, our encoder-decoder model simultaneously reconstructs acoustic and natural language features from audio. Our results show that spoken sentence embeddings outperform phoneme and word-level baselines on speech recognition and emotion recognition tasks. Ablation studies show that our embeddings can better model high-level acoustic concepts while retaining linguistic content. Overall, our work illustrates the viability of generic, multi-modal sentence embeddings for spoken language understanding.}
}

@book{harari_sapiens_2011,
  title = {Sapiens: {{A Brief History}} of {{Humankind}}},
  author = {Harari, Yuval Noah},
  date = {2011},
  isbn = {978-0-06-231609-7}
}

@unpublished{haroush_statistical_2021,
  title = {A Statistical Framework for Efficient out of Distribution Detection in Deep Neural Networks},
  author = {Haroush, Matan and Frostig, Tzviel and Heller, Ruth and Soudry, Daniel},
  date = {2021},
  eprint = {2102.12967},
  eprinttype = {arxiv}
}

@inproceedings{harwath_deep_2015,
  title = {Deep Multimodal Semantic Embeddings for Speech and Images},
  booktitle = {Ieee {{Workshop}} on {{Automatic Speech Recognition}} and {{Understanding}}},
  author = {Harwath, David and Glass, James R},
  date = {2015}
}

@inproceedings{harwath_jointly_2018,
  title = {Jointly Discovering Visual Objects and Spoken Words from Raw Sensory Input},
  booktitle = {Proceedings of the {{European Conference}} on {{Computer Vision}} ({{ECCV}})},
  author = {Harwath, David and Recasens, Adria and Surís, Dídac and Chuang, Galen and Torralba, Antonio and Glass, James},
  date = {2018},
  pages = {649--665}
}

@inproceedings{harwath_learning_2019,
  title = {Learning Hierarchical Discrete Linguistic Units from Visually-Grounded Speech},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Harwath, David and Hsu, Wei-Ning and Glass, James},
  date = {2019}
}

@inproceedings{harwath_unsupervised_2016,
  title = {Unsupervised Learning of Spoken Language with Visual Context},
  booktitle = {Conference on {{Neural Information Processing Systems}} (Neurips)},
  author = {Harwath, David and Torralba, Antonio and Glass, James R},
  date = {2016}
}

@inproceedings{harwath_vision_2018,
  title = {Vision as an Interlingua: {{Learning}} Multilingual Semantic Embeddings of Untranscribed Speech},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Harwath, David and Chuang, Galen and Glass, James},
  date = {2018},
  pages = {4969--4973}
}

@misc{hasanpour_lets_2016,
  title = {Lets Keep It Simple, {{Using}} Simple Architectures to Outperform Deeper and More Complex Architectures},
  author = {Hasanpour, Seyyed Hossein and Rouhani, Mohammad and Fayyaz, Mohsen and Sabokrou, Mohammad},
  date = {2016},
  eprint = {1608.06037},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1608.06037},
  urldate = {2018-04-03},
  abstract = {Major winning Convolutional Neural Networks (CNNs), such as AlexNet, VGGNet, ResNet, GoogleNet, include tens to hundreds of millions of parameters, which impose considerable computation and memory overhead. This limits their practical use for training, optimization and memory efficiency. On the contrary, light-weight architectures, being proposed to address this issue, mainly suffer from low accuracy. These inefficiencies mostly stem from following an ad hoc procedure. We propose a simple architecture, called SimpleNet, based on a set of designing principles, with which we empirically show, a well-crafted yet simple and reasonably deep architecture can perform on par with deeper and more complex architectures. SimpleNet provides a good tradeoff between the computation/memory efficiency and the accuracy. Our simple 13-layer architecture outperforms most of the deeper and complex architectures to date such as VGGNet, ResNet, and GoogleNet on several well-known benchmarks while having 2 to 25 times fewer number of parameters and operations. This makes it very handy for embedded system or system with computational and memory limitations. We achieved state-of-the-art result on CIFAR10 outperforming several heavier architectures, near state of the art on MNIST and competitive results on CIFAR100 and SVHN. Models are made available at: https://github.com/Coderx7/SimpleNet}
}

@misc{hashemi_learning_2018,
  title = {Learning {{Memory Access Patterns}}},
  author = {Hashemi, Milad and Swersky, Kevin and Smith, Jamie A. and Ayers, Grant and Litz, Heiner and Chang, Jichuan and Kozyrakis, Christos and Ranganathan, Parthasarathy},
  date = {2018},
  eprint = {1803.02329},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1803.02329},
  urldate = {2018-04-01},
  abstract = {The explosion in workload complexity and the recent slow-down in Moore's law scaling call for new approaches towards efficient computing. Researchers are now beginning to use recent advances in machine learning in software optimizations, augmenting or replacing traditional heuristics and data structures. However, the space of machine learning for computer hardware architecture is only lightly explored. In this paper, we demonstrate the potential of deep learning to address the von Neumann bottleneck of memory performance. We focus on the critical problem of learning memory access patterns, with the goal of constructing accurate and efficient memory prefetchers. We relate contemporary prefetching strategies to n-gram models in natural language processing, and show how recurrent neural networks can serve as a drop-in replacement. On a suite of challenging benchmark datasets, we find that neural networks consistently demonstrate superior performance in terms of precision and recall. This work represents the first step towards practical neural-network based prefetching, and opens a wide range of exciting directions for machine learning in computer architecture research.}
}

@book{hastie_elements_2009,
  title = {The {{Elements}} of {{Statistical Learning}}},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  date = {2009},
  journaltitle = {Bayesian Forecasting and Dynamic Models},
  edition = {2},
  eprint = {12377617},
  eprinttype = {pmid},
  publisher = {{Springer}},
  issn = {0172-7397},
  doi = {10.1007/b94608},
  url = {http://www.springerlink.com/index/10.1007/b94608},
  abstract = {During the past decade there has been an explosion in computation and information technology. With it has come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It should be a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting-the first comprehensive treatment of this topic in any book. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie wrote much of the statistical modeling software in S-PLUS and invented principal curves and surfaces. Tibshirani proposed the Lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, and projection pursuit. FROM THE REVIEWS: TECHNOMETRICS "This is a vast and complex book. Generally, it concentrates on explaining why and how the methods work, rather than how to use them. Examples and especially the visualizations are principle features...As a source for the methods of statistical learning...it will probably be a long time before there is a competitor to this book."},
  isbn = {978-0-387-84857-0}
}

@book{hauberg_differential_2022,
  title = {Differential {{Geometry}} for {{Generative Modelling}}},
  author = {Hauberg, Søren},
  date = {2022},
  url = {http://www2.compute.dtu.dk/~sohau/weekendwithbernie/Differential_geometry_for_generative_modeling.pdf},
  langid = {english}
}

@inproceedings{hauberg_geometric_2012,
  title = {A {{Geometric}} Take on {{Metric Learning}}},
  booktitle = {Proceedings of the 26th {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Hauberg, Søren and Freifeld, Oren and Black, Michael J},
  date = {2012},
  pages = {2033--2041},
  location = {{Lake Tahoe, Nevada, USA}},
  url = {https://proceedings.neurips.cc/paper/2012/hash/ec5aa0b7846082a2415f0902f0da88f2-Abstract.html},
  abstract = {Multi-metric learning techniques learn local metric tensors in different parts of a feature space. With such an approach, even simple classifiers can be competitive with the state-of-the-art because the distance measure locally adapts to the structure of the data. The learned distance measure is, however, non-metric, which has prevented multi-metric learning from generalizing to tasks such as dimensionality reduction and regression in a principled way. We prove that, with appropriate changes, multi-metric learning corresponds to learning the structure of a Riemannian manifold. We then show that this structure gives us a principled way to perform dimensionality reduction and regression according to the learned metrics. Algorithmically, we provide the first practical algorithm for computing geodesics according to the learned metrics, as well as algorithms for computing exponential and logarithmic maps on the Riemannian manifold. Together, these tools let many Euclidean algorithms take advantage of multi-metric learning. We illustrate the approach on regression and dimensionality reduction tasks that involve predicting measurements of the human body from shape data.},
  langid = {english}
}

@inproceedings{havard_models_2019,
  title = {Models of Visually Grounded Speech Signal Pay Attention to Nouns: {{A}} Bilingual Experiment on {{English}} and {{Japanese}}},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Havard, William N and Chevrot, Jean-Pierre and Besacier, Laurent},
  date = {2019},
  pages = {8618--8622}
}

@article{havasi_training_2021,
  title = {Training {{Independent Subnetworks}} for {{Robust Prediction}}},
  author = {Havasi, Marton and Jenatton, Rodolphe and Fort, Stanislav},
  date = {2021},
  pages = {13},
  abstract = {Recent approaches to efficiently ensemble neural networks have shown that strong robustness and uncertainty performance can be achieved with a negligible gain in parameters over the original network. However, these methods still require multiple forward passes for prediction, leading to a significant computational cost. In this work, we show a surprising result: the benefits of using multiple predictions can be achieved ‘for free’ under a single model’s forward pass. In particular, we show that, using a multi-input multi-output (MIMO) configuration, one can utilize a single model’s capacity to train multiple subnetworks that independently learn the task at hand. By ensembling the predictions made by the subnetworks, we improve model robustness without increasing compute. We observe a significant improvement in negative log-likelihood, accuracy, and calibration error on CIFAR10, CIFAR100, ImageNet, and their out-of-distribution variants compared to previous methods.},
  langid = {english}
}

@inproceedings{havtorn_benchmarking_2022,
  title = {Benchmarking {{Generative Latent Variable Models}} for {{Speech}}},
  booktitle = {Proceedings of the {{Workshop}} on {{Deep Generative Models}} for {{Highly Structured Data}}},
  author = {Havtorn, Jakob D. and Borgholt, Lasse and Hauberg, Søren and Frellsen, Jes and Maaløe, Lars},
  date = {2022-04-05},
  eprint = {2202.12707},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2202.12707},
  abstract = {Stochastic latent variable models (LVMs) achieve state-of-the-art performance on natural image generation but are still inferior to deterministic models on speech. In this paper, we develop a speech benchmark of popular temporal LVMs and compare them against state-of-the-art deterministic models. We report the likelihood, which is a much used metric in the image domain, but rarely, or incomparably, reported for speech models. To assess the quality of the learned representations, we also compare their usefulness for phoneme recognition. Finally, we adapt the Clockwork VAE, a state-of-the-art temporal LVM for video generation, to the speech domain. Despite being autoregressive only in latent space, we find that the Clockwork VAE can outperform previous LVMs and reduce the gap to deterministic models by using a hierarchy of latent variables.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}}
}

@inproceedings{havtorn_hierarchical_2021,
  title = {Hierarchical {{VAEs Know What They Don}}’t {{Know}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Havtorn, Jakob D. and Frellsen, Jes and Hauberg, Søren and Maaløe, Lars},
  date = {2021-07},
  eprint = {2102.08248},
  eprinttype = {arxiv},
  pages = {4117--4128},
  publisher = {{PMLR}},
  location = {{Virtual}},
  url = {https://proceedings.mlr.press/v139/havtorn21a.html},
  abstract = {Deep generative models have been demonstrated as state-of-the-art density estimators. Yet, recent work has found that they often assign a higher likelihood to data from outside the training distribution. This seemingly paradoxical behavior has caused concerns over the quality of the attained density estimates. In the context of hierarchical variational autoencoders, we provide evidence to explain this behavior by out-of-distribution data having in-distribution low-level features. We argue that this is both expected and desirable behavior. With this insight in hand, we develop a fast, scalable and fully unsupervised likelihood-ratio score for OOD detection that requires data to be in-distribution across all feature-levels. We benchmark the method on a vast set of data and model combinations and achieve state-of-the-art results on out-of-distribution detection.},
  keywords = {Read}
}

@inproceedings{havtorn_multiqt_2020,
  title = {{{MultiQT}}: {{Multimodal}} Learning for Real-Time Question Tracking in Speech},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{ACL}})},
  author = {Havtorn, Jakob D. and Latko, Jan and Edin, Joakim and Borgholt, Lasse and Maaløe, Lars and Belgrano, Lorenzo and Jakobsen, Nicolai Frost and Sdun, Regitze and Agić, Željko},
  date = {2020},
  eprint = {2005.00812},
  eprinttype = {arxiv},
  publisher = {{Association for Computational Linguistics}},
  location = {{Virtual}},
  abstract = {We address a challenging and practical task of labeling questions in speech in real time during telephone calls to emergency medical services in English, which embeds within a broader decision support system for emergency call-takers. We propose a novel multimodal approach to real-time sequence labeling in speech. Our model treats speech and its own textual representation as two separate modalities or views, as it jointly learns from streamed audio and its noisy transcription into text via automatic speech recognition. Our results show significant gains of jointly learning from the two modalities when compared to text or audio only, under adverse noise and limited volume of training data. The results generalize to medical symptoms detection where we observe a similar pattern of improvements with multimodal learning.},
  eventtitle = {Annual {{Meeting}} of the {{Association}} for {{Computational Linguistics}}}
}

@thesis{havtorn_physics_2014,
  type = {Second Year Project},
  title = {Physics {{Project Contact Resistance Measurement Rig}} for {{Thermoelectric Materials}}},
  author = {Havtorn, Jakob D. and Skovhus, Thorbjørn and Blaabjerg, Lasse},
  date = {2014},
  institution = {{Technical University of Denmark}}
}

@thesis{havtorn_variational_2018,
  type = {mathesis},
  title = {Variational {{Optimization}} of {{Neural Networks}}},
  author = {Havtorn, Jakob D.},
  date = {2018-07},
  institution = {{Technical University of Denmark}},
  location = {{Kongens Lyngby}},
  abstract = {This thesis presents evolution strategies as a competitive alternative to classical methods for reinforcement learning and unifies previous work within black-box optimization with deep learning. It presents variational optimization as an encompassing mathematical framework, which maintains a search distribution over the optimized parameters during training, and describes how it can be efficiently applied for optimization of neural networks without the use of backpropagation. The natural gradient is derived and implemented as a way to update a search distribution subject to a similarity constraint w.r.t. the previous iterate. It is shown to be superior to using the regular gradient, especially when adapting the variance of a Gaussian search distribution. Antithetic sampling and the method of common random numbers are derived and applied to reduce the variance of the gradient. Experiments run primarily in the supervised setting on the MNIST dataset show that while antithetic sampling is rather efficient at achieving this goal, common random numbers is not. A novel approach for reduction of gradient variance as well as computation based on a local reparameterization of feedforward neural networks is presented and treated theoretically. Different search distributions based on the Gaussian are derived and implemented and the effect of adapting the mean and variance is compared to adapting only the mean which parameterizes the network parameters. It is found that while using an isotropic Gaussian with fixed variance provides good results, adapting the variance can lead to divergence. Separable Gaussians with a variance per network layer or per network weight are shown to perform similarly to using a fixed variance but not significantly better. These results are discussed and related to the geometry of the loss surface.},
  langid = {english},
  pagetotal = {152}
}

@article{hawkins_waymo_2023,
  entrysubtype = {newspaper},
  title = {Waymo’s Driverless Cars Were Involved in Two Crashes and 18 ‘Minor Contact Events’ over 1 Million Miles},
  author = {Hawkins, Andrew J.},
  date = {2023-02-28},
  journaltitle = {The Verge},
  url = {https://www.theverge.com/2023/2/28/23617278/waymo-self-driving-driverless-crashes-av},
  urldate = {2023-09-18},
  abstract = {The Alphabet-owned company pulls back the curtain on more stats from its public road testing. Of the 20 incidents, only two met the federal government’s reporting criteria, and no one was injured.},
  journalsubtitle = {Tech},
  langid = {english}
}

@inproceedings{hayashi_backtranslationstyle_2018,
  title = {Back-Translation-Style Data Augmentation for End-to-End {{ASR}}},
  booktitle = {{{IEEE Spoken Language Technology Workshop}}},
  author = {Hayashi, Tomoki and Watanabe, Shinji and Zhang, Yu and Toda, Tomoki and Hori, Takaaki and Astudillo, Ramon and Takeda, Kazuya},
  date = {2018},
  pages = {426--433}
}

@online{hayashi_discretalk_2020,
  title = {{{DiscreTalk}}: {{Text-to-Speech}} as a {{Machine Translation Problem}}},
  shorttitle = {{{DiscreTalk}}},
  author = {Hayashi, Tomoki and Watanabe, Shinji},
  date = {2020-05-11},
  eprint = {2005.05525},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2005.05525},
  urldate = {2023-04-12},
  abstract = {This paper proposes a new end-to-end text-to-speech (E2E-TTS) model based on neural machine translation (NMT). The proposed model consists of two components; a non-autoregressive vector quantized variational autoencoder (VQ-VAE) model and an autoregressive Transformer-NMT model. The VQ-VAE model learns a mapping function from a speech waveform into a sequence of discrete symbols, and then the Transformer-NMT model is trained to estimate this discrete symbol sequence from a given input text. Since the VQ-VAE model can learn such a mapping in a fully-data-driven manner, we do not need to consider hyperparameters of the feature extraction required in the conventional E2E-TTS models. Thanks to the use of discrete symbols, we can use various techniques developed in NMT and automatic speech recognition (ASR) such as beam search, subword units, and fusions with a language model. Furthermore, we can avoid an over smoothing problem of predicted features, which is one of the common issues in TTS. The experimental evaluation with the JSUT corpus shows that the proposed method outperforms the conventional Transformer-TTS model with a non-autoregressive neural vocoder in naturalness, achieving the performance comparable to the reconstruction of the VQ-VAE model.},
  pubstate = {preprint}
}

@article{he_deep_2015,
  title = {Deep Residual Learning for Image Recognition},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2015-12-10},
  journaltitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  eprint = {1512.03385},
  eprinttype = {arxiv},
  pages = {770--778},
  url = {http://arxiv.org/abs/1512.03385},
  urldate = {2018-03-24}
}

@article{he_delving_2015,
  title = {Delving {{Deep}} into {{Rectifiers}}: {{Surpassing Human-Level Performance}} on {{ImageNet Classification}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2015},
  journaltitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  eprint = {1502.01852v1},
  eprinttype = {arxiv},
  pages = {1026--1034},
  abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra com-putational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94\% top-5 test error on the ImageNet 2012 clas-sification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\% [33]). To our knowledge, our result is the first 1 to surpass the reported human-level performance (5.1\%, [26]) on this dataset.}
}

@inproceedings{he_identity_2016,
  title = {Identity Mappings in Deep Residual Networks},
  booktitle = {European {{Conference}} on {{Computer Vision}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2016},
  pages = {630--645}
}

@misc{he_lagging_2019,
  title = {Lagging {{Inference Networks}} and {{Posterior Collapse}} in {{Variational Autoencoders}}},
  author = {He, Junxian and Spokoyny, Daniel and Neubig, Graham and Berg-Kirkpatrick, Taylor},
  date = {2019-01-28},
  eprint = {1901.05534},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1901.05534},
  urldate = {2021-03-02},
  abstract = {The variational autoencoder (VAE) is a popular combination of deep latent variable model and accompanying variational learning technique. By using a neural inference network to approximate the model's posterior on latent variables, VAEs efficiently parameterize a lower bound on marginal data likelihood that can be optimized directly via gradient methods. In practice, however, VAE training often results in a degenerate local optimum known as "posterior collapse" where the model learns to ignore the latent variable and the approximate posterior mimics the prior. In this paper, we investigate posterior collapse from the perspective of training dynamics. We find that during the initial stages of training the inference network fails to approximate the model's true posterior, which is a moving target. As a result, the model is encouraged to ignore the latent encoding and posterior collapse occurs. Based on this observation, we propose an extremely simple modification to VAE training to reduce inference lag: depending on the model's current mutual information between latent variable and observation, we aggressively optimize the inference network before performing each model update. Despite introducing neither new model components nor significant complexity over basic VAE, our approach is able to avoid the problem of collapse that has plagued a large amount of previous work. Empirically, our approach outperforms strong autoregressive baselines on text and image benchmarks in terms of held-out likelihood, and is competitive with more complex techniques for avoiding collapse while being substantially faster.},
  langid = {english}
}

@inproceedings{he_momentum_2020,
  title = {Momentum Contrast for Unsupervised Visual Representation Learning},
  booktitle = {{{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition},
  author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  date = {2020}
}

@unpublished{he_multiview_2017,
  title = {Multi-View {{Recurrent Neural Acoustic Word Embeddings}}},
  author = {He, Wanjia and Wang, Weiran and Livescu, Karen},
  date = {2017-03-10},
  eprint = {1611.04496},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1611.04496},
  urldate = {2021-10-20},
  abstract = {Recent work has begun exploring neural acoustic word embeddings---fixed-dimensional vector representations of arbitrary-length speech segments corresponding to words. Such embeddings are applicable to speech retrieval and recognition tasks, where reasoning about whole words may make it possible to avoid ambiguous sub-word representations. The main idea is to map acoustic sequences to fixed-dimensional vectors such that examples of the same word are mapped to similar vectors, while different-word examples are mapped to very different vectors. In this work we take a multi-view approach to learning acoustic word embeddings, in which we jointly learn to embed acoustic sequences and their corresponding character sequences. We use deep bidirectional LSTM embedding models and multi-view contrastive losses. We study the effect of different loss variants, including fixed-margin and cost-sensitive losses. Our acoustic word embeddings improve over previous approaches for the task of word discrimination. We also present results on other tasks that are enabled by the multi-view approach, including cross-view word discrimination and word similarity.}
}

@online{he_outofdistribution_2022,
  title = {Out-{{Of-Distribution Detection In Unsupervised Continual Learning}}},
  author = {He, Jiangpeng and Zhu, Fengqing},
  date = {2022-04-11},
  number = {2204.05462},
  eprint = {2204.05462},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2204.05462},
  urldate = {2022-09-04},
  abstract = {Unsupervised continual learning aims to learn new tasks incrementally without requiring human annotations. However, most existing methods, especially those targeted on image classification, only work in a simplified scenario by assuming all new data belong to new tasks, which is not realistic if the class labels are not provided. Therefore, to perform unsupervised continual learning in real life applications, an out-of-distribution detector is required at beginning to identify whether each new data corresponds to a new task or already learned tasks, which still remains under-explored yet. In this work, we formulate the problem for Out-of-distribution Detection in Unsupervised Continual Learning (OOD-UCL) with the corresponding evaluation protocol. In addition, we propose a novel OOD detection method by correcting the output bias at first and then enhancing the output confidence for in-distribution data based on task discriminativeness, which can be applied directly without modifying the learning procedures and objectives of continual learning. Our method is evaluated on CIFAR-100 dataset by following the proposed evaluation protocol and we show improved performance compared with existing OOD detection methods under the unsupervised continual learning scenario.},
  pubstate = {preprint}
}

@inproceedings{he_spatial_2014,
  title = {Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition},
  booktitle = {European {{Conference}} on {{Computer Vision}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2014},
  pages = {346--361}
}

@misc{he_streaming_2018,
  title = {Streaming {{End-to-end Speech Recognition For Mobile Devices}}},
  author = {He, Yanzhang and Sainath, Tara N. and Prabhavalkar, Rohit and McGraw, Ian and Alvarez, Raziel and Zhao, Ding and Rybach, David and Kannan, Anjuli and Wu, Yonghui and Pang, Ruoming and Liang, Qiao and Bhatia, Deepti and Shangguan, Yuan and Li, Bo and Pundak, Golan and Sim, Khe Chai and Bagby, Tom and Chang, Shuo-yiin and Rao, Kanishka and Gruenstein, Alexander},
  date = {2018-11-15},
  eprint = {1811.06621},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1811.06621},
  urldate = {2018-12-18},
  abstract = {End-to-end (E2E) models, which directly predict output character sequences given input speech, are good candidates for on-device speech recognition. E2E models, however, present numerous challenges: In order to be truly useful, such models must decode speech utterances in a streaming fashion, in real time; they must be robust to the long tail of use cases; they must be able to leverage user-specific context (e.g., contact lists); and above all, they must be extremely accurate. In this work, we describe our efforts at building an E2E speech recognizer using a recurrent neural network transducer. In experimental evaluations, we find that the proposed approach can outperform a conventional CTC-based model in terms of both latency and accuracy in a number of evaluation categories.}
}

@inproceedings{heck_feature_2017,
  title = {Feature Optimized {{DPGMM}} Clustering for Unsupervised Subword Modeling: {{A}} Contribution to Zerospeech 2017},
  booktitle = {2017 {{IEEE}} Automatic Speech Recognition and Understanding Workshop, {{ASRU}} 2017, Okinawa, Japan, December 16-20, 2017},
  author = {Heck, Michael and Sakti, Sakriani and Nakamura, Satoshi},
  date = {2017},
  pages = {740--746},
  doi = {10.1109/ASRU.2017.8269011},
  url = {https://doi.org/10.1109/ASRU.2017.8269011}
}

@misc{heigold_endtoend_2015,
  title = {End-to-{{End Text-Dependent Speaker Verification}}},
  author = {Heigold, Georg and Moreno, Ignacio and Bengio, Samy and Shazeer, Noam},
  date = {2015-09-27},
  eprint = {1509.08062},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1509.08062},
  urldate = {2019-03-23},
  abstract = {In this paper we present a data-driven, integrated approach to speaker verification, which maps a test utterance and a few reference utterances directly to a single score for verification and jointly optimizes the system's components using the same evaluation protocol and metric as at test time. Such an approach will result in simple and efficient systems, requiring little domain-specific knowledge and making few model assumptions. We implement the idea by formulating the problem as a single neural network architecture, including the estimation of a speaker model on only a few utterances, and evaluate it on our internal "Ok Google" benchmark for text-dependent speaker verification. The proposed approach appears to be very effective for big data applications like ours that require highly accurate, easy-to-maintain systems with a small footprint.}
}

@article{heinrich_estimating_2011,
  title = {Estimating {{Speaking Rate}} by {{Means}} of {{Rhythmicity Parameters}}},
  author = {Heinrich, Christian and Schiel, Florian},
  date = {2011},
  pages = {4},
  abstract = {In this paper we present a speech rate estimator based on so-called rhythmicity features derived from a modified version of the short-time energy envelope. To evaluate the new method, it is compared to a traditional speech rate estimator on the basis of semi-automatic segmentation. Speech material from the Alcohol Language Corpus (ALC) covering intoxicated and sober speech of different speech styles provides a statistically sound foundation to test upon. The proposed measure clearly correlates with the semi-automatically determined speech rate and seems to be robust across speech styles and speaker states.},
  langid = {english}
}

@misc{henaff_dataefficient_2019,
  title = {Data-{{Efficient Image Recognition}} with {{Contrastive Predictive Coding}}},
  author = {Hénaff, Olivier J. and Razavi, Ali and Doersch, Carl and Eslami, S. M. Ali and family=Oord, given=Aäron, prefix=van den, useprefix=false},
  date = {2019-05-22},
  eprint = {1905.09272},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1905.09272},
  urldate = {2019-05-24},
  abstract = {Large scale deep learning excels when labeled images are abundant, yet data-efficient learning remains a longstanding challenge. While biological vision is thought to leverage vast amounts of unlabeled data to solve classification problems with limited supervision, computer vision has so far not succeeded in this `semi-supervised' regime. Our work tackles this challenge with Contrastive Predictive Coding, an unsupervised objective which extracts stable structure from still images. The result is a representation which, equipped with a simple linear classifier, separates ImageNet categories better than all competing methods, and surpasses the performance of a fully-supervised AlexNet model. When given a small number of labeled images (as few as 13 per class), this representation retains a strong classification performance, outperforming state-of-the-art semi-supervised methods by 10\% Top-5 accuracy and supervised methods by 20\%. Finally, we find our unsupervised representation to serve as a useful substrate for image detection on the PASCAL-VOC 2007 dataset, approaching the performance of representations trained with a fully annotated ImageNet dataset. We expect these results to open the door to pipelines that use scalable unsupervised representations as a drop-in replacement for supervised ones for real-world vision tasks where labels are scarce.}
}

@article{henderson_deep_2013,
  title = {Deep {{Neural Network Approach}} for the {{Dialog State Tracking Challenge}}},
  author = {Henderson, Matthew and Thomson, Blaise and Young, Steve},
  date = {2013},
  journaltitle = {Proceedings of the SIGDIAL 2013 Conference},
  pages = {467--471},
  url = {http://www.aclweb.org/anthology/W/W13/W13-4073},
  abstract = {While belief tracking is known to be im-portant in allowing statistical dialog sys-tems to manage dialogs in a highly robust manner, until recently little attention has been given to analysing the behaviour of belief tracking techniques. The Dialogue State Tracking Challenge has allowed for such an analysis, comparing multiple be-lief tracking approaches on a shared task. Recent success in using deep learning for speech research motivates the Deep Neu-ral Network approach presented here. The model parameters can be learnt by directly maximising the likelihood of the training data. The paper explores some aspects of the training, and the resulting tracker is found to perform competitively, particu-larly on a corpus of dialogs from a system not found in the training.},
  isbn = {9781937284954}
}

@inproceedings{hendrycks_baseline_2017,
  title = {A {{Baseline}} for {{Detecting Misclassified}} and {{Out-of-Distribution Examples}} in {{Neural Networks}}},
  booktitle = {Proceedings of the 5th {{International Conference}} on {{Learning Representations}} ({{ICRL}})},
  author = {Hendrycks, Dan and Gimpel, Kevin},
  date = {2017},
  location = {{Toulon, France}},
  url = {http://arxiv.org/abs/1610.02136},
  urldate = {2021-01-23},
  abstract = {We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english}
}

@inproceedings{hendrycks_deep_2019,
  title = {Deep Anomaly Detection with Outlier Exposure},
  booktitle = {Proceedings of the 7th {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Hendrycks, Dan and Mazeika, Mantas and Dietterich, Thomas G.},
  date = {2019},
  location = {{New Orleans, LA, USA}},
  url = {https://openreview.net/forum?id=HyxCxhRcY7},
  abstract = {It is important to detect anomalous inputs when deploying machine learning systems. The use of larger and more complex inputs in deep learning magnifies the difficulty of distinguishing between anomalous and in-distribution examples. At the same time, diverse image and text data are available in enormous quantities. We propose leveraging these data to improve deep anomaly detection by training anomaly detectors against an auxiliary dataset of outliers, an approach we call Outlier Exposure (OE). This enables anomaly detectors to generalize and detect unseen anomalies. In extensive experiments on natural language processing and small- and large-scale vision tasks, we find that Outlier Exposure significantly improves detection performance. We also observe that cutting-edge generative models trained on CIFAR-10 may assign higher likelihoods to SVHN images than to CIFAR-10 images; we use OE to mitigate this issue. We also analyze the flexibility and robustness of Outlier Exposure, and identify characteristics of the auxiliary dataset that improve performance.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}}
}

@inproceedings{hendrycks_scaling_2022,
  title = {Scaling Out-of-Distribution Detection for Real-World Settings},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Hendrycks, Dan and Basart, Steven and Mazeika, Mantas and Zou, Andy and Kwon, Joe and Mostajabi, Mohammadreza and Steinhardt, Jacob and Song, Dawn},
  date = {2022},
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {162},
  eprint = {1911.11132},
  eprinttype = {arxiv},
  pages = {8759--8773},
  publisher = {{PMLR}},
  location = {{Baltimore, Maryland, USA}},
  eventtitle = {International {{Conference}} on {{Machine Learning}}}
}

@article{hendrycks_using_2019,
  title = {Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty},
  author = {Hendrycks, Dan and Mazeika, Mantas and Kadavath, Saurav and Song, Dawn},
  date = {2019},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {32}
}

@inproceedings{henning_are_2021,
  title = {Are {{Bayesian}} Neural Networks Intrinsically Good at Out-of-Distribution Detection?},
  booktitle = {Proceedings of the {{ICML}} 2021 {{Workshop}} on {{Uncertainty}} \& {{Robustness}} in {{Deep Learning}}},
  author = {Henning, Christian and D'Angelo, Francesco and Grewe, Benjamin F},
  date = {2021-07},
  location = {{Virtual}}
}

@book{herlau_introduction_2016,
  title = {Introduction to {{Machine Learning}} and {{Data Mining}}},
  author = {Herlau, Tue and Schmidt, Mikkel N. and Mørup, Morten},
  date = {2016},
  abstract = {Course notes/book for 02450 Introduction to Machine Learning and Data Mining}
}

@inproceedings{hermann_multilingual_2013,
  title = {Multilingual {{Distributed Representations Without Word Alignment}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Hermann, Karl Moritz and Blunsom, Phil},
  date = {2013-12-20},
  eprint = {1312.6173},
  eprinttype = {arxiv}
}

@article{hermann_teaching_2015,
  title = {Teaching {{Machines}} to {{Read}} and {{Comprehend}}},
  author = {Hermann, Karl Moritz and Kočiský, Tomáš and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
  date = {2015},
  journaltitle = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
  eprint = {18263409},
  eprinttype = {pmid},
  pages = {1693--1701},
  issn = {10495258},
  doi = {10.1109/72.410363},
  url = {http://arxiv.org/abs/1506.03340},
  abstract = {Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.},
  isbn = {1045-9227}
}

@article{hermansky_rasta_1994,
  title = {{{RASTA Processing}} of {{Speech}}},
  author = {Hermansky, Hynek and Morgan, Nelson},
  date = {1994-10},
  journaltitle = {IEEE Transactions on Speech and Audio Processing},
  volume = {2},
  number = {4},
  pages = {587--589}
}

@inproceedings{hernandez_tedlium_2018,
  title = {{{TED-LIUM}} 3: {{Twice}} as Much Data and Corpus Repartition for Experiments on Speaker Adaptation},
  booktitle = {International {{Conference}} on {{Speech}} and {{Computer}}},
  author = {Hernandez, François and Nguyen, Vincent and Ghannay, Sahar and Tomashenko, Natalia and Esteve, Yannick},
  date = {2018},
  pages = {198--208}
}

@inproceedings{hernandez-lobato_black-box_2016,
  title = {Black-Box \$\textbackslash alpha\$-Divergence {{Minimization}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Hernández-Lobato, José Miguel and Li, Yingzhen and Rowland, Mark and Hernández-Lobato, Daniel and Bui, Thang and Turner, Richard E.},
  date = {2016-11-10},
  eprint = {1511.03243},
  eprinttype = {arxiv},
  location = {{New York, NY, USA}},
  url = {http://arxiv.org/abs/1511.03243},
  urldate = {2018-05-06},
  abstract = {Black-box alpha (BB-\$\textbackslash alpha\$) is a new approximate inference method based on the minimization of \$\textbackslash alpha\$-divergences. BB-\$\textbackslash alpha\$ scales to large datasets because it can be implemented using stochastic gradient descent. BB-\$\textbackslash alpha\$ can be applied to complex probabilistic models with little effort since it only requires as input the likelihood function and its gradients. These gradients can be easily obtained using automatic differentiation. By changing the divergence parameter \$\textbackslash alpha\$, the method is able to interpolate between variational Bayes (VB) (\$\textbackslash alpha \textbackslash rightarrow 0\$) and an algorithm similar to expectation propagation (EP) (\$\textbackslash alpha = 1\$). Experiments on probit regression and neural network regression and classification problems show that BB-\$\textbackslash alpha\$ with non-standard settings of \$\textbackslash alpha\$, such as \$\textbackslash alpha = 0.5\$, usually produces better predictions than with \$\textbackslash alpha \textbackslash rightarrow 0\$ (VB) or \$\textbackslash alpha = 1\$ (EP).}
}

@misc{hernandez-lobato_probabilistic_2015,
  title = {Probabilistic {{Backpropagation}} for {{Scalable Learning}} of {{Bayesian Neural Networks}}},
  author = {Hernández-Lobato, José Miguel and Adams, Ryan P.},
  date = {2015},
  eprint = {1502.05336},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1502.05336},
  urldate = {2018-05-06},
  abstract = {Large multilayer neural networks trained with backpropagation have recently achieved state-of-the-art results in a wide range of problems. However, using backprop for neural net learning still has some disadvantages, e.g., having to tune a large number of hyperparameters to the data, lack of calibrated probabilistic predictions, and a tendency to overfit the training data. In principle, the Bayesian approach to learning neural networks does not have these problems. However, existing Bayesian techniques lack scalability to large dataset and network sizes. In this work we present a novel scalable method for learning Bayesian neural networks, called probabilistic backpropagation (PBP). Similar to classical backpropagation, PBP works by computing a forward propagation of probabilities through the network and then doing a backward computation of gradients. A series of experiments on ten real-world datasets show that PBP is significantly faster than other techniques, while offering competitive predictive abilities. Our experiments also show that PBP provides accurate estimates of the posterior variance on the network weights.},
  isbn = {9781510810587}
}

@inproceedings{hertel_deep_2015,
  title = {Deep Convolutional Neural Networks as Generic Feature Extractors},
  booktitle = {Proceedings of the {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Hertel, Lars and Barth, Erhardt and Kaster, Thomas and Martinetz, Thomas},
  date = {2015},
  volume = {2015-Septe},
  eprint = {1710.02286},
  eprinttype = {arxiv},
  issn = {2161-4393},
  doi = {10.1109/IJCNN.2015.7280683},
  abstract = {Recognizing objects in natural images is an intricate problem involving multiple conflicting objectives. Deep convolutional neural networks, trained on large datasets, achieve convincing results and are currently the state-of-the-art approach for this task. However, the long time needed to train such deep networks is a major drawback. We tackled this problem by reusing a previously trained network. For this purpose, we first trained a deep convolutional network on the ILSVRC2012 dataset. We then maintained the learned convolution kernels and only retrained the classification part on different datasets. Using this approach, we achieved an accuracy of 67.68 \% on CIFAR-100, compared to the previous state-of-the-art result of 65.43 \%. Furthermore, our findings indicate that convolutional networks are able to learn generic feature extractors that can be used for different tasks.},
  isbn = {978-1-4799-1960-4}
}

@inproceedings{heusel_gans_2017,
  title = {{{GANs Trained}} by a {{Two Time-Scale Update Rule Converge}} to a {{Local Nash Equilibrium}}},
  booktitle = {Proceedings of the {{Conference}} on {{Neural Information Processing Systems}} ({{NIPS}})},
  author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  date = {2017},
  eprint = {1706.08500},
  eprinttype = {arxiv},
  issn = {10495258},
  abstract = {Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the "Fr\textbackslash 'echet Inception Distance" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.}
}

@inproceedings{higgins_vvae_2017,
  title = {β-{{VAE}}: Learning Basic Visual Concepts with a Constrained Variational Framework},
  booktitle = {Proceedings of the {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
  date = {2017},
  location = {{Palais des Congrès Neptune, Toulon, France}},
  abstract = {Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce β-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter β that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that β-VAE with appropriately tuned β {$>$} 1 qualitatively outperforms VAE (β = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also significantly outperforms all baselines quantitatively. Unlike InfoGAN, β-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter β, which can be directly optimised through a hyperparameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.},
  langid = {english},
  keywords = {Read}
}

@inproceedings{hinton_autoencoders_1994,
  title = {Autoencoders, Minimum Description Length and Helmholtz Free Energy},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hinton, Geoffrey E and Zemel, Richard},
  editor = {Cowan, J. and Tesauro, G. and Alspector, J.},
  date = {1994},
  volume = {6},
  publisher = {{Morgan-Kaufmann}},
  url = {https://proceedings.neurips.cc/paper/1993/file/9e3cfc48eccf81a0d57663e129aef3cb-Paper.pdf}
}

@article{hinton_deep_2012,
  title = {Deep Neural Networks for Acoustic Modeling in Speech Recognition: {{The}} Shared Views of Four Research Groups},
  author = {Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George E. and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara N. and Kingsbury, Brian},
  date = {2012},
  journaltitle = {IEEE Signal Processing Magazine},
  volume = {29},
  number = {6},
  pages = {82--97},
  doi = {10.1109/MSP.2012.2205597}
}

@report{hinton_development_1988,
  title = {The {{Development}} of {{Time-Delay Neural Network Architecture}} for {{Speech Recognition}}},
  author = {Hinton, Geoffrey E. and McClelland, James L.},
  date = {1988},
  institution = {{Carnegie Mellon University}},
  isbn = {0780374029}
}

@misc{hinton_distilling_2015,
  title = {Distilling the Knowledge in a Neural Network},
  author = {Hinton, Geoffrey E. and Vinyals, Oriol and Dean, Jeff},
  date = {2015},
  eprint = {1503.02531},
  eprinttype = {arxiv},
  url = {https://arxiv.org/abs/1503.02531}
}

@article{hinton_fast_2006,
  title = {A Fast Learning Algorithm for Deep Belief Nets},
  author = {Hinton, Geoffrey E and Osindero, Simon and Teh, Yee-Whye},
  date = {2006},
  journaltitle = {Neural computation},
  volume = {18},
  number = {7},
  pages = {1527--1554},
  publisher = {{MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info …}}
}

@misc{hinton_how_2021,
  title = {How to Represent Part-Whole Hierarchies in a Neural Network},
  author = {Hinton, Geoffrey},
  date = {2021-02-24},
  eprint = {2102.12627},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2102.12627},
  urldate = {2021-05-07},
  abstract = {This paper does not describe a working system. Instead, it presents a single idea about representation which allows advances made by several different groups to be combined into an imaginary system called GLOM1. The advances include transformers, neural fields, contrastive representation learning, distillation and capsules. GLOM answers the question: How can a neural network with a fixed architecture parse an image into a partwhole hierarchy which has a different structure for each image? The idea is simply to use islands of identical vectors to represent the nodes in the parse tree. If GLOM can be made to work, it should significantly improve the interpretability of the representations produced by transformer-like systems when applied to vision or language.},
  langid = {english}
}

@misc{hinton_improving_2012,
  title = {Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors},
  author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
  date = {2012},
  eprint = {1207.0580},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1207.0580},
  urldate = {2017-10-13},
  abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.}
}

@inproceedings{hinton_keeping_1993,
  title = {Keeping the Neural Networks Simple by Minimizing the Description Length of the Weights},
  booktitle = {Proceedings of {{Annual Conference}} on {{Computational Learning Theory}} ({{COLT}})},
  author = {Hinton, Geoffrey E. and family=Camp, given=Drew, prefix=van, useprefix=true},
  date = {1993},
  pages = {5--13},
  doi = {10.1145/168304.168306},
  url = {http://portal.acm.org/citation.cfm?doid=168304.168306},
  urldate = {2018-05-06},
  abstract = {Supervised neural networks generalize there is much less information learning, well if in the weights than there is in the output vectors of the train- ing cases. So during it is impor- tant to keep the weights simple by penaliz- ing the amount of information The amount of information be controlled they contain. in a weight can by adding Gaussian noise and the noise level can be adapted during learning to optimize a method of computing the trade-off between the expected squared error of the network and the amount of information in the noisy weights can be computed is required in the weights. We describe the derivatives work that contains a layer of non-linear time-consuming that of the expected squared error and of the amount of information in a net- hidden units. Provided the output units are linear, the exact derivatives without efficiently tions. The idea of minimizing information Monte Carlo simula- the amount of to communicate the weights of a neural network leads to a number of intereating schemes for encoding the weights.},
  isbn = {0-89791-611-5}
}

@article{hinton_learning_2007,
  title = {Learning Multiple Layers of Representation},
  author = {Hinton, Geoffrey E},
  date = {2007},
  journaltitle = {Trends in cognitive sciences},
  volume = {11},
  number = {10},
  pages = {428--434},
  publisher = {{Elsevier}}
}

@article{hinton_reducing_2006,
  title = {Reducing the Dimensionality of Data with Neural Networks},
  author = {Hinton, Geoffrey E and Salakhutdinov, Ruslan R},
  date = {2006},
  journaltitle = {Science (New York, N.Y.)},
  shortjournal = {science},
  volume = {313},
  number = {5786},
  pages = {504--507},
  publisher = {{American Association for the Advancement of Science}}
}

@article{hinton_training_2002,
  title = {Training Products of Experts by Minimizing Contrastive Divergence},
  author = {Hinton, Geoffrey E.},
  date = {2002-08},
  journaltitle = {Neural Computation},
  volume = {14},
  number = {8},
  pages = {1771--1800},
  issn = {0899-7667}
}

@misc{ho_denoising_2020,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  date = {2020-12-16},
  eprint = {2006.11239},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2006.11239},
  urldate = {2021-06-29},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion.},
  langid = {english},
  keywords = {Unread}
}

@inproceedings{ho_flow_2019,
  title = {Flow++: {{Improving Flow-Based Generative Models}} with {{Variational Dequantization}} and {{Architecture Design}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Ho, Jonathan and Chen, Xi and Srinivas, Aravind and Duan, Yan and Abbeel, Pieter},
  date = {2019},
  pages = {9},
  location = {{Long Beach, CA, USA}},
  url = {http://proceedings.mlr.press/v97/ho19a/ho19a.pdf},
  abstract = {Flow-based generative models are powerful exact likelihood models with efficient sampling and inference. Despite their computational efficiency, flow-based models generally have much worse density modeling performance compared to stateof-the-art autoregressive models. In this paper, we investigate and improve upon three limiting design choices employed by flow-based models in prior work: the use of uniform noise for dequantization, the use of inexpressive affine flows, and the use of purely convolutional conditioning networks in coupling layers. Based on our findings, we propose Flow++, a new flow-based model that is now the state-of-the-art non-autoregressive model for unconditional density estimation on standard image benchmarks. Our work has begun to close the significant performance gap that has so far existed between autoregressive models and flow-based models. Our implementation is available at: https://github.com/ aravindsrinivas/flowpp.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english}
}

@article{hochreiter_long_1997,
  title = {Long {{Short-Term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  date = {1997},
  journaltitle = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  doi = {10.1162/neco.1997.9.8.1735},
  url = {http://www.mitpressjournals.org/doi/10.1162/neco.1997.9.8.1735},
  urldate = {2017-09-17},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}
}

@online{hoedt_normalization_2022,
  title = {Normalization Is Dead, Long Live Normalization!},
  author = {Hoedt, Pieter-Jan and Hochreiter, Sepp and Klambauer, Günter},
  date = {2022},
  url = {https://iclr-blog-track.github.io/2022/03/25/unnormalized-resnets/},
  organization = {{ICLR Blog Track}}
}

@misc{hoffer_norm_2018,
  title = {Norm Matters: Efficient and Accurate Normalization Schemes in Deep Networks},
  shorttitle = {Norm Matters},
  author = {Hoffer, Elad and Banner, Ron and Golan, Itay and Soudry, Daniel},
  date = {2018-03-05},
  eprint = {1803.01814},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1803.01814},
  urldate = {2018-09-01},
  abstract = {Over the past few years batch-normalization has been commonly used in deep networks, allowing faster training and high performance for a wide variety of applications. However, the reasons behind its merits remained unanswered, with several shortcomings that hindered its use for certain tasks. In this work we present a novel view on the purpose and function of normalization methods and weight-decay, as tools to decouple weights' norm from the underlying optimized objective. We also improve the use of weight-normalization and show the connection between practices such as normalization, weight decay and learning-rate adjustments. Finally, we suggest several alternatives to the widely used \$L\^2\$ batch-norm, using normalization in \$L\^1\$ and \$L\^\textbackslash infty\$ spaces that can substantially improve numerical stability in low-precision implementations as well as provide computational and memory benefits. We demonstrate that such methods enable the first batch-norm alternative to work for half-precision implementations.}
}

@inproceedings{hoffman_elbo_2016,
  title = {{{ELBO}} Surgery: Yet Another Way to Carve up the Variational Evidence Lower Bound},
  booktitle = {Workshop in {{Advances}} in {{Approximate Bayesian Inference}}},
  author = {Hoffman, Matthew D and Johnson, Matthew J},
  date = {2016},
  pages = {4},
  abstract = {We rewrite the variational evidence lower bound objective (ELBO) of variational autoencoders in a way that highlights the role of the encoded data distribution. This perspective suggests that to improve our variational bounds we should improve our priors and not just the encoder and decoder.},
  eventtitle = {{{NIPS}}},
  langid = {english}
}

@article{hoffman_learning_nodate,
  title = {Learning {{Deep Latent Gaussian Models}} with {{Markov Chain Monte Carlo}}},
  author = {Hoffman, Matthew D},
  pages = {10},
  url = {http://proceedings.mlr.press/v70/hoffman17a/hoffman17a.pdf},
  abstract = {Deep latent Gaussian models are powerful and popular probabilistic models of highdimensional data. These models are almost always fit using variational expectationmaximization, an approximation to true maximum-marginal-likelihood estimation. In this paper, we propose a different approach: rather than use a variational approximation (which produces biased gradient signals), we use Markov chain Monte Carlo (MCMC, which allows us to trade bias for computation). We find that our MCMC-based approach has several advantages: it yields higher held-out likelihoods, produces sharper images, and does not suffer from the variational overpruning effect. MCMC’s additional computational overhead proves to be significant, but not prohibitive.},
  langid = {english}
}

@article{hoffman_stochastic_2012,
  title = {Stochastic {{Variational Inference}}},
  author = {Hoffman, Matt and Blei, David M. and Wang, Chong and Paisley, John},
  date = {2012},
  journaltitle = {Journal of Machine Learning Research},
  eprint = {19926898},
  eprinttype = {pmid},
  issn = {1532-4435},
  doi = {citeulike-article-id:10852147},
  url = {http://arxiv.org/abs/1206.7051},
  urldate = {2018-05-09},
  abstract = {We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets.},
  isbn = {1532-4435},
  keywords = {★}
}

@inproceedings{holzenberger_learning_2018,
  title = {Learning Word Embeddings: {{Unsupervised}} Methods for Fixed-Size Representations of Variable-Length Speech Segments},
  booktitle = {Annual Conference of the International Speech Communication Association},
  author = {Holzenberger, Nils and Du, Mingxing and Karadayi, Julien and Riad, Rachid and Dupoux, Emmanuel},
  date = {2018}
}

@article{honkela_variational_2004,
  title = {Variational Learning and Bits-Back Coding: {{An}} Information-Theoretic View to {{Bayesian}} Learning},
  author = {Honkela, Antti and Valpola, Harri},
  date = {2004},
  journaltitle = {IEEE Transactions on Neural Networks},
  volume = {15},
  number = {4},
  eprint = {15461074},
  eprinttype = {pmid},
  pages = {800--810},
  issn = {10459227},
  doi = {10.1109/TNN.2004.828762},
  abstract = {The bits-back coding first introduced by Wallace in 1990 and later by Hinton and van Camp in 1993 provides an interesting link between Bayesian learning and information-theoretic minimum-description-length (MDL) learning approaches. The bits-back coding allows interpreting the cost function used in the variational Bayesian method called ensemble learning as a code length in addition to the Bayesian view of misfit of the posterior approximation and a lower bound of model evidence. Combining these two viewpoints provides interesting insights to the learning process and the functions of different parts of the model. In this paper, the problem of variational Bayesian learning of hierarchical latent variable models is used to demonstrate the benefits of the two views. The code-length interpretation provides new views to many parts of the problem such as model comparison and pruning and helps explain many phenomena occurring in learning.}
}

@misc{hoogeboom_argmax_2021,
  title = {Argmax {{Flows}} and {{Multinomial Diffusion}}: {{Towards Non-Autoregressive Language Models}}},
  shorttitle = {Argmax {{Flows}} and {{Multinomial Diffusion}}},
  author = {Hoogeboom, Emiel and Nielsen, Didrik and Jaini, Priyank and Forré, Patrick and Welling, Max},
  date = {2021-02-10},
  eprint = {2102.05379},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2102.05379},
  urldate = {2021-05-07},
  abstract = {The field of language modelling has been largely dominated by autoregressive models, for which sampling is inherently difficult to parallelize. This paper introduces two new classes of generative models for categorical data such as language or image segmentation: Argmax Flows and Multinomial Diffusion. Argmax Flows are defined by a composition of a continuous distribution (such as a normalizing flow), and an argmax function. To optimize this model, we learn a probabilistic inverse for the argmax that lifts the categorical data to a continuous space. Multinomial Diffusion gradually adds categorical noise in a diffusion process, for which the generative denoising process is learned. We demonstrate that our models perform competitively on language modelling and modelling of image segmentation maps.},
  langid = {english}
}

@inproceedings{hori_cycleconsistency_2019,
  title = {Cycle-Consistency Training for End-to-End Speech Recognition},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Hori, Takaaki and Astudillo, Ramon and Hayashi, Tomoki and Zhang, Yu and Watanabe, Shinji and Le Roux, Jonathan},
  date = {2019},
  pages = {6271--6275}
}

@misc{hospedales_metalearning_2020,
  title = {Meta-{{Learning}} in {{Neural Networks}}: {{A Survey}}},
  shorttitle = {Meta-{{Learning}} in {{Neural Networks}}},
  author = {Hospedales, Timothy and Antoniou, Antreas and Micaelli, Paul and Storkey, Amos},
  date = {2020-04-11},
  eprint = {2004.05439},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2004.05439},
  urldate = {2020-06-10},
  abstract = {The field of meta-learning, or learning-to-learn, has seen a dramatic rise in interest in recent years. Contrary to conventional approaches to AI where a given task is solved from scratch using a fixed learning algorithm, meta-learning aims to improve the learning algorithm itself, given the experience of multiple learning episodes. This paradigm provides an opportunity to tackle many of the conventional challenges of deep learning, including data and computation bottlenecks, as well as the fundamental issue of generalization. In this survey we describe the contemporary meta-learning landscape. We first discuss definitions of meta-learning and position it with respect to related fields, such as transfer learning, multi-task learning, and hyperparameter optimization. We then propose a new taxonomy that provides a more comprehensive breakdown of the space of meta-learning methods today. We survey promising applications and successes of meta-learning including few-shot learning, reinforcement learning and architecture search. Finally, we discuss outstanding challenges and promising areas for future research.},
  langid = {english}
}

@article{hotelling_relations_1936,
  title = {Relations between Two Sets of Variates},
  author = {Hotelling, H.},
  date = {1936},
  journaltitle = {Biometrika},
  volume = {28},
  number = {3/4},
  pages = {321--377}
}

@inproceedings{houlsby_parameterefficient_2019,
  title = {Parameter-Efficient Transfer Learning for {{NLP}}},
  booktitle = {International {{Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  date = {2019-06-09/2019-06-15},
  volume = {97},
  pages = {2790--2799}
}

@inproceedings{hozjan_interface_2002,
  title = {Interface Databases: {{Design}} and Collection of a Multilingual Emotional Speech Database},
  booktitle = {Proceedings of the {{International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}})},
  author = {Hozjan, Vladimir and Kacic, Zdravko and Moreno, Asuncion and Bonafonte, Antonio and Nogueiras, Albino},
  date = {2002},
  eventtitle = {International {{Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}})}
}

@article{hsiao_online_2020,
  title = {Online Automatic Speech Recognition with Listen, Attend and Spell Model},
  author = {Hsiao, Roger and Can, Dogan and Ng, Tim and Travadi, Ruchir and Ghoshal, Arnab},
  date = {2020},
  journaltitle = {IEEE Signal Processing Letters},
  volume = {27},
  eprint = {2008.05514},
  eprinttype = {arxiv},
  pages = {1889--1893},
  doi = {10.1109/LSP.2020.3031480}
}

@misc{hsu_extracting_2018,
  title = {Extracting {{Domain Invariant Features}} by {{Unsupervised Learning}} for {{Robust Automatic Speech Recognition}}},
  author = {Hsu, Wei-Ning and Glass, James},
  date = {2018-03-07},
  eprint = {1803.02551},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1803.02551},
  urldate = {2019-09-15},
  abstract = {The performance of automatic speech recognition (ASR) systems can be significantly compromised by previously unseen conditions, which is typically due to a mismatch between training and testing distributions. In this paper, we address robustness by studying domain invariant features, such that domain information becomes transparent to ASR systems, resolving the mismatch problem. Specifically, we investigate a recent model, called the Factorized Hierarchical Variational Autoencoder (FHVAE). FHVAEs learn to factorize sequence-level and segment-level attributes into different latent variables without supervision. We argue that the set of latent variables that contain segment-level information is our desired domain invariant feature for ASR. Experiments are conducted on Aurora-4 and CHiME-4, which demonstrate 41\% and 27\% absolute word error rate reductions respectively on mismatched domains.},
  langid = {english}
}

@inproceedings{hsu_generalized_2020,
  title = {Generalized {{ODIN}}: Detecting out-of-Distribution Image without Learning from out-of-Distribution Data},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition},
  author = {Hsu, Yen-Chang and Shen, Yilin and Jin, Hongxia and Kira, Zsolt},
  date = {2020},
  pages = {10951--10960}
}

@inproceedings{hsu_hierarchical_2019,
  title = {Hierarchical Generative Modeling for Controllable Speech Synthesis},
  booktitle = {Proceedings of the International Conference on Learning Representations ({{ICLR}})},
  author = {Hsu, Wei-Ning and Zhang, Yu and Weiss, Ron J. and Zen, Heiga and Wu, Yonghui and Wang, Yuxuan and Cao, Yuan and Jia, Ye and Chen, Zhifeng and Shen, Jonathan and Nguyen, Patrick and Pang, Ruoming},
  date = {2019},
  location = {{New Orleans, LA, USA}},
  url = {https://openreview.net/forum?id=rygkk305YQ}
}

@article{hsu_hubert_2021,
  title = {{{HuBERT}}: {{Self-Supervised Speech Representation Learning}} by {{Masked Prediction}} of {{Hidden Units}}},
  shorttitle = {{{HuBERT}}},
  author = {Hsu, Wei-Ning and Bolte, Benjamin and Tsai, Yao-Hung Hubert and Lakhotia, Kushal and Salakhutdinov, Ruslan and Mohamed, Abdelrahman},
  date = {2021-06-14},
  url = {https://arxiv.org/abs/2106.07447v1},
  urldate = {2021-10-11},
  abstract = {Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19\% and 13\% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.},
  langid = {english}
}

@inproceedings{hsu_hubert_2021a,
  title = {Hubert: {{How Much Can}} a {{Bad Teacher Benefit ASR Pre-Training}}?},
  shorttitle = {Hubert},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Hsu, Wei-Ning and Tsai, Yao-Hung Hubert and Bolte, Benjamin and Salakhutdinov, Ruslan and Mohamed, Abdelrahman},
  date = {2021-06},
  pages = {6533--6537},
  issn = {2379-190X},
  doi = {10.1109/ICASSP39728.2021.9414460},
  abstract = {Compared to vision and language applications, self-supervised pre-training approaches for ASR are challenged by three unique problems: (1) There are multiple sound units in each input utterance, (2) With audio-only pre-training, there is no lexicon of sound units, and (3) Sound units have variable lengths with no explicit segmentation. In this paper, we propose the Hidden-Unit BERT (HUBERT) model which utilizes a cheap k-means clustering step to provide aligned target labels for pre-training of a BERT model. A key ingredient of our approach is applying the predictive loss over the masked regions only. This allows the pre-training stage to benefit from the consistency of the unsupervised teacher rather that its intrinsic quality. Starting with a simple k-means teacher of 100 cluster, and using two iterations of clustering, the HUBERT model matches the state-of-the-art wav2vec 2.0 performance on the ultra low-resource Libri-light 10h, 1h, 10min supervised subsets.},
  eventtitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})}
}

@misc{hsu_learning_2017,
  title = {Learning {{Latent Representations}} for {{Speech Generation}} and {{Transformation}}},
  author = {Hsu, Wei-Ning and Zhang, Yu and Glass, James},
  date = {2017-09-22},
  eprint = {1704.04222},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1704.04222},
  urldate = {2020-05-22},
  abstract = {An ability to model a generative process and learn a latent representation for speech in an unsupervised fashion will be crucial to process vast quantities of unlabelled speech data. Recently, deep probabilistic generative models such as Variational Autoencoders (VAEs) have achieved tremendous success in modeling natural images. In this paper, we apply a convolutional VAE to model the generative process of natural speech. We derive latent space arithmetic operations to disentangle learned latent representations. We demonstrate the capability of our model to modify the phonetic content or the speaker identity for speech segments using the derived operations, without the need for parallel supervisory data.},
  langid = {english}
}

@inproceedings{hsu_robust_2021,
  title = {Robust Wav2vec 2.0: {{Analyzing}} Domain Shift in Self-Supervised Pre-Training},
  booktitle = {Proceedings of the 22nd {{Annual Conference}} of the {{International Speech Communication Association}} ({{Interspeech}})},
  author = {Hsu, Wei-Ning and Sriram, Anuroop and {baevski\textsubscript{u}nsupervised₂021}, Alexei and Likhomanenko, Tatiana and Xu, Qiantong and Pratap, Vineel and Kahn, Jacob and Lee, Ann and Collobert, Ronan and Synnaeve, Gabriel and Auli, Michael},
  date = {2021},
  publisher = {{ISCA}}
}

@inproceedings{hsu_unsupervised_2017,
  title = {Unsupervised {{Learning}} of {{Disentangled}} and {{Interpretable Representations}} from {{Sequential Data}}},
  booktitle = {Proceedings of the 31st {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Hsu, Wei-Ning and Zhang, Yu and Glass, James},
  date = {2017},
  location = {{Long Beach, CA, USA}},
  abstract = {We present a factorized hierarchical variational autoencoder, which learns disentangled and interpretable representations from sequential data without supervision. Specifically, we exploit the multi-scale nature of information in sequential data by formulating it explicitly within a factorized hierarchical graphical model that imposes sequence-dependent priors and sequence-independent priors to different sets of latent variables. The model is evaluated on two speech corpora to demonstrate, qualitatively, its ability to transform speakers or linguistic content by manipulating different sets of latent variables; and quantitatively, its ability to outperform an i-vector baseline for speaker verification and reduce the word error rate by as much as 35\% in mismatched train/test scenarios for automatic speech recognition tasks.},
  eventtitle = {Neural {{Information Processing Systems}}},
  langid = {english}
}

@misc{hsu_wgwavenet_2020,
  title = {{{WG-WaveNet}}: {{Real-Time High-Fidelity Speech Synthesis}} without {{GPU}}},
  shorttitle = {{{WG-WaveNet}}},
  author = {Hsu, Po-chun and Lee, Hung-yi},
  date = {2020-08-20},
  eprint = {2005.07412},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2005.07412},
  urldate = {2021-05-12},
  abstract = {In this paper, we propose WG-WaveNet, a fast, lightweight, and high-quality waveform generation model. WG-WaveNet is composed of a compact flow-based model and a post-filter. The two components are jointly trained by maximizing the likelihood of the training data and optimizing loss functions on the frequency domains. As we design a flow-based model that is heavily compressed, the proposed model requires much less computational resources compared to other waveform generation models during both training and inference time; even though the model is highly compressed, the post-filter maintains the quality of generated waveform. Our PyTorch implementation can be trained using less than 8 GB GPU memory and generates audio samples at a rate of more than 960 kHz on an NVIDIA 1080Ti GPU. Furthermore, even if synthesizing on a CPU, we show that the proposed method is capable of generating 44.1 kHz speech waveform 1.2 times faster than real-time. Experiments also show that the quality of generated audio is comparable to those of other methods. Audio samples are publicly available online.},
  langid = {english}
}

@inproceedings{huang_hierarchical_2019,
  title = {Hierarchical {{Importance Weighted Autoencoders}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Huang, Chin-Wei and Sankaran, Kris and Dhekane, Eeshan and Lacoste, Alexandre and Courville, Aaron},
  editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  date = {2019-06-09/2019-06-15},
  series = {Proceedings of Machine Learning Research},
  volume = {97},
  pages = {2869--2878},
  publisher = {{PMLR}},
  url = {http://proceedings.mlr.press/v97/huang19d.html},
  abstract = {Importance weighted variational inference (Burda et al., 2015) uses multiple i.i.d. samples to have a tighter variational lower bound. We believe a joint proposal has the potential of reducing the number of redundant samples, and introduce a hierarchical structure to induce correlation. The hope is that the proposals would coordinate to make up for the error made by one another to reduce the variance of the importance estimator. Theoretically, we analyze the condition under which convergence of the estimator variance can be connected to convergence of the lower bound. Empirically, we confirm that maximization of the lower bound does implicitly minimize variance. Further analysis shows that this is a result of negative correlation induced by the proposed hierarchical meta sampling scheme, and performance of inference also improves when the number of samples increases.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}}
}

@article{huang_importance_2021,
  title = {On the Importance of Gradients for Detecting Distributional Shifts in the Wild},
  author = {Huang, Rui and Geng, Andrew and Li, Yixuan},
  date = {2021},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {34},
  pages = {677--689}
}

@inproceedings{huang_improving_2022,
  title = {Improving Distortion Robustness of Self-Supervised Speech Processing Tasks with Domain Adaptation},
  booktitle = {Proceedings of the {{Annual Conference}} of the {{International Speech Communication Association}} ({{Interspeech}})},
  author = {Huang, Kuan Po and Fu, Yu-Kuan and Zhang, Yu and Lee, Hung-yi},
  date = {2022}
}

@inproceedings{huang_learning_2013,
  title = {Learning Deep Structured Semantic Models for Web Search Using Clickthrough Data},
  booktitle = {International {{Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Huang, Po-Sen and He, Xiaodong and Gao, Jianfeng and Deng, Li and Acero, Alex and Heck, Larry},
  date = {2013}
}

@misc{huang_lookuptable_2021,
  title = {Lookup-{{Table Recurrent Language Models}} for {{Long Tail Speech Recognition}}},
  author = {Huang, W. Ronny and Sainath, Tara N. and Peyser, Cal and Kumar, Shankar and Rybach, David and Strohman, Trevor},
  date = {2021-04-09},
  eprint = {2104.04552},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2104.04552},
  urldate = {2021-05-07},
  abstract = {We introduce Lookup-Table Language Models (LookupLM), a method for scaling up the size of RNN language models with only a constant increase in the floating point operations, by increasing the expressivity of the embedding table. In particular, we instantiate an (additional) embedding table which embeds the previous n-gram token sequence, rather than a single token. This allows the embedding table to be scaled up arbitrarily—with a commensurate increase in performance—without changing the token vocabulary. Since embeddings are sparsely retrieved from the table via a lookup; increasing the size of the table adds neither extra operations to each forward pass nor extra parameters that need to be stored on limited GPU/TPU memory. We explore scaling n-gram embedding tables up to nearly a billion parameters. When trained on a 3-billion sentence corpus, we find that LookupLM improves long tail log perplexity by 2.44 and long tail WER by 23.4\% on a downstream speech recognition task over a standard RNN language model baseline, an improvement comparable to a scaling up the baseline by 6.2x the number of floating point operations.},
  langid = {english}
}

@inproceedings{huang_mos_2021,
  title = {{{MoS}}: {{Towards Scaling Out-of-Distribution Detection}} for {{Large Semantic Space}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Huang, Rui and Li, Yixuan},
  date = {2021},
  pages = {8706--8715},
  eventtitle = {Conference on {{Computer Vision}} and {{Pattern Recognition}}}
}

@inproceedings{huang_strassen_2016,
  title = {Strassen's Algorithm Reloaded},
  author = {Huang, Jianyu and Smith, Tyler and Henry, Greg and family=Geijn, given=Robert, prefix=van de, useprefix=true},
  date = {2016-11},
  pages = {690--701},
  doi = {10.1109/SC.2016.58}
}

@inproceedings{huang_using_2020,
  title = {Using Personalized Speech Synthesis and Neural Language Generator for Rapid Speaker Adaptation},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Huang, Yan and He, Lei and Wei, Wenning and Gale, William and Li, Jinyu and Gong, Yifan},
  date = {2020},
  pages = {7399--7403}
}

@inproceedings{huangPLMICDAutomaticICD2022,
  title = {{{PLM-ICD}}: {{Automatic ICD Coding}} with {{Pretrained Language Models}}},
  shorttitle = {{{PLM-ICD}}},
  booktitle = {Proceedings of the 4th {{Clinical Natural Language Processing Workshop}}},
  author = {Huang, Chao-Wei and Tsai, Shang-Chi and Chen, Yun-Nung},
  date = {2022-07},
  pages = {10--20},
  publisher = {{Association for Computational Linguistics}},
  location = {{Seattle, WA}},
  doi = {10.18653/v1/2022.clinicalnlp-1.2},
  abstract = {Automatically classifying electronic health records (EHRs) into diagnostic codes has been challenging to the NLP community. State-of-the-art methods treated this problem as a multi-label classification problem and proposed various architectures to model this problem. However, these systems did not leverage the superb performance of pretrained language models, which achieved superb performance on natural language understanding tasks. Prior work has shown that pretrained language models underperformed on this task with the regular fine-tuning scheme. Therefore, this paper aims at analyzing the causes of the underperformance and developing a framework for automatic ICD coding with pretrained language models. We spotted three main issues through the experiments: 1) large label space, 2) long input sequences, and 3) domain mismatch between pretraining and fine-tuning. We propose PLM-ICD, a framework that tackles the challenges with various strategies. The experimental results show that our proposed framework can overcome the challenges and achieves state-of-the-art performance in terms of multiple metrics on the benchmark MIMIC data. Our source code is available at https://github.com/MiuLab/PLM-ICD.}
}

@book{hubbard_how_2014,
  title = {How to {{Measure Anything}} - {{Finding}} the {{Value}} of {{Intangibles}} in {{Business}}},
  author = {Hubbard, Douglas W.},
  date = {2014-04-25},
  edition = {1},
  publisher = {{John Wiley \& Sons Inc}},
  isbn = {978-1-118-53927-9},
  langid = {english},
  pagetotal = {432}
}

@article{huici_speech_2016,
  title = {Speech Rate Estimation in Disordered Speech Based on Spectral Landmark Detection},
  author = {Huici, Hernandez-Diaz and Kairuz, Hector A. and Martens, Heidi and Van Nuffelen, Gwen and De Bodt, Marc},
  date = {2016-05},
  journaltitle = {Biomedical Signal Processing and Control},
  volume = {27},
  pages = {1--6},
  issn = {17468094},
  doi = {10/f8kn5n},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1746809416000069},
  urldate = {2018-11-28},
  abstract = {Speech rate (SR) plays an important role in the assessment of disordered speech. Clinicians rely primarily on manual or semi-automatic methods to determine SR. The reported algorithms are designed for normal speech and show many restrictions with respect to disordered speech that are predominantly characterized by slow SR. This research presents an algorithm that in addition to energy and pitch, relies on information regarding the spectral characteristics of the borders of the syllables (landmarks). Speech samples (three sentences per speaker) for 66 healthy and dysarthric speakers were analyzed with four algorithms (Mrate, robust SR estimation method, Praat’s script and the proposed algorithm based on landmark detection). The landmark approach is demonstrated to be more accurate for speakers with slow SR. The Pearson correlation coefficient between the calculated SR and the reference remains over 0.84 for the 198 sentences analyzed, while the other algorithms’ correlations are below the values reported in literature for fluent speech. In samples where SR is high, the algorithm shows similar limitations versus other algorithms due to the merging of syllables. The landmark-based algorithm is an adequate method for determining SR in disordered speech.},
  langid = {english}
}

@online{huszar_evolution_2017,
  title = {Evolution {{Strategies}}, {{Variational Optimisation}} and {{Natural ES}}},
  author = {Huszár, Ferenc},
  date = {2017},
  url = {http://www.inference.vc/evolution-strategies-variational-optimisation-and-natural-es-2/},
  urldate = {2018-03-23},
  abstract = {In my last post I conveyed my enthusiasm about evolution strategies (ES), and particularly the highly scalable distributed version. I have to admit, this was the first time I had come across this particular formulation, and unexpectedly, people were quick to point out a whole body of research that I probably should have read or known about. Here, I want to highlight two such papers: - Staines and Barber (2013) Optimization by Variational Bounding ESANN - Wierstra et al (2014) Natural Evolution Strategies JMLR And I also highly recommend David's blog post. This post is just a summary of what I've learnt about ES, a bit of an addendum to last week's post. Thanks to David Barber, Olivier Grisel and Nando de Freitas for their comments and pointers.}
}

@article{huszar_how_2015,
  title = {How (Not) to {{Train}} Your {{Generative Model}}: {{Scheduled Sampling}}, {{Likelihood}}, {{Adversary}}?},
  shorttitle = {How (Not) to {{Train}} Your {{Generative Model}}},
  author = {Huszár, Ferenc},
  date = {2015-11-16},
  url = {https://arxiv.org/abs/1511.05101},
  urldate = {2018-10-29},
  langid = {english}
}

@online{huszar_is_2017,
  title = {Is {{Maximum Likelihood Useful}} for {{Representation Learning}}?},
  author = {Huszár, Ferenc},
  date = {2017-05-04},
  url = {https://www.inference.vc/maximum-likelihood-for-representation-learning-2/},
  urldate = {2021-12-16},
  langid = {english},
  organization = {{inFERENCe}}
}

@inproceedings{hvingelby_dane_2020,
  title = {{{DaNE}}: {{A Named Entity Resource}} for {{Danish}}},
  booktitle = {Proceedings of the 12th Language Resources and Evaluation Conference},
  author = {Hvingelby, Rasmus and Pauli, Amalie Brogaard and Barrett, Maria and Rosted, Christina and Lidegaard, Lasse Malm and Søgaard, Anders},
  date = {2020},
  pages = {4597--4604}
}

@misc{iandola_squeezenet_2016,
  title = {{{SqueezeNet}}: {{AlexNet-level}} Accuracy with 50x Fewer Parameters And{$<$} 0.5 {{MB}} Model Size},
  author = {Iandola, Forrest N. and Han, Song and Moskewicz, Matthew W. and Ashraf, Khalid and Dally, William J. and Keutzer, Kurt},
  date = {2016},
  eprint = {1602.07360},
  eprinttype = {arxiv},
  url = {https://arxiv.org/abs/1602.07360}
}

@inproceedings{ilharco_largescale_2019,
  title = {Large-Scale Representation Learning from Visually Grounded Untranscribed Speech},
  booktitle = {Proceedings of the 23rd {{Conference}} on {{Computational Natural Language Learning}} ({{CoNLL}})},
  author = {Ilharco, Gabriel and Zhang, Yuan and Baldridge, Jason},
  date = {2019}
}

@misc{im_denoising_2015,
  title = {Denoising {{Criterion}} for {{Variational Auto-Encoding Framework}}},
  author = {Im, Daniel Jiwoong and Ahn, Sungjin and Memisevic, Roland and Bengio, Yoshua},
  date = {2015-11-19},
  eprint = {1511.06406},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1511.06406},
  urldate = {2018-06-07},
  abstract = {Denoising autoencoders (DAE) are trained to reconstruct their clean inputs with noise injected at the input level, while variational autoencoders (VAE) are trained with noise injected in their stochastic hidden layer, with a regularizer that encourages this noise injection. In this paper, we show that injecting noise both in input and in the stochastic hidden layer can be advantageous and we propose a modified variational lower bound as an improved objective function in this setup. When input is corrupted, then the standard VAE lower bound involves marginalizing the encoder conditional distribution over the input noise, which makes the training criterion intractable. Instead, we propose a modified training criterion which corresponds to a tractable bound when input is corrupted. Experimentally, we find that the proposed denoising variational autoencoder (DVAE) yields better average log-likelihood than the VAE and the importance weighted autoencoder on the MNIST and Frey Face datasets.}
}

@book{internationalbureauofweightsandmeasures_systeme_2019,
  title = {Le {{Système}} International d’unités [{{The International System}} of {{Units}}]},
  author = {{International Bureau of Weights and Measures,}},
  date = {2019},
  edition = {9},
  url = {https://www.bipm.org/documents/20126/41483022/SI-Brochure-9.pdf/fcf090b2-04e6-88cc-1149-c3e029ad8232},
  isbn = {978-92-822-2272-0},
  langid = {english}
}

@unpublished{ioannou_decision_2016,
  title = {Decision {{Forests}}, {{Convolutional Networks}} and the {{Models}} in-{{Between}}},
  author = {Ioannou, Yani and Robertson, Duncan and Zikic, Darko and Kontschieder, Peter and Shotton, Jamie and Brown, Matthew and Criminisi, Antonio},
  date = {2016-03-03},
  eprint = {1603.01250},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1603.01250},
  urldate = {2022-03-30},
  abstract = {This paper investigates the connections between two state of the art classifiers: decision forests (DFs, including decision jungles) and convolutional neural networks (CNNs). Decision forests are computationally efficient thanks to their conditional computation property (computation is confined to only a small region of the tree, the nodes along a single branch). CNNs achieve state of the art accuracy, thanks to their representation learning capabilities. We present a systematic analysis of how to fuse conditional computation with representation learning and achieve a continuum of hybrid models with different ratios of accuracy vs. efficiency. We call this new family of hybrid models conditional networks. Conditional networks can be thought of as: i) decision trees augmented with data transformation operators, or ii) CNNs, with block-diagonal sparse weight matrices, and explicit data routing functions. Experimental validation is performed on the common task of image classification on both the CIFAR and Imagenet datasets. Compared to state of the art CNNs, our hybrid models yield the same accuracy with a fraction of the compute cost and much smaller number of parameters.}
}

@inproceedings{ioffe_batch_2015,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Ioffe, Sergey and Szegedy, Christian},
  date = {2015},
  eprint = {1502.03167},
  eprinttype = {arxiv},
  location = {{Lille, France}},
  url = {http://arxiv.org/abs/1502.03167},
  urldate = {2018-03-18},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.}
}

@inproceedings{ipsen_not-miwae_2021,
  title = {Not-{{MIWAE}}: {{Deep}} Generative Modelling with Missing Not at Random Data},
  booktitle = {Proceedings of the 9th {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Ipsen, Niels Bruun and Mattei, Pierre-Alexandre and Frellsen, Jes},
  date = {2021},
  location = {{Virtual}},
  url = {https://openreview.net/forum?id=tu29GQT0JFy},
  eventtitle = {International {{Conference}} on {{Learning Representations}}}
}

@misc{ishikawa_efficient_2020,
  title = {Efficient {{Debiased Variational Bayes}} by {{Multilevel Monte Carlo Methods}}},
  author = {Ishikawa, Kei and Goda, Takashi},
  date = {2020-01-14},
  eprint = {2001.04676},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2001.04676},
  urldate = {2020-11-18},
  abstract = {Variational Bayes is a method to find a good approximation of the posterior probability distribution of latent variables from a parametric family of distributions. The evidence lower bound (ELBO), which is nothing but the model evidence minus the KullbackLeibler divergence, has been commonly used as a quality measure in the optimization process. However, the model evidence itself has been considered computationally intractable since it is expressed as a nested expectation with an outer expectation with respect to the training dataset and an inner conditional expectation with respect to latent variables. Similarly, if the KullbackLeibler divergence is replaced with another divergence metric, the corresponding lower bound on the model evidence is often given by such a nested expectation. The standard (nested) Monte Carlo method can be used to estimate such quantities, whereas the resulting estimate is biased and the variance is often quite large. Recently the authors provided an unbiased estimator of the model evidence with small variance by applying the idea from multilevel Monte Carlo (MLMC) methods. In this article, we give more examples involving nested expectations in the context of variational Bayes where MLMC methods can help construct low-variance unbiased estimators, and provide numerical results which demonstrate the effectiveness of our proposed estimators.},
  langid = {english}
}

@misc{itu-t_recommendation_1988,
  title = {Recommendation {{G}}. 711. {{Pulse Code Modulation}} ({{PCM}}) of Voice Frequencies},
  author = {ITU-T},
  date = {1988}
}

@inproceedings{iyyer_deep_2015,
  title = {Deep Unordered Composition Rivals Syntactic Methods for Text Classification},
  booktitle = {Proceedings of the 53rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 7th {{International Joint Conference}} on {{Natural Language Processing}}},
  author = {Iyyer, Mohit and Manjunatha, Varun and Boyd-Graber, Jordan and Daumé III, Hal},
  date = {2015-07},
  pages = {1681--1691},
  publisher = {{Association for Computational Linguistics}},
  location = {{Beijing, China}},
  doi = {10.3115/v1/P15-1162},
  url = {https://www.aclweb.org/anthology/P15-1162}
}

@article{izbicki_high-dimensional_2014,
  title = {High-Dimensional Density Ratio Estimation with Extensions to Approximate Likelihood Computation},
  author = {Izbicki, Rafael and Lee, Ann B. and Schafer, Chad M.},
  date = {2014},
  journaltitle = {Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)},
  volume = {33},
  eprint = {1404.7063},
  eprinttype = {arxiv},
  pages = {420--429},
  issn = {15337928},
  abstract = {The ratio between two probability density functions is an important component of various tasks, including selection bias correction, novelty detection and classification. Recently, several estimators of this ratio have been proposed. Most of these methods fail if the sample space is high-dimensional, and hence require a dimension reduction step, the result of which can be a significant loss of information. Here we propose a simple-to-implement, fully nonparametric density ratio estimator that expands the ratio in terms of the eigenfunctions of a kernel-based operator; these functions reflect the underlying geometry of the data (e.g., submanifold structure), often leading to better estimates without an explicit dimension reduction step. We show how our general framework can be extended to address another important problem, the estimation of a likelihood function in situations where that function cannot be well-approximated by an analytical form. One is often faced with this situation when performing statistical inference with data from the sciences, due the complexity of the data and of the processes that generated those data. We emphasize applications where using existing likelihood-free methods of inference would be challenging due to the high dimensionality of the sample space, but where our spectral series method yields a reasonable estimate of the likelihood function. We provide theoretical guarantees and illustrate the effectiveness of our proposed method with numerical experiments.}
}

@misc{izmailov_averaging_2018,
  title = {Averaging {{Weights Leads}} to {{Wider Optima}} and {{Better Generalization}}},
  author = {Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
  date = {2018-03-14},
  eprint = {1803.05407},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1803.05407},
  urldate = {2018-09-07},
  abstract = {Deep neural networks are typically trained by optimizing a loss function with an SGD variant, in conjunction with a decaying learning rate, until convergence. We show that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training. We also show that this Stochastic Weight Averaging (SWA) procedure finds much broader optima than SGD, and approximates the recent Fast Geometric Ensembling (FGE) approach with a single model. Using SWA we achieve notable improvement in test accuracy over conventional SGD training on a range of state-of-the-art residual networks, PyramidNets, DenseNets, and Shake-Shake networks on CIFAR-10, CIFAR-100, and ImageNet. In short, SWA is extremely easy to implement, improves generalization, and has almost no computational overhead.}
}

@article{jaakkola_exploiting_1999,
  title = {Exploiting Generative Models in Discriminative Classifiers},
  author = {Jaakkola, T. S. and Haussler, D.},
  date = {1999},
  journaltitle = {Advances in Neural Information Processing Systems (NIPS)}
}

@inproceedings{jacobs_acoustic_2021,
  title = {Acoustic Word Embeddings for Zero-Resource Languages Using Self-Supervised Contrastive Learning and Multilingual Adaptation},
  booktitle = {2021 {{IEEE}} Spoken Language Technology Workshop ({{SLT}})},
  author = {Jacobs, Christiaan and Matusevych, Yevgen and Kamper, Herman},
  date = {2021},
  pages = {919--926},
  publisher = {{IEEE}}
}

@misc{jaderberg_population_2017,
  title = {Population {{Based Training}} of {{Neural Networks}}},
  author = {Jaderberg, Max and Dalibard, Valentin and Osindero, Simon and Czarnecki, Wojciech M. and Donahue, Jeff and Razavi, Ali and Vinyals, Oriol and Green, Tim and Dunning, Iain and Simonyan, Karen and Fernando, Chrisantha and Kavukcuoglu, Koray},
  date = {2017-11-27},
  eprint = {1711.09846},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1711.09846},
  urldate = {2018-09-17},
  abstract = {Neural networks dominate the modern machine learning landscape, but their training and success still suffer from sensitivity to empirical choices of hyperparameters such as model architecture, loss function, and optimisation algorithm. In this work we present \textbackslash emph\{Population Based Training (PBT)\}, a simple asynchronous optimisation algorithm which effectively utilises a fixed computational budget to jointly optimise a population of models and their hyperparameters to maximise performance. Importantly, PBT discovers a schedule of hyperparameter settings rather than following the generally sub-optimal strategy of trying to find a single fixed set to use for the whole course of training. With just a small modification to a typical distributed hyperparameter training framework, our method allows robust and reliable training of models. We demonstrate the effectiveness of PBT on deep reinforcement learning problems, showing faster wall-clock convergence and higher final performance of agents by optimising over a suite of hyperparameters. In addition, we show the same method can be applied to supervised learning for machine translation, where PBT is used to maximise the BLEU score directly, and also to training of Generative Adversarial Networks to maximise the Inception score of generated images. In all cases PBT results in the automatic discovery of hyperparameter schedules and model selection which results in stable training and better final performance.}
}

@unpublished{jaderberg_spatial_2016,
  title = {Spatial {{Transformer Networks}}},
  author = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and Kavukcuoglu, Koray},
  date = {2016-02-04},
  eprint = {1506.02025},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1506.02025},
  urldate = {2022-05-03},
  abstract = {Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.},
  keywords = {Unread}
}

@misc{jaegle_perceiver_2021,
  title = {Perceiver: {{General Perception}} with {{Iterative Attention}}},
  shorttitle = {Perceiver},
  author = {Jaegle, Andrew and Gimeno, Felix and Brock, Andrew and Zisserman, Andrew and Vinyals, Oriol and Carreira, Joao},
  date = {2021-06-22},
  eprint = {2103.03206},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2103.03206},
  urldate = {2022-02-03},
  abstract = {Biological systems perceive the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domain-specific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver - a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture is competitive with or outperforms strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video, and video+audio. The Perceiver obtains performance comparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly attending to 50,000 pixels. It is also competitive in all modalities in AudioSet.},
  keywords = {Read}
}

@article{jaegle_perceiver_2022,
  title = {Perceiver {{IO}}: {{A General Architecture}} for {{Structured Inputs}} \& {{Outputs}}},
  author = {Jaegle, Andrew and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Doersch, Carl and Ionescu, Catalin and Ding, David and Koppula, Skanda and Zoran, Daniel and Brock, Andrew and Shelhamer, Evan and Hénaff, Olivier and Botvinick, Matthew M and Zisserman, Andrew and Vinyals, Oriol and Carreira, João},
  date = {2022},
  pages = {29},
  abstract = {A central goal of machine learning is the development of systems that can solve many problems in as many data domains as possible. Current architectures, however, cannot be applied beyond a small set of stereotyped settings, as they bake in domain \& task assumptions or scale poorly to large inputs or outputs. In this work, we propose Perceiver IO, a general-purpose architecture that handles data from arbitrary settings while scaling linearly with the size of inputs and outputs. Our model augments the Perceiver with a flexible querying mechanism that enables outputs of various sizes and semantics, doing away with the need for task-specific architecture engineering. The same architecture achieves strong results on tasks spanning natural language and visual understanding, multi-task and multi-modal reasoning, and StarCraft II. As highlights, Perceiver IO outperforms a Transformer-based BERT baseline on the GLUE language benchmark despite removing input tokenization and achieves state-of-the-art performance on Sintel optical flow estimation with no explicit mechanisms for multiscale correspondence.},
  langid = {english},
  keywords = {Read}
}

@misc{jain_attention_2019,
  title = {Attention Is Not {{Explanation}}},
  author = {Jain, Sarthak and Wallace, Byron C.},
  date = {2019-05-08},
  eprint = {1902.10186},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1902.10186},
  urldate = {2021-03-30},
  abstract = {Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful “explanations" for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do. Code to reproduce all experiments is available at https://github.com/successar/ AttentionExplanation.},
  langid = {english}
}

@inproceedings{jaitly_vocal_2013,
  title = {Vocal {{Tract Length Perturbation}} ({{VTLP}}) Improves Speech Recognition},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{Machine Learning}}},
  author = {Jaitly, Navdeep and Hinton, Geoffrey E},
  date = {2013},
  pages = {5},
  abstract = {Augmenting datasets by transforming inputs in a way that does not change the label is a crucial ingredient of the state of the art methods for object recognition using neural networks. However this approach has (to our knowledge) not been exploited successfully in speech recognition (with or without neural networks). In this paper we lay the foundation for this approach, and show one way of augmenting speech datasets by transforming spectrograms, using a random linear warping along the frequency dimension. In practice this can be achieved by using warping techniques that are used for vocal tract length normalization (VTLN) - with the difference that a warp factor is generated randomly each time, during training, rather than fitting a single warp factor to each training and test speaker (or utterance). At test time, a prediction is made by averaging the predictions over multiple warp factors. When this technique is applied to TIMIT using Deep Neural Networks (DNN) of different depths, the Phone Error Rate (PER) improved by an average of 0.65\% on the test set. For a Convolutional neural network (CNN) with convolutional layer in the bottom, a gain of 1.0\% was observed. These improvements were achieved without increasing the number of training epochs, and suggest that data transformations should be an important component of training neural networks for speech, especially for data limited projects.},
  eventtitle = {International {{Conference}} on {{Machine Learning}} ({{ICML}})},
  langid = {english}
}

@misc{jang_categorical_2016,
  title = {Categorical {{Reparameterization}} with {{Gumbel-Softmax}}},
  author = {Jang, Eric and Gu, Shixiang and Poole, Ben},
  date = {2016},
  eprint = {1611.01144},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1611.01144},
  urldate = {2019-12-05},
  abstract = {Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.},
  langid = {english}
}

@inproceedings{jansen_summary_2013,
  title = {A Summary of the 2012 {{JHU CLSP}} Workshop on Zero Resource Speech Technologies and Models of Early Language Acquisition},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Jansen, Aren and Dupoux, Emmanuel and Goldwater, Sharon and Johnson, Mark and Khudanpur, Sanjeev and Church, Kenneth and Feldman, Naomi and Hermansky, Hynek and Metze, Florian and Rose, Richard and others},
  date = {2013},
  pages = {8111--8115}
}

@article{jansen_towards_nodate,
  title = {Towards {{Unsupervised Training}} of {{Speaker Independent Acoustic Models}}},
  author = {Jansen, Aren and Church, Kenneth},
  pages = {4},
  abstract = {Can we automatically discover speaker independent phonemelike subword units with zero resources in a surprise language? There have been a number of recent efforts to automatically discover repeated spoken terms without a recognizer. This paper investigates the feasibility of using these results as constraints for unsupervised acoustic model training. We start with a relatively small set of word types, as well as their locations in the speech. The training process assumes that repetitions of the same (unknown) word share the same (unknown) sequence of subword units. For each word type, we train a whole-word hidden Markov model with Gaussian mixture observation densities and collapse correlated states across the word types using spectral clustering. We find that the resulting state clusters align reasonably well along phonetic lines. In evaluating cross-speaker word similarity, the proposed techniques outperform both raw acoustic features and language-mismatched acoustic models.},
  langid = {english}
}

@inproceedings{jansen_unsupervised_2018,
  title = {Unsupervised {{Learning}} of {{Semantic Audio Representations}}},
  booktitle = {International {{Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Jansen, Aren and Plakal, Manoj and Pandya, Ratheet and Ellis, Daniel P. W. and Hershey, Shawn and Liu, Jiayang and Moore, R. Channing and Saurous, Rif A.},
  date = {2018-04},
  pages = {126--130},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2018.8461684},
  abstract = {Even in the absence of any explicit semantic annotation, vast collections of audio recordings provide valuable information for learning the categorical structure of sounds. We consider several class-agnostic semantic constraints that apply to unlabeled nonspeech audio: (i) noise and translations in time do not change the underlying sound category, (ii) a mixture of two sound events inherits the categories of the constituents, and (iii) the categories of events in close temporal proximity are likely to be the same or related. Without labels to ground them, these constraints are incompatible with classification loss functions. However, they may still be leveraged to identify geometric inequalities needed for triplet loss-based training of convolutional neural networks. The result is low-dimensional embeddings of the input spectrograms that recover 41\% and 84\% of the performance of their fully-supervised counterparts when applied to downstream query-by-example sound retrieval and sound event classification tasks, respectively. Moreover, in limited-supervision settings, our unsupervised embeddings double the state-of-the-art classification performance.},
  eventtitle = {International {{Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})}
}

@inproceedings{jansen_weak_2013,
  title = {Weak Top-down Constraints for Unsupervised Acoustic Model Training},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Jansen, Aren and Thomas, Samuel and Hermansky, Hynek},
  date = {2013},
  pages = {8091--8095},
  location = {{Vancouver, BC, Canada}},
  doi = {10.1109/ICASSP.2013.6639241},
  url = {https://doi.org/10.1109/ICASSP.2013.6639241}
}

@article{jati_neural_2019,
  title = {Neural {{Predictive Coding Using Convolutional Neural Networks Toward Unsupervised Learning}} of {{Speaker Characteristics}}},
  author = {Jati, Arindam and Georgiou, Panayiotis},
  date = {2019-10},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {27},
  number = {10},
  pages = {1577--1589},
  issn = {2329-9304},
  doi = {10.1109/TASLP.2019.2921890},
  abstract = {Learning speaker-specific features is vital in many applications like speaker recognition, diarization, and speech recognition. This paper provides a novel approach, we term neural predictive coding (NPC), to learn speaker-specific characteristics in a completely unsupervised manner from large amounts of unlabeled training data that even contain many non-speech events and multi-speaker audio streams. The NPC framework exploits the proposed short-term active-speaker stationarity hypothesis which assumes two temporally close short speech segments belong to the same speaker, and thus a common representation that can encode the commonalities of both the segments, should capture the vocal characteristics of that speaker. We train a convolutional deep siamese network to produce “speaker embeddings” by learning to separate “same” versus “different” speaker pairs which are generated from an unlabeled data of audio streams. Two sets of experiments are done in different scenarios to evaluate the strength of NPC embeddings and compare with state-of-the-art in-domain supervised methods. First, two speaker identification experiments with different context lengths are performed in a scenario with comparatively limited within-speaker channel variability. NPC embeddings are found to perform the best at short duration experiment, and they provide complementary information to i-vectors for full utterance experiments. Second, a large-scale speaker verification task having a wide range of within-speaker channel variability is adopted as an upper-bound experiment where comparisons are drawn with in-domain supervised methods.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}}
}

@inproceedings{jati_speaker2vec_2017,
  title = {{{Speaker2Vec}}: {{Unsupervised Learning}} and {{Adaptation}} of a {{Speaker Manifold Using Deep Neural Networks}} with an {{Evaluation}} on {{Speaker Segmentation}}},
  shorttitle = {{{Speaker2Vec}}},
  booktitle = {Proceedings of the 18th {{Annual Conference}} of the {{International Speech Communication Association}} ({{Interspeech}})},
  author = {Jati, Arindam and Georgiou, Panayiotis},
  date = {2017-08-20},
  pages = {3567--3571},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2017-1650},
  url = {https://www.isca-speech.org/archive/interspeech_2017/jati17_interspeech.html},
  urldate = {2021-11-11},
  abstract = {This paper presents a novel approach, we term Speaker2Vec, to derive a speaker-characteristics manifold learned in an unsupervised manner. The proposed representation can be employed in different applications such as diarization, speaker identification or, as in our evaluation test case, speaker segmentation. Speaker2Vec exploits large amounts of unlabeled training data and the assumption of short-term active-speaker stationarity to derive a speaker embedding using Deep Neural Networks (DNN). We assume that temporally-near speech segments belong to the same speaker, and as such a joint representation connecting these nearby segments can encode their common information. Thus, this bottleneck representation will be capturing mainly speaker-specific information. Such training can take place in a completely unsupervised manner. For testing, our trained model generates the embeddings for the test audio, and applies a simple distance metric to detect speaker-change points. The paper also proposes a strategy for unsupervised adaptation of the DNN models to the application domain. The proposed method outperforms the state-of-the-art speaker segmentation algorithms and MFCC based baseline methods on four evaluation datasets, while it allows for further improvements by employing this embedding into supervised training methods.},
  langid = {english}
}

@unpublished{jayashankar_detecting_2020,
  title = {Detecting Audio Attacks on {{ASR}} Systems with Dropout Uncertainty},
  author = {Jayashankar, Tejas and Roux, Jonathan Le and Moulin, Pierre},
  date = {2020},
  eprint = {2006.01906},
  eprinttype = {arxiv}
}

@article{jaynes_information_1957,
  title = {Information {{Theory}} and {{Statistical Mechanics}}},
  author = {Jaynes, Edwin T},
  date = {1957},
  journaltitle = {Physical review},
  volume = {106},
  number = {4},
  pages = {620},
  publisher = {{APS}}
}

@article{jaynes_prior_1968,
  title = {Prior {{Probabilities}}},
  author = {Jaynes, Edwin T},
  date = {1968},
  journaltitle = {IEEE Transactions on Systems Science and Cybernetics},
  volume = {4},
  number = {3},
  pages = {227--241},
  publisher = {{IEEE}}
}

@article{jeffreys_invariant_1946,
  title = {An {{Invariant Form}} for the {{Prior Probability}} in {{Estimation Problems}}},
  author = {Jeffreys, H.},
  date = {1946},
  journaltitle = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {186},
  number = {1007},
  eprint = {20998741},
  eprinttype = {pmid},
  pages = {453--461},
  issn = {1364-5021},
  doi = {10.1098/rspa.1946.0056},
  abstract = {It is shown that a certain differential form depending on the values of the parameters in a law of chance is invariant for all transformations of the parameters when the law is differentiable with regard to all parameters. For laws containing a location and a scale parameter a form with a somewhat restricted type of invariance is found even when the law is not everywhere differentiable with regard to the parameters. This form has the properties required to give a general rule for stating the prior probability in a large class of estimation problems.},
  isbn = {1364-5021}
}

@article{jegou_product_2011,
  title = {Product Quantization for Nearest Neighbor Search},
  author = {Jégou, Herve and Douze, Matthijs and Schmid, Cordelia},
  date = {2011},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {33},
  number = {1},
  pages = {117--128},
  doi = {10.1109/TPAMI.2010.57}
}

@book{jelinek_statistical_1997,
  title = {Statistical Methods for Speech Recognition},
  author = {Jelinek, Frederick},
  date = {1997},
  publisher = {{MIT Press}}
}

@article{ji_3d_2013,
  title = {{{3D}} Convolutional Neural Networks for Human Action Recognition},
  author = {Ji, Shuiwang and Xu, Wei and Yang, Ming and Yu, Kai},
  date = {2013},
  journaltitle = {IEEE transactions on pattern analysis and machine intelligence},
  volume = {35},
  number = {1},
  pages = {221--231},
  publisher = {{IEEE}}
}

@article{ji_blackout_2016,
  title = {{{BlackOut}}: {{Speeding}} up {{Recurrent Neural Network Language Models With Very Large Vocabularies}}},
  author = {Ji, Shihao and Vishwanathan, S. V. N. and Satish, Nadathur and Anderson, Michael J. and Dubey, Pradeep},
  date = {2016},
  journaltitle = {Under review of ICLR},
  eprint = {1511.06909},
  eprinttype = {arxiv},
  pages = {1--12},
  url = {http://arxiv.org/abs/1511.06909},
  urldate = {2018-05-02},
  abstract = {We propose BlackOut, an approximation algorithm to efficiently train massive recurrent neural network language models (RNNLMs) with million word vocabularies. BlackOut is motivated by using a discriminative loss, and we describe a new sampling strategy which significantly reduces computation while improving stability, sample efficiency, and rate of convergence. One way to understand BlackOut is to view it as an extension of the DropOut strategy to the output layer, wherein we use a discriminative training loss and a weighted sampling scheme. We also establish close connections between BlackOut, importance sampling, and noise contrastive estimation (NCE). Our experiments, on the recently released one billion word language modeling benchmark, demonstrate scalability and accuracy of BlackOut; we outperform the state-of-the art, and achieve the lowest perplexity scores on this dataset. Moreover, unlike other established methods which typically require GPUs or CPU clusters, we show that a carefully implemented version of BlackOut requires only 1-10 days on a single machine to train a RNNLM with a million word vocabulary and billions of parameters on one billion of words.}
}

@misc{jia_direct_2019,
  title = {Direct Speech-to-Speech Translation with a Sequence-to-Sequence Model},
  author = {Jia, Ye and Weiss, Ron J. and Biadsy, Fadi and Macherey, Wolfgang and Johnson, Melvin and Chen, Zhifeng and Wu, Yonghui},
  date = {2019-04-12},
  eprint = {1904.06037},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1904.06037},
  urldate = {2019-06-24},
  abstract = {We present an attention-based sequence-to-sequence neural network which can directly translate speech from one language into speech in another language, without relying on an intermediate text representation. The network is trained end-to-end, learning to map speech spectrograms into target spectrograms in another language, corresponding to the translated content (in a different canonical voice). We further demonstrate the ability to synthesize translated speech using the voice of the source speaker. We conduct experiments on two Spanish-to-English speech translation datasets, and find that the proposed model slightly underperforms a baseline cascade of a direct speech-to-text translation model and a text-to-speech synthesis model, demonstrating the feasibility of the approach on this very challenging task.},
  langid = {english}
}

@misc{jiang_fantastic_2019,
  title = {Fantastic {{Generalization Measures}} and {{Where}} to {{Find Them}}},
  author = {Jiang, Yiding and Neyshabur, Behnam and Mobahi, Hossein and Krishnan, Dilip and Bengio, Samy},
  date = {2019-12-04},
  eprint = {1912.02178},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1912.02178},
  urldate = {2021-05-07},
  abstract = {Generalization of deep networks has been of great interest in recent years, resulting in a number of theoretically and empirically motivated complexity measures. However, most papers proposing such measures study only a small set of models, leaving open the question of whether the conclusion drawn from those experiments would remain valid in other settings. We present the first large scale study of generalization in deep networks. We investigate more then 40 complexity measures taken from both theoretical bounds and empirical studies. We train over 10,000 convolutional networks by systematically varying commonly used hyperparameters. Hoping to uncover potentially causal relationships between each measure and generalization, we analyze carefully controlled experiments and show surprising failures of some measures as well as promising measures for further research.},
  langid = {english}
}

@unpublished{jiang_further_2020,
  title = {A {{Further Study}} of {{Unsupervised Pre-training}} for {{Transformer Based Speech Recognition}}},
  author = {Jiang, Dongwei and Li, Wubo and Zhang, Ruixiong and Cao, Miao and Luo, Ne and Han, Yang and Zou, Wei and Li, Xiangang},
  date = {2020-06-22},
  eprint = {2005.09862},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2005.09862},
  urldate = {2021-10-12},
  abstract = {Building a good speech recognition system usually requires large amounts of transcribed data, which is expensive to collect. To tackle this problem, many unsupervised pre-training methods have been proposed. Among these methods, Masked Predictive Coding achieved significant improvements on various speech recognition datasets with BERT-like Masked Reconstruction loss and Transformer backbone. However, many aspects of MPC have not been fully investigated. In this paper, we conduct a further study on MPC and focus on three important aspects: the effect of pre-training data speaking style, its extension on streaming model, and how to better transfer learned knowledge from pre-training stage to downstream tasks. Experiments reveled that pre-training data with matching speaking style is more useful on downstream recognition tasks. A unified training objective with APC and MPC provided 8.46\% relative error reduction on streaming model trained on HKUST. Also, the combination of target data adaption and layer-wise discriminative training helped the knowledge transfer of MPC, which achieved 3.99\% relative error reduction on AISHELL over a strong baseline.}
}

@inproceedings{jiang_further_2021,
  title = {A Further Study of Unsupervised Pretraining for {{Transformer}} Based Speech Recognition},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Jiang, Dongwei and Li, Wubo and Zhang, Ruixiong and Cao, Miao and Luo, Ne and Han, Yang and Zou, Wei and Han, Kun and Li, Xiangang},
  date = {2021}
}

@unpublished{jiang_improving_2019,
  title = {Improving {{Transformer-based Speech Recognition Using Unsupervised Pre-training}}},
  author = {Jiang, Dongwei and Lei, Xiaoning and Li, Wubo and Luo, Ne and Hu, Yuxuan and Zou, Wei and Li, Xiangang},
  date = {2019-10-31},
  eprint = {1910.09932},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/1910.09932},
  urldate = {2021-10-12},
  abstract = {Speech recognition technologies are gaining enormous popularity in various industrial applications. However, building a good speech recognition system usually requires large amounts of transcribed data, which is expensive to collect. To tackle this problem, an unsupervised pre-training method called Masked Predictive Coding is proposed, which can be applied for unsupervised pre-training with Transformer based model. Experiments on HKUST show that using the same training data, we can achieve CER 23.3\%, exceeding the best end-to-end model by over 0.2\% absolute CER. With more pre-training data, we can further reduce the CER to 21.0\%, or a 11.8\% relative CER reduction over baseline.}
}

@inproceedings{jiang_speech_2021,
  title = {Speech {{SIMCLR}}: {{Combining}} Contrastive and Reconstruction Objective for Self-Supervised Speech Representation Learning},
  booktitle = {Annual {{Conference}} of the {{International Speech Communication Association}}},
  author = {Jiang, Dongwei and Li, Wubo and Cao, Miao and Zou, Wei and Li, Xiangang},
  date = {2021}
}

@inproceedings{jiao_online_2016,
  title = {Online Speaking Rate Estimation Using Recurrent Neural Networks},
  booktitle = {2016 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Jiao, Yishan and Tu, Ming and Berisha, Visar and Liss, Julie},
  date = {2016-03},
  pages = {5245--5249},
  publisher = {{IEEE}},
  location = {{Shanghai}},
  doi = {10.1109/ICASSP.2016.7472678},
  url = {http://ieeexplore.ieee.org/document/7472678/},
  urldate = {2018-11-23},
  abstract = {A reliable online speaking rate estimation tool is useful in many domains, including speech recognition, speech therapy intervention, speaker identification, etc. This paper proposes an online speaking rate estimation model based on recurrent neural networks (RNNs). Speaking rate is a long-term feature of speech, which depends on how many syllables were spoken over an extended time window (seconds). We posit that since RNNs can capture long-term dependencies through the memory of previous hidden states, they are a good match for the speaking rate estimation task. Here we train a long shortterm memory (LSTM) RNN on a set of speech features that are known to correlate with speech rhythm. An evaluation on spontaneous speech shows that the method yields a higher correlation between the estimated rate and the ground-truth rate when compared to the state-of-the-art alternatives. The evaluation on longitudinal pathological speech shows that the proposed method can capture long-term and short-term changes in speaking rate.},
  eventtitle = {2016 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  isbn = {978-1-4799-9988-0},
  langid = {english}
}

@article{jiDoesMagicBERT2021,
  title = {Does the Magic of {{BERT}} Apply to Medical Code Assignment? {{A}} Quantitative Study},
  shorttitle = {Does the Magic of {{BERT}} Apply to Medical Code Assignment?},
  author = {Ji, Shaoxiong and Hölttä, Matti and Marttinen, Pekka},
  date = {2021-12},
  journaltitle = {Computers in Biology and Medicine},
  volume = {139},
  pages = {104998},
  issn = {0010-4825},
  doi = {10.1016/j.compbiomed.2021.104998},
  abstract = {Unsupervised pretraining is an integral part of many natural language processing systems, and transfer learning with language models has achieved remarkable results in downstream tasks. In the clinical application of medical code assignment, diagnosis and procedure codes are inferred from lengthy clinical notes such as hospital discharge summaries. However, it is not clear if pretrained models are useful for medical code prediction without further architecture engineering. This paper conducts a comprehensive quantitative analysis of various contextualized language models' performances, pretrained in different domains, for medical code assignment from clinical notes. We propose a hierarchical fine-tuning architecture to capture interactions between distant words and adopt label-wise attention to exploit label information. Contrary to current trends, we demonstrate that a carefully trained classical CNN outperforms attention-based models on a MIMIC-III subset with frequent codes. Our empirical findings suggest directions for building robust medical code assignment models.},
  langid = {english}
}

@article{jing_selfsupervised_2021,
  title = {Self-{{Supervised Visual Feature Learning}} with {{Deep Neural Networks}}: {{A Survey}}},
  author = {Jing, L. and Tian, Y.},
  date = {2021-11},
  journaltitle = {IEEE Transactions on Pattern Analysis \& Machine Intelligence},
  volume = {43},
  number = {11},
  pages = {4037--4058},
  publisher = {{IEEE Computer Society}},
  location = {{Los Alamitos, CA, USA}},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2020.2992393},
  keywords = {annotations,feature extraction,learning systems,task analysis,training,videos,visualization}
}

@misc{jiUnifiedReviewDeep2022,
  title = {A {{Unified Review}} of {{Deep Learning}} for {{Automated Medical Coding}}},
  author = {Ji, Shaoxiong and Sun, Wei and Dong, Hang and Wu, Honghan and Marttinen, Pekka},
  date = {2022-01},
  eprint = {2201.02797},
  eprinttype = {arxiv},
  eprintclass = {cs},
  abstract = {Automated medical coding, an essential task for healthcare operation and delivery, makes unstructured data manageable by predicting medical codes from clinical documents. Recent advances in deep learning models in natural language processing have been widely applied to this task. However, it lacks a unified view of the design of neural network architectures for medical coding. This review proposes a unified framework to provide a general understanding of the building blocks of medical coding models and summarizes recent advanced models under the proposed framework. Our unified framework decomposes medical coding into four main components, i.e., encoder modules for text feature extraction, mechanisms for building deep encoder architectures, decoder modules for transforming hidden representations into medical codes, and the usage of auxiliary information. Finally, we discuss key research challenges and future directions.},
  issue = {arXiv:2201.02797},
  organization = {{arXiv}}
}

@article{johnson_accelerating_2013,
  title = {Accelerating {{Stochastic Gradient Descent}} Using {{Predictive Variance Reduction}}},
  author = {Johnson, Rie and Zhang, Tong},
  date = {2013},
  journaltitle = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
  volume = {1},
  number = {3},
  eprint = {880145},
  eprinttype = {pmid},
  pages = {315--323},
  issn = {10495258},
  abstract = {Stochastic gradient descent is popular for large scale optimization but has slow convergence asymptotically due to the inherent variance. To remedy this prob-lem, we introduce an explicit variance reduction method for stochastic gradient descent which we call stochastic variance reduced gradient (SVRG). For smooth and strongly convex functions, we prove that this method enjoys the same fast con-vergence rate as those of stochastic dual coordinate ascent (SDCA) and Stochastic Average Gradient (SAG). However, our analysis is significantly simpler and more intuitive. Moreover, unlike SDCA or SAG, our method does not require the stor-age of gradients, and thus is more easily applicable to complex problems such as some structured prediction problems and neural network learning.}
}

@inproceedings{johnson_composing_2016,
  title = {Composing Graphical Models with Neural Networks for Structured Representations and Fast Inference},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Johnson, Matthew J and Duvenaud, David K and Wiltschko, Alex and Adams, Ryan P and Datta, Sandeep R},
  editor = {Lee, D. and Sugiyama, M. and Luxburg, U. and Guyon, I. and Garnett, R.},
  date = {2016},
  volume = {29},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2016/file/7d6044e95a16761171b130dcb476a43e-Paper.pdf}
}

@article{johnsonMIMICIIIFreelyAccessible2016,
  title = {{{MIMIC-III}}, a Freely Accessible Critical Care Database},
  author = {Johnson, Alistair E. W. and Pollard, Tom J. and Shen, Lu and Lehman, Li-wei H. and Feng, Mengling and Ghassemi, Mohammad and Moody, Benjamin and Szolovits, Peter and Anthony Celi, Leo and Mark, Roger G.},
  date = {2016-05},
  journaltitle = {Scientific Data},
  volume = {3},
  number = {1},
  pages = {160035},
  publisher = {{Nature Publishing Group}},
  issn = {2052-4463},
  doi = {10.1038/sdata.2016.35},
  abstract = {MIMIC-III (`Medical Information Mart for Intensive Care') is a large, single-center database comprising information relating to patients admitted to critical care units at a large tertiary care hospital. Data includes vital signs, medications, laboratory measurements, observations and notes charted by care providers, fluid balance, procedure codes, diagnostic codes, imaging reports, hospital length of stay, survival data, and more. The database supports applications including academic and industrial research, quality improvement initiatives, and higher education coursework.},
  copyright = {2016 The Author(s)},
  langid = {english}
}

@article{johnsonMIMICIVFreelyAccessible2023,
  title = {{{MIMIC-IV}}, a Freely Accessible Electronic Health Record Dataset},
  author = {Johnson, Alistair E. W. and Bulgarelli, Lucas and Shen, Lu and Gayles, Alvin and Shammout, Ayad and Horng, Steven and Pollard, Tom J. and Moody, Benjamin and Gow, Brian and Lehman, Li-wei H. and Celi, Leo A. and Mark, Roger G.},
  date = {2023-01},
  journaltitle = {Scientific Data},
  volume = {10},
  number = {1},
  pages = {1},
  publisher = {{Nature Publishing Group}},
  issn = {2052-4463},
  doi = {10.1038/s41597-022-01899-x},
  abstract = {Digital data collection during routine clinical practice is now ubiquitous within hospitals. The data contains valuable information on the care of patients and their response to treatments, offering exciting opportunities for research. Typically, data are stored within archival systems that are not intended to support research. These systems are often inaccessible to researchers and structured for optimal storage, rather than interpretability and analysis. Here we present MIMIC-IV, a publicly available database sourced from the electronic health record of the Beth Israel Deaconess Medical Center. Information available includes patient measurements, orders, diagnoses, procedures, treatments, and deidentified free-text clinical notes. MIMIC-IV is intended to support a wide array of research studies and educational material, helping to reduce barriers to conducting clinical research.},
  copyright = {2023 The Author(s)},
  langid = {english}
}

@inproceedings{jones_is_2020,
  title = {Is the Discrete {{VAE}}'s Power Stuck in Its Prior?},
  booktitle = {”{{I}} Can't Believe It's Not Better!” {{NeurIPS}} 2020 Workshop},
  author = {Jones, Haydn Thomas and Moore, Juston},
  date = {2020},
  url = {https://openreview.net/forum?id=Ws7NeFj3s9i}
}

@article{jones_taxonomy_,
  title = {A {{Taxonomy}} of {{Global Optimization Methods Based}} on {{Response Surfaces}}},
  author = {Jones, Donald R},
  pages = {39},
  abstract = {This paper presents a taxonomy of existing approaches for using response surfaces for global optimization. Each method is illustrated with a simple numerical example that brings out its advantages and disadvantages. The central theme is that methods that seem quite reasonable often have non-obvious failure modes. Understanding these failure modes is essential for the development of practical algorithms that fulfill the intuitive promise of the response surface approach.},
  langid = {english}
}

@article{jordan_hierarchical_1994,
  title = {Hierarchical Mixtures of Experts and the {{EM}} Algorithm},
  author = {Jordan, Michael I and Jacobs, Robert A},
  date = {1994},
  journaltitle = {Neural Computation},
  volume = {6},
  number = {2},
  pages = {181--214}
}

@article{jordan_introduction_1999,
  title = {An Introduction to Variational Methods for Graphical Models},
  author = {Jordan, Michael I. and Ghahramani, Zoubin and Jaakkola, Tommi S. and Saul, Lawrence K.},
  date = {1999},
  journaltitle = {Machine Learning},
  volume = {37},
  number = {2},
  pages = {183--233}
}

@article{jordan_machine_2015,
  title = {Machine Learning: {{Trends}}, Perspectives, and Prospects},
  author = {Jordan, Michael I and Mitchell, Tom M},
  date = {2015},
  journaltitle = {Science (New York, N.Y.)},
  shortjournal = {Science},
  volume = {349},
  number = {6245},
  pages = {255--260},
  publisher = {{American Association for the Advancement of Science}}
}

@article{jorgensen_isometric_nodate,
  title = {Isometric {{Gaussian Process Latent Variable Model}}  for {{Dissimilarity Data}}},
  author = {Jørgensen, Martin and Hauberg, Søren},
  pages = {10},
  abstract = {We present a probabilistic model where the latent variable respects both the distances and the topology of the modeled data. The model leverages the Riemannian geometry of the generated manifold to endow the latent space with a well-defined stochastic distance measure, which is modeled locally as Nakagami distributions. These stochastic distances are sought to be as similar as possible to observed distances along a neighborhood graph through a censoring process. The model is inferred by variational inference based on observations of pairwise distances. We demonstrate how the new model can encode invariances in the learned manifolds.},
  langid = {english}
}

@misc{joy_learning_2021,
  title = {Learning {{Multimodal VAEs}} through {{Mutual Supervision}}},
  author = {Joy, Tom and Shi, Yuge and Torr, Philip H. S. and Rainforth, Tom and Schmon, Sebastian M. and Siddharth, N.},
  date = {2021-06-23},
  eprint = {2106.12570},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2106.12570},
  urldate = {2021-07-01},
  abstract = {Multimodal variational autoencoders (VAEs) seek to model the joint distribution over heterogeneous data (e.g. vision, language), whilst also capturing a shared representation across such modalities. Prior work has typically combined information from the modalities by reconciling idiosyncratic representations directly in the recognition model through explicit products, mixtures, or other such factorisations. Here we introduce a novel alternative, the Mutually supErvised Multimodal VAE (MEME), that avoids such explicit combinations by repurposing semi-supervised VAEs to combine information between modalities implicitly through mutual supervision. This formulation naturally allows learning from partially-observed data where some modalities can be entirely missing—something that most existing approaches either cannot handle, or do so to a limited extent. We demonstrate that MEME outperforms baselines on standard metrics across both partial and complete observation schemes on the MNIST-SVHN (image–image) and CUB (image–text) datasets. We also contrast the quality of the representations learnt by mutual supervision against standard approaches and observe interesting trends in its ability to capture relatedness between data.},
  langid = {english}
}

@article{jozefowicz_empirical_nodate,
  title = {An {{Empirical Exploration}} of {{Recurrent Network Architectures}}},
  author = {Jozefowicz, Rafal and Zaremba, Wojciech and Sutskever, Ilya},
  abstract = {The Recurrent Neural Network (RNN) is an ex-tremely powerful sequence model that is often difficult to train. The Long Short-Term Memory (LSTM) is a specific RNN architecture whose design makes it much easier to train. While wildly successful in practice, the LSTM's archi-tecture appears to be ad-hoc so it is not clear if it is optimal, and the significance of its individual components is unclear. In this work, we aim to determine whether the LSTM architecture is optimal or whether much better architectures exist. We conducted a thor-ough architecture search where we evaluated over ten thousand different RNN architectures, and identified an architecture that outperforms both the LSTM and the recently-introduced Gated Recurrent Unit (GRU) on some but not all tasks. We found that adding a bias of 1 to the LSTM's forget gate closes the gap between the LSTM and the GRU.}
}

@misc{jozefowicz_exploring_2016,
  title = {Exploring the Limits of Language Modeling},
  author = {Jozefowicz, Rafal and Vinyals, Oriol and Schuster, Mike and Shazeer, Noam and Wu, Yonghui},
  date = {2016},
  eprint = {1602.02410},
  eprinttype = {arxiv},
  url = {https://arxiv.org/abs/1602.02410}
}

@article{jumper_highly_2021,
  title = {Highly Accurate Protein Structure Prediction with {{AlphaFold}}},
  author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and Žídek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
  date = {2021-08-26},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {596},
  number = {7873},
  pages = {583--589},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-021-03819-2},
  url = {https://www.nature.com/articles/s41586-021-03819-2},
  urldate = {2021-09-09},
  abstract = {Abstract                            Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort               1–4               , the structures of around 100,000 unique proteins have been determined               5               , but this represents a small fraction of the billions of known protein sequences               6,7               . Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence—the structure prediction component of the ‘protein folding problem’               8               —has been an important open research problem for more than 50~years               9               . Despite recent progress               10–14               , existing methods fall far~short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)               15               , demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
  langid = {english}
}

@article{jumper_supplementary_nodate,
  title = {Supplementary {{Information}} for: {{Highly}} Accurate Protein Structure Prediction with {{AlphaFold}}},
  author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and Žídek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, A A and Ballard, Andrew J and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Senior, Andrew W and Kavukcuoglu, Koray and Kohli, Pushmeet},
  pages = {62},
  langid = {english}
}

@inproceedings{jun_distribution_2020,
  title = {Distribution {{Augmentation}} for {{Generative Modeling}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Jun, Heewoo and Child, Rewon and Chen, Mark and Schulman, John and Ramesh, Aditya and Radford, Alec and Sutskever, Ilya},
  date = {2020},
  series = {Proceedings of {{Machine Learning Research}}},
  pages = {5006--5019},
  publisher = {{PMLR}},
  location = {{Vienna, Austria}},
  url = {http://proceedings.mlr.press/v119/jun20a.html},
  abstract = {We present distribution augmentation (DistAug), a simple and powerful method of regularizing generative models. Our approach applies augmentation functions to data and, importantly, conditions the generative model on the specific function used. Unlike typical data augmentation, DistAug allows usage of functions which modify the target density, enabling aggressive augmentations more commonly seen in supervised and self-supervised learning. We demonstrate this is a more effective regularizer than standard methods, and use it to train a 152M parameter autoregressive model on CIFAR-10 to 2.56 bits per dim (relative to the state-of-the-art 2.80). Samples from this model attain FID 12.75 and IS 8.40, outperforming the majority of GANs. We further demonstrate the technique is broadly applicable across model architectures and problem domains.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english}
}

@article{jung_current_2014,
  title = {Current Status and Future Advances for Wind Speed and Power Forecasting},
  author = {Jung, Jaesung and Broadwater, Robert P.},
  date = {2014},
  journaltitle = {Renewable and Sustainable Energy Reviews},
  volume = {31},
  pages = {762--777},
  issn = {13640321},
  doi = {10.1016/j.rser.2013.12.054},
  abstract = {This paper presents an overview of existing research on wind speed and power forecasting. It first discusses state-of-the-art wind speed and power forecasting approaches. Then, forecasting accuracy is presented based on variable factors. Finally, potential techniques to improve the accuracy of forecasting models are reviewed. A full survey on all existing models is not presented, but attempts to highlight the most promising body of knowledge concerning wind speed and power forecasting. © 2014 Elsevier Ltd.},
  isbn = {1364-0321}
}

@book{jurafsky_speech_2019,
  title = {Speech and {{Language Processing}} - {{An Introduction}} to {{Natural Language Processing}}, {{Computational Linguistics}}, and {{Speech Recognition}}},
  author = {Jurafsky, Daniel and Martin, James H.},
  date = {2019},
  edition = {3},
  location = {{Boulder, Colorado}},
  keywords = {★}
}

@article{kabir_neural_2018,
  title = {Neural {{Network-Based Uncertainty Quantification}}: {{A Survey}} of {{Methodologies}} and {{Applications}}},
  author = {Kabir, H. M. Dipu and Khosravi, Abbas and Hosen, Mohammad Anwar and Nahavandi, Saeid},
  date = {2018},
  journaltitle = {IEEE Access: Practical Innovations, Open Solutions},
  shortjournal = {IEEE Access},
  volume = {6},
  pages = {36218--36234},
  doi = {10.1109/ACCESS.2018.2836917}
}

@unpublished{kaddour_questions_2022,
  title = {Questions for {{Flat-Minima Optimization}} of {{Modern Neural Networks}}},
  author = {Kaddour, Jean and Liu, Linqing and Silva, Ricardo and Kusner, Matt J.},
  date = {2022-02-02},
  eprint = {2202.00661},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2202.00661},
  urldate = {2022-03-17},
  abstract = {For training neural networks, flat-minima optimizers that seek to find parameters in neighborhoods having uniformly low loss (flat minima) have been shown to improve upon stochastic and adaptive gradient-based methods. Two methods for finding flat minima stand out: 1. Averaging methods (i.e., Stochastic Weight Averaging, SWA), and 2. Minimax methods (i.e., Sharpness Aware Minimization, SAM). However, despite similar motivations, there has been limited investigation into their properties and no comprehensive comparison between them. In this work, we investigate the loss surfaces from a systematic benchmarking of these approaches across computer vision, natural language processing, and graph learning tasks. The results lead to a simple hypothesis: since both approaches find different flat solutions, combining them should improve generalization even further. We verify this improves over either flat-minima approach in 39 out of 42 cases. When it does not, we investigate potential reasons. We hope our results across image, graph, and text data will help researchers to improve deep learning optimizers, and practitioners to pinpoint the optimizer for the problem at hand.}
}

@article{kaelbling_artificial_1998,
  title = {Artificial {{Intelligence Planning}} and Acting in Partially Observable Stochastic Domains},
  author = {Kaelbling, Leslie Pack and Littman, Michael L. and Cassandra ', Anthony R.},
  date = {1998},
  journaltitle = {Artificial IntelligenceArti\$cial Intelligence},
  volume = {101},
  number = {101},
  pages = {99--134},
  url = {http://production.datastore.cvt.dk/filestore?oid=539d2d94e2a1a1d725047a22&targetid=539d2d94e2a1a1d725047a26},
  urldate = {2017-09-27},
  abstract = {In this paper, we bring techniques from operations research to bear on the problem of choosing optimal actions in partially observable stochastic domains. We begin by introducing the theory of Markov decision processes (MDPS) and partially observable MDPS (POMDPS). We then outline a novel algorithm for solving POMDPS off line and show how, in some cases, a finite-memory controller can be extracted from the solution to a POMDP. We conclude with a discussion of how our approach relates to previous work, the complexity of finding exact solutions to POMDPS, and of some possibilities for finding approximate solutions. 0 1998 Consider the problem of a robot navigating in a large office building. The robot can move from hallway intersection to intersection and can make local observations of its world. Its actions are not completely reliable, however. Sometimes, when it intends to move, it stays where it is or goes too far; sometimes, when it intends to turn, it overshoots. It has similar problems with observation. Sometimes a corridor looks like a comer; sometimes a T-junction looks like an L-junction. How can such an error-plagued robot navigate, even given a map of the corridors? In general, the robot will have to remember something about its history of actions and observations and use this information, together with its knowledge of the underlying dynamics of the world (the map and other information), to maintain an estimate of its location. Many engineering applications follow this approach, using methods like the Kalman filter [26] to maintain a running estimate of the robot's spatial uncertainty, expressed as an ellipsoid or normal distribution in Cartesian space. This approach will not do for our robot, though. Its uncertainty may be discrete: it might be almost certain that it is in the northeast comer of either the fourth or the seventh floors, though it admits a chance that it is on the fifth floor, as well. Then, given an uncertain estimate of its location, the robot has to decide what actions to take. In some cases, it might be sufficient to ignore its uncertainty and take actions that would be appropriate for the most likely location. In other cases, it might be better for the robot to take actions for the purpose of gathering information, such as searching for a landmark or reading signs on the wall. In general, it will take actions that fulfill both purposes simultaneously.}
}

@article{kaelbling_reinforcement_1996,
  title = {Reinforcement Learning: {{A Survey}}},
  author = {Kaelbling, Leslie Pack and Littman, Michael L. and Moore, Andrew W.},
  date = {1996},
  journaltitle = {Journal of Artificial Intelligence Research},
  volume = {4},
  pages = {237--285},
  doi = {10.1613/jair.301},
  abstract = {This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word ``reinforcement.'' The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.}
}

@inproceedings{kahn_libri-light_2020,
  title = {Libri-Light: {{A}} Benchmark for {{ASR}} with Limited or No Supervision},
  booktitle = {Proceedings of the {{International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Kahn, J. and Riviere, M. and Zheng, W. and Kharitonov, E. and Xu, Q. and Mazare, P.E. and Karadayi, J. and Liptchinsky, V. and Collobert, R. and Fuegen, C. and {al.}, et},
  date = {2020-05},
  location = {{Virtual}},
  doi = {10.1109/icassp40776.2020.9052942},
  url = {http://dx.doi.org/10.1109/ICASSP40776.2020.9052942},
  eventtitle = {International {{Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}}
}

@misc{kalatzis_variational_2020,
  title = {Variational {{Autoencoders}} with {{Riemannian Brownian Motion Priors}}},
  author = {Kalatzis, Dimitris and Eklund, David and Arvanitidis, Georgios and Hauberg, Søren},
  date = {2020-02-12},
  eprint = {2002.05227},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2002.05227},
  urldate = {2020-02-18},
  abstract = {Variational Autoencoders (VAEs) represent the given data in a low-dimensional latent space, which is generally assumed to be Euclidean. This assumption naturally leads to the common choice of a standard Gaussian prior over continuous latent variables. Recent work has, however, shown that this prior has a detrimental effect on model capacity, leading to subpar performance. We propose that the Euclidean assumption lies at the heart of this failure mode. To counter this, we assume a Riemannian structure over the latent space, which constitutes a more principled geometric view of the latent codes, and replace the standard Gaussian prior with a Riemannian Brownian motion prior. We propose an efficient inference scheme that does not rely on the unknown normalizing factor of this prior. Finally, we demonstrate that this prior significantly increases model capacity using only one additional scalar parameter.},
  langid = {english}
}

@inproceedings{kalchbrenner_convolutional_2014,
  title = {A {{Convolutional Neural Network}} for {{Modelling Sentences}}},
  booktitle = {Proceedings of the 52nd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Kalchbrenner, Nal and Grefenstette, Edward and Blunsom, Phil},
  date = {2014},
  eprint = {15003161},
  eprinttype = {pmid},
  pages = {655--665},
  issn = {03064573},
  doi = {10.3115/v1/P14-1062},
  url = {https://arxiv.org/abs/1404.2188},
  urldate = {2018-05-02},
  abstract = {The ability to accurately represent sen- tences is central to language understand- ing. We describe a convolutional architec- ture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences. The network uses Dynamic k-Max Pool- ing, a global pooling operation over lin- ear sequences. The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations. The network does not rely on a parse tree and is easily ap- plicable to any language. We test the DCNN in four experiments: small scale binary and multi-class sentiment predic- tion, six-way question classification and Twitter sentiment prediction by distant su- pervision. The network achieves excellent performance in the first three tasks and a greater than 25\% error reduction in the last task with respect to the strongest baseline.},
  isbn = {978-1-937284-72-5}
}

@inproceedings{kamper_deep_2016,
  title = {Deep Convolutional Acoustic Word Embeddings Using Word-Pair Side Information},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Kamper, Herman and Wang, Weiran and Livescu, Karen},
  date = {2016-03},
  pages = {4950--4954},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2016.7472619},
  abstract = {Recent studies have been revisiting whole words as the basic modelling unit in speech recognition and query applications, instead of phonetic units. Such whole-word segmental systems rely on a function that maps a variable-length speech segment to a vector in a fixed-dimensional space; the resulting acoustic word embeddings need to allow for accurate discrimination between different word types, directly in the embedding space. We compare several old and new approaches in a word discrimination task. Our best approach uses side information in the form of known word pairs to train a Siamese convolutional neural network (CNN): a pair of tied networks that take two speech segments as input and produce their embeddings, trained with a hinge loss that separates same-word pairs and different-word pairs by some margin. A word classifier CNN performs similarly, but requires much stronger supervision. Both types of CNNs yield large improvements over the best previously published results on the word discrimination task.},
  eventtitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})}
}

@inproceedings{kamper_embedded_2017,
  title = {An Embedded Segmental {{K-means}} Model for Unsupervised Segmentation and Clustering of Speech},
  booktitle = {2017 {{IEEE Automatic Speech Recognition}} and {{Understanding Workshop}} ({{ASRU}})},
  author = {Kamper, Herman and Livescu, Karen and Goldwater, Sharon},
  date = {2017-12},
  pages = {719--726},
  doi = {10.1109/ASRU.2017.8269008},
  abstract = {Unsupervised segmentation and clustering of unlabelled speech are core problems in zero-resource speech processing. Most approaches lie at methodological extremes: some use probabilistic Bayesian models with convergence guarantees, while others opt for more efficient heuristic techniques. Despite competitive performance in previous work, the full Bayesian approach is difficult to scale to large speech corpora. We introduce an approximation to a recent Bayesian model that still has a clear objective function but improves efficiency by using hard clustering and segmentation rather than full Bayesian inference. Like its Bayesian counterpart, this embedded segmental K-means model (ES-KMeans) represents arbitrary-length word segments as fixed-dimensional acoustic word embeddings. We first compare ES-KMeans to previous approaches on common English and Xitsonga data sets (5 and 2.5 hours of speech): ES-KMeans outperforms a leading heuristic method in word segmentation, giving similar scores to the Bayesian model while being 5 times faster with fewer hyperparameters. However, its clusters are less pure than those of the other models. We then show that ES-KMeans scales to larger corpora by applying it to the 5 languages of the Zero Resource Speech Challenge 2017 (up to 45 hours), where it performs competitively compared to the challenge baseline.},
  eventtitle = {2017 {{IEEE Automatic Speech Recognition}} and {{Understanding Workshop}} ({{ASRU}})}
}

@article{kamper_segmental_2017,
  title = {A Segmental Framework for Fully-Unsupervised Large-Vocabulary Speech Recognition},
  author = {Kamper, Herman and Jansen, Aren and Goldwater, Sharon},
  date = {2017-11-01},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {46},
  pages = {154--174},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2017.04.008},
  url = {https://www.sciencedirect.com/science/article/pii/S0885230816301905},
  urldate = {2021-10-21},
  abstract = {Zero-resource speech technology is a growing research area that aims to develop methods for speech processing in the absence of transcriptions, lexicons, or language modelling text. Early term discovery systems focused on identifying isolated recurring patterns in a corpus, while more recent full-coverage systems attempt to completely segment and cluster the audio into word-like units—effectively performing unsupervised speech recognition. This article presents the first attempt we are aware of to apply such a system to large-vocabulary multi-speaker data. Our system uses a Bayesian modelling framework with segmental word representations: each word segment is represented as a fixed-dimensional acoustic embedding obtained by mapping the sequence of feature frames to a single embedding vector. We compare our system on English and Xitsonga datasets to state-of-the-art baselines, using a variety of measures including word error rate (obtained by mapping the unsupervised output to ground truth transcriptions). Very high word error rates are reported—in the order of 70–80\% for speaker-dependent and 80–95\% for speaker-independent systems—highlighting the difficulty of this task. Nevertheless, in terms of cluster quality and word segmentation metrics, we show that by imposing a consistent top-down segmentation while also using bottom-up knowledge from detected syllable boundaries, both single-speaker and multi-speaker versions of our system outperform a purely bottom-up single-speaker syllable-based approach. We also show that the discovered clusters can be made less speaker- and gender-specific by using an unsupervised autoencoder-like feature extractor to learn better frame-level features (prior to embedding). Our system’s discovered clusters are still less pure than those of unsupervised term discovery systems, but provide far greater coverage.},
  langid = {english}
}

@inproceedings{kamper_truly_2019,
  title = {Truly Unsupervised Acoustic Word Embeddings Using Weak Top-down Constraints in Encoder-Decoder Models},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Kamper, Herman},
  date = {2019},
  pages = {6535--3539}
}

@inproceedings{kamper_unsupervised_2015,
  title = {Unsupervised Neural Network Based Feature Extraction Using Weak Top-down Constraints},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Kamper, Herman and Elsner, Micha and Jansen, Aren and Goldwater, Sharon},
  date = {2015-04},
  pages = {5818--5822},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2015.7179087},
  abstract = {Deep neural networks (DNNs) have become a standard component in supervised ASR, used in both data-driven feature extraction and acoustic modelling. Supervision is typically obtained from a forced alignment that provides phone class targets, requiring transcriptions and pronunciations. We propose a novel unsupervised DNN-based feature extractor that can be trained without these resources in zero-resource settings. Using unsupervised term discovery, we find pairs of isolated word examples of the same unknown type; these provide weak top-down supervision. For each pair, dynamic programming is used to align the feature frames of the two words. Matching frames are presented as input-output pairs to a deep autoencoder (AE) neural network. Using this AE as feature extractor in a word discrimination task, we achieve 64\% relative improvement over a previous state-of-the-art system, 57\% improvement relative to a bottom-up trained deep AE, and come to within 23\% of a supervised system.},
  eventtitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})}
}

@article{kamper_unsupervised_2021,
  title = {Towards Unsupervised Phone and Word Segmentation Using Self-Supervised Vector-Quantized Neural Networks},
  author = {Kamper, Herman and family=Niekerk, given=Benjamin, prefix=van, useprefix=true},
  date = {2021},
  journaltitle = {Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech)}
}

@inproceedings{kamper_visually_2018,
  title = {Visually Grounded Cross-Lingual Keyword Spotting in Speech},
  booktitle = {Proceedings of the 6th {{Workshop}} on {{Spoken Language Technologies}} for {{Under-Resourced Languages}} ({{SLTU}})},
  author = {Kamper, Herman and Roth, Michael},
  date = {2018}
}

@online{kanda_simultaneous_2019,
  title = {Simultaneous {{Speech Recognition}} and {{Speaker Diarization}} for {{Monaural Dialogue Recordings}} with {{Target-Speaker Acoustic Models}}},
  author = {Kanda, Naoyuki and Horiguchi, Shota and Fujita, Yusuke and Xue, Yawen and Nagamatsu, Kenji and Watanabe, Shinji},
  date = {2019-09-17},
  eprint = {1909.08103},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1909.08103},
  urldate = {2023-04-14},
  abstract = {This paper investigates the use of target-speaker automatic speech recognition (TS-ASR) for simultaneous speech recognition and speaker diarization of single-channel dialogue recordings. TS-ASR is a technique to automatically extract and recognize only the speech of a target speaker given a short sample utterance of that speaker. One obvious drawback of TS-ASR is that it cannot be used when the speakers in the recordings are unknown because it requires a sample of the target speakers in advance of decoding. To remove this limitation, we propose an iterative method, in which (i) the estimation of speaker embeddings and (ii) TS-ASR based on the estimated speaker embeddings are alternately executed. We evaluated the proposed method by using very challenging dialogue recordings in which the speaker overlap ratio was over 20\%. We confirmed that the proposed method significantly reduced both the word error rate (WER) and diarization error rate (DER). Our proposed method combined with i-vector speaker embeddings ultimately achieved a WER that differed by only 2.1 \% from that of TS-ASR given oracle speaker embeddings. Furthermore, our method can solve speaker diarization simultaneously as a by-product and achieved better DER than that of the conventional clustering-based speaker diarization method based on i-vector.},
  pubstate = {preprint}
}

@online{kang_deep_2023,
  title = {Deep {{Neural Networks Tend To Extrapolate Predictably}}},
  author = {Kang, Katie and Setlur, Amrith and Tomlin, Claire and Levine, Sergey},
  date = {2023-10-01},
  eprint = {2310.00873},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2310.00873},
  urldate = {2023-10-18},
  abstract = {Conventional wisdom suggests that neural network predictions tend to be unpredictable and overconfident when faced with out-of-distribution (OOD) inputs. Our work reassesses this assumption for neural networks with high-dimensional inputs. Rather than extrapolating in arbitrary ways, we observe that neural network predictions often tend towards a constant value as input data becomes increasingly OOD. Moreover, we find that this value often closely approximates the optimal constant solution (OCS), i.e., the prediction that minimizes the average loss over the training data without observing the input. We present results showing this phenomenon across 8 datasets with different distributional shifts (including CIFAR10-C and ImageNet-R, S), different loss functions (cross entropy, MSE, and Gaussian NLL), and different architectures (CNNs and transformers). Furthermore, we present an explanation for this behavior, which we first validate empirically and then study theoretically in a simplified setting involving deep homogeneous networks with ReLU activations. Finally, we show how one can leverage our insights in practice to enable risk-sensitive decision-making in the presence of OOD inputs.},
  pubstate = {preprint}
}

@misc{kannan_largescale_2019,
  title = {Large-{{Scale Multilingual Speech Recognition}} with a {{Streaming End-to-End Model}}},
  author = {Kannan, Anjuli and Datta, Arindrima and Sainath, Tara N. and Weinstein, Eugene and Ramabhadran, Bhuvana and Wu, Yonghui and Bapna, Ankur and Chen, Zhifeng and Lee, Seungji},
  date = {2019-09-11},
  eprint = {1909.05330},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1909.05330},
  urldate = {2019-10-29},
  abstract = {Multilingual end-to-end (E2E) models have shown great promise in expansion of automatic speech recognition (ASR) coverage of the world’s languages. They have shown improvement over monolingual systems, and have simplified training and serving by eliminating language-specific acoustic, pronunciation, and language models. This work presents an E2E multilingual system which is equipped to operate in low-latency interactive applications, as well as handle a key challenge of real world data: the imbalance in training data across languages. Using nine Indic languages, we compare a variety of techniques, and find that a combination of conditioning on a language vector and training language-specific adapter layers produces the best model. The resulting E2E multilingual model achieves a lower word error rate (WER) than both monolingual E2E models (eight of nine languages) and monolingual conventional systems (all nine languages).},
  langid = {english}
}

@unpublished{karimi_automated_2022,
  title = {Automated {{Detection}} of {{Doxing}} on {{Twitter}}},
  author = {Karimi, Younes and Squicciarini, Anna and Wilson, Shomir},
  date = {2022-02-02},
  eprint = {2202.0879},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2202.00879},
  urldate = {2022-04-05},
  abstract = {Doxing refers to the practice of disclosing sensitive personal information about a person without their consent. This form of cyberbullying is an unpleasant and sometimes dangerous phenomenon for online social networks. Although prior work exists on automated identification of other types of cyberbullying, a need exists for methods capable of detecting doxing on Twitter specifically. We propose and evaluate a set of approaches for automatically detecting second- and third-party disclosures on Twitter of sensitive private information, a subset of which constitutes doxing. We summarize our findings of common intentions behind doxing episodes and compare nine different approaches for automated detection based on string-matching and one-hot encoded heuristics, as well as word and contextualized string embedding representations of tweets. We identify an approach providing 96.86\% accuracy and 97.37\% recall using contextualized string embeddings and conclude by discussing the practicality of our proposed methods.}
}

@inproceedings{karpathy_deep_2015,
  title = {Deep Visual-Semantic Alignments for Generating Image Descriptions},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Karpathy, Andrej and Fei-Fei, Li},
  date = {2015},
  pages = {3128--3137}
}

@inproceedings{karpathy_largescale_2014,
  title = {Large-Scale Video Classification with Convolutional Neural Networks},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Karpathy, Andrej and Toderici, George and Shetty, Sanketh and Leung, Thomas and Sukthankar, Rahul and Fei-Fei, Li},
  date = {2014},
  pages = {1725--1732}
}

@online{karpathy_unreasonable_2015,
  title = {The {{Unreasonable Effectiveness}} of {{Recurrent Neural Networks}}},
  author = {Karpathy, Andrej},
  date = {2015},
  url = {http://karpathy.github.io/2015/05/21/rnn-effectiveness/},
  urldate = {2018-04-18}
}

@misc{karpathy_visualizing_2015,
  title = {Visualizing and {{Understanding Recurrent Networks}}},
  author = {Karpathy, Andrej and Johnson, Justin and Fei-Fei, Li},
  date = {2015},
  eprint = {26353135},
  eprinttype = {pmid},
  url = {http://arxiv.org/abs/1506.02078},
  urldate = {2018-04-07},
  abstract = {Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-Term Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing an analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, our comparative analysis with finite horizon n-gram models traces the source of the LSTM improvements to long-range structural dependencies. Finally, we provide analysis of the remaining errors and suggests areas for further study.},
  isbn = {978-3-319-10589-5}
}

@misc{karras_progressive_2018,
  title = {Progressive {{Growing}} of {{GANs}} for {{Improved Quality}}, {{Stability}}, and {{Variation}}},
  author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
  date = {2018-02-26},
  eprint = {1710.10196},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1710.10196},
  urldate = {2020-03-18},
  abstract = {We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CELEBA images at 10242. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CELEBA dataset.},
  langid = {english}
}

@inproceedings{karras_stylebased_2019,
  title = {A Style-Based Generator Architecture for Generative Adversarial Networks},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Karras, Tero and Laine, Samuli and Aila, Timo},
  date = {2019},
  pages = {4401--4410}
}

@misc{katharopoulos_biased_2017,
  title = {Biased {{Importance Sampling}} for {{Deep Neural Network Training}}},
  author = {Katharopoulos, Angelos and Fleuret, François},
  date = {2017-05-31},
  eprint = {1706.00043},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1706.00043},
  urldate = {2018-04-08},
  abstract = {Importance sampling has been successfully used to accelerate stochastic optimization in many convex problems. However, the lack of an efficient way to calculate the importance still hinders its application to Deep Learning. In this paper, we show that the loss value can be used as an alternative importance metric, and propose a way to efficiently approximate it for a deep model, using a small model trained for that purpose in parallel. This method allows in particular to utilize a biased gradient estimate that implicitly optimizes a soft max-loss, and leads to better generalization performance. While such method suffers from a prohibitively high variance of the gradient estimate when using a standard stochastic optimizer, we show that when it is combined with our sampling mechanism, it results in a reliable procedure. We showcase the generality of our method by testing it on both image classification and language modeling tasks using deep convolutional and recurrent neural networks. In particular, our method results in 30\% faster training of a CNN for CIFAR10 than when using uniform sampling.}
}

@misc{katharopoulos_not_2018,
  title = {Not {{All Samples Are Created Equal}}: {{Deep Learning}} with {{Importance Sampling}}},
  author = {Katharopoulos, Angelos and Fleuret, François},
  date = {2018-03-02},
  eprint = {1803.00942},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1803.00942},
  urldate = {2018-04-08},
  abstract = {Deep neural network training spends most of the computation on examples that are properly handled, and could be ignored. We propose to mitigate this phenomenon with a principled importance sampling scheme that focuses computation on "informative" examples, and reduces the variance of the stochastic gradients during training. Our contribution is twofold: first, we derive a tractable upper bound to the per-sample gradient norm, and second we derive an estimator of the variance reduction achieved with importance sampling, which enables us to switch it on when it will result in an actual speedup. The resulting scheme can be used by changing a few lines of code in a standard SGD procedure, and we demonstrate experimentally, on image classification, CNN fine-tuning, and RNN training, that for a fixed wall-clock time budget, it provides a reduction of the train losses of up to an order of magnitude and a relative improvement of test errors between 5\% and 17\%.}
}

@article{kavuluruEmpiricalEvaluationSupervised2015,
  title = {An Empirical Evaluation of Supervised Learning Approaches in Assigning Diagnosis Codes to Electronic Medical Records},
  author = {Kavuluru, Ramakanth and Rios, Anthony and Lu, Yuan},
  date = {2015-10},
  journaltitle = {Artificial Intelligence in Medicine},
  series = {Intelligent Healthcare Informatics in Big Data Era},
  volume = {65},
  number = {2},
  pages = {155--166},
  issn = {0933-3657},
  doi = {10.1016/j.artmed.2015.04.007},
  abstract = {Background Diagnosis codes are assigned to medical records in healthcare facilities by trained coders by reviewing all physician authored documents associated with a patient's visit. This is a necessary and complex task involving coders adhering to coding guidelines and coding all assignable codes. With the popularity of electronic medical records (EMRs), computational approaches to code assignment have been proposed in the recent years. However, most efforts have focused on single and often short clinical narratives, while realistic scenarios warrant full EMR level analysis for code assignment. Objective We evaluate supervised learning approaches to automatically assign international classification of diseases (ninth revision) – clinical modification (ICD-9-CM) codes to EMRs by experimenting with a large realistic EMR dataset. The overall goal is to identify methods that offer superior performance in this task when considering such datasets. Methods We use a dataset of 71,463 EMRs corresponding to in-patient visits with discharge date falling in a two year period (2011–2012) from the University of Kentucky (UKY) Medical Center. We curate a smaller subset of this dataset and also use a third gold standard dataset of radiology reports. We conduct experiments using different problem transformation approaches with feature and data selection components and employing suitable label calibration and ranking methods with novel features involving code co-occurrence frequencies and latent code associations. Results Over all codes with at least 50 training examples we obtain a micro F-score of 0.48. On the set of codes that occur at least in 1\% of the two year dataset, we achieve a micro F-score of 0.54. For the smaller radiology report dataset, the classifier chaining approach yields best results. For the smaller subset of the UKY dataset, feature selection, data selection, and label calibration offer best performance. Conclusions We show that datasets at different scale (size of the EMRs, number of distinct codes) and with different characteristics warrant different learning approaches. For shorter narratives pertaining to a particular medical subdomain (e.g., radiology, pathology), classifier chaining is ideal given the codes are highly related with each other. For realistic in-patient full EMRs, feature and data selection methods offer high performance for smaller datasets. However, for large EMR datasets, we observe that the binary relevance approach with learning-to-rank based code reranking offers the best performance. Regardless of the training dataset size, for general EMRs, label calibration to select the optimal number of labels is an indispensable final step.},
  langid = {english}
}

@article{kawakami_learning_2020,
  title = {Learning {{Robust}} and {{Multilingual Speech Representations}}},
  author = {Kawakami, Kazuya and Wang, Luyu and Dyer, Chris and Blunsom, Phil and family=Oord, given=Aaron, prefix=van den, useprefix=false},
  date = {2020-01-29},
  journaltitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  eprint = {2001.11128},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2001.11128},
  urldate = {2021-10-12},
  abstract = {Unsupervised speech representation learning has shown remarkable success at finding representations that correlate with phonetic structures and improve downstream speech recognition performance. However, most research has been focused on evaluating the representations in terms of their ability to improve the performance of speech recognition systems on read English (e.g. Wall Street Journal and LibriSpeech). This evaluation methodology overlooks two important desiderata that speech representations should have: robustness to domain shifts and transferability to other languages. In this paper we learn representations from up to 8000 hours of diverse and noisy speech data and evaluate the representations by looking at their robustness to domain shifts and their ability to improve recognition performance in many languages. We find that our representations confer significant robustness advantages to the resulting recognition systems: we see significant improvements in out-of-domain transfer relative to baseline feature sets and the features likewise provide improvements in 25 phonetically diverse languages including tonal languages and low-resource languages.}
}

@article{kawakami_unsupervised_2019,
  title = {Unsupervised {{Learning}} of {{Efficient}} and {{Robust Speech Representations}}},
  author = {Kawakami, Kazuya and Wang, Luyu and Dyer, Chris and Blunsom, Phil and family=Oord, given=Aaron, prefix=van den, useprefix=false},
  date = {2019-09-25},
  url = {https://openreview.net/forum?id=HJe-blSYvH},
  urldate = {2021-10-12},
  abstract = {We present an unsupervised method for learning speech representations based on a bidirectional contrastive predictive coding that implicitly discovers phonetic structure from large-scale corpora of...},
  langid = {english}
}

@inproceedings{kemp_unsupervised_1999,
  title = {Unsupervised Training of a Speech Recognizer: {{Recent}} Experiments},
  booktitle = {In {{Proceedings}} of the {{European Conference}} on {{Speech Communication}} and {{Technology}} ({{EUROSPEECH}})},
  author = {Kemp, Thomas and Waibel, Alex},
  date = {1999},
  eventtitle = {European {{Conference}} on {{Speech Communication}} and {{Technology}} ({{EUROSPEECH}})}
}

@inproceedings{kendall_what_2017,
  title = {What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Kendall, Alex and Gal, Yarin},
  editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  date = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/2650d6089a6d640c5e85b2b88265dc2b-Paper.pdf}
}

@online{keng_variational_2017,
  title = {Variational {{Bayes}} and {{The Mean-Field Approximation}} | {{Bounded Rationality}}},
  author = {Keng, Brian},
  date = {2017-04-03},
  url = {https://bjlkeng.github.io/posts/variational-bayes-and-the-mean-field-approximation/},
  urldate = {2021-04-22}
}

@online{khalid_rodd_2022,
  title = {{{RODD}}: {{A Self-Supervised Approach}} for {{Robust Out-of-Distribution Detection}}},
  shorttitle = {{{RODD}}},
  author = {Khalid, Umar and Esmaeili, Ashkan and Karim, Nazmul and Rahnavard, Nazanin},
  date = {2022-04-13},
  eprint = {2204.02553},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2204.02553},
  urldate = {2022-09-04},
  abstract = {Recent studies have addressed the concern of detecting and rejecting the out-of-distribution (OOD) samples as a major challenge in the safe deployment of deep learning (DL) models. It is desired that the DL model should only be confident about the in-distribution (ID) data which reinforces the driving principle of the OOD detection. In this paper, we propose a simple yet effective generalized OOD detection method independent of out-of-distribution datasets. Our approach relies on self-supervised feature learning of the training samples, where the embeddings lie on a compact low-dimensional space. Motivated by the recent studies that show self-supervised adversarial contrastive learning helps robustify the model, we empirically show that a pre-trained model with self-supervised contrastive learning yields a better model for uni-dimensional feature learning in the latent space. The method proposed in this work referred to as RODD outperforms SOTA detection performance on an extensive suite of benchmark datasets on OOD detection tasks. On the CIFAR-100 benchmarks, RODD achieves a 26.97 \$\textbackslash\%\$ lower false-positive rate (FPR@95) compared to SOTA methods.},
  pubstate = {preprint}
}

@article{khan_fast_nodate,
  title = {Fast and {{Scalable Bayesian Deep Learning}} by {{Weight-Perturbation}} in {{Adam}}},
  author = {Khan, Mohammad Emtiyaz and Nielsen, Didrik and Tangkaratt, Voot and Lin, Wu and Gal, Yarin and Srivastava, Akash},
  abstract = {Uncertainty computation in deep learning is es-sential to design robust and reliable systems. Vari-ational inference (VI) is a promising approach for such computation, but requires more effort to implement and execute compared to maximum-likelihood methods. In this paper, we propose new natural-gradient algorithms to reduce such ef-forts for Gaussian mean-field VI. Our algorithms can be implemented within the Adam optimizer by perturbing the network weights during gradi-ent evaluations, and uncertainty estimates can be cheaply obtained by using the vector that adapts the learning rate. This requires lower memory, computation, and implementation effort than ex-isting VI methods, while obtaining uncertainty estimates of comparable quality. Our empirical results confirm this and further suggest that the weight-perturbation in our algorithm could be use-ful for exploration in reinforcement learning and stochastic optimization.}
}

@article{khandelwal_sharp_nodate,
  title = {Sharp {{Nearby}}, {{Fuzzy Far Away}}: {{How Neural Language Models Use Context}}},
  author = {Khandelwal, Urvashi and He, He and Qi, Peng and Jurafsky, Dan},
  abstract = {We know very little about how neural lan-guage models (LM) use prior linguistic context. In this paper, we investigate the role of context in an LSTM LM, through ablation studies. Specifically, we ana-lyze the increase in perplexity when prior context words are shuffled, replaced, or dropped. On two standard datasets, Penn Treebank and WikiText-2, we find that the model is capable of using about 200 to-kens of context on average, but sharply distinguishes nearby context (recent 50 to-kens) from the distant history. The model is highly sensitive to the order of words within the most recent sentence, but ig-nores word order in the long-range context (beyond 50 tokens), suggesting the distant past is modeled only as a rough seman-tic field or topic. We further find that the neural caching model (Grave et al., 2017b) especially helps the LSTM to copy words from within this distant context. Overall, our analysis not only provides a better un-derstanding of how neural LMs use their context, but also sheds light on recent suc-cess from cache-based models.}
}

@inproceedings{kharitonov_data_2021,
  title = {Data {{Augmenting Contrastive Learning}} of {{Speech Representations}} in the {{Time Domain}}},
  booktitle = {2021 {{IEEE Spoken Language Technology Workshop}} ({{SLT}})},
  author = {Kharitonov, Eugene and Rivière, Morgane and Synnaeve, Gabriel and Wolf, Lior and Mazaré, Pierre-Emmanuel and Douze, Matthijs and Dupoux, Emmanuel},
  date = {2021-01},
  pages = {215--222},
  doi = {10.1109/SLT48900.2021.9383605},
  abstract = {Contrastive Predictive Coding (CPC), based on predicting future segments of speech from past segments is emerging as a powerful algorithm for representation learning of speech signal. However, it still under-performs compared to other methods on unsupervised evaluation benchmarks. Here, we intro-duce WavAugment, a time-domain data augmentation library which we adapt and optimize for the specificities of CPC (raw waveform input, contrastive loss, past versus future structure). We find that applying augmentation only to the segments from which the CPC prediction is performed yields better results than applying it also to future segments from which the samples (both positive and negative) of the contrastive loss are drawn. After selecting the best combination of pitch modification, additive noise and reverberation on unsupervised metrics on LibriSpeech (with a gain of 18-22\% relative on the ABX score), we apply this combination without any change to three new datasets in the Zero Resource Speech Benchmark 2017 and beat the state-of-the-art using out-of-domain training data. Finally, we show that the data-augmented pretrained features improve a downstream phone recognition task in the Libri-light semi-supervised setting (10 min, 1 h or 10 h of labelled data) reducing the PER by 15\% relative.},
  eventtitle = {2021 {{IEEE Spoken Language Technology Workshop}} ({{SLT}})}
}

@online{kharitonov_textfree_2021,
  title = {Text-Free Prosody-Aware Generative Spoken Language Modeling},
  author = {Kharitonov, Eugene and Lee, Ann and Polyak, Adam and Adi, Yossi and Copet, Jade and Lakhotia, Kushal and Nguyen, Tu Anh and Rivière, Morgane and Mohamed, Abdelrahman and Dupoux, Emmanuel and Hsu, Wei-Ning},
  date = {2021},
  eprint = {2109.03264},
  eprinttype = {arxiv},
  pubstate = {preprint}
}

@misc{khurana_convolutional_2020,
  title = {A {{Convolutional Deep Markov Model}} for {{Unsupervised Speech Representation Learning}}},
  author = {Khurana, Sameer and Laurent, Antoine and Hsu, Wei-Ning and Chorowski, Jan and Lancucki, Adrian and Marxer, Ricard and Glass, James},
  date = {2020-06-03},
  eprint = {2006.02547},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2006.02547},
  urldate = {2020-07-14},
  abstract = {Probabilistic Latent Variable Models (LVMs) provide an alternative to self-supervised learning approaches for linguistic representation learning from speech. LVMs admit an intuitive probabilistic interpretation where the latent structure shapes the information extracted from the signal. Even though LVMs have recently seen a renewed interest due to the introduction of Variational Autoencoders (VAEs), their use for speech representation learning remains largely unexplored. In this work, we propose Convolutional Deep Markov Model (ConvDMM), a Gaussian state-space model with non-linear emission and transition functions modelled by deep neural networks. This unsupervised model is trained using black box variational inference. A deep convolutional neural network is used as an inference network for structured variational approximation. When trained on a large scale speech dataset (LibriSpeech), ConvDMM produces features that significantly outperform multiple self-supervised feature extracting methods on linear phone classification and recognition on the Wall Street Journal dataset. Furthermore, we found that ConvDMM complements self-supervised methods like Wav2Vec and PASE, improving on the results achieved with any of the methods alone. Lastly, we find that ConvDMM features enable learning better phone recognizers than any other features in an extreme low-resource regime with few labelled training examples.},
  langid = {english}
}

@inproceedings{khurana_factorial_2019,
  title = {A {{Factorial Deep Markov Model}} for {{Unsupervised Disentangled Representation Learning}} from {{Speech}}},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Khurana, Sameer and Joty, Shafiq Rayhan and Ali, Ahmed and Glass, James},
  date = {2019-05},
  pages = {6540--6544},
  publisher = {{IEEE}},
  location = {{Brighton, United Kingdom}},
  doi = {10.1109/ICASSP.2019.8683131},
  url = {https://ieeexplore.ieee.org/document/8683131/},
  urldate = {2021-10-30},
  abstract = {We present the Factorial Deep Markov Model (FDMM) for representation learning of speech. The FDMM learns disentangled, interpretable and lower dimensional latent representations from speech without supervision. We use a static and dynamic latent variable to exploit the fact that information in a speech signal evolves at different time scales. Latent representations learned by the FDMM outperform a baseline ivector system on speaker verification and dialect identification while also reducing the error rate of a phone recognition system in a domain mismatch scenario.},
  eventtitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  isbn = {978-1-4799-8131-1},
  langid = {english}
}

@unpublished{khurana_magic_2021,
  title = {Magic Dust for Cross-Lingual Adaptation of Monolingual Wav2vec-2.0},
  author = {Khurana, Sameer and Laurent, Antoine and Glass, James},
  date = {2021-10-07},
  eprint = {2110.03560},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2110.03560},
  urldate = {2021-10-29},
  abstract = {We propose a simple and effective cross-lingual transfer learning method to adapt monolingual wav2vec-2.0 models for Automatic Speech Recognition (ASR) in resource-scarce languages. We show that a monolingual wav2vec-2.0 is a good few-shot ASR learner in several languages. We improve its performance further via several iterations of Dropout Uncertainty-Driven Self-Training (DUST) by using a moderate-sized unlabeled speech dataset in the target language. A key finding of this work is that the adapted monolingual wav2vec-2.0 achieves similar performance as the topline multilingual XLSR model, which is trained on fifty-three languages, on the target language ASR task.}
}

@misc{kim_boosting_2019,
  title = {Boosting {{Vector Calculus}} with the {{Graphical Notation}}},
  author = {Kim, Joon-Hwi and Oh, Maverick S. H. and Kim, Keun-Young},
  date = {2019-11-03},
  eprint = {1911.00892},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1911.00892},
  urldate = {2019-12-05},
  abstract = {Learning vector calculus techniques is one of the major missions to be accomplished by physics undergraduates. However, beginners report various difficulties dealing with the index notation due to its bulkiness. Meanwhile, there have been graphical notations for tensor algebra that are intuitive and effective in calculations and can serve as a quick mnemonic for algebraic identities. Although they have been introduced and applied in vector algebra in the educational context, to the best of our knowledge, there have been no publications that employ the graphical notation to three-dimensional Euclidean vector \textbackslash textit\{calculus\}, involving differentiation and integration of vector fields. Aiming for physics students and educators, we introduce such ``graphical vector calculus,'' demonstrate its pedagogical advantages, and provide enough exercises containing both purely mathematical identities and practical calculations in physics. The graphical notation can readily be utilized in the educational environment to not only lower the barriers in learning and practicing vector calculus but also make students interested and self-motivated to manipulate the vector calculus syntax and heuristically comprehend the language of tensors by themselves.},
  langid = {english}
}

@misc{kim_convolutional_2014,
  title = {Convolutional {{Neural Networks}} for {{Sentence Classification}}},
  author = {Kim, Yoon},
  date = {2014},
  eprint = {1408.5882},
  eprinttype = {arxiv},
  abstract = {We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vec-tors for sentence-level classification tasks. We show that a simple CNN with lit-tle hyperparameter tuning and static vec-tors achieves excellent results on multi-ple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the ar-chitecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.}
}

@misc{kim_disentangling_2019,
  title = {Disentangling by {{Factorising}}},
  author = {Kim, Hyunjik and Mnih, Andriy},
  date = {2019-07-09},
  eprint = {1802.05983},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1802.05983},
  urldate = {2021-04-20},
  abstract = {We define and address the problem of unsupervised learning of disentangled representations on data generated from independent factors of variation. We propose FactorVAE, a method that disentangles by encouraging the distribution of representations to be factorial and hence independent across the dimensions. We show that it improves upon β-VAE by providing a better trade-off between disentanglement and reconstruction quality. Moreover, we highlight the problems of a commonly used disentanglement metric and introduce a new metric that does not suffer from them.},
  langid = {english}
}

@online{kim_generalizing_2022,
  title = {Generalizing {{RNN-Transducer}} to {{Out-Domain Audio}} via {{Sparse Self-Attention Layers}}},
  author = {Kim, Juntae and Lee, Jeehye},
  date = {2022-06-17},
  eprint = {2108.10752},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2108.10752},
  urldate = {2023-03-21},
  abstract = {Recurrent neural network transducer (RNN-T) is an end-to-end speech recognition framework converting input acoustic frames into a character sequence. The state-of-the-art encoder network for RNN-T is the Conformer, which can effectively model the local-global context information via its convolution and self-attention layers. Although Conformer RNN-T has shown outstanding performance, most studies have been verified in the setting where the train and test data are drawn from the same domain. The domain mismatch problem for Conformer RNN-T has not been intensively investigated yet, which is an important issue for the product-level speech recognition system. In this study, we identified that fully connected self-attention layers in the Conformer caused high deletion errors, specifically in the long-form out-domain utterances. To address this problem, we introduce sparse self-attention layers for Conformer-based encoder networks, which can exploit local and generalized global information by pruning most of the in-domain fitted global connections. Also, we propose a state reset method for the generalization of the prediction network to cope with long-form utterances. Applying proposed methods to an out-domain test, we obtained 27.6\% relative character error rate (CER) reduction compared to the fully connected self-attention layer-based Conformers.},
  pubstate = {preprint}
}

@inproceedings{kim_interpretability_2018,
  title = {Interpretability {{Beyond Feature Attribution}}: {{Quantitative Testing}} with {{Concept Activation Vectors}} ({{TCAV}})},
  shorttitle = {Interpretability {{Beyond Feature Attribution}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and Sayres, Rory},
  date = {2018-06-07},
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {80},
  eprint = {1711.11279},
  eprinttype = {arxiv},
  pages = {2673--2682},
  publisher = {{PMLR}},
  location = {{Stockholmsmässan, Stockholm, Sweden}},
  url = {http://arxiv.org/abs/1711.11279},
  urldate = {2023-04-20},
  abstract = {The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net's internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result--for example, how sensitive a prediction of "zebra" is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.}
}

@online{kim_squeezeformer_2022,
  title = {Squeezeformer: {{An Efficient Transformer}} for {{Automatic Speech Recognition}}},
  shorttitle = {Squeezeformer},
  author = {Kim, Sehoon and Gholami, Amir and Shaw, Albert and Lee, Nicholas and Mangalam, Karttikeya and Malik, Jitendra and Mahoney, Michael W. and Keutzer, Kurt},
  date = {2022-10-15},
  eprint = {2206.00888},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2206.00888},
  urldate = {2023-02-14},
  abstract = {The recently proposed Conformer model has become the de facto backbone model for various downstream speech tasks based on its hybrid attention-convolution architecture that captures both local and global features. However, through a series of systematic studies, we find that the Conformer architecture's design choices are not optimal. After re-examining the design choices for both the macro and micro-architecture of Conformer, we propose Squeezeformer which consistently outperforms the state-of-the-art ASR models under the same training schemes. In particular, for the macro-architecture, Squeezeformer incorporates (i) the Temporal U-Net structure which reduces the cost of the multi-head attention modules on long sequences, and (ii) a simpler block structure of multi-head attention or convolution modules followed up by feed-forward module instead of the Macaron structure proposed in Conformer. Furthermore, for the micro-architecture, Squeezeformer (i) simplifies the activations in the convolutional block, (ii) removes redundant Layer Normalization operations, and (iii) incorporates an efficient depthwise down-sampling layer to efficiently sub-sample the input signal. Squeezeformer achieves state-of-the-art results of 7.5\%, 6.5\%, and 6.0\% word-error-rate (WER) on LibriSpeech test-other without external language models, which are 3.1\%, 1.4\%, and 0.6\% better than Conformer-CTC with the same number of FLOPs. Our code is open-sourced and available online.},
  pubstate = {preprint}
}

@misc{kim_variational_2019,
  title = {Variational {{Temporal Abstraction}}},
  author = {Kim, Taesup and Ahn, Sungjin and Bengio, Yoshua},
  date = {2019-10-02},
  eprint = {1910.00775},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1910.00775},
  urldate = {2021-04-28},
  abstract = {We introduce a variational approach to learning and inference of temporally hierarchical structure and representation for sequential data. We propose the Variational Temporal Abstraction (VTA), a hierarchical recurrent state space model that can infer the latent temporal structure and thus perform the stochastic state transition hierarchically. We also propose to apply this model to implement the jumpyimagination ability in imagination-augmented agent-learning in order to improve the efficiency of the imagination. In experiments, we demonstrate that our proposed method can model 2D and 3D visual sequence datasets with interpretable temporal structure discovery and that its application to jumpy imagination enables more efficient agent-learning in a 3D navigation task.},
  langid = {english}
}

@inproceedings{kim_vilt_2021,
  title = {{{ViLT}}: {{Vision-and-Language Transformer Without Convolution}} or {{Region Supervision}}},
  shorttitle = {{{ViLT}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Kim, Wonjae and Son, Bokyung and Kim, Ildoo},
  date = {2021-06-10},
  volume = {139},
  pages = {5583--5594},
  publisher = {{\{PMLR\}}},
  location = {{Virtual}},
  url = {http://proceedings.mlr.press/v139/kim21k.html},
  urldate = {2022-02-03},
  abstract = {Vision-and-Language Pre-training (VLP) has improved performance on various joint vision-and-language downstream tasks. Current approaches to VLP heavily rely on image feature extraction processes, most of which involve region supervision (e.g., object detection) and the convolutional architecture (e.g., ResNet). Although disregarded in the literature, we find it problematic in terms of both (1) efficiency/speed, that simply extracting input features requires much more computation than the multimodal interaction steps; and (2) expressive power, as it is upper bounded to the expressive power of the visual embedder and its predefined visual vocabulary. In this paper, we present a minimal VLP model, Vision-and-Language Transformer (ViLT), monolithic in the sense that the processing of visual inputs is drastically simplified to just the same convolution-free manner that we process textual inputs. We show that ViLT is up to tens of times faster than previous VLP models, yet with competitive or better downstream task performance. Our code and pre-trained weights are available at https://github.com/dandelin/vilt.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}}
}

@inproceedings{kimReadAttendCode2021,
  title = {Read, {{Attend}}, and {{Code}}: {{Pushing}} the {{Limits}} of {{Medical Codes Prediction}} from {{Clinical Notes}} by {{Machines}}},
  shorttitle = {Read, {{Attend}}, and {{Code}}},
  booktitle = {Proceedings of the 6th {{Machine Learning}} for {{Healthcare Conference}}},
  author = {Kim, Byung-Hak and Ganapathi, Varun},
  date = {2021-10},
  pages = {196--208},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Prediction of medical codes from clinical notes is both a practical and essential need for every healthcare delivery organization within current medical systems. Automating annotation will save significant time and excessive effort spent by human coders today. However, the biggest challenge is directly identifying appropriate medical codes out of several thou- sands of high-dimensional codes from unstructured free-text clinical notes. In the past three years, with Convolutional Neural Networks (CNN) and Long Short-Term Memory (LTSM) networks, there have been vast improvements in tackling the most challenging benchmark of the MIMIC-III-full-label inpatient clinical notes dataset. This progress raises the fundamental question of how far automated machine learning (ML) systems are from human coders' working performance. We assessed the baseline of human coders' performance on the same subsampled testing set. We also present our Read, Attend, and Code (RAC) model for learning the medical code assignment mappings. By connecting convolved embeddings with self-attention and code-title guided attention modules, combined with sentence permutation-based data augmentations and stochastic weight averaging training, RAC establishes a new state of the art (SOTA), considerably outperforming the current best Macro-F1 by 18.7\%, and reaches past the human-level coding baseline. This new milestone marks a meaningful step toward fully autonomous medical coding (AMC) in machines reaching parity with human coders' performance in medical code prediction.},
  langid = {english}
}

@inproceedings{kingma_adam_2015,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  booktitle = {Proceedings of the the 3rd {{International Conference}} for {{Learning Representations}} ({{ICLR}})},
  author = {Kingma, Diederik P. and Ba, Jimmy Lei},
  date = {2015},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  location = {{San Diego, CA, USA}},
  url = {http://arxiv.org/abs/1412.6980},
  urldate = {2018-05-24},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  keywords = {★}
}

@inproceedings{kingma_autoencoding_2014,
  title = {Auto-{{Encoding Variational Bayes}}},
  booktitle = {Proceedings of the 2nd {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Kingma, Diederik P and Welling, Max},
  date = {2014},
  eprint = {1312.6114},
  eprinttype = {arxiv},
  location = {{Banff, AB, Canada}},
  url = {http://arxiv.org/abs/1312.6114},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  keywords = {★}
}

@inproceedings{kingma_glow_2018,
  title = {Glow: {{Generative Flow}} with {{Invertible}} 1×1 {{Convolutions}}},
  booktitle = {Proceedings of the 32nd {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Kingma, Diederik P and Dhariwal, Prafulla},
  date = {2018},
  pages = {10},
  location = {{Montréal, Canada}},
  abstract = {Flow-based generative models (Dinh et al., 2014) are conceptually attractive due to tractability of the exact log-likelihood, tractability of exact latent-variable inference, and parallelizability of both training and synthesis. In this paper we propose Glow, a simple type of generative flow using an invertible 1 × 1 convolution. Using our method we demonstrate a significant improvement in log-likelihood on standard benchmarks. Perhaps most strikingly, we demonstrate that a flow-based generative model optimized towards the plain log-likelihood objective is capable of efficient realistic-looking synthesis and manipulation of large images. The code for our model is available at https://github.com/openai/glow.},
  eventtitle = {Conference on {{Neural Information Processing Systems}}},
  langid = {english}
}

@inproceedings{kingma_improved_2016,
  title = {Improved {{Variational Inference}} with {{Inverse Autoregressive Flow}}},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Kingma, Diederik P. and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max},
  date = {2016},
  series = {{{NIPS}}'16},
  pages = {4743--4751},
  location = {{Barcelona, Spain}},
  url = {http://arxiv.org/abs/1606.04934},
  abstract = {The framework of normalizing flows provides a general strategy for flexible variational inference of posteriors over latent variables. We propose a new type of normalizing flow, inverse autoregressive flow (IAF), that, in contrast to earlier published flows, scales well to high-dimensional latent spaces. The proposed flow consists of a chain of invertible transformations, where each transformation is based on an autoregressive neural network. In experiments, we show that IAF significantly improves upon diagonal Gaussian approximate posteriors. In addition, we demonstrate that a novel type of variational autoencoder, coupled with IAF, is competitive with neural autoregressive models in terms of attained log-likelihood on natural images, while allowing significantly faster synthesis.},
  eventtitle = {International {{Conference}} on {{Neural Information Processing Systems}}},
  isbn = {978-1-5108-3881-9}
}

@article{kingma_introduction_2019,
  title = {An {{Introduction}} to {{Variational Autoencoders}}},
  author = {Kingma, Diederik P. and Welling, Max},
  date = {2019},
  journaltitle = {Foundations and Trends® in Machine Learning},
  shortjournal = {FNT in Machine Learning},
  volume = {12},
  number = {4},
  eprint = {1906.02691},
  eprinttype = {arxiv},
  pages = {307--392},
  issn = {1935-8237, 1935-8245},
  doi = {10/ggfm34},
  url = {http://arxiv.org/abs/1906.02691},
  urldate = {2020-02-03},
  abstract = {Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.},
  langid = {english}
}

@inproceedings{kingma_semi-supervised_2014,
  title = {Semi-{{Supervised Learning}} with {{Deep Generative Models}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Kingma, Diederik P. and Rezende, Danilo J. and Mohamed, Shakir and Welling, Max},
  date = {2014-06-20},
  eprint = {1406.5298},
  eprinttype = {arxiv},
  location = {{Montréal, Quebec, Canada}},
  url = {http://arxiv.org/abs/1406.5298},
  urldate = {2018-06-07},
  abstract = {The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.}
}

@misc{kingma_variational_2015,
  title = {Variational {{Dropout}} and the {{Local Reparameterization Trick}}},
  author = {Kingma, Diederik P. and Salimans, Tim and Welling, Max},
  date = {2015-06-08},
  eprint = {1506.02557},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1506.02557},
  abstract = {We investigate a local reparameterizaton technique for greatly reducing the variance of stochastic gradients for variational Bayesian inference (SGVB) of a posterior over model parameters, while retaining parallelizability. This local reparameterization translates uncertainty about global parameters into local noise that is independent across datapoints in the minibatch. Such parameterizations can be trivially parallelized and have variance that is inversely proportional to the minibatch size, generally leading to much faster convergence. Additionally, we explore a connection with dropout: Gaussian dropout objectives correspond to SGVB with local reparameterization, a scale-invariant prior and proportionally fixed posterior variance. Our method allows inference of more flexibly parameterized posteriors; specifically, we propose variational dropout, a generalization of Gaussian dropout where the dropout rates are learned, often leading to better models. The method is demonstrated through several experiments.}
}

@misc{kingma_variational_2021,
  title = {Variational {{Diffusion Models}}},
  author = {Kingma, Diederik P. and Salimans, Tim and Poole, Ben and Ho, Jonathan},
  date = {2021-07-01},
  eprint = {2107.00630},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2107.00630},
  urldate = {2021-07-07},
  abstract = {Diffusion-based generative models have demonstrated a capacity for perceptually impressive synthesis, but can they also be great likelihood-based models? We answer this in the affirmative, and introduce a family of diffusion-based generative models that obtain state-of-the-art likelihoods on standard image density estimation benchmarks. Unlike other diffusion-based models, our method allows for efficient optimization of the noise schedule jointly with the rest of the model. We show that the variational lower bound (VLB) simplifies to a remarkably short expression in terms of the signal-to-noise ratio of the diffused data, thereby improving our theoretical understanding of this model class. Using this insight, we prove an equivalence between several models proposed in the literature. In addition, we show that the continuous-time VLB is invariant to the noise schedule, except for the signal-to-noise ratio at its endpoints. This enables us to learn a noise schedule that minimizes the variance of the resulting VLB estimator, leading to faster optimization. Combining these advances with architectural improvements, we obtain state-of-the-art likelihoods on image density estimation benchmarks, outperforming autoregressive models that have dominated these benchmarks for many years, with often significantly faster optimization. In addition, we show how to turn the model into a bits-back compression scheme, and demonstrate lossless compression rates close to the theoretical optimum.},
  langid = {english},
  keywords = {Unread}
}

@misc{kingmaAdamMethodStochastic2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  date = {2017-01},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1412.6980},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  issue = {arXiv:1412.6980},
  organization = {{arXiv}}
}

@misc{kipf_variational_2016,
  title = {Variational {{Graph Auto-Encoders}}},
  author = {Kipf, Thomas N. and Welling, Max},
  date = {2016-11-21},
  eprint = {1611.07308},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1611.07308},
  urldate = {2021-01-29},
  abstract = {We introduce the variational graph auto-encoder (VGAE), a framework for unsupervised learning on graph-structured data based on the variational auto-encoder (VAE). This model makes use of latent variables and is capable of learning interpretable latent representations for undirected graphs. We demonstrate this model using a graph convolutional network (GCN) encoder and a simple inner product decoder. Our model achieves competitive results on a link prediction task in citation networks. In contrast to most existing models for unsupervised learning on graph-structured data and link prediction, our model can naturally incorporate node features, which significantly improves predictive performance on a number of benchmark datasets.}
}

@misc{kirichenko_why_2020,
  title = {Why {{Normalizing Flows Fail}} to {{Detect Out-of-Distribution Data}}},
  author = {Kirichenko, Polina and Izmailov, Pavel and Wilson, Andrew Gordon},
  date = {2020-06-15},
  eprint = {2006.08545},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2006.08545},
  urldate = {2020-10-21},
  abstract = {Detecting out-of-distribution (OOD) data is crucial for robust machine learning systems. Normalizing flows are flexible deep generative models that often surprisingly fail to distinguish between in- and out-of-distribution data: a flow trained on pictures of clothing assigns higher likelihood to handwritten digits. We investigate why normalizing flows perform poorly for OOD detection. We demonstrate that flows learn local pixel correlations and generic image-to-latentspace transformations which are not specific to the target image dataset. We show that by modifying the architecture of flow coupling layers we can bias the flow towards learning the semantic structure of the target data, improving OOD detection. Our investigation reveals that properties that enable flows to generate high-fidelity images can have a detrimental effect on OOD detection.},
  langid = {english}
}

@article{kirkpatrick_overcoming_2017,
  title = {Overcoming Catastrophic Forgetting in Neural Networks},
  author = {Kirkpatrick, J. and Pascanu, R. and Rabinowitz, N. and Veness, J. and Desjardins, G. and Rusu, A. A. and Milan, K. and Quan, J. and Ramalho, T. and Grabska-Barwinska, A. and Hassabis, D. and Clopath, C. and Kumaran, D. and Hadsell, R.},
  date = {2017},
  journaltitle = {Proceedings of the National Academy of Sciences (PNAS)}
}

@inproceedings{kirsch_batchbald_2019,
  title = {{{BatchBALD}}: {{Efficient}} and {{Diverse Batch Acquisition}} for {{Deep Bayesian Active Learning}}},
  shorttitle = {{{BatchBALD}}},
  booktitle = {Proceedings of the {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Kirsch, Andreas and family=Amersfoort, given=Joost, prefix=van, useprefix=true and Gal, Yarin},
  date = {2019-10-28},
  eprint = {1906.08158},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1906.08158},
  urldate = {2023-06-28},
  abstract = {We develop BatchBALD, a tractable approximation to the mutual information between a batch of points and model parameters, which we use as an acquisition function to select multiple informative points jointly for the task of deep Bayesian active learning. BatchBALD is a greedy linear-time \$1 - \textbackslash frac\{1\}\{e\}\$-approximate algorithm amenable to dynamic programming and efficient caching. We compare BatchBALD to the commonly used approach for batch data acquisition and find that the current approach acquires similar and redundant points, sometimes performing worse than randomly acquiring data. We finish by showing that, using BatchBALD to consider dependencies within an acquisition batch, we achieve new state of the art performance on standard benchmarks, providing substantial data efficiency improvements in batch acquisition.}
}

@online{kitaev_reformer_2020,
  title = {Reformer: {{The Efficient Transformer}}},
  author = {Kitaev, Nikita and Kaiser, L. and Levskaya, Anselm},
  date = {2020},
  eprint = {2001.04451},
  eprinttype = {arxiv},
  url = {https://arxiv.org/abs/2001.04451},
  pubstate = {preprint}
}

@misc{klambauer_selfnormalizing_2017,
  title = {Self-{{Normalizing Neural Networks}}},
  author = {Klambauer, Günter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
  date = {2017},
  eprint = {1706.02515},
  eprinttype = {arxiv},
  url = {https://arxiv.org/abs/1706.02515},
  urldate = {2018-04-06},
  abstract = {Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are "scaled exponential linear units" (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance -- even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs and other machine learning methods such as random forests and support vector machines. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: github.com/bioinf-jku/SNNs.}
}

@article{klarreich_multiplication_2019,
  title = {Multiplication Hits the Speed Limit},
  author = {Klarreich, Erica},
  date = {2019-12-20},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {63},
  number = {1},
  pages = {11--13},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3371387},
  url = {https://dl.acm.org/doi/10.1145/3371387},
  urldate = {2023-07-10},
  abstract = {A problem "around since antiquity" may have been resolved by a new algorithm.},
  langid = {english}
}

@inproceedings{klejch_deciphering_2022,
  title = {Deciphering {{Speech}}: {{A Zero-Resource Approach}} to {{Cross-Lingual Transfer}} in {{Asr}}},
  booktitle = {Proceedings of the 23rd {{Annual Conference}} of the {{International Speech Communication Association}} ({{Interspeech}})},
  author = {Klejch, Ondrej and Wallington, Electra and Bell, Peter},
  editor = {Ko, Hanseok and Hansen, John H. L.},
  date = {2022-09},
  pages = {2288--2292},
  publisher = {{ISCA}},
  location = {{Incheon, South Korea}},
  doi = {10.21437/Interspeech.2022-10170},
  url = {https://doi.org/10.21437/Interspeech.2022-10170}
}

@article{ko_audio_,
  title = {Audio {{Augmentation}} for {{Speech Recognition}}},
  author = {Ko, Tom and Peddinti, Vijayaditya and Povey, Daniel and Khudanpur, Sanjeev},
  pages = {4},
  abstract = {Data augmentation is a common strategy adopted to increase the quantity of training data, avoid overfitting and improve robustness of the models. In this paper, we investigate audio-level speech augmentation methods which directly process the raw signal. The method we particularly recommend is to change the speed of the audio signal, producing 3 versions of the original signal with speed factors of 0.9, 1.0 and 1.1. The proposed technique has a low implementation cost, making it easy to adopt. We present results on 4 different LVCSR tasks with training data ranging from 100 hours to 1000 hours, to examine the effectiveness of audio augmentation in a variety of data scenarios. An average relative improvement of 4.3\% was observed across the 4 tasks.},
  langid = {english}
}

@article{kobak_optimal_,
  title = {The {{Optimal Ridge Penalty}} for {{Real-world High-dimensional Data Can Be Zero}} or {{Negative}} Due to the {{Implicit Ridge Regularization}}},
  author = {Kobak, Dmitry},
  pages = {16},
  abstract = {A conventional wisdom in statistical learning is that large models require strong regularization to prevent overfitting. Here we show that this rule can be violated by linear regression in the underdetermined n p situation under realistic conditions. Using simulations and real-life highdimensional datasets, we demonstrate that an explicit positive ridge penalty can fail to provide any improvement over the minimum-norm least squares estimator. Moreover, the optimal value of ridge penalty in this situation can be negative. This happens when the high-variance directions in the predictor space can predict the response variable, which is often the case in the real-world highdimensional data. In this regime, low-variance directions provide an implicit ridge regularization and can make any further positive ridge penalty detrimental. We prove that augmenting any linear model with random covariates and using minimum-norm estimator is asymptotically equivalent to adding the ridge penalty. We use a spiked covariance model as an analytically tractable example and prove that the optimal ridge penalty in this case is negative when n p.},
  langid = {english}
}

@inproceedings{kocabiyikoglu_augmenting_2018,
  title = {Augmenting {{LibriSpeech}} with {{French}} Translations: {{A}} Multimodal Corpus for Direct Speech Translation Evaluation},
  booktitle = {Proceedings of the International Conference on Language Resources and Evaluation ({{LREC}})},
  author = {Kocabiyikoglu, Ali Can and Besacier, Laurent and Kraif, Olivier},
  date = {2018}
}

@article{koenecke_racial_2020,
  title = {Racial Disparities in Automated Speech Recognition},
  author = {Koenecke, Allison and Nam, Andrew and Lake, Emily and Nudell, Joe and Quartey, Minnie and Mengesha, Zion and Toups, Connor and Rickford, John R. and Jurafsky, Dan and Goel, Sharad},
  date = {2020},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. USA},
  volume = {117},
  number = {14},
  pages = {7684--7689},
  doi = {10.1073/pnas.1915768117},
  url = {https://doi.org/10.1073/pnas.1915768117}
}

@article{kolmogorov_tables_1963,
  title = {On Tables of Random Numbers},
  author = {Kolmogorov, A. N.},
  date = {1963},
  journaltitle = {Sankhyā: The Indian Journal of Statistics, Series A (1961-2002)},
  volume = {25},
  number = {4},
  eprint = {25049284},
  eprinttype = {jstor},
  pages = {369--376},
  issn = {0581572X},
  url = {http://www.jstor.org/stable/25049284}
}

@article{komarek_making_2005,
  title = {Making {{Logistic Regression A Core Data Mining Tool}}},
  author = {Komarek, Paul and Moore, Andrew},
  date = {2005},
  url = {http://repository.cmu.edu/cgi/viewcontent.cgi?article=1217&context=robotics},
  urldate = {2018-05-01},
  abstract = {Binary classification is a core data mining task. For large datasets or real-time applications, desirable classifiers are accurate, fast, and automatic (i.e. no parameter tuning). Naive Bayes and decision trees are fast and parameter-free, but their accuracy is often below state-of-the-art. Linear support vector machines (SVM) are fast and have good accuracy, but current implementations are sensitive to the capacity parameter. SVMs with radial basis function kernels are accurate but slow, and have multiple parameters that require tuning. In this paper we demonstrate that a very simple parameter-free implementation of logistic regression (LR) is suffi-ciently accurate and fast to compete with state-of-the-art binary classifiers on large real-world datasets. The accuracy is comparable to per-dataset tuned linear SVMs and, in higher dimensions, to tuned RBF SVMs. A combination of reg-ularization, truncated-Newton methods, and iteratively re-weighted least squares make this implementation faster than SVMs and relatively insensitive to parameters. Our fitting procedure, TR-IRLS, appears to outperform several common LR fitting procedures in our experiments. TR-IRLS is robust to linear dependencies and scaling problems in the data, and no data preprocessing is necessary. TR-IRLS is easy to implement and can be used anywhere that IRLS is used. Convergence guarantees can be stated for generalized linear models with canonical links.}
}

@online{koner_oodformer_2021,
  title = {{{OODformer}}: {{Out-Of-Distribution Detection Transformer}}},
  shorttitle = {{{OODformer}}},
  author = {Koner, Rajat and Sinhamahapatra, Poulami and Roscher, Karsten and Günnemann, Stephan and Tresp, Volker},
  date = {2021-12-06},
  eprint = {2107.08976},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2107.08976},
  urldate = {2022-09-04},
  abstract = {A serious problem in image classification is that a trained model might perform well for input data that originates from the same distribution as the data available for model training, but performs much worse for out-of-distribution (OOD) samples. In real-world safety-critical applications, in particular, it is important to be aware if a new data point is OOD. To date, OOD detection is typically addressed using either confidence scores, auto-encoder based reconstruction, or by contrastive learning. However, the global image context has not yet been explored to discriminate the non-local objectness between in-distribution and OOD samples. This paper proposes a first-of-its-kind OOD detection architecture named OODformer that leverages the contextualization capabilities of the transformer. Incorporating the trans\textbackslash -former as the principal feature extractor allows us to exploit the object concepts and their discriminate attributes along with their co-occurrence via visual attention. Using the contextualised embedding, we demonstrate OOD detection using both class-conditioned latent space similarity and a network confidence score. Our approach shows improved generalizability across various datasets. We have achieved a new state-of-the-art result on CIFAR-10/-100 and ImageNet30.},
  pubstate = {preprint}
}

@online{kong_diffwave_2021,
  title = {{{DiffWave}}: {{A Versatile Diffusion Model}} for {{Audio Synthesis}}},
  shorttitle = {{{DiffWave}}},
  author = {Kong, Zhifeng and Ping, Wei and Huang, Jiaji and Zhao, Kexin and Catanzaro, Bryan},
  date = {2021-03-30},
  eprint = {2009.09761},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2009.09761},
  urldate = {2022-09-21},
  abstract = {In this work, we propose DiffWave, a versatile diffusion probabilistic model for conditional and unconditional waveform generation. The model is non-autoregressive, and converts the white noise signal into structured waveform through a Markov chain with a constant number of steps at synthesis. It is efficiently trained by optimizing a variant of variational bound on the data likelihood. DiffWave produces high-fidelity audios in different waveform generation tasks, including neural vocoding conditioned on mel spectrogram, class-conditional generation, and unconditional generation. We demonstrate that DiffWave matches a strong WaveNet vocoder in terms of speech quality (MOS: 4.44 versus 4.43), while synthesizing orders of magnitude faster. In particular, it significantly outperforms autoregressive and GAN-based waveform models in the challenging unconditional generation task in terms of audio quality and sample diversity from various automatic and human evaluations.},
  langid = {english},
  pubstate = {preprint}
}

@online{kong_spvit_2022,
  title = {{{SPViT}}: {{Enabling Faster Vision Transformers}} via {{Soft Token Pruning}}},
  shorttitle = {{{SPViT}}},
  author = {Kong, Zhenglun and Dong, Peiyan and Ma, Xiaolong and Meng, Xin and Sun, Mengshu and Niu, Wei and Shen, Xuan and Yuan, Geng and Ren, Bin and Qin, Minghai and Tang, Hao and Wang, Yanzhi},
  date = {2022-09-20},
  eprint = {2112.13890},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2112.13890},
  urldate = {2022-10-08},
  abstract = {Recently, Vision Transformer (ViT) has continuously established new milestones in the computer vision field, while the high computation and memory cost makes its propagation in industrial production difficult. Pruning, a traditional model compression paradigm for hardware efficiency, has been widely applied in various DNN structures. Nevertheless, it stays ambiguous on how to perform exclusive pruning on the ViT structure. Considering three key points: the structural characteristics, the internal data pattern of ViTs, and the related edge device deployment, we leverage the input token sparsity and propose a computation-aware soft pruning framework, which can be set up on vanilla Transformers of both flatten and CNN-type structures, such as Pooling-based ViT (PiT). More concretely, we design a dynamic attention-based multi-head token selector, which is a lightweight module for adaptive instance-wise token selection. We further introduce a soft pruning technique, which integrates the less informative tokens generated by the selector module into a package token that will participate in subsequent calculations rather than being completely discarded. Our framework is bound to the trade-off between accuracy and computation constraints of specific edge devices through our proposed computation-aware training strategy. Experimental results show that our framework significantly reduces the computation cost of ViTs while maintaining comparable performance on image classification. Moreover, our framework can guarantee the identified model to meet resource specifications of mobile devices and FPGA, and even achieve the real-time execution of DeiT-T on mobile platforms. For example, our method reduces the latency of DeiT-T to 26 ms (26\%\$\textbackslash sim \$41\% superior to existing works) on the mobile device with 0.25\%\$\textbackslash sim \$4\% higher top-1 accuracy on ImageNet.},
  pubstate = {preprint},
  keywords = {Medium}
}

@online{kool_stochastic_2019,
  title = {Stochastic {{Beams}} and {{Where}} to {{Find Them}}: {{The Gumbel-Top-k Trick}} for {{Sampling Sequences Without Replacement}}},
  shorttitle = {Stochastic {{Beams}} and {{Where}} to {{Find Them}}},
  author = {Kool, Wouter and family=Hoof, given=Herke, prefix=van, useprefix=true and Welling, Max},
  date = {2019-05-29},
  eprint = {1903.06059},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1903.06059},
  urldate = {2022-08-08},
  abstract = {The well-known Gumbel-Max trick for sampling from a categorical distribution can be extended to sample \$k\$ elements without replacement. We show how to implicitly apply this 'Gumbel-Top-\$k\$' trick on a factorized distribution over sequences, allowing to draw exact samples without replacement using a Stochastic Beam Search. Even for exponentially large domains, the number of model evaluations grows only linear in \$k\$ and the maximum sampled sequence length. The algorithm creates a theoretical connection between sampling and (deterministic) beam search and can be used as a principled intermediate alternative. In a translation task, the proposed method compares favourably against alternatives to obtain diverse yet good quality translations. We show that sequences sampled without replacement can be used to construct low-variance estimators for expected sentence-level BLEU score and model entropy.},
  pubstate = {preprint}
}

@misc{kopp_differentiable_2016,
  title = {A {{Differentiable Transition Between Additive}} and {{Multiplicative Neurons}}},
  author = {Köpp, Wiebke and family=Smagt, given=Patrick, prefix=van der, useprefix=true and Urban, Sebastian},
  date = {2016-04-13},
  eprint = {1604.03736},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1604.03736},
  urldate = {2018-04-18},
  abstract = {Existing approaches to combine both additive and multiplicative neural units either use a fixed assignment of operations or require discrete optimization to determine what function a neuron should perform. However, this leads to an extensive increase in the computational complexity of the training procedure. We present a novel, parameterizable transfer function based on the mathematical concept of non-integer functional iteration that allows the operation each neuron performs to be smoothly and, most importantly, differentiablely adjusted between addition and multiplication. This allows the decision between addition and multiplication to be integrated into the standard backpropagation training procedure.}
}

@inproceedings{kornblith_similarity_2019,
  title = {Similarity of {{Neural Network Representations Revisited}}},
  booktitle = {36th {{International Conference}} on {{Machine Learning}} ({{ICML}} 2019)},
  author = {Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
  date = {2019},
  pages = {20},
  location = {{Long Beach, CA, USA}},
  url = {https://arxiv.org/pdf/1905.00414.pdf},
  abstract = {Recent work has sought to understand the behavior of neural networks by comparing representations between layers and between different trained models. We examine methods for comparing neural network representations based on canonical correlation analysis (CCA). We show that CCA belongs to a family of statistics for measuring multivariate similarity, but that neither CCA nor any other statistic that is invariant to invertible linear transformation can measure meaningful similarities between representations of higher dimension than the number of data points. We introduce a similarity index that measures the relationship between representational similarity matrices and does not suffer from this limitation. This similarity index is equivalent to centered kernel alignment (CKA) and is also closely connected to CCA. Unlike CCA, CKA can reliably identify correspondences between representations in networks trained from different initializations.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english}
}

@online{kosiorek_adam_what_2018,
  title = {What Is Wrong with {{VAEs}}?},
  author = {Kosiorek, Adam},
  date = {2018-03-18},
  url = {http://akosiorek.github.io/ml/2018/03/14/what_is_wrong_with_vaes.html},
  urldate = {2021-04-22}
}

@online{kosiorek_attention_2017,
  title = {Attention in {{Neural Networks}} and {{How}} to {{Use It}}},
  author = {Kosiorek, Adam},
  date = {2017},
  url = {http://akosiorek.github.io/ml/2017/10/14/visual-attention.html},
  urldate = {2018-04-18},
  abstract = {Attention mechanisms in neural networks, otherwise known as neural attention or just attention, have recently attracted a lot of attention (pun intended). In this post, I will try to find a common denominator for different mechanisms and use-cases and I will describe (and implement!) two mechanisms of soft visual attention.}
}

@online{koutnik_clockwork_2014,
  title = {A {{Clockwork RNN}}},
  author = {Koutník, Jan and Greff, Klaus and Gomez, Faustino and Schmidhuber, Jürgen},
  date = {2014-02-14},
  number = {1402.3511},
  eprint = {1402.3511},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1402.3511},
  urldate = {2021-04-21},
  abstract = {Sequence prediction and classification are ubiquitous and challenging problems in machine learning that can require identifying complex dependencies between temporally distant inputs. Recurrent Neural Networks (RNNs) have the ability, in theory, to cope with these temporal dependencies by virtue of the short-term memory implemented by their recurrent (feedback) connections. However, in practice they are difficult to train successfully when the long-term memory is required. This paper introduces a simple, yet powerful modification to the standard RNN architecture, the Clockwork RNN (CW-RNN), in which the hidden layer is partitioned into separate modules, each processing inputs at its own temporal granularity, making computations only at its prescribed clock rate. Rather than making the standard RNN models more complex, CW-RNN reduces the number of RNN parameters, improves the performance significantly in the tasks tested, and speeds up the network evaluation. The network is demonstrated in preliminary experiments involving two tasks: audio signal generation and TIMIT spoken word classification, where it outperforms both RNN and LSTM networks.},
  langid = {english},
  pubstate = {preprint}
}

@inproceedings{koutnik_waveletbased_2016,
  title = {A {{Wavelet-based Encoding}} for {{Neuroevolution}}},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference}} ({{GECCO}})},
  author = {Koutník, Jan and Driessens, Kurt},
  date = {2016},
  pages = {517--524},
  location = {{Denver, CO, USA}},
  doi = {10.1145/2908812.2908905},
  abstract = {A new indirect scheme for encoding neural network connec-tion weights as sets of wavelet-domain coefficients is pro-posed in this paper. It exploits spatial regularities in the weight-space to reduce the gene-space dimension by con-sidering the low-frequency wavelet coefficients only. The wavelet-based encoding builds on top of a frequency-domain encoding, but unlike when using a Fourier-type transform, it offers gene locality while preserving continuity of the genotype-phenotype mapping. We argue that this added property al-lows for more efficient evolutionary search and demonstrate this on the octopus-arm control task, where superior solu-tions were found in fewer generations. The scalability of the wavelet-based encoding is shown by evolving networks with many parameters to control game-playing agents in the Arcade Learning Environment.},
  isbn = {978-1-4503-4206-3}
}

@misc{krahenbuhl_datadependent_2016,
  title = {Data-Dependent {{Initializations}} of {{Convolutional Neural Networks}}},
  author = {Krähenbühl, Philipp and Doersch, Carl and Donahue, Jeff and Darrell, Trevor},
  date = {2016-09-22},
  eprint = {1511.06856},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1511.06856},
  urldate = {2021-12-19},
  abstract = {Convolutional Neural Networks spread through computer vision like a wildfire, impacting almost all visual tasks imaginable. Despite this, few researchers dare to train their models from scratch. Most work builds on one of a handful of ImageNet pre-trained models, and fine-tunes or adapts these for specific tasks. This is in large part due to the difficulty of properly initializing these networks from scratch. A small miscalibration of the initial weights leads to vanishing or exploding gradients, as well as poor convergence properties. In this work we present a fast and simple data-dependent initialization procedure, that sets the weights of a network such that all units in the network train at roughly the same rate, avoiding vanishing or exploding gradients. Our initialization matches the current state-of-the-art unsupervised or self-supervised pre-training methods on standard computer vision tasks, such as image classification and object detection, while reducing the pre-training time by three orders of magnitude. When combined with pre-training methods, our initialization significantly outperforms prior work, narrowing the gap between supervised and unsupervised pre-training.},
  langid = {english}
}

@article{kramer_nonlinear_1991,
  title = {Nonlinear Principal Component Analysis Using Autoassociative Neural Networks},
  author = {Kramer, Mark A},
  date = {1991},
  journaltitle = {AIChE journal},
  volume = {37},
  number = {2},
  pages = {233--243},
  publisher = {{Wiley Online Library}}
}

@online{kreuk_selfsupervised_2020,
  title = {Self-{{Supervised Contrastive Learning}} for {{Unsupervised Phoneme Segmentation}}},
  author = {Kreuk, Felix and Keshet, Joseph and Adi, Yossi},
  date = {2020-08-06},
  eprint = {2007.13465},
  eprinttype = {arxiv},
  eprintclass = {cs, eess, stat},
  url = {http://arxiv.org/abs/2007.13465},
  urldate = {2023-09-23},
  abstract = {We propose a self-supervised representation learning model for the task of unsupervised phoneme boundary detection. The model is a convolutional neural network that operates directly on the raw waveform. It is optimized to identify spectral changes in the signal using the Noise-Contrastive Estimation principle. At test time, a peak detection algorithm is applied over the model outputs to produce the final boundaries. As such, the proposed model is trained in a fully unsupervised manner with no manual annotations in the form of target boundaries nor phonetic transcriptions. We compare the proposed approach to several unsupervised baselines using both TIMIT and Buckeye corpora. Results suggest that our approach surpasses the baseline models and reaches state-of-the-art performance on both data sets. Furthermore, we experimented with expanding the training set with additional examples from the Librispeech corpus. We evaluated the resulting model on distributions and languages that were not seen during the training phase (English, Hebrew and German) and showed that utilizing additional untranscribed data is beneficial for model performance.},
  pubstate = {preprint}
}

@inproceedings{kreuk_textless_2022,
  title = {Textless Speech Emotion Conversion Using Discrete \& Decomposed Representations},
  booktitle = {Proceedings of the {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Kreuk, Felix and Polyak, Adam and Copet, Jade and Kharitonov, Eugene and Nguyen, Tu Anh and Rivière, Morgane and Hsu, Wei-Ning and Mohamed, Abdelrahman and Dupoux, Emmanuel and Adi, Yossi},
  editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
  date = {2022},
  pages = {11200--11214},
  publisher = {{Association for Computational Linguistics}},
  location = {{Abu Dhabi, United Arab Emirates}},
  url = {https://aclanthology.org/2022.emnlp-main.769}
}

@misc{krishnan_deep_2015,
  title = {Deep {{Kalman Filters}}},
  author = {Krishnan, Rahul G. and Shalit, Uri and Sontag, David},
  date = {2015-11-25},
  eprint = {1511.05121},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1511.05121},
  urldate = {2021-02-19},
  abstract = {Kalman Filters are one of the most influential models of time-varying phenomena. They admit an intuitive probabilistic interpretation, have a simple functional form, and enjoy widespread adoption in a variety of disciplines. Motivated by recent variational methods for learning deep generative models, we introduce a unified algorithm to efficiently learn a broad spectrum of Kalman filters. Of particular interest is the use of temporal generative models for counterfactual inference. We investigate the efficacy of such models for counterfactual inference, and to that end we introduce the “Healing MNIST” dataset where long-term structure, noise and actions are applied to sequences of digits. We show the efficacy of our method for modeling this dataset. We further show how our model can be used for counterfactual inference for patients, based on electronic health record data of 8,000 patients over 4.5 years.},
  langid = {english}
}

@inproceedings{krishnan_structured_2017,
  title = {Structured Inference Networks for Nonlinear State Space Models},
  booktitle = {Proceedings of the Thirty-First {{AAAI}} Conference on Artificial Intelligence},
  author = {Krishnan, Rahul G. and Shalit, Uri and Sontag, David},
  date = {2017},
  series = {{{AAAI}}'17},
  pages = {2101--2109},
  publisher = {{AAAI Press}},
  location = {{San Francisco, California, USA}},
  abstract = {Gaussian state space models have been used for decades as generative models of sequential data. They admit an intuitive probabilistic interpretation, have a simple functional form, and enjoy widespread adoption. We introduce a unified algorithm to efficiently learn a broad class of linear and non-linear state space models, including variants where the emission and transition distributions are modeled by deep neural networks. Our learning algorithm simultaneously learns a compiled inference network and the generative model, leveraging a structured variational approximation parameterized by recurrent neural networks to mimic the posterior distribution. We apply the learning algorithm to both synthetic and real-world datasets, demonstrating its scalability and versatility. We find that using the structured approximation to the posterior results in models with significantly higher held-out likelihood.},
  pagetotal = {9}
}

@article{krizhevsky_imagenet_2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  date = {2012},
  journaltitle = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
  eprint = {7491034},
  eprinttype = {pmid},
  pages = {1--9},
  issn = {10495258},
  doi = {http://dx.doi.org/10.1016/j.protcy.2014.09.007},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  isbn = {9781627480031}
}

@thesis{krizhevsky_learning_2009,
  title = {Learning {{Multiple Layers}} of {{Features}} from {{Tiny Images}}},
  author = {Krizhevsky, Alex},
  date = {2009},
  eprint = {25246403},
  eprinttype = {pmid},
  institution = {{University of Toronto}},
  issn = {1098-6596},
  doi = {10.1.1.222.9220},
  abstract = {Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it difficult to learn a good set of filters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is significantly improved by pre-training a layer of features on a large set of unlabeled tiny images.},
  isbn = {9788578110796},
  pagetotal = {1-60}
}

@article{krizhevsky_one_2014,
  title = {One Weird Trick for Parallelizing Convolutional Neural Networks},
  author = {Krizhevsky, Alex},
  date = {2014},
  abstract = {I present a new way to parallelize the training of convolutional neural networks across multiple GPUs. The method scales significantly better than all alternatives when applied to modern convolutional neural networks.}
}

@article{kuleshov_calibrated_2015,
  title = {Calibrated {{Structured Prediction}}},
  author = {Kuleshov, Volodymyr and Liang, Percy S},
  date = {2015},
  journaltitle = {Advances in Neural Information Processing Systems 28},
  url = {http://papers.nips.cc/paper/5658-calibrated-structured-prediction.pdf},
  urldate = {2018-10-31}
}

@inproceedings{kull_beta_2017,
  title = {Beta Calibration: A Well-Founded and Easily Implemented Improvement on Logistic Calibration for Binary Classifiers},
  booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
  author = {Kull, Meelis and Filho, Telmo Silva and Flach, Peter},
  editor = {Singh, Aarti and Zhu, Jerry},
  date = {2017-04-20/2017-04-22},
  series = {Proceedings of Machine Learning Research},
  volume = {54},
  pages = {623--631},
  publisher = {{PMLR}},
  url = {https://proceedings.mlr.press/v54/kull17a.html},
  abstract = {For optimal decision making under variable class distributions and misclassification costs a classifier needs to produce well-calibrated estimates of the posterior probability. Isotonic calibration is a powerful non-parametric method that is however prone to overfitting on smaller datasets; hence a parametric method based on the logistic curve is commonly used. While logistic calibration is designed for normally distributed per-class scores, we demonstrate experimentally that many classifiers including Naive Bayes and Adaboost suffer from a particular distortion where these score distributions are heavily skewed. In such cases logistic calibration can easily yield probability estimates that are worse than the original scores. Moreover, the logistic curve family does not include the identity function, and hence logistic calibration can easily uncalibrate a perfectly calibrated classifier. In this paper we solve all these problems with a richer class of calibration maps based on the beta distribution. We derive the method from first principles and show that fitting it is as easy as fitting a logistic curve. Extensive experiments show that beta calibration is superior to logistic calibration for Naive Bayes and Adaboost.}
}

@incollection{kull_temperature_2019,
  title = {Beyond Temperature Scaling: {{Obtaining}} Well-Calibrated Multiclass Probabilities with Dirichlet Calibration},
  booktitle = {Proceedings of the 33rd {{International Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Kull, Meelis and Perello-Nieto, Miquel and Kängsepp, Markus and Filho, Telmo Silva and Song, Hao and Flach, Peter},
  date = {2019},
  publisher = {{Curran Associates Inc.}},
  location = {{Red Hook, NY, USA}},
  abstract = {Class probabilities predicted by most multiclass classifiers are uncalibrated, often tending towards over-confidence. With neural networks, calibration can be improved by temperature scaling, a method to learn a single corrective multiplicative factor for inputs to the last softmax layer. On non-neural models the existing methods apply binary calibration in a pairwise or one-vs-rest fashion. We propose a natively multiclass calibration method applicable to classifiers from any model class, derived from Dirichlet distributions and generalising the beta calibration method from binary classification. It is easily implemented with neural nets since it is equivalent to log-transforming the uncalibrated probabilities, followed by one linear layer and softmax. Experiments demonstrate improved probabilistic predictions according to multiple measures (confidence-ECE, classwise-ECE, log-loss, Brier score) across a wide range of datasets and classifiers. Parameters of the learned Dirichlet calibration map provide insights to the biases in the uncalibrated model.}
}

@article{kullback_information_1951,
  title = {On {{Information}} and {{Sufficiency}}},
  author = {Kullback, Solomon and Leibler, R. A.},
  date = {1951},
  journaltitle = {The Annals of Mathematical Statistics},
  volume = {22},
  number = {1},
  eprint = {10896709},
  eprinttype = {pmid},
  pages = {79--86},
  issn = {0003-4851},
  doi = {10.1214/aoms/1177729694},
  url = {http://projecteuclid.org/euclid.aoms/1177729694},
  urldate = {2018-04-21},
  abstract = {This note generalizes to the abstract case Shannon's definition of information.},
  isbn = {00034851}
}

@book{kullback_information_1959,
  title = {Information {{Theory}} and {{Statistics}}},
  author = {Kullback, Solomon},
  date = {1959},
  publisher = {{John Wiley \& Sons}},
  isbn = {0-8446-5625-9},
  annotation = {Republished by Dover Publications in 1968. Reprinted in 1978.}
}

@inproceedings{kumar_ask_2016,
  title = {Ask Me Anything: {{Dynamic}} Memory Networks for Natural Language Processing},
  booktitle = {Proceedings of the 33rd {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Kumar, Ankit and Irsoy, Ozan and Ondruska, Peter and Iyyer, Mohit and Bradbury, James and Gulrajani, Ishaan and Zhong, Victor and Paulus, Romain and Socher, Richard},
  editor = {Balcan, Maria Florina and Weinberger, Kilian Q.},
  date = {2016-06-20/2016-06-22},
  series = {Proceedings of Machine Learning Research},
  volume = {48},
  pages = {1378--1387},
  publisher = {{PMLR}},
  location = {{New York, New York, USA}},
  url = {http://proceedings.mlr.press/v48/kumar16.html},
  abstract = {Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a neural network architecture which processes input sequences and questions, forms episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the inputs and the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The DMN can be trained end-to-end and obtains state-of-the-art results on several types of tasks and datasets: question answering (Facebook’s bAbI dataset), text classification for sentiment analysis (Stanford Sentiment Treebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). The training for these different tasks relies exclusively on trained word vector representations and input-question-answer triplets.}
}

@misc{kurata_language_2017,
  title = {Language {{Modeling}} with {{Highway LSTM}}},
  author = {Kurata, Gakuto and Ramabhadran, Bhuvana and Saon, George and Sethy, Abhinav},
  date = {2017-09-19},
  eprint = {1709.06436},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1709.06436},
  urldate = {2019-09-20},
  abstract = {Language models (LMs) based on Long Short Term Memory (LSTM) have shown good gains in many automatic speech recognition tasks. In this paper, we extend an LSTM by adding highway networks inside an LSTM and use the resulting Highway LSTM (HW-LSTM) model for language modeling. The added highway networks increase the depth in the time dimension. Since a typical LSTM has two internal states, a memory cell and a hidden state, we compare various types of HW-LSTM by adding highway networks onto the memory cell and/or the hidden state. Experimental results on English broadcast news and conversational telephone speech recognition show that the proposed HW-LSTM LM improves speech recognition accuracy on top of a strong LSTM LM baseline. We report 5.1\% and 9.9\% on the Switchboard and CallHome subsets of the Hub5 2000 evaluation, which reaches the best performance numbers reported on these tasks to date.},
  langid = {english}
}

@misc{kuzina_diagnosing_2021,
  title = {Diagnosing {{Vulnerability}} of {{Variational Auto-Encoders}} to {{Adversarial Attacks}}},
  author = {Kuzina, Anna and Welling, Max and Tomczak, Jakub M.},
  date = {2021-03-10},
  eprint = {2103.06701},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2103.06701},
  urldate = {2021-03-15},
  abstract = {In this work, we explore adversarial attacks on the Variational Autoencoders (VAE). We show how to modify data point to obtain a prescribed latent code (supervised attack) or just get a drastically different code (unsupervised attack). We examine the influence of model modifications (β-VAE, NVAE) on the robustness of VAEs and suggest metrics to quantify it.},
  langid = {english}
}

@online{kuzmin_pruning_2023a,
  title = {Pruning vs {{Quantization}}: {{Which}} Is {{Better}}?},
  shorttitle = {Pruning vs {{Quantization}}},
  author = {Kuzmin, Andrey and Nagel, Markus and family=Baalen, given=Mart, prefix=van, useprefix=true and Behboodi, Arash and Blankevoort, Tijmen},
  date = {2023-07-06},
  eprint = {2307.02973},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2307.02973},
  urldate = {2023-10-18},
  abstract = {Neural network pruning and quantization techniques are almost as old as neural networks themselves. However, to date only ad-hoc comparisons between the two have been published. In this paper, we set out to answer the question on which is better: neural network quantization or pruning? By answering this question, we hope to inform design decisions made on neural network hardware going forward. We provide an extensive comparison between the two techniques for compressing deep neural networks. First, we give an analytical comparison of expected quantization and pruning error for general data distributions. Then, we provide lower bounds for the per-layer pruning and quantization error in trained networks, and compare these to empirical error after optimization. Finally, we provide an extensive experimental comparison for training 8 large-scale models on 3 tasks. Our results show that in most cases quantization outperforms pruning. Only in some scenarios with very high compression ratio, pruning might be beneficial from an accuracy standpoint.},
  pubstate = {preprint},
  keywords = {Pruning,Quantisation}
}

@article{lai_kernel_2000,
  title = {Kernel and Nonlinear Canonical Correlation Analysis},
  author = {Lai, Pei Ling and Fyfe, Colin},
  date = {2000},
  journaltitle = {International Journal of Neural Systems},
  shortjournal = {Int. J. Neural Syst.},
  volume = {10},
  number = {5},
  pages = {365--377},
  doi = {10.1142/S012906570000034X}
}

@article{lai_neural_1999,
  title = {A Neural Implementation of Canonical Correlation Analysis},
  author = {Lai, Pei Ling and Fyfe, Colin},
  date = {1999},
  journaltitle = {Neural Networks},
  volume = {12},
  number = {10},
  pages = {1391--1397}
}

@inproceedings{lai_parp_2021,
  title = {{{PARP}}: {{Prune}}, Adjust and Re-Prune for Self-Supervised Speech Recognition},
  booktitle = {Conference on {{Neural Information Processing Systems}} (Neurips)},
  author = {Lai, Cheng-I Jeff and Zhang, Yang and Liu, Alexander H. and Chang, Shiyu and Liao, Yi-Lun and Chuang, Yung-Sung and Qian, Kaizhi and Khurana, Sameer and Cox, David and Glass, James},
  date = {2021}
}

@misc{lai_race_2017,
  title = {{{RACE}}: {{Large-scale ReAding Comprehension Dataset From Examinations}}},
  shorttitle = {{{RACE}}},
  author = {Lai, Guokun and Xie, Qizhe and Liu, Hanxiao and Yang, Yiming and Hovy, Eduard},
  date = {2017-04-15},
  eprint = {1704.04683},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1704.04683},
  urldate = {2019-09-08},
  abstract = {We present RACE, a new dataset for benchmark evaluation of methods in the reading comprehension task. Collected from the English exams for middle and high school Chinese students in the age range between 12 to 18, RACE consists of near 28,000 passages and near 100,000 questions generated by human experts (English instructors), and covers a variety of topics which are carefully designed for evaluating the students' ability in understanding and reasoning. In particular, the proportion of questions that requires reasoning is much larger in RACE than that in other benchmark datasets for reading comprehension, and there is a significant gap between the performance of the state-of-the-art models (43\%) and the ceiling human performance (95\%). We hope this new dataset can serve as a valuable resource for research and evaluation in machine comprehension. The dataset is freely available at http://www.cs.cmu.edu/\textasciitilde glai1/data/race/ and the code is available at https://github.com/qizhex/RACE\_AR\_baselines.},
  langid = {english}
}

@inproceedings{lai_re-examination_2019,
  title = {Re-Examination of the Role of Latent Variables in Sequence Modeling},
  booktitle = {Proceedings of the 32nd {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Lai, Guokun and Dai, Zihang and Yang, Yiming and Yoo, Shinjae},
  date = {2019},
  location = {{Vancouver, BC, Canada}},
  url = {https://proceedings.neurips.cc/paper/2019/hash/d0ac1ed0c5cb9ecbca3d2496ec1ad984-Abstract.html},
  eventtitle = {Neural {{Information Processing Systems}}}
}

@inproceedings{lai_semisupervised_2021,
  title = {Semi-Supervised Spoken Language Understanding via Self-Supervised Speech and Language Model Pretraining},
  booktitle = {Proceedings of the {{International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Lai, Cheng-I and Chuang, Yung-Sung and Lee, Hung-Yi and Li, Shang-Wen and Glass, James},
  date = {2021},
  pages = {7468--7472},
  publisher = {{IEEE}}
}

@misc{lai_stochastic_2018,
  title = {Stochastic {{WaveNet}}: {{A Generative Latent Variable Model}} for {{Sequential Data}}},
  shorttitle = {Stochastic {{WaveNet}}},
  author = {Lai, Guokun and Li, Bohan and Zheng, Guoqing and Yang, Yiming},
  date = {2018-06-15},
  eprint = {1806.06116},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1806.06116},
  urldate = {2021-11-13},
  abstract = {How to model distribution of sequential data, including but not limited to speech and human motions, is an important ongoing research problem. It has been demonstrated that model capacity can be significantly enhanced by introducing stochastic latent variables in the hidden states of recurrent neural networks. Simultaneously, WaveNet, equipped with dilated convolutions, achieves astonishing empirical performance in natural speech generation task. In this paper, we combine the ideas from both stochastic latent variables and dilated convolutions, and propose a new architecture to model sequential data, termed as Stochastic WaveNet, where stochastic latent variables are injected into the WaveNet structure. We argue that Stochastic WaveNet enjoys powerful distribution modeling capacity and the advantage of parallel training from dilated convolutions. In order to efficiently infer the posterior distribution of the latent variables, a novel inference network structure is designed based on the characteristics of WaveNet architecture. State-of-the-art performances on benchmark datasets are obtained by Stochastic WaveNet on natural speech modeling and high quality human handwriting samples can be generated as well.},
  langid = {english}
}

@article{lake_humanlevel_2015,
  title = {Human-Level Concept Learning through Probabilistic Program Induction},
  author = {Lake, Brenden M. and Salakhutdinov, Ruslan and Tenenbaum, Joshua B.},
  date = {2015},
  journaltitle = {Science},
  volume = {350},
  number = {6266},
  pages = {1332--1338},
  issn = {0036-8075},
  doi = {10.1126/science.aab3050},
  url = {https://science.sciencemag.org/content/350/6266/1332},
  abstract = {Not only do children learn effortlessly, they do so quickly and with a remarkable ability to use what they have learned as the raw material for creating new stuff. Lake et al. describe a computational model that learns in a similar fashion and does so better than current deep learning algorithms. The model classifies, parses, and recreates handwritten characters, and can generate new letters of the alphabet that look “right” as judged by Turing-like tests of the model’s output in comparison to what real humans produce.Science, this issue p. 1332People learning new concepts can often generalize successfully from just a single example, yet machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy. People can also use learned concepts in richer ways than conventional algorithms—for action, imagination, and explanation. We present a computational model that captures these human learning abilities for a large class of simple visual concepts: handwritten characters from the world’s alphabets. The model represents concepts as simple programs that best explain observed examples under a Bayesian criterion. On a challenging one-shot classification task, the model achieves human-level performance while outperforming recent deep learning approaches. We also present several “visual Turing tests” probing the model’s creative generalization abilities, which in many cases are indistinguishable from human behavior.}
}

@inproceedings{lakshminarayanan_simple_2017,
  title = {Simple and {{Scalable Predictive Uncertainty Estimation}} Using {{Deep Ensembles}}},
  booktitle = {In {{Proceddings}} of the 31st {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
  date = {2017},
  location = {{Long Beach, CA, USA}},
  url = {http://arxiv.org/abs/1612.01474},
  urldate = {2018-10-31},
  abstract = {Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet.},
  eventtitle = {Conference on {{Neural Information Processing Systems}}}
}

@inproceedings{laleye_first_2016,
  title = {First Automatic Fongbe Continuous Speech Recognition System: {{Development}} of Acoustic Models and Language Models},
  booktitle = {Proceedings of the 2016 {{Federated Conference}} on {{Computer Science}} and {{Information Systems}} ({{FedCSIS}})},
  author = {Laleye, Fréjus A. A. and Besacier, Laurent and Ezin, Eugène C. and Motamed, Cina},
  editor = {Ganzha, Maria and Maciaszek, Leszek A. and Paprzycki, Marcin},
  date = {2016-09},
  series = {Annals of Computer Science and Information Systems},
  volume = {8},
  pages = {477--482},
  publisher = {{IEEE}},
  location = {{Gdańsk, Poland}},
  doi = {10.15439/2016F153},
  url = {https://doi.org/10.15439/2016F153}
}

@article{lamb_professor_2016,
  title = {Professor {{Forcing}}: {{A New Algorithm}} for {{Training Recurrent Networks}}},
  shorttitle = {Professor {{Forcing}}},
  author = {Lamb, Alex and Goyal, Anirudh and Zhang, Ying and Zhang, Saizheng and Courville, Aaron and Bengio, Yoshua},
  date = {2016-10-27},
  url = {https://arxiv.org/abs/1610.09038},
  urldate = {2018-10-29},
  langid = {english}
}

@article{lamel_lightly_2002,
  title = {Lightly Supervised and Unsupervised Acoustic Model Training},
  author = {Lamel, Lori and Gauvain, Jean-Luc and Adda, Gilles},
  date = {2002},
  journaltitle = {Computer Speech \& Language},
  volume = {16},
  number = {1},
  pages = {115--129},
  publisher = {{Elsevier}}
}

@inproceedings{lample_large_2019,
  title = {Large Memory Layers with Product Keys},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32: {{Annual Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Lample, Guillaume and Sablayrolles, Alexandre and Ranzato, Marc'Aurelio and Denoyer, Ludovic and Jégou, Hervé},
  date = {2019},
  pages = {8546--8557},
  location = {{Vancouver, Canada}},
  url = {https://proceedings.neurips.cc/paper/2019/hash/9d8df73a3cfbf3c5b47bc9b50f214aff-Abstract.html}
}

@misc{lample_phrasebased_2018,
  title = {Phrase-{{Based}} \& {{Neural Unsupervised Machine Translation}}},
  author = {Lample, Guillaume and Ott, Myle and Conneau, Alexis and Denoyer, Ludovic and Ranzato, Marc'Aurelio},
  date = {2018},
  eprint = {1804.07755},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1804.07755},
  urldate = {2018-04-25},
  abstract = {Machine translation systems achieve near human-level performance on some languages, yet their effectiveness strongly relies on the availability of large amounts of bitexts, which hinders their applicability to the majority of language pairs. This work investigates how to learn to translate when having access to only large monolingual corpora in each language. We propose two model variants, a neural and a phrase-based model. Both versions leverage automatic generation of parallel data by backtranslating with a backward model operating in the other direction, and the denoising effect of a language model trained on the target side. These models are significantly better than methods from the literature, while being simpler and having fewer hyper-parameters. On the widely used WMT14 English-French and WMT16 German-English benchmarks, our models respectively obtain 27.1 and 23.6 BLEU points without using a single parallel sentence, outperforming the state of the art by more than 11 BLEU points.}
}

@article{lample_unsupervised_2018,
  title = {Unsupervised Machine Translation Using Monolingual Corpora Only},
  author = {Lample, Guillaume and Conneau, Alexis and Denoyer, Ludovic and Ranzato, Marc'Aurelio},
  date = {2018},
  journaltitle = {International Conference on Learning Representations (ICLR)}
}

@online{landsberg_border_2006,
  title = {The Border Rank of the Multiplication of Two by Two Matrices Is Seven},
  author = {Landsberg, J. M.},
  date = {2006-04-05},
  eprint = {math/0407224},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/math/0407224},
  urldate = {2023-07-11},
  abstract = {I show that 2x2 matrices cannot be approximately multiplied using less than seven multiplications. In other words I show that the matrix multiplication operator is not in the sixth secant variety of the Segre produt of three \$P\^3\$'s.},
  langid = {english},
  pubstate = {preprint}
}

@inproceedings{laptev_you_2020,
  title = {You Do Not Need More Data: {{Improving}} End-to-End Speech Recognition by Text-to-Speech Data Augmentation},
  booktitle = {Proceedings of the {{International Congress}} on {{Image}} and {{Signal Processing}}, {{Biomedical Engineering}} and {{Informatics}} ({{CISP-BMEI}})},
  author = {Laptev, Aleksandr and Korostik, Roman and Svischev, Aleksey and Andrusenko, Andrei and Medennikov, Ivan and Rybin, Sergey},
  date = {2020},
  pages = {439--444}
}

@inproceedings{larochelle_classification_2008,
  title = {Classification Using Discriminative Restricted {{Boltzmann}} Machines},
  booktitle = {Proceedings of the 25th International Conference on {{Machine}} Learning - {{ICML}} '08},
  author = {Larochelle, Hugo and Bengio, Yoshua},
  date = {2008},
  pages = {536--543},
  publisher = {{ACM Press}},
  location = {{Helsinki, Finland}},
  doi = {10/b95gx6},
  url = {http://portal.acm.org/citation.cfm?doid=1390156.1390224},
  urldate = {2020-01-16},
  abstract = {Recently, many applications for Restricted Boltzmann Machines (RBMs) have been developed for a large variety of learning problems. However, RBMs are usually used as feature extractors for another learning algorithm or to provide a good initialization for deep feed-forward neural network classifiers, and are not considered as a standalone solution to classification problems. In this paper, we argue that RBMs provide a self-contained framework for deriving competitive non-linear classifiers. We present an evaluation of different learning algorithms for RBMs which aim at introducing a discriminative component to RBM training and improve their performance as classifiers. This approach is simple in that RBMs are used directly to build a classifier, rather than as a stepping stone. Finally, we demonstrate how discriminative RBMs can also be successfully employed in a semi-supervised setting.},
  eventtitle = {The 25th International Conference},
  isbn = {978-1-60558-205-4},
  langid = {english}
}

@inproceedings{larochelle_neural_2011,
  title = {The {{Neural Autoregressive Distribution Estimator}}},
  booktitle = {Proceedings of the 14th {{International Conference}} on {{Artiﬁcial Intelligence}} and {{Statistics}} ({{AISTATS}})},
  author = {Larochelle, Hugo and Murray, Iain},
  date = {2011},
  volume = {15},
  pages = {9},
  publisher = {{Journal of Machine Learning}},
  location = {{Fort Lauderdale, FL, USA}},
  abstract = {We describe a new approach for modeling the distribution of high-dimensional vectors of discrete variables. This model is inspired by the restricted Boltzmann machine (RBM), which has been shown to be a powerful model of such distributions. However, an RBM typically does not provide a tractable distribution estimator, since evaluating the probability it assigns to some given observation requires the computation of the so-called partition function, which itself is intractable for RBMs of even moderate size. Our model circumvents this difficulty by decomposing the joint distribution of observations into tractable conditional distributions and modeling each conditional using a non-linear function similar to a conditional of an RBM. Our model can also be interpreted as an autoencoder wired such that its output can be used to assign valid probabilities to observations. We show that this new model outperforms other multivariate binary distribution estimators on several datasets and performs similarly to a large (but intractable) RBM.},
  eventtitle = {International {{Conference}} on {{Artiﬁcial Intelligence}} and {{Statistics}}},
  langid = {english}
}

@misc{larsen_autoencoding_2016,
  title = {Autoencoding {{Beyond Pixels Using}} a {{Learned Similarity Metric}}},
  author = {Larsen, Anders Boesen Lindbo and Sønderby, Søren Kaae and Larochelle, Hugo and Winther, Ole},
  date = {2016-02-10},
  eprint = {1512.09300},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1512.09300},
  urldate = {2020-12-01},
  abstract = {We present an autoencoder that leverages learned representations to better measure similarities in data space. By combining a variational autoencoder with a generative adversarial network we can use learned feature representations in the GAN discriminator as basis for the VAE reconstruction objective. Thereby, we replace element-wise errors with feature-wise errors to better capture the data distribution while offering invariance towards e.g. translation. We apply our method to images of faces and show that it outperforms VAEs with element-wise similarity measures in terms of visual fidelity. Moreover, we show that the method learns an embedding in which high-level abstract visual features (e.g. wearing glasses) can be modified using simple arithmetic.},
  langid = {english}
}

@online{latif_deep_2021,
  title = {Deep Representation Learning in Speech Processing: {{Challenges}}, Recent Advances, and Future Trends},
  author = {Latif, Siddique and Rana, Rajib and Khalifa, Sara and Jurdak, Raja and Qadir, Junaid and Schuller, Björn W.},
  date = {2021},
  eprint = {2001.00378},
  eprinttype = {arxiv},
  pubstate = {preprint}
}

@unpublished{latif_variational_2020,
  title = {Variational {{Autoencoders}} for {{Learning Latent Representations}} of {{Speech Emotion}}: {{A Preliminary Study}}},
  shorttitle = {Variational {{Autoencoders}} for {{Learning Latent Representations}} of {{Speech Emotion}}},
  author = {Latif, Siddique and Rana, Rajib and Qadir, Junaid and Epps, Julien},
  date = {2020-07-27},
  eprint = {1712.08708},
  eprinttype = {arxiv},
  eprintclass = {cs, eess, stat},
  url = {http://arxiv.org/abs/1712.08708},
  urldate = {2021-11-16},
  abstract = {Learning the latent representation of data in unsupervised fashion is a very interesting process that provides relevant features for enhancing the performance of a classifier. For speech emotion recognition tasks, generating effective features is crucial. Currently, handcrafted features are mostly used for speech emotion recognition, however, features learned automatically using deep learning have shown strong success in many problems, especially in image processing. In particular, deep generative models such as Variational Autoencoders (VAEs) have gained enormous success for generating features for natural images. Inspired by this, we propose VAEs for deriving the latent representation of speech signals and use this representation to classify emotions. To the best of our knowledge, we are the first to propose VAEs for speech emotion classification. Evaluations on the IEMOCAP dataset demonstrate that features learned by VAEs can produce state-of-the-art results for speech emotion classification.}
}

@article{lawrence_gaussian_2004,
  title = {Gaussian {{Process Latent Variable Models}} for {{Visualisation}} of {{High Dimensional Data}}},
  author = {Lawrence, Neil D.},
  date = {2004},
  journaltitle = {Proceedings of the Conference on Neural Information Processing Systems (NeurIPS)},
  abstract = {In this paper we introduce a new underlying probabilistic model for prin-cipal component analysis (PCA). Our formulation interprets PCA as a particular Gaussian process prior on a mapping from a latent space to the observed data-space. We show that if the prior's covariance func-tion constrains the mappings to be linear the model is equivalent to PCA, we then extend the model by considering less restrictive covariance functions which allow non-linear mappings. This more general Gaussian pro-cess latent variable model (GPLVM) is then evaluated as an approach to the visualisation of high dimensional data for three different data-sets. Additionally our non-linear algorithm can be further kernelised leading to 'twin kernel PCA' in which a mapping between feature spaces occurs.}
}

@article{lawrence_probabilistic_2005,
  title = {Probabilistic {{Non-linear Principal Component Analysis}} with {{Gaussian Process Latent Variable Models}}},
  author = {Lawrence, Neil},
  date = {2005},
  pages = {34},
  abstract = {Summarising a high dimensional data set with a low dimensional embedding is a standard approach for exploring its structure. In this paper we provide an overview of some existing techniques for discovering such embeddings. We then introduce a novel probabilistic interpretation of principal component analysis (PCA) that we term dual probabilistic PCA (DPPCA). The DPPCA model has the additional advantage that the linear mappings from the embedded space can easily be nonlinearised through Gaussian processes. We refer to this model as a Gaussian process latent variable model (GP-LVM). Through analysis of the GP-LVM objective function, we relate the model to popular spectral techniques such as kernel PCA and multidimensional scaling. We then review a practical algorithm for GP-LVMs in the context of large data sets and develop it to also handle discrete valued data and missing attributes. We demonstrate the model on a range of real-world and artificially generated data sets.},
  langid = {english}
}

@inproceedings{le_autoencoding_2018,
  title = {Auto-{{Encoding Sequential Monte Carlo}}},
  booktitle = {Proceedings of the 6th {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Le, Tuan Anh and Igl, Maximilian and Rainforth, Tom and Jin, Tom and Wood, Frank},
  date = {2018},
  publisher = {{OpenReview.net}},
  location = {{Vancouver, Canada}},
  url = {https://openreview.net/forum?id=BJ8c3f-0b},
  eventtitle = {International {{Conference}} on {{Learning Representations}}}
}

@inproceedings{le_building_2013,
  title = {Building High-Level Features Using Large Scale Unsupervised Learning},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Le, Quoc V.},
  date = {2013},
  pages = {8595--8598}
}

@inproceedings{le_distributed_2014,
  title = {Distributed {{Representations}} of {{Sentences}} and {{Documents}}.},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Le, Quoc V. and Mikolov, Tomas},
  date = {2014},
  volume = {14},
  pages = {1188--1196}
}

@misc{le_nonautoregressive_2020,
  title = {Non-{{Autoregressive Dialog State Tracking}}},
  author = {Le, Hung and Socher, Richard and Hoi, Steven C. H.},
  date = {2020-02-19},
  eprint = {2002.08024},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2002.08024},
  urldate = {2020-02-25},
  abstract = {Recent efforts in Dialogue State Tracking (DST) for task-oriented dialogues have progressed toward open-vocabulary or generation-based approaches where the models can generate slot value candidates from the dialogue history itself. These approaches have shown good performance gain, especially in complicated dialogue domains with dynamic slot values. However, they fall short in two aspects: (1) they do not allow models to explicitly learn signals across domains and slots to detect potential dependencies among (domain, slot) pairs; and (2) existing models follow auto-regressive approaches which incur high time cost when the dialogue evolves over multiple domains and multiple turns. In this paper, we propose a novel framework of Non-Autoregressive Dialog State Tracking (NADST) which can factor in potential dependencies among domains and slots to optimize the models towards better prediction of dialogue states as a complete set rather than separate slots. In particular, the non-autoregressive nature of our method not only enables decoding in parallel to significantly reduce the latency of DST for realtime dialogue response generation, but also detect dependencies among slots at token level in addition to slot and domain level. Our empirical results show that our model achieves the state-of-the-art joint accuracy across all domains on the MultiWOZ 2.1 corpus, and the latency of our model is an order of magnitude lower than the previous state of the art as the dialogue history extends over time.},
  langid = {english}
}

@article{lecun_backpropagation_1989,
  title = {Backpropagation {{Applied}} to {{Handwritten Zip Code Recognition}}},
  author = {LeCun, Yann A. and Boser, B. and Denker, John S. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
  date = {1989},
  journaltitle = {Neural Computation},
  number = {1},
  pages = {541--551},
  abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.}
}

@article{lecun_deep_2015,
  title = {Deep {{Learning}}},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  date = {2015},
  journaltitle = {Nature Publishing Group, UK, London},
  volume = {521},
  number = {7553},
  pages = {436--444}
}

@article{lecun_efficient_2012,
  title = {Efficient Backprop},
  author = {LeCun, Yann A. and Bottou, Léon and Orr, Genevieve B. and Müller, Klaus Robert},
  date = {2012},
  journaltitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  volume = {7700 LECTU},
  eprint = {15003161},
  eprinttype = {pmid},
  pages = {9--48},
  issn = {03029743},
  doi = {10.1007/978-3-642-35289-8-3},
  abstract = {The convergence of back-propagation learning is analyzed so as to explain common phenomenon observedb y practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposedin serious technical publications. This paper gives some of those tricks, ando.ers explanations of why they work. Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most “classical” second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations.},
  isbn = {9783642352881}
}

@article{lecun_gradientbased_1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {LeCun, Yann A. and Bottou, Léon and Bengio, Yoshua and Haffner, Patrick},
  date = {1998},
  journaltitle = {Proceedings of the IEEE},
  volume = {86},
  number = {11},
  pages = {2278--2323},
  abstract = {Multilayer neural networks trained with the back-propagation\textbackslash nalgorithm constitute the best example of a successful gradient based\textbackslash nlearning technique. Given an appropriate network architecture,\textbackslash ngradient-based learning algorithms can be used to synthesize a complex\textbackslash ndecision surface that can classify high-dimensional patterns, such as\textbackslash nhandwritten characters, with minimal preprocessing. This paper reviews\textbackslash nvarious methods applied to handwritten character recognition and\textbackslash ncompares them on a standard handwritten digit recognition task.\textbackslash nConvolutional neural networks, which are specifically designed to deal\textbackslash nwith the variability of 2D shapes, are shown to outperform all other\textbackslash ntechniques. Real-life document recognition systems are composed of\textbackslash nmultiple modules including field extraction, segmentation recognition,\textbackslash nand language modeling. A new learning paradigm, called graph transformer\textbackslash nnetworks (GTN), allows such multimodule systems to be trained globally\textbackslash nusing gradient-based methods so as to minimize an overall performance\textbackslash nmeasure. Two systems for online handwriting recognition are described.\textbackslash nExperiments demonstrate the advantage of global training, and the\textbackslash nflexibility of graph transformer networks. A graph transformer network\textbackslash nfor reading a bank cheque is also described. It uses convolutional\textbackslash nneural network character recognizers combined with global training\textbackslash ntechniques to provide record accuracy on business and personal cheques.\textbackslash nIt is deployed commercially and reads several million cheques per day\textbackslash n}
}

@article{lecun_handwritten_1989,
  title = {Handwritten {{Digit Recognition}}: {{Applications}} of {{Neural Network Chips}} and {{Automatic Learning}}},
  author = {LeCun, Yann A. and Jackel, L. D. and Boser, B. and Denker, J. S. and Graf, H. P. and Guyon, I. and Henderson, D. and Howard, R. E. and Hubbard, W.},
  date = {1989},
  journaltitle = {IEEE Communications Magazine},
  volume = {27},
  number = {11},
  pages = {41--46},
  doi = {10.1109/35.41400},
  url = {http://ieeexplore.ieee.org/document/41400/},
  abstract = {Two novel methods for achieving handwritten digit recognition are described. The first method is based on a neural network chip that performs line thinning and feature extraction using local template matching. The second method is implemented on a digital signal processor and makes extensive use of constrained automatic learning. Experimental results obtained using isolated handwritten digits taken from postal zip codes, a rather difficult data set, are reported and discussed.}
}

@inproceedings{lecun_learning_2004,
  title = {Learning Methods for Generic Object Recognition with Invariance to Pose and Lighting},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {LeCun, Y. and Huang, F. and Bottou, L.},
  date = {2004},
  volume = {2},
  pages = {II-104 Vol.2},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})}
}

@article{lecun_mnist_1998,
  title = {The {{MNIST}} Database of Handwritten Digits},
  author = {LeCun, Yann},
  date = {1998},
  journaltitle = {http://yann. lecun. com/exdb/mnist/}
}

@article{lecun_modeles_1987,
  title = {Modeles Connexionnistes de l'apprentissage},
  author = {Lecun, Yann and Soulie Fogelman, Francoise},
  date = {1987-01},
  journaltitle = {Intellectica, special issue apprentissage et machine},
  volume = {2},
  doi = {10.3406/intel.1987.1804}
}

@article{lecun_tutorial_2006,
  title = {A Tutorial on Energy-Based Learning},
  author = {LeCun, Yann and Chopra, Sumit and Hadsell, Raia and Ranzato, M and Huang, Fujie},
  date = {2006},
  journaltitle = {Predicting structured data},
  volume = {1},
  number = {0}
}

@inproceedings{lecuyer_randomized_2016,
  title = {Randomized {{Quasi-Monte Carlo}}: {{An Introduction}} for {{Practitioners}}},
  booktitle = {International {{Conference}} on {{Monte Carlo}} and {{Quasi-Monte Carlo Methods}} in {{Scientific Computing}} ({{MCQMC}})},
  author = {L'Ecuyer, Pierre},
  date = {2016},
  url = {https://hal.inria.fr/hal-01561550/document},
  urldate = {2018-06-26},
  abstract = {We survey basic ideas and results on randomized quasi-Monte Carlo (RQMC) methods, discuss their practical aspects, and give numerical illustrations. RQMC can improve accuracy compared with standard Monte Carlo (MC) when estimating an integral interpreted as a mathematical expectation. RQMC estimators are unbiased and their variance converges at a faster rate (under certain conditions) than MC estimators, as a function of the sample size. Variants of RQMC also work for the simulation of Markov chains, for function approximation and optimization, for solving partial differential equations, etc. In this introductory survey, we look at how RQMC point sets and sequences are constructed, how we measure their uniformity, why they can work for high-dimensional integrals, and how can they work when simulating Markov chains over a large number of steps.}
}

@inproceedings{lee_avicar_2004,
  title = {{{AVICAR}}: {{Audio-visual}} Speech Corpus in a Car Environment},
  booktitle = {Eighth {{International Conference}} on {{Spoken Language Processing}}},
  author = {Lee, Bowon and Hasegawa-Johnson, Mark and Goudeseune, Camille and Kamdar, Suketu and Borys, Sarah and Liu, Ming and Huang, Thomas},
  date = {2004}
}

@article{lee_bidirectional_2021,
  title = {Bidirectional {{Variational Inference}} for {{Non-Autoregressive Text-to-Speech}}},
  author = {Lee, Yoonhyung and Shin, Joongbo and Jung, Kyomin},
  date = {2021},
  pages = {19},
  abstract = {Although early text-to-speech (TTS) models such as Tacotron 2 have succeeded in generating human-like speech, their autoregressive architectures have several limitations: (1) They require a lot of time to generate a mel-spectrogram consisting of hundreds of steps. (2) The autoregressive speech generation lacks robustness due to its error propagation property. In this paper, we propose a novel non-autoregressive TTS model called BVAE-TTS, which eliminates the architectural limitations and generates a mel-spectrogram in parallel. BVAE-TTS adopts a bidirectional-inference variational autoencoder (BVAE) that learns hierarchical latent representations using both bottom-up and top-down paths to increase its expressiveness. To apply BVAE to TTS, we design our model to utilize text information via an attention mechanism. By using attention maps that BVAE-TTS generates, we train a duration predictor so that the model uses the predicted duration of each phoneme at inference. In experiments conducted on LJSpeech dataset, we show that our model generates a mel-spectrogram 27 times faster than Tacotron 2 with similar speech quality. Furthermore, our BVAE-TTS outperforms GlowTTS, which is one of the state-of-the-art non-autoregressive TTS models, in terms of both speech quality and inference speed while having 58\% fewer parameters.},
  langid = {english}
}

@article{lee_efficient_2006,
  title = {Efficient {{Sparse}} Coding Algorithms},
  author = {Lee, Honglak and Battle, Alexis and Raina, Rajat and Ng, Andrew Y.},
  date = {2006},
  journaltitle = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
  volume = {19},
  number = {2},
  eprint = {17051527},
  eprinttype = {pmid},
  pages = {801--808},
  issn = {10495258},
  doi = {10.1.1.69.2112},
  abstract = {Sparse coding provides a class of algorithms for finding succinct representations of stimuli; given only unlabeled input data, it discovers basis functions that cap- ture higher-level features in the data. However, finding sparse codes remains a very difficult computational problem. In this paper, we present efficient sparse coding algorithms that are based on iteratively solving two convex optimization problems: an L1-regularized least squares problem and an L2-constrained least squares problem. We propose novel algorithms to solve both of these optimiza- tion problems. Our algorithms result in a significant speedup for sparse coding, allowing us to learn larger sparse codes than possible with previously described algorithms. We apply these algorithms to natural images and demonstrate that the inferred sparse codes exhibit end-stopping and non-classical receptive field sur- round suppression and, therefore, may provide a partial explanation for these two phenomena in V1 neurons. 1},
  isbn = {0262195682}
}

@misc{lee_endtoend_2017,
  title = {End-to-End {{Neural Coreference Resolution}}},
  author = {Lee, Kenton and He, Luheng and Lewis, Mike and Zettlemoyer, Luke},
  date = {2017},
  eprint = {1707.07045},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1707.07045},
  urldate = {2018-05-02},
  abstract = {We introduce the first end-to-end coreference resolution model and show that it significantly outperforms all previous work without using a syntactic parser or hand-engineered mention detector. The key idea is to directly consider all spans in a document as potential mentions and learn distributions over possible antecedents for each. The model computes span embeddings that combine context-dependent boundary representations with a head-finding attention mechanism. It is trained to maximize the marginal likelihood of gold antecedent spans from coreference clusters and is factored to enable aggressive pruning of potential mentions. Experiments demonstrate state-of-the-art performance, with a gain of 1.5 F1 on the OntoNotes benchmark and by 3.1 F1 using a 5-model ensemble, despite the fact that this is the first approach to be successfully trained with no external resources.}
}

@misc{lee_fully_2016,
  title = {Fully {{Character-Level Neural Machine Translation}} without {{Explicit Segmentation}}},
  author = {Lee, Jason and Cho, Kyunghyun and Hofmann, Thomas},
  date = {2016-03-19},
  eprint = {1610.03017},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1610.03017},
  urldate = {2018-03-18},
  abstract = {Most existing machine translation systems operate at the level of words, relying on explicit segmentation to extract tokens. We introduce a neural machine translation (NMT) model that maps a source character sequence to a target character sequence without any segmentation. We employ a character-level convolutional network with max-pooling at the encoder to reduce the length of source representation, allowing the model to be trained at a speed comparable to subword-level models while capturing local regularities. Our character-to-character model outperforms a recently proposed baseline with a subword-level encoder on WMT'15 DE-EN and CS-EN, and gives comparable performance on FI-EN and RU-EN. We then demonstrate that it is possible to share a single character-level encoder across multiple languages by training a model on a many-to-one translation task. In this multilingual setting, the character-level encoder significantly outperforms the subword-level encoder on all the language pairs. We observe that on CS-EN, FI-EN and RU-EN, the quality of the multilingual character-level translation even surpasses the models specifically trained on that language pair alone, both in terms of BLEU score and human judgment.},
  isbn = {9781510827585}
}

@article{lee_learning_1999,
  title = {Learning the Parts of Objects by Non-Negative Matrix Factorization},
  author = {Lee, Daniel D and Seung, H Sebastian},
  date = {1999},
  journaltitle = {Nature},
  volume = {401},
  number = {6755},
  pages = {788--791}
}

@inproceedings{lee_nonparametric_2012,
  title = {A {{Nonparametric Bayesian Approach}} to {{Acoustic Model Discovery}}},
  booktitle = {Proceedings of the 50th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Lee, Chia-ying and Glass, James},
  date = {2012-07},
  pages = {40--49},
  publisher = {{Association for Computational Linguistics}},
  location = {{Jeju Island, Korea}},
  url = {https://aclanthology.org/P12-1005},
  urldate = {2021-10-26},
  eventtitle = {{{ACL}} 2012}
}

@inproceedings{lee_set_2019,
  title = {Set {{Transformer}}: {{A Framework}} for {{Attention-based Permutation-Invariant Neural Networks}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Lee, Juho and Lee, Yoonho and Kim, Jungtaek and Kosiorek, Adam R and Choi, Seungjin and Teh, Yee Whye},
  date = {2019-07},
  pages = {10},
  location = {{Long Beach, CA, USA}},
  abstract = {Many machine learning tasks such as multiple instance learning, 3D shape recognition and fewshot image classification are defined on sets of instances. Since solutions to such problems do not depend on the order of elements of the set, models used to address them should be permutation invariant. We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce computational complexity, we introduce an attention scheme inspired by inducing point methods from sparse Gaussian process literature. It reduces computation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating increased performance compared to recent methods for set-structured data.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english}
}

@inproceedings{lee_simple_2018,
  title = {A {{Simple Unified Framework}} for {{Detecting Out-of-Distribution Samples}} and {{Adversarial Attacks}}},
  booktitle = {Proceedings of the 32nd {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Lee, Kimin and Lee, Kibok and Lee, Honglak and Shin, Jinwoo},
  date = {2018},
  pages = {11},
  location = {{Montréal, Quebec, Canada}},
  url = {https://papers.nips.cc/paper/2018/file/abdeb6f575ac5c6676b747bca8d09cc2-Paper.pdf},
  abstract = {Detecting test samples drawn sufficiently far away from the training distribution statistically or adversarially is a fundamental requirement for deploying a good classifier in many real-world machine learning applications. However, deep neural networks with the softmax classifier are known to produce highly overconfident posterior distributions even for such abnormal samples. In this paper, we propose a simple yet effective method for detecting any abnormal samples, which is applicable to any pre-trained softmax neural classifier. We obtain the class conditional Gaussian distributions with respect to (low- and upper-level) features of the deep models under Gaussian discriminant analysis, which result in a confidence score based on the Mahalanobis distance. While most prior methods have been evaluated for detecting either out-of-distribution or adversarial samples, but not both, the proposed method achieves the state-of-the-art performances for both cases in our experiments. Moreover, we found that our proposed method is more robust in harsh cases, e.g., when the training dataset has noisy labels or small number of samples. Finally, we show that the proposed method enjoys broader usage by applying it to class-incremental learning: whenever out-of-distribution samples are detected, our classification rule can incorporate new classes well without further training deep models.},
  eventtitle = {Conference on {{Neural Information Processing Systems}}}
}

@inproceedings{lee_textless_2022,
  title = {Textless Speech-to-Speech Translation on Real Data},
  booktitle = {Proceedings of the {{Annual Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}} ({{NAACL}})},
  author = {Lee, Ann and Gong, Hongyu and Duquenne, Paul-Ambroise and Schwenk, Holger and Chen, Peng-Jen and Wang, Changhan and Popuri, Sravya and Adi, Yossi and Pino, Juan Miguel and Gu, Jiatao and Hsu, Wei-Ning},
  editor = {Carpuat, Marine and family=Marneffe, given=Marie-Catherine, prefix=de, useprefix=true and Ruíz, Iván Vladimir Meza},
  date = {2022},
  pages = {860--872},
  publisher = {{Association for Computational Linguistics}},
  location = {{Seattle, WA, USA}},
  doi = {10.18653/v1/2022.naacl-main.63},
  url = {https://doi.org/10.18653/v1/2022.naacl-main.63}
}

@inproceedings{lee_training_2018,
  title = {Training {{Confidence-calibrated Classifiers}} for {{Detecting Out-of-Distribution Samples}}},
  booktitle = {Proceedings of the 6th {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Lee, Kimin and Lee, Honglak and Lee, Kibok and Shin, Jinwoo},
  date = {2018},
  url = {https://openreview.net/forum?id=ryiAv2xAZ},
  abstract = {The problem of detecting whether a test sample is from in-distribution (i.e., training distribution by a classifier) or out-of-distribution sufficiently different from it arises in many real-world machine learning applications. However, the state-of-art deep neural networks are known to be highly overconfident in their predictions, i.e., do not distinguish in- and out-of-distributions. Recently, to handle this issue, several threshold-based detectors have been proposed given pre-trained neural classifiers. However, the performance of prior works highly depends on how to train the classifiers since they only focus on improving inference procedures. In this paper, we develop a novel training method for classifiers so that such inference algorithms can work better. In particular, we suggest two additional terms added to the original loss (e.g., cross entropy). The first one forces samples from out-of-distribution less confident by the classifier and the second one is for (implicitly) generating most effective training samples for the first one. In essence, our method jointly trains both classification and generative neural networks for out-of-distribution. We demonstrate its effectiveness using deep convolutional neural networks on various popular image datasets.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  annotation = {http://arxiv.org/abs/1711.09325}
}

@inproceedings{lee_unsupervised_2009,
  title = {Unsupervised Feature Learning for Audio Classification Using Convolutional Deep Belief Networks},
  booktitle = {Proceedings of the 22nd {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Lee, Honglak and Largman, Yan and Pham, Peter and Ng, Andrew Y.},
  date = {2009},
  series = {{{NIPS}}'09},
  pages = {1096--1104},
  location = {{Vancouver, BC, Canada}},
  abstract = {In recent years, deep learning approaches have gained significant interest as a way of building hierarchical representations from unlabeled data. However, to our knowledge, these deep learning approaches have not been extensively studied for auditory data. In this paper, we apply convolutional deep belief networks to audio data and empirically evaluate them on various audio classification tasks. In the case of speech data, we show that the learned features correspond to phones/phonemes. In addition, our feature representations learned from unlabeled audio data show very good performance for multiple audio classification tasks. We hope that this paper will inspire more research on deep learning approaches applied to a wide range of audio recognition tasks.},
  eventtitle = {International {{Conference}} on {{Neural Information Processing Systems}}},
  isbn = {978-1-61567-911-9}
}

@article{lee_wide_2020,
  title = {Wide {{Neural Networks}} of {{Any Depth Evolve}} as {{Linear Models Under Gradient Descent}}},
  author = {Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel S. and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  date = {2020-12-22},
  journaltitle = {Journal of Statistical Mechanics: Theory and Experiment},
  shortjournal = {J. Stat. Mech.},
  volume = {2020},
  number = {12},
  eprint = {1902.06720},
  eprinttype = {arxiv},
  pages = {124002},
  issn = {1742-5468},
  doi = {10.1088/1742-5468/abc62b},
  url = {http://arxiv.org/abs/1902.06720},
  urldate = {2021-05-07},
  abstract = {A longstanding goal in deep learning research has been to precisely characterize training and generalization. However, the often complex loss landscapes of neural networks have made a theory of learning dynamics elusive. In this work, we show that for wide neural networks the learning dynamics simplify considerably and that, in the infinite width limit, they are governed by a linear model obtained from the first-order Taylor expansion of the network around its initial parameters. Furthermore, mirroring the correspondence between wide Bayesian neural networks and Gaussian processes, gradient-based training of wide neural networks with a squared loss produces test set predictions drawn from a Gaussian process with a particular compositional kernel. While these theoretical results are only exact in the infinite width limit, we nevertheless find excellent empirical agreement between the predictions of the original network and those of the linearized version even for finite practically-sized networks. This agreement is robust across different architectures, optimization methods, and loss functions.},
  langid = {english}
}

@misc{lee-thorp_fnet_2021,
  title = {{{FNet}}: {{Mixing Tokens}} with {{Fourier Transforms}}},
  shorttitle = {{{FNet}}},
  author = {Lee-Thorp, James and Ainslie, Joshua and Eckstein, Ilya and Ontanon, Santiago},
  date = {2021-05-08},
  eprint = {2105.03824},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2105.03824},
  urldate = {2021-05-18},
  abstract = {We show that Transformer encoder architectures can be massively sped up, with limited accuracy costs, by replacing the self-attention sublayers with simple linear transformations that “mix” input tokens. These linear transformations, along with simple nonlinearities in feed-forward layers, are sufficient to model semantic relationships in several text classification tasks. Perhaps most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder with a standard, unparameterized Fourier Transform achieves 92\% of the accuracy of BERT on the GLUE benchmark, but pre-trains and runs up to seven times faster on GPUs and twice as fast on TPUs. The resulting model, which we name FNet, scales very efficiently to long inputs, matching the accuracy of the most accurate “efficient” Transformers on the Long Range Arena benchmark, but training and running faster across all sequence lengths on GPUs and relatively shorter sequence lengths on TPUs. Finally, FNet has a light memory footprint and is particularly efficient at smaller model sizes: for a fixed speed and accuracy budget, small FNet models outperform Transformer counterparts.},
  langid = {english}
}

@article{leeOpenaccessMIMICIIDatabase2011,
  title = {Open-Access {{MIMIC-II Database}} for {{Intensive Care Research}}},
  author = {Lee, Joon and Scott, Daniel J. and Villarroel, Mauricio and Clifford, Gari D. and Saeed, Mohammed and Mark, Roger G.},
  date = {2011},
  journaltitle = {Conference proceedings : ... Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual Conference},
  volume = {2011},
  eprint = {22256274},
  eprinttype = {pmid},
  pages = {8315--8318},
  issn = {1557-170X},
  doi = {10.1109/IEMBS.2011.6092050},
  abstract = {The critical state of intensive care unit (ICU) patients demands close monitoring, and as a result a large volume of multi-parameter data is collected continuously. This represents a unique opportunity for researchers interested in clinical data mining. We sought to foster a more transparent and efficient intensive care research community by building a publicly available ICU database, namely Multiparameter Intelligent Monitoring in Intensive Care II (MIMIC-II). The data harnessed in MIMIC-II were collected from the ICUs of Beth Israel Deaconess Medical Center from 2001 to 2008 and represent 26,870 adult hospital admissions (version 2.6). MIMIC-II consists of two major components: clinical data and physiological waveforms. The clinical data, which include patient demographics, intravenous medication drip rates, and laboratory test results, were organized into a relational database. The physiological waveforms, including 125 Hz signals recorded at bedside and corresponding vital signs, were stored in an open-source format. MIMIC-II data were also deidentified in order to remove protected health information. Any interested researcher can gain access to MIMIC-II free of charge after signing a data use agreement and completing human subjects training. MIMIC-II can support a wide variety of research studies, ranging from the development of clinical decision support algorithms to retrospective clinical studies. We anticipate that MIMIC-II will be an invaluable resource for intensive care research by stimulating fair comparisons among different studies.},
  pmcid = {PMC6339457}
}

@article{legerstee_infants_1990,
  title = {Infants Use Multimodal Information to Imitate Speech Sounds},
  author = {Legerstee, Maria},
  date = {1990},
  journaltitle = {Infant Behavior and Development},
  volume = {13},
  number = {3},
  pages = {343--354}
}

@misc{lehman_es_2017,
  title = {{{ES Is More Than Just}} a {{Traditional Finite-Difference Approximator}}},
  author = {Lehman, Joel and Chen, Jay and Clune, Jeff and Stanley, Kenneth O.},
  date = {2017},
  eprint = {1712.06568},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1712.06568},
  urldate = {2018-01-03},
  abstract = {An evolution strategy (ES) variant recently attracted significant attention due to its surprisingly good performance at optimizing neural networks in challenging deep reinforcement learning domains. It searches directly in the parameter space of neural networks by generating perturbations to the current set of parameters, checking their performance, and moving in the direction of higher reward. The resemblance of this algorithm to a traditional finite-difference approximation of the reward gradient in parameter space naturally leads to the assumption that it is just that. However, this assumption is incorrect. The aim of this paper is to definitively demonstrate this point empirically. ES is a gradient approximator, but optimizes for a different gradient than just reward (especially when the magnitude of candidate perturbations is high). Instead, it optimizes for the average reward of the entire population, often also promoting parameters that are robust to perturbation. This difference can channel ES into significantly different areas of the search space than gradient descent in parameter space, and also consequently to networks with significantly different properties. This unique robustness-seeking property, and its consequences for optimization, are demonstrated in several domains. They include humanoid locomotion, where networks from policy gradient-based reinforcement learning are far less robust to parameter perturbation than ES-based policies that solve the same task. While the implications of such robustness and robustness-seeking remain open to further study, the main contribution of this work is to highlight that such differences indeed exist and deserve attention.}
}

@misc{lehman_safe_2017,
  title = {Safe {{Mutations}} for {{Deep}} and {{Recurrent Neural Networks}} through {{Output Gradients}}},
  author = {Lehman, Joel and Chen, Jay and Clune, Jeff and Stanley, Kenneth O.},
  date = {2017},
  eprint = {1712.06563},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1712.06563},
  urldate = {2018-01-03},
  abstract = {While neuroevolution (evolving neural networks) has a successful track record across a variety of domains from reinforcement learning to artificial life, it is rarely applied to large, deep neural networks. A central reason is that while random mutation generally works in low dimensions, a random perturbation of thousands or millions of weights is likely to break existing functionality, providing no learning signal even if some individual weight changes were beneficial. This paper proposes a solution by introducing a family of safe mutation (SM) operators that aim within the mutation operator itself to find a degree of change that does not alter network behavior too much, but still facilitates exploration. Importantly, these SM operators do not require any additional interactions with the environment. The most effective SM variant capitalizes on the intriguing opportunity to scale the degree of mutation of each individual weight according to the sensitivity of the network's outputs to that weight, which requires computing the gradient of outputs with respect to the weights (instead of the gradient of error, as in conventional deep learning). This safe mutation through gradients (SM-G) operator dramatically increases the ability of a simple genetic algorithm-based neuroevolution method to find solutions in high-dimensional domains that require deep and/or recurrent neural networks (which tend to be particularly brittle to mutation), including domains that require processing raw pixels. By improving our ability to evolve deep neural networks, this new safer approach to mutation expands the scope of domains amenable to neuroevolution.}
}

@misc{lei_rationalizing_2016,
  title = {Rationalizing {{Neural Predictions}}},
  author = {Lei, Tao and Barzilay, Regina and Jaakkola, Tommi},
  date = {2016-11-02},
  eprint = {1606.04155},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1606.04155},
  urldate = {2021-04-19},
  abstract = {Prediction without justification has limited applicability. As a remedy, we learn to extract pieces of input text as justifications -- rationales -- that are tailored to be short and coherent, yet sufficient for making the same prediction. Our approach combines two modular components, generator and encoder, which are trained to operate well together. The generator specifies a distribution over text fragments as candidate rationales and these are passed through the encoder for prediction. Rationales are never given during training. Instead, the model is regularized by desiderata for rationales. We evaluate the approach on multi-aspect sentiment analysis against manually annotated test cases. Our approach outperforms attention-based baseline by a significant margin. We also successfully illustrate the method on the question retrieval task.},
  langid = {english}
}

@incollection{lemarechal_nondifferentiable_1989,
  title = {Nondifferentiable Optimization},
  booktitle = {Handbooks in {{Operations Research}} and {{Management Science}}},
  author = {Lemaréchal, Claude},
  date = {1989},
  volume = {1},
  number = {C},
  pages = {529--572},
  issn = {09270507},
  doi = {10.1016/S0927-0507(89)01008-X},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S092705078901008X},
  urldate = {2018-03-23},
  abstract = {This chapter discusses the nondifferentiable optimization (NDO). Nondifferentiable optimization or nonsmooth optimization (NSO) deals with the situations in operations research where a function that fails to have derivatives for some values of the variables has to be optimized. For this situation, new tools are required to replace standard differential calculus, and these new tools come from convex analysis. Functions with discontinuous derivatives are frequent in operations research. Sometimes they arise when modeling the problem, sometimes they are introduced artificially during the solution procedure. The chapter discusses the necessary concepts and the basic properties and some examples of practical problems motivating the use of NSO. It is shown how and why classical methods fail. The chapter also discusses some possibilities that can be used when a special structure exists in the nonsmooth problem. It also presents subgradient methods and more recent methods and also covers some orientations for future research. © 1989 Elsevier Science Publishers B.V.},
  isbn = {978-0-444-87284-5}
}

@book{lemonte_gradient_2016,
  title = {The {{Gradient Test}}: {{Another Likelihood-Based Test}}},
  author = {Lemonte, Artur},
  date = {2016},
  publisher = {{Academic Press}}
}

@article{lenz_deep_2015,
  title = {Deep Learning for Detecting Robotic Grasps},
  author = {Lenz, Ian and Lee, Honglak and Saxena, Ashutosh},
  date = {2015},
  journaltitle = {The International Journal of Robotics Research},
  volume = {34},
  number = {4-5},
  pages = {705--724},
  publisher = {{SAGE Publications Sage UK: London, England}}
}

@online{lepikhin_gshard_2020,
  title = {{{GShard}}: {{Scaling Giant Models}} with {{Conditional Computation}} and {{Automatic Sharding}}},
  shorttitle = {{{GShard}}},
  author = {Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng},
  date = {2020-06-30},
  eprint = {2006.16668},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2006.16668},
  urldate = {2022-10-10},
  abstract = {Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art.},
  pubstate = {preprint}
}

@article{leung_machine_2016,
  title = {Machine Learning in Genomic Medicine: {{A}} Review of Computational Problems and Data Sets},
  author = {Leung, Michael K. K. and Delong, Andrew and Alipanahi, Babak and Frey, Brendan J.},
  date = {2016},
  journaltitle = {Proceedings of the IEEE},
  volume = {104},
  number = {1},
  issn = {00189219},
  doi = {10.1109/JPROC.2015.2494198},
  abstract = {In this paper, we provide an introduction to machine learning tasks that address important problems in genomic medicine. One of the goals of genomic medicine is to determine how variations in the DNA of individuals can affect the risk of different diseases, and to find causal explanations so that targeted therapies can be designed. Here we focus on how machine learning can help to model the relationship between DNA and the quantities of key molecules in the cell, with the premise that these quantities, which we refer to as cell variables, may be associated with disease risks. Modern biology allows high-throughput measurement of many such cell variables, including gene expression, splicing, and proteins binding to nucleic acids, which can all be treated as training targets for predictive models. With the growing availability of large-scale data sets and advanced computational techniques such as deep learning, researchers can help to usher in a new era of effective genomic medicine.},
  isbn = {0018-9219 VO - 104}
}

@article{leveson_investigation_1993,
  title = {An Investigation of the {{Therac-25}} Accidents},
  author = {Leveson, Nancy G and Turner, Clark S},
  date = {1993},
  journaltitle = {Computer},
  volume = {26},
  number = {7},
  pages = {18--41},
  publisher = {{IEEE}}
}

@inproceedings{levin_fixeddimensional_2013,
  title = {Fixed-Dimensional Acoustic Embeddings of Variable-Length Segments in Low-Resource Settings},
  booktitle = {Proceedings of the {{IEEE Workshop}} on {{Automatic Speech Recognition}} and {{Understanding}} ({{ASRU}})},
  author = {Levin, Keith and Henry, Katharine and Jansen, Aren and Livescu, Karen},
  date = {2013-12},
  pages = {410--415},
  doi = {10.1109/ASRU.2013.6707765},
  abstract = {Measures of acoustic similarity between words or other units are critical for segmental exemplar-based acoustic models, spoken term discovery, and query-by-example search. Dynamic time warping (DTW) alignment cost has been the most commonly used measure, but it has well-known inadequacies. Some recently proposed alternatives require large amounts of training data. In the interest of finding more efficient, accurate, and low-resource alternatives, we consider the problem of embedding speech segments of arbitrary length into fixed-dimensional spaces in which simple distances (such as cosine or Euclidean) serve as a proxy for linguistically meaningful (phonetic, lexical, etc.) dissimilarities. Such embeddings would enable efficient audio indexing and permit application of standard distance learning techniques to segmental acoustic modeling. In this paper, we explore several supervised and unsupervised approaches to this problem and evaluate them on an acoustic word discrimination task. We identify several embedding algorithms that match or improve upon the DTW baseline in low-resource settings.}
}

@inproceedings{levin_segmental_2015,
  title = {Segmental Acoustic Indexing for Zero Resource Keyword Search},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Levin, Keith and Jansen, Aren and Van Durme, Benjamin},
  date = {2015-04},
  pages = {5828--5832},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2015.7179089},
  abstract = {The task of zero resource query-by-example keyword search has received much attention in recent years as the speech technology needs of the developing world grow. These systems traditionally rely upon dynamic time warping (DTW) based retrieval algorithms with runtimes that are linear in the size of the search collection. As a result, their scalability substantially lags that of their supervised counterparts, which take advantage of efficient word-based indices. In this paper, we present a novel audio indexing approach called Segmental Randomized Acoustic Indexing and Logarithmic-time Search (S-RAILS). S-RAILS generalizes the original frame-based RAILS methodology to word-scale segments by exploiting a recently proposed acoustic segment embedding technique. By indexing word-scale segments directly, we avoid higher cost frame-based processing of RAILS while taking advantage of the improved lexical discrimination of the embeddings. Using the same conversational telephone speech benchmark, we demonstrate major improvements in both speed and accuracy over the original RAILS system.},
  eventtitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})}
}

@article{levine_end--end_2016,
  title = {End-to-End Training of Deep Visuomotor Policies},
  author = {Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
  date = {2016},
  journaltitle = {Journal of Machine Learning Research},
  volume = {17},
  number = {39},
  pages = {1--40}
}

@misc{levine_learning_2016,
  title = {Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection},
  author = {Levine, Sergey and Pastor, Peter and Krizhevsky, Alex and Quillen, Deirdre},
  date = {2016},
  eprint = {1603.02199},
  eprinttype = {arxiv},
  url = {https://arxiv.org/abs/1603.02199}
}

@inproceedings{lewis_sequential_1995,
  title = {A Sequential Algorithm for Training Text Classifiers: {{Corrigendum}} and Additional Data},
  booktitle = {Acm Sigir Forum},
  author = {Lewis, David D},
  date = {1995},
  volume = {29},
  number = {2},
  pages = {13--19},
  publisher = {{ACM New York, NY, USA}}
}

@misc{li_advancing_2018,
  title = {Advancing {{Acoustic-to-Word CTC Model}}},
  author = {Li, Jinyu and Ye, Guoli and Das, Amit and Zhao, Rui and Gong, Yifan},
  date = {2018},
  eprint = {1803.05566},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1803.05566},
  abstract = {The acoustic-to-word model based on the connectionist temporal classification (CTC) criterion was shown as a natural end-to-end (E2E) model directly targeting words as output units. However, the word-based CTC model suffers from the out-of-vocabulary (OOV) issue as it can only model limited number of words in the output layer and maps all the remaining words into an OOV output node. Hence, such a word-based CTC model can only recognize the frequent words modeled by the network output nodes. Our first attempt to improve the acoustic-to-word model is a hybrid CTC model which consults a letter-based CTC when the word-based CTC model emits OOV tokens during testing time. Then, we propose a much better solution by training a mixed-unit CTC model which decomposes all the OOV words into sequences of frequent words and multi-letter units. Evaluated on a 3400 hours Microsoft Cortana voice assistant task, the final acoustic-to-word solution improves the baseline word-based CTC by relative 12.09\% word error rate (WER) reduction when combined with our proposed attention CTC. Such an E2E model without using any language model (LM) or complex decoder outperforms the traditional context-dependent phoneme CTC which has strong LM and decoder by relative 6.79\%.},
  isbn = {9781538646588}
}

@online{li_anomaly_2019,
  title = {Anomaly {{Detection}} with {{Generative Adversarial Networks}} for {{Multivariate Time Series}}},
  author = {Li, Dan and Chen, Dacheng and Goh, Jonathan and Ng, See-kiong},
  date = {2019-01-15},
  eprint = {1809.04758},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1809.04758},
  urldate = {2023-09-17},
  abstract = {Today's Cyber-Physical Systems (CPSs) are large, complex, and affixed with networked sensors and actuators that are targets for cyber-attacks. Conventional detection techniques are unable to deal with the increasingly dynamic and complex nature of the CPSs. On the other hand, the networked sensors and actuators generate large amounts of data streams that can be continuously monitored for intrusion events. Unsupervised machine learning techniques can be used to model the system behaviour and classify deviant behaviours as possible attacks. In this work, we proposed a novel Generative Adversarial Networks-based Anomaly Detection (GAN-AD) method for such complex networked CPSs. We used LSTM-RNN in our GAN to capture the distribution of the multivariate time series of the sensors and actuators under normal working conditions of a CPS. Instead of treating each sensor's and actuator's time series independently, we model the time series of multiple sensors and actuators in the CPS concurrently to take into account of potential latent interactions between them. To exploit both the generator and the discriminator of our GAN, we deployed the GAN-trained discriminator together with the residuals between generator-reconstructed data and the actual samples to detect possible anomalies in the complex CPS. We used our GAN-AD to distinguish abnormal attacked situations from normal working conditions for a complex six-stage Secure Water Treatment (SWaT) system. Experimental results showed that the proposed strategy is effective in identifying anomalies caused by various attacks with high detection rate and low false positive rate as compared to existing methods.},
  pubstate = {preprint}
}

@article{li_applications_2012,
  title = {Applications of {{Bayesian}} Methods in Wind Energy Conversion Systems},
  author = {Li, Gong and Shi, Jing},
  date = {2012},
  journaltitle = {Renewable Energy},
  volume = {43},
  pages = {1--8},
  doi = {10.1016/j.renene.2011.12.006},
  abstract = {The fast growth of wind power is in urgent need of more accurate, reliable, and adaptive modeling and data analysis methods for the characterization and prediction of wind resource and wind power, as well as reliability evaluation of wind energy conversion systems. Bayesian methods have shown unique advantages in statistical modeling and data analysis for the quantity of interest with uncertainty and variability. The adoption of Bayesian methods carries great potentials for various aspects in wind energy conversion systems such as improving the accuracy and reliability of wind resource estimation and short-term forecasts. This paper summarizes the basic theories of several Bayesian methods, and extensively reviews the literature addressing the applications of Bayesian methods in wind energy conversion systems. Based on the state-of-the-art review, the prospects of Bayesian methods in wind energy conversion systems are discussed on how to develop new applications and enhance the methods for existing applications. It is believed that Bayesian methods will be gaining more momentum in wind energy applications in the near future.}
}

@inproceedings{li_deep_2016,
  title = {Deep {{Reinforcement Learning}} for {{Dialogue Generation}}},
  booktitle = {Proceedings of the {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Li, Jiwei and Monroe, Will and Ritter, Alan and Jurafsky, Dan and Galley, Michel and Gao, Jianfeng},
  date = {2016},
  eprint = {1606.01541},
  eprinttype = {arxiv},
  pages = {1192--1202},
  issn = {24699969},
  doi = {10.18653/v1/D16-1127},
  url = {http://aclweb.org/anthology/D16-1127},
  urldate = {2018-04-24},
  abstract = {Recent neural models of dialogue generation offer great promise for generating responses for conversational agents, but tend to be shortsighted, predicting utterances one at a time while ignoring their influence on future outcomes. Modeling the future direction of a dialogue is crucial to generating coherent, interesting dialogues, a need which led traditional NLP models of dialogue to draw on reinforcement learning. In this paper, we show how to integrate these goals, applying deep reinforcement learning to model future reward in chatbot dialogue. The model simulates dialogues between two virtual agents, using policy gradient methods to reward sequences that display three useful conversational properties: informativity (non-repetitive turns), coherence, and ease of answering (related to forward-looking function). We evaluate our model on diversity, length as well as with human judges, showing that the proposed algorithm generates more interactive responses and manages to foster a more sustained conversation in dialogue simulation. This work marks a first step towards learning a neural conversational model based on the long-term success of dialogues.},
  isbn = {978-1-5090-6182-2}
}

@misc{li_deep_2017,
  title = {Deep {{Reinforcement Learning}}: {{An Overview}}},
  author = {Li, Yuxi},
  date = {2017-01-25},
  eprint = {15040217},
  eprinttype = {pmid},
  url = {http://arxiv.org/abs/1701.07274},
  urldate = {2017-10-23},
  abstract = {We give an overview of recent exciting achievements of deep reinforcement learning (RL). We discuss six core elements, six important mechanisms, and twelve applications. We start with background of machine learning, deep learning and reinforcement learning. Next we discuss core RL elements, including value function, in particular, Deep Q-Network (DQN), policy, reward, model, planning, and exploration. After that, we discuss important mechanisms for RL, including attention and memory, unsupervised learning, transfer learning, multi-agent RL, hierarchical RL, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, natural language processing, including dialogue systems, machine translation, and text generation, computer vision, neural architecture design, business management, finance, healthcare, Industry 4.0, smart grid, intelligent transportation systems, and computer systems. We mention topics not reviewed yet, and list a collection of RL resources. After presenting a brief summary, we close with discussions.},
  isbn = {9781509011216}
}

@online{li_efficientformer_2022,
  title = {{{EfficientFormer}}: {{Vision Transformers}} at {{MobileNet Speed}}},
  shorttitle = {{{EfficientFormer}}},
  author = {Li, Yanyu and Yuan, Geng and Wen, Yang and Hu, Eric and Evangelidis, Georgios and Tulyakov, Sergey and Wang, Yanzhi and Ren, Jian},
  date = {2022-06-02},
  eprint = {2206.01191},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2206.01191},
  urldate = {2022-06-08},
  abstract = {Vision Transformers (ViT) have shown rapid progress in computer vision tasks, achieving promising results on various benchmarks. However, due to the massive number of parameters and model design, e.g., attention mechanism, ViT-based models are generally times slower than lightweight convolutional networks. Therefore, the deployment of ViT for real-time applications is particularly challenging, especially on resource-constrained hardware such as mobile devices. Recent efforts try to reduce the computation complexity of ViT through network architecture search or hybrid design with MobileNet block, yet the inference speed is still unsatisfactory. This leads to an important question: can transformers run as fast as MobileNet while obtaining high performance? To answer this, we first revisit the network architecture and operators used in ViT-based models and identify inefficient designs. Then we introduce a dimension-consistent pure transformer (without MobileNet blocks) as design paradigm. Finally, we perform latency-driven slimming to get a series of final models dubbed EfficientFormer. Extensive experiments show the superiority of EfficientFormer in performance and speed on mobile devices. Our fastest model, EfficientFormer-L1, achieves 79.2\% top-1 accuracy on ImageNet-1K with only 1.6 ms inference latency on iPhone 12 (compiled with CoreML), which is even a bit faster than MobileNetV2 (1.7 ms, 71.8\% top-1), and our largest model, EfficientFormer-L7, obtains 83.3\% accuracy with only 7.0 ms latency. Our work proves that properly designed transformers can reach extremely low latency on mobile devices while maintaining high performance},
  pubstate = {preprint}
}

@inproceedings{li_improving_2020,
  title = {Improving {{Transformer-Based Speech Recognition}} with {{Unsupervised Pre-Training}} and {{Multi-Task Semantic Knowledge Learning}}},
  booktitle = {Proceedings of the 21st {{Annual Conference}} of the {{International Speech Communication Association}} ({{Interspeech}})},
  author = {Li, Song and Li, Lin and Hong, Qingyang and Liu, Lingling},
  date = {2020-10-25},
  pages = {5006--5010},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2020-2007},
  url = {https://www.isca-speech.org/archive/interspeech_2020/li20na_interspeech.html},
  urldate = {2021-10-12},
  abstract = {Recently, the Transformer-based end-to-end speech recognition system has become a state-of-the-art technology. However, one prominent problem with current end-to-end speech recognition systems is that an extensive amount of paired data are required to achieve better recognition performance. In order to grapple with such an issue, we propose two unsupervised pre-training strategies for the encoder and the decoder of Transformer respectively, which make full use of unpaired data for training. In addition, we propose a new semi-supervised fine-tuning method named multi-task semantic knowledge learning to strengthen the Transformer’s ability to learn about semantic knowledge, thereby improving the system performance. We achieve the best CER with our proposed methods on AISHELL-1 test set: 5.9\%, which exceeds the best end-to-end model by 10.6\% relative CER. Moreover, relative CER reduction of 20.3\% and 17.8\% are obtained for low-resource Mandarin and English data sets, respectively.},
  langid = {english}
}

@misc{li_jasper_2019,
  title = {Jasper: {{An End-to-End Convolutional Neural Acoustic Model}}},
  shorttitle = {Jasper},
  author = {Li, Jason and Lavrukhin, Vitaly and Ginsburg, Boris and Leary, Ryan and Kuchaiev, Oleksii and Cohen, Jonathan M. and Nguyen, Huyen and Gadde, Ravi Teja},
  date = {2019-04-05},
  eprint = {1904.03288},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1904.03288},
  urldate = {2019-06-18},
  abstract = {In this paper, we report state-of-the-art results on LibriSpeech among end-to-end speech recognition models without any external training data. Our model, Jasper, uses only 1D convolutions, batch normalization, ReLU, dropout, and residual connections. To improve training, we further introduce a new layer-wise optimizer called NovoGrad. Through experiments, we demonstrate that the proposed deep architecture performs as well or better than more complex choices. Our deepest Jasper variant uses 54 convolutional layers. With this architecture, we achieve 2.95\% WER using beam-search decoder with an external neural language model and 3.86\% WER with a greedy decoder on LibriSpeech test-clean. We also report competitive results on the Wall Street Journal and the Hub5'00 conversational evaluation datasets.}
}

@inproceedings{li_learning_2016,
  title = {Learning to Generate with Memory},
  booktitle = {Proceedings of the 33nd {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Li, Chongxuan and Zhu, Jun and Zhang, Bo},
  date = {2016},
  pages = {1177--1186},
  location = {{New York, NY, USA}},
  url = {http://proceedings.mlr.press/v48/lie16.html}
}

@inproceedings{li_madgan_2019,
  title = {{{MAD-GAN}}: {{Multivariate}} Anomaly Detection for Time Series Data with Generative Adversarial Networks},
  booktitle = {Proceedings of the {{International Conference}} on {{Artificial Neural Networks}} ({{ICANN}})},
  author = {Li, Dan and Chen, Dacheng and Jin, Baihong and Shi, Lei and Goh, Jonathan and Ng, See-Kiong},
  editor = {Tetko, Igor V. and Kurková, Vera and Karpov, Pavel and Theis, Fabian J.},
  date = {2019},
  volume = {11730},
  pages = {703--716},
  publisher = {{Springer}},
  location = {{Munich, Germany}},
  doi = {10.1007/978-3-030-30490-4\_56},
  eventtitle = {International {{Conference}} on {{Artificial Neural Networks}}}
}

@inproceedings{li_measuring_2018,
  title = {Measuring the {{Intrinsic Dimension}} of {{Objective Landscapes}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Li, Chunyuan and Farkhoor, Heerad and Liu, Rosanne and Yosinski, Jason},
  date = {2018},
  eprint = {1804.08838},
  eprinttype = {arxiv},
  location = {{Vancouver, Canada}},
  url = {http://arxiv.org/abs/1804.08838},
  abstract = {Many recently trained neural networks employ large numbers of parameters to achieve good performance. One may intuitively use the number of parameters required as a rough gauge of the difficulty of a problem. But how accurate are such notions? How many parameters are really needed? In this paper we attempt to answer this question by training networks not in their native parameter space, but instead in a smaller, randomly oriented subspace. We slowly increase the dimension of this subspace, note at which dimension solutions first appear, and define this to be the intrinsic dimension of the objective landscape. The approach is simple to implement, computationally tractable, and produces several suggestive conclusions. Many problems have smaller intrinsic dimensions than one might suspect, and the intrinsic dimension for a given dataset varies little across a family of models with vastly different sizes. This latter result has the profound implication that once a parameter space is large enough to solve a problem, extra parameters serve directly to increase the dimensionality of the solution manifold. Intrinsic dimension allows some quantitative comparison of problem difficulty across supervised, reinforcement, and other types of learning where we conclude, for example, that solving the inverted pendulum problem is 100 times easier than classifying digits from MNIST, and playing Atari Pong from pixels is about as hard as classifying CIFAR-10. In addition to providing new cartography of the objective landscapes wandered by parameterized models, the method is a simple technique for constructively obtaining an upper bound on the minimum description length of a solution. A byproduct of this construction is a simple approach for compressing networks, in some cases by more than 100 times.},
  keywords = {★}
}

@inproceedings{li_outofdistribution_,
  title = {Out-of-{{Distribution Detection}} with {{An Adaptive Likelihood Ratio}} on {{Informative Hierarchical VAE}}},
  booktitle = {Proceedings of the 36th {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Li, Yewen and Wang, Chaojie and Xia, Xiaobo and Liu, Tongliang and Miao, Xin and An, Bo},
  pages = {28},
  url = {https://personal.ntu.edu.sg/boan/papers/NeurIPS_22_OOD.pdf},
  abstract = {Unsupervised out-of-distribution (OOD) detection is essential for the reliability of machine learning. In the literature, existing work has shown that higher-level semantics captured by hierarchical VAEs can be used to detect OOD instances. However, we empirically show that, the inheirt “posterior collapse” of hierarchical VAEs would seriously limit their capacity for OOD detection. Based on a thorough analysis, we propose an informative hierarchical VAE to alleviate this issue through enhancing the connections between the data sample and its multi-layer stochastic latent representations during training. Furthermore, we propose a novel score function for unsupervised OOD detection, referred to as Adaptive Likelihood Ratio, which can selectively aggregate the semantic information on multiple hidden layers of hierarchical VAEs, leading to a strong separability between in-distribution and OOD samples. Experimental results demonstrate that our method can significantly outperform existing state-of-the-art unsupervised OOD detection approaches.},
  eventtitle = {Conference on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  langid = {english}
}

@misc{li_renyi_2016,
  title = {Rényi {{Divergence Variational Inference}}},
  author = {Li, Yingzhen and Turner, Richard E.},
  date = {2016},
  eprint = {1602.02311},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1602.02311},
  urldate = {2018-04-23},
  abstract = {This paper introduces the variational R\textbackslash 'enyi bound (VR) that extends traditional variational inference to R\textbackslash 'enyi's alpha-divergences. This new family of variational methods unifies a number of existing approaches, and enables a smooth interpolation from the evidence lower-bound to the log (marginal) likelihood that is controlled by the value of alpha that parametrises the divergence. The reparameterization trick, Monte Carlo approximation and stochastic optimisation methods are deployed to obtain a tractable and unified framework for optimisation. We further consider negative alpha values and propose a novel variational inference method as a new special case in the proposed framework. Experiments on Bayesian neural networks and variational auto-encoders demonstrate the wide applicability of the VR bound.}
}

@online{li_training_2018,
  title = {Training {{Neural Speech Recognition Systems}} with {{Synthetic Speech Augmentation}}},
  author = {Li, Jason and Gadde, Ravi and Ginsburg, Boris and Lavrukhin, Vitaly},
  date = {2018-11-01},
  eprint = {1811.00707},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1811.00707},
  urldate = {2023-04-12},
  abstract = {Building an accurate automatic speech recognition (ASR) system requires a large dataset that contains many hours of labeled speech samples produced by a diverse set of speakers. The lack of such open free datasets is one of the main issues preventing advancements in ASR research. To address this problem, we propose to augment a natural speech dataset with synthetic speech. We train very large end-to-end neural speech recognition models using the LibriSpeech dataset augmented with synthetic speech. These new models achieve state of the art Word Error Rate (WER) for character-level based models without an external language model.},
  pubstate = {preprint}
}

@misc{li_understanding_2018,
  title = {Understanding the {{Disharmony}} between {{Dropout}} and {{Batch Normalization}} by {{Variance Shift}}},
  author = {Li, Xiang and Chen, Shuo and Hu, Xiaolin and Yang, Jian},
  date = {2018},
  eprint = {1801.05134},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1801.05134},
  urldate = {2018-04-06},
  abstract = {This paper first answers the question "why do the two most powerful techniques Dropout and Batch Normalization (BN) often lead to a worse performance when they are combined together?" in both theoretical and statistical aspects. Theoretically, we find that Dropout would shift the variance of a specific neural unit when we transfer the state of that network from train to test. However, BN would maintain its statistical variance, which is accumulated from the entire learning procedure, in the test phase. The inconsistency of that variance (we name this scheme as "variance shift") causes the unstable numerical behavior in inference that leads to more erroneous predictions finally, when applying Dropout before BN. Thorough experiments on DenseNet, ResNet, ResNeXt and Wide ResNet confirm our findings. According to the uncovered mechanism, we next explore several strategies that modifies Dropout and try to overcome the limitations of their combination by avoiding the variance shift risks.}
}

@inproceedings{liang_enhancing_2018,
  title = {Enhancing the Reliability of Out-of-Distribution Image Detection in Neural Networks},
  booktitle = {Proceedings of the 6th {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Liang, Shiyu and Li, Yixuan and Srikant, R.},
  date = {2018},
  location = {{Vancouver, Canada}},
  url = {https://openreview.net/forum?id=H1VGkIxRZ},
  eventtitle = {International {{Conference}} on {{Learning Representations}}}
}

@misc{liang_learning_2018,
  title = {Learning {{Noise-Invariant Representations}} for {{Robust Speech Recognition}}},
  author = {Liang, Davis and Huang, Zhiheng and Lipton, Zachary C.},
  date = {2018-07-17},
  eprint = {1807.06610},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1807.06610},
  urldate = {2018-09-01},
  abstract = {Despite rapid advances in speech recognition, current models remain brittle to superficial perturbations to their inputs. Small amounts of noise can destroy the performance of an otherwise state-of-the-art model. To harden models against background noise, practitioners often perform data augmentation, adding artificially-noised examples to the training set, carrying over the original label. In this paper, we hypothesize that a clean example and its superficially perturbed counterparts shouldn't merely map to the same class --- they should map to the same representation. We propose invariant-representation-learning (IRL): At each training iteration, for each training example,we sample a noisy counterpart. We then apply a penalty term to coerce matched representations at each layer (above some chosen layer). Our key results, demonstrated on the Librispeech dataset are the following: (i) IRL significantly reduces character error rates (CER) on both 'clean' (3.3\% vs 6.5\%) and 'other' (11.0\% vs 18.1\%) test sets; (ii) on several out-of-domain noise settings (different from those seen during training), IRL's benefits are even more pronounced. Careful ablations confirm that our results are not simply due to shrinking activations at the chosen layers.}
}

@misc{liang_learning_2019,
  title = {Learning {{Representations}} from {{Imperfect Time Series Data}} via {{Tensor Rank Regularization}}},
  author = {Liang, Paul Pu and Liu, Zhun and Tsai, Yao-Hung Hubert and Zhao, Qibin and Salakhutdinov, Ruslan and Morency, Louis-Philippe},
  date = {2019-07-01},
  eprint = {1907.01011},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1907.01011},
  urldate = {2019-11-18},
  abstract = {There has been an increased interest in multimodal language processing including multimodal dialog, question answering, sentiment analysis, and speech recognition. However, naturally occurring multimodal data is often imperfect as a result of imperfect modalities, missing entries or noise corruption. To address these concerns, we present a regularization method based on tensor rank minimization. Our method is based on the observation that high-dimensional multimodal time series data often exhibit correlations across time and modalities which leads to low-rank tensor representations. However, the presence of noise or incomplete values breaks these correlations and results in tensor representations of higher rank. We design a model to learn such tensor representations and effectively regularize their rank. Experiments on multimodal language data show that our model achieves good results across various levels of imperfection.},
  langid = {english}
}

@online{liang_not_2022,
  title = {Not {{All Patches}} Are {{What You Need}}: {{Expediting Vision Transformers}} via {{Token Reorganizations}}},
  shorttitle = {Not {{All Patches}} Are {{What You Need}}},
  author = {Liang, Youwei and Ge, Chongjian and Tong, Zhan and Song, Yibing and Wang, Jue and Xie, Pengtao},
  date = {2022-04-13},
  eprint = {2202.07800},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2202.07800},
  urldate = {2022-10-08},
  abstract = {Vision Transformers (ViTs) take all the image patches as tokens and construct multi-head self-attention (MHSA) among them. Complete leverage of these image tokens brings redundant computations since not all the tokens are attentive in MHSA. Examples include that tokens containing semantically meaningless or distractive image backgrounds do not positively contribute to the ViT predictions. In this work, we propose to reorganize image tokens during the feed-forward process of ViT models, which is integrated into ViT during training. For each forward inference, we identify the attentive image tokens between MHSA and FFN (i.e., feed-forward network) modules, which is guided by the corresponding class token attention. Then, we reorganize image tokens by preserving attentive image tokens and fusing inattentive ones to expedite subsequent MHSA and FFN computations. To this end, our method EViT improves ViTs from two perspectives. First, under the same amount of input image tokens, our method reduces MHSA and FFN computation for efficient inference. For instance, the inference speed of DeiT-S is increased by 50\% while its recognition accuracy is decreased by only 0.3\% for ImageNet classification. Second, by maintaining the same computational cost, our method empowers ViTs to take more image tokens as input for recognition accuracy improvement, where the image tokens are from higher resolution images. An example is that we improve the recognition accuracy of DeiT-S by 1\% for ImageNet classification at the same computational cost of a vanilla DeiT-S. Meanwhile, our method does not introduce more parameters to ViTs. Experiments on the standard benchmarks show the effectiveness of our method. The code is available at https://github.com/youweiliang/evit},
  pubstate = {preprint},
  keywords = {Medium}
}

@article{liao_sharpening_2017,
  title = {Sharpening {{Jensen}}'s {{Inequality}}},
  author = {Liao, J. and Berg, Arthur},
  date = {2017-07},
  journaltitle = {The American Statistician},
  volume = {73},
  eprint = {1707.08644},
  eprinttype = {arxiv},
  doi = {10.1080/00031305.2017.1419145}
}

@inproceedings{lievin_hierarchical_2019,
  title = {Towards {{Hierarchical Discrete Variational Autoencoders}}},
  booktitle = {2nd {{Symposium}} on {{Advances}} in {{Approximate Bayesian Inference}}},
  author = {Lievin, Valentin and Dittadi, Andrea and Maaløe, Lars and Winther, Ole},
  date = {2019},
  pages = {16},
  abstract = {Variational Autoencoders (VAEs) have proven to be powerful latent variable models. However, the form of the approximate posterior can limit the expressiveness of the model. Categorical distributions are flexible and useful building blocks for example in neural memory layers. We introduce the Hierarchical Discrete Variational Autoencoder (HD-VAE): a hierarchy of variational memory layers. The Concrete/Gumbel-Softmax relaxation allows maximizing a surrogate of the Evidence Lower Bound by stochastic gradient ascent. We show that, when using a limited number of latent variables, HD-VAE outperforms the Gaussian baseline on modelling multiple binary image datasets. Training very deep HD-VAE remains a challenge due to the relaxation bias that is induced by the use of a surrogate objective. We introduce a formal definition and conduct a preliminary theoretical and empirical study of the bias.},
  langid = {english}
}

@misc{lievin_optimal_2020,
  title = {Optimal {{Variance Control}} of the {{Score Function Gradient Estimator}} for {{Importance Weighted Bounds}}},
  author = {Liévin, Valentin and Dittadi, Andrea and Christensen, Anders and Winther, Ole},
  date = {2020-08-05},
  eprint = {2008.01998},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2008.01998},
  urldate = {2020-08-20},
  abstract = {This paper introduces novel results for the score function gradient estimator of the importance weighted variational bound (IWAE). We prove that in the limit of large K (number of importance samples) one can choose th√e control variate such that the Signal-to-Noise ratio (SNR) of the estimator grows as K. This is in√contrast to the standard pathwise gradient estimator where the SNR decreases as 1/ K. Based on our theoretical findings we develop a novel control variate that extends on VIMCO. Empirically, for the training of both continuous and discrete generative models, the proposed method yields superior variance reduction, resulting in an SNR for IWAE that increases with K without relying on the reparameterization trick. The novel estimator is competitive with state-of-the-art reparameterization-free gradient estimators such as Reweighted Wake-Sleep (RWS) and the thermodynamic variational objective (TVO) when training generative models.},
  langid = {english}
}

@report{lighthill_lighthill_1973,
  title = {Lighthill {{Report}}: {{Artificial Intelligence}}: {{A Paper Symposium}}},
  author = {Lighthill, James},
  date = {1973},
  institution = {{Science Research Council}},
  location = {{London, United Kingdom}},
  url = {http://www.chilton-computing.org.uk/inf/literature/reports/lighthill_report/contents.htm%5Cnhttp://www.aiai.ed.ac.uk/events/lighthill1973/.},
  urldate = {2018-04-21},
  abstract = {Lighthill's report was commissioned by the Science Research Council (SRC) to give an unbiased view of the state of AI research primarily in the UK in 1973. The two main research groups were at Sussex and Edinburgh. There was pressure from Edinburgh to buy a US machine, the Digital Equipment Corporation DEC10 which was used by most US researchers. AI research was funded by the Engineering Board of SRC as part of its Computer Science funding. The Lighthill Report was published early in 1973. Although it supported AI research related to automation and to computer simulation of neurophysiological and psychological processes, it was highly critical of basic research in the foundation areas such as robotics and language processing. Lighthill's report provoked a massive loss of confidence in AI by the academic establishment in the UK including the funding body. It persisted for almost a decade. AI research continued but the next attempt to mount a major activity in the area did not come until the September 1982 Research Area Review Meeting on Intelligent Knowledge-Based Systems. The findings of which were accepted by SERC (Science and Engineering Research Council, a change of name) and became the IKBS part of the Alvey Programme.}
}

@article{liICDCodingClinical2020,
  title = {{{ICD Coding}} from {{Clinical Text Using Multi-Filter Residual Convolutional Neural Network}}},
  author = {Li, Fei and Yu, Hong},
  date = {2020-04},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {05},
  pages = {8180--8187},
  issn = {2374-3468},
  doi = {10.1609/aaai.v34i05.6331},
  abstract = {Automated ICD coding, which assigns the International Classification of Disease codes to patient visits, has attracted much research attention since it can save time and labor for billing. The previous state-of-the-art model utilized one convolutional layer to build document representations for predicting ICD codes. However, the lengths and grammar of text fragments, which are closely related to ICD coding, vary a lot in different documents. Therefore, a flat and fixed-length convolutional architecture may not be capable of learning good document representations. In this paper, we proposed a Multi-Filter Residual Convolutional Neural Network (MultiResCNN) for ICD coding. The innovations of our model are two-folds: it utilizes a multi-filter convolutional layer to capture various text patterns with different lengths and a residual convolutional layer to enlarge the receptive field. We evaluated the effectiveness of our model on the widely-used MIMIC dataset. On the full code set of MIMIC-III, our model outperformed the state-of-the-art model in 4 out of 6 evaluation metrics. On the top-50 code set of MIMIC-III and the full code set of MIMIC-II, our model outperformed all the existing and state-of-the-art models in all evaluation metrics. The code is available at https://github.com/foxlf823/Multi-Filter-Residual-Convolutional-Neural-Network.},
  copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
  langid = {english}
}

@inproceedings{likhomanenko_slimipl_2021,
  title = {{{slimIPL}}: {{Language-model-free}} Iterative Pseudo-Labeling},
  booktitle = {Annual {{Conference}} of the {{International Speech Communication Association}}},
  author = {Likhomanenko, Tatiana and Xu, Qiantong and Kahn, Jacob and Synnaeve, Gabriel and Collobert, Ronan},
  date = {2021}
}

@misc{lillicrap_continuous_2015,
  title = {Continuous Control with Deep Reinforcement Learning},
  author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  date = {2015},
  eprint = {1509.02971},
  eprinttype = {arxiv},
  url = {https://arxiv.org/abs/1509.02971}
}

@inproceedings{lin_analyzing_2022,
  title = {Analyzing the Robustness of Unsupervised Speech Recognition},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Lin, Guan-Ting and Hsu, Chan-Jan and Liu, Da-Rong and Lee, Hung-Yi and Tsao, Yu},
  date = {2022},
  pages = {8202--8206},
  publisher = {{IEEE}},
  location = {{Singapore}},
  doi = {10.1109/ICASSP43922.2022.9747357},
  url = {https://doi.org/10.1109/ICASSP43922.2022.9747357}
}

@inproceedings{lin_dual_2022,
  title = {{{DUAL}}: {{Discrete}} Spoken Unit Adaptive Learning for Textless Spoken Question Answering},
  booktitle = {{{IEEE Annual Conference}} of the {{International Speech Communication Association}} ({{Interspeech}})},
  author = {Lin, Guan-Ting and Chuang, Yung-Sung and Chung, Ho-Lam and Yang, Shu-Wen and Chen, Hsuan-Jui and Dong, Shuyan Annie and Li, Shang-Wen and Mohamed, Abdelrahman and Lee, Hung-yi and Lee, Lin-Shan},
  editor = {Ko, Hanseok and Hansen, John H. L.},
  date = {2022},
  pages = {5165--5169},
  publisher = {{ISCA}},
  location = {{Incheon, South Korea}},
  doi = {10.21437/Interspeech.2022-612},
  url = {https://doi.org/10.21437/Interspeech.2022-612}
}

@misc{lin_network_2013,
  title = {Network {{In Network}}},
  author = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
  date = {2013-12-16},
  eprint = {1312.4400},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1312.4400},
  urldate = {2018-05-22},
  abstract = {We propose a novel deep network structure called "Network In Network" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.}
}

@online{lin_super_2022,
  title = {Super {{Vision Transformer}}},
  author = {Lin, Mingbao and Chen, Mengzhao and Zhang, Yuxin and Li, Ke and Shen, Yunhang and Shen, Chunhua and Ji, Rongrong},
  date = {2022-05-25},
  eprint = {2205.11397},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2205.11397},
  urldate = {2022-10-08},
  abstract = {We attempt to reduce the computational costs in vision transformers (ViTs), which increase quadratically in the token number. We present a novel training paradigm that trains only one ViT model at a time, but is capable of providing improved image recognition performance with various computational costs. Here, the trained ViT model, termed super vision transformer (SuperViT), is empowered with the versatile ability to solve incoming patches of multiple sizes as well as preserve informative tokens with multiple keeping rates (the ratio of keeping tokens) to achieve good hardware efficiency for inference, given that the available hardware resources often change from time to time. Experimental results on ImageNet demonstrate that our SuperViT can considerably reduce the computational costs of ViT models with even performance increase. For example, we reduce 2x FLOPs of DeiT-S while increasing the Top-1 accuracy by 0.2\% and 0.7\% for 1.5x reduction. Also, our SuperViT significantly outperforms existing studies on efficient vision transformers. For example, when consuming the same amount of FLOPs, our SuperViT surpasses the recent state-of-the-art (SoTA) EViT by 1.1\% when using DeiT-S as their backbones. The project of this work is made publicly available at https://github.com/lmbxmu/SuperViT.},
  pubstate = {preprint}
}

@article{lin_why_2017,
  title = {Why Does Deep and Cheap Learning Work so Well?},
  author = {Lin, Henry W. and Tegmark, Max and Rolnick, David},
  date = {2017-09},
  journaltitle = {Journal of Statistical Physics},
  shortjournal = {J Stat Phys},
  volume = {168},
  number = {6},
  pages = {1223--1247},
  issn = {0022-4715, 1572-9613},
  doi = {10/gbvgmm},
  url = {http://arxiv.org/abs/1608.08225},
  urldate = {2019-10-27},
  abstract = {We show how the success of deep learning could depend not only on mathematics but also on physics: although well-known mathematical theorems guarantee that neural networks can approximate arbitrary functions well, the class of functions of practical interest can frequently be approximated through "cheap learning" with exponentially fewer parameters than generic ones. We explore how properties frequently encountered in physics such as symmetry, locality, compositionality, and polynomial log-probability translate into exceptionally simple neural networks. We further argue that when the statistical process generating the data is of a certain hierarchical form prevalent in physics and machine-learning, a deep neural network can be more efficient than a shallow one. We formalize these claims using information theory and discuss the relation to the renormalization group. We prove various "no-flattening theorems" showing when efficient linear deep networks cannot be accurately approximated by shallow ones without efficiency loss, for example, we show that \$n\$ variables cannot be multiplied using fewer than 2\^n neurons in a single hidden layer.},
  langid = {english},
  keywords = {★}
}

@inproceedings{ling_bertphone_2020,
  title = {{{BERTphone}}: {{Phonetically-aware}} Encoder Representations for Utterance-Level Speaker and Language Recognition},
  author = {Ling, Shaoshi and Salazar, Julian and Liu, Yuzong and Kirchhoff, Katrin},
  date = {2020},
  pages = {9--16},
  doi = {10.21437/Odyssey.2020-2},
  eventtitle = {Odyssey: {{The Speaker}} and {{Language Recognition Workshop}}}
}

@unpublished{ling_decoar_2020,
  title = {{{DeCoAR}} 2.0: {{Deep Contextualized Acoustic Representations}} with {{Vector Quantization}}},
  shorttitle = {{{DeCoAR}} 2.0},
  author = {Ling, Shaoshi and Liu, Yuzong},
  date = {2020-12-11},
  eprint = {2012.06659},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2012.06659},
  urldate = {2021-10-11},
  abstract = {Recent success in speech representation learning enables a new way to leverage unlabeled data to train speech recognition model. In speech representation learning, a large amount of unlabeled data is used in a self-supervised manner to learn a feature representation. Then a smaller amount of labeled data is used to train a downstream ASR system using the new feature representations. Based on our previous work DeCoAR and inspirations from other speech representation learning, we propose DeCoAR 2.0, a Deep Contextualized Acoustic Representation with vector quantization. We introduce several modifications over the DeCoAR: first, we use Transformers in encoding module instead of LSTMs; second, we introduce a vector quantization layer between encoder and reconstruction modules; third, we propose an objective that combines the reconstructive loss with vector quantization diversity loss to train speech representations. Our experiments show consistent improvements over other speech representations in different data-sparse scenarios. Without fine-tuning, a light-weight ASR model trained on 10 hours of LibriSpeech labeled data with DeCoAR 2.0 features outperforms the model trained on the full 960-hour dataset with filterbank features.}
}

@inproceedings{ling_deep_2020,
  title = {Deep {{Contextualized Acoustic Representations}} for {{Semi-Supervised Speech Recognition}}},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Ling, Shaoshi and Liu, Yuzong and Salazar, Julian and Kirchhoff, Katrin},
  date = {2020-05},
  pages = {6429--6433},
  issn = {2379-190X},
  doi = {10.1109/ICASSP40776.2020.9053176},
  abstract = {We propose a novel approach to semi-supervised automatic speech recognition (ASR). We first exploit a large amount of unlabeled audio data via representation learning, where we reconstruct a temporal slice of filterbank features from past and future context frames. The resulting deep contextualized acoustic representations (DeCoAR) are then used to train a CTC-based end-to-end ASR system using a smaller amount of labeled audio data. In our experiments, we show that systems trained on DeCoAR consistently outperform ones trained on conventional filterbank features, giving 42\% and 19\% relative improvement over the baseline on WSJ eval92 and LibriSpeech test-clean, respectively. Our approach can drastically reduce the amount of labeled data required; unsupervised training on LibriSpeech then supervision with 100 hours of labeled data achieves performance on par with training on all 960 hours directly.},
  eventtitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})}
}

@unpublished{linPretrainedTransformersText2021,
  title = {Pretrained {{Transformers}} for {{Text Ranking}}: {{BERT}} and {{Beyond}}},
  shorttitle = {Pretrained {{Transformers}} for {{Text Ranking}}},
  author = {Lin, Jimmy and Nogueira, Rodrigo and Yates, Andrew},
  date = {2021-08},
  eprint = {2010.06467},
  eprinttype = {arxiv},
  eprintclass = {cs},
  abstract = {The goal of text ranking is to generate an ordered list of texts retrieved from a corpus in response to a query. Although the most common formulation of text ranking is search, instances of the task can also be found in many natural language processing applications. This survey provides an overview of text ranking with neural network architectures known as transformers, of which BERT is the best-known example. The combination of transformers and self-supervised pretraining has been responsible for a paradigm shift in natural language processing (NLP), information retrieval (IR), and beyond. In this survey, we provide a synthesis of existing work as a single point of entry for practitioners who wish to gain a better understanding of how to apply transformers to text ranking problems and researchers who wish to pursue work in this area. We cover a wide range of modern techniques, grouped into two high-level categories: transformer models that perform reranking in multi-stage architectures and dense retrieval techniques that perform ranking directly. There are two themes that pervade our survey: techniques for handling long documents, beyond typical sentence-by-sentence processing in NLP, and techniques for addressing the tradeoff between effectiveness (i.e., result quality) and efficiency (e.g., query latency, model and index size). Although transformer architectures and pretraining techniques are recent innovations, many aspects of how they are applied to text ranking are relatively well understood and represent mature techniques. However, there remain many open research questions, and thus in addition to laying out the foundations of pretrained transformers for text ranking, this survey also attempts to prognosticate where the field is heading.}
}

@article{lippe_categorical_2021,
  title = {Categorical {{Normalizing Flows}} via {{Continuous Transformations}}},
  author = {Lippe, Phillip and Gavves, Efstratios},
  date = {2021},
  pages = {27},
  abstract = {Despite their popularity, to date, the application of normalizing flows on categorical data stays limited. The current practice of using dequantization to map discrete data to a continuous space is inapplicable as categorical data has no intrinsic order. Instead, categorical data have complex and latent relations that must be inferred, like the synonymy between words. In this paper, we investigate Categorical Normalizing Flows, that is normalizing flows for categorical data. By casting the encoding of categorical data in continuous space as a variational inference problem, we jointly optimize the continuous representation and the model likelihood. Using a factorized decoder, we introduce an inductive bias to model any interactions in the normalizing flow. As a consequence, we do not only simplify the optimization compared to having a joint decoder, but also make it possible to scale up to a large number of categories that is currently impossible with discrete normalizing flows. Based on Categorical Normalizing Flows, we propose GraphCNF a permutationinvariant generative model on graphs. GraphCNF implements a three-step approach modeling the nodes, edges, and adjacency matrix stepwise to increase efficiency. On molecule generation, GraphCNF outperforms both one-shot and autoregressive flow-based state-of-the-art.},
  langid = {english}
}

@inproceedings{liu_completely_2018,
  title = {Completely Unsupervised Phoneme Recognition by Adversarially Learning Mapping Relationships from Audio Embeddings},
  booktitle = {Proceedings of the 19th {{Annual Conference}} of the {{International Speech Communication Association}} ({{Interspeech}})},
  author = {Liu, Da-Rong and Chen, Kuan-Yu and Lee, Hung-Yi and Lee, Lin-shan},
  date = {2018},
  pages = {3748--3752},
  publisher = {{ISCA}}
}

@inproceedings{liu_deep_2015,
  title = {Deep {{Learning Face Attributes}} in the {{Wild}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
  date = {2015},
  eventtitle = {International {{Conference}} on {{Computer Vision}}}
}

@inproceedings{liu_efficient_2018,
  title = {Efficient {{Low-rank Multimodal Fusion With Modality-Specific Factors}}},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Liu, Zhun and Shen, Ying and Lakshminarasimhan, Varun Bharadhwaj and Liang, Paul Pu and Bagher Zadeh, AmirAli and Morency, Louis-Philippe},
  date = {2018},
  pages = {2247--2256},
  publisher = {{Association for Computational Linguistics}},
  location = {{Melbourne, Australia}},
  doi = {10/ggcmm8},
  url = {http://aclweb.org/anthology/P18-1209},
  urldate = {2019-11-08},
  abstract = {Multimodal research is an emerging field of artificial intelligence, and one of the main research problems in this field is multimodal fusion. The fusion of multimodal data is the process of integrating multiple unimodal representations into one compact multimodal representation. Previous research in this field has exploited the expressiveness of tensors for multimodal representation. However, these methods often suffer from exponential increase in dimensions and in computational complexity introduced by transformation of input into tensor. In this paper, we propose the Lowrank Multimodal Fusion method, which performs multimodal fusion using low-rank tensors to improve efficiency. We evaluate our model on three different tasks: multimodal sentiment analysis, speaker trait analysis, and emotion recognition. Our model achieves competitive results on all these tasks while drastically reducing computational complexity. Additional experiments also show that our model can perform robustly for a wide range of low-rank settings, and is indeed much more efficient in both training and inference compared to other methods that utilize tensor representations.},
  eventtitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  langid = {english}
}

@inproceedings{liu_endtoend_2023,
  title = {Towards End-to-End Unsupervised Speech Recognition},
  booktitle = {{{IEEE Spoken Language Technology Workshop}} ({{SLT}})},
  author = {Liu, Alexander H. and Hsu, Wei-Ning and Auli, Michael and Baevski, Alexei},
  date = {2023-01-09/2023-01-12},
  pages = {221--228},
  publisher = {{IEEE}},
  location = {{Doha, Qatar}},
  doi = {10.1109/SLT54892.2023.10023187},
  url = {https://doi.org/10.1109/SLT54892.2023.10023187}
}

@misc{liu_energybased_2020,
  title = {Energy-Based {{Out-of-distribution Detection}}},
  author = {Liu, Weitang and Wang, Xiaoyun and Owens, John D. and Li, Yixuan},
  date = {2020-10-13},
  eprint = {2010.03759},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2010.03759},
  urldate = {2020-11-08},
  abstract = {Determining whether inputs are out-of-distribution (OOD) is an essential building block for safely deploying machine learning models in the open world. However, previous methods relying on the softmax confidence score suffer from overconfident posterior distributions for OOD data. We propose a unified framework for OOD detection that uses an energy score. We show that energy scores better distinguish in- and out-of-distribution samples than the traditional approach using the softmax scores. Unlike softmax confidence scores, energy scores are theoretically aligned with the probability density of the inputs and are less susceptible to the overconfidence issue. Within this framework, energy can be flexibly used as a scoring function for any pre-trained neural classifier as well as a trainable cost function to shape the energy surface explicitly for OOD detection. On a CIFAR-10 pre-trained WideResNet, using the energy score reduces the average FPR (at TPR 95\%) by 18.03\% compared to the softmax confidence score. With energy-based training, our method outperforms the state-of-the-art on common benchmarks.},
  langid = {english}
}

@article{liu_gated_2017,
  title = {Gated {{End-to-End Memory Networks}}},
  author = {Liu, Fei and Perez, Julien},
  date = {2017},
  journaltitle = {Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics (EACL)},
  volume = {1},
  eprint = {26774160},
  eprinttype = {pmid},
  pages = {1--10},
  url = {http://www.aclweb.org/anthology/E17-1001},
  urldate = {2018-06-07},
  abstract = {Machine reading using differentiable rea-soning models has recently shown re-markable progress. In this context, End-to-End trainable Memory Networks (MemN2N) have demonstrated promising performance on simple natural language based reasoning tasks such as factual rea-soning and basic deduction. However, other tasks, namely multi-fact question-answering, positional reasoning or dialog related tasks, remain challenging particu-larly due to the necessity of more com-plex interactions between the memory and controller modules composing this family of models. In this paper, we introduce a novel end-to-end memory access regu-lation mechanism inspired by the current progress on the connection short-cutting principle in the field of computer vision. Concretely, we develop a Gated End-to-End trainable Memory Network architec-ture (GMemN2N). From the machine learn-ing perspective, this new capability is learned in an end-to-end fashion without the use of any additional supervision sig-nal which is, as far as our knowledge goes, the first of its kind. Our experi-ments show significant improvements on the most challenging tasks in the 20 bAbI dataset, without the use of any domain knowledge. Then, we show improvements on the Dialog bAbI tasks including the real human-bot conversion-based Di-alog State Tracking Challenge (DSTC-2) dataset. On these two datasets, our model sets the new state of the art.},
  isbn = {0896-6273}
}

@misc{liu_gramctc_2017,
  title = {Gram-{{CTC}}: {{Automatic Unit Selection}} and {{Target Decomposition}} for {{Sequence Labelling}}},
  shorttitle = {Gram-{{CTC}}},
  author = {Liu, Hairong and Zhu, Zhenyao and Li, Xiangang and Satheesh, Sanjeev},
  date = {2017-02-28},
  eprint = {1703.00096},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1703.00096},
  urldate = {2018-10-31},
  abstract = {Most existing sequence labelling models rely on a fixed decomposition of a target sequence into a sequence of basic units. These methods suffer from two major drawbacks: 1) the set of basic units is fixed, such as the set of words, characters or phonemes in speech recognition, and 2) the decomposition of target sequences is fixed. These drawbacks usually result in sub-optimal performance of modeling sequences. In this pa- per, we extend the popular CTC loss criterion to alleviate these limitations, and propose a new loss function called Gram-CTC. While preserving the advantages of CTC, Gram-CTC automatically learns the best set of basic units (grams), as well as the most suitable decomposition of tar- get sequences. Unlike CTC, Gram-CTC allows the model to output variable number of characters at each time step, which enables the model to capture longer term dependency and improves the computational efficiency. We demonstrate that the proposed Gram-CTC improves CTC in terms of both performance and efficiency on the large vocabulary speech recognition task at multiple scales of data, and that with Gram-CTC we can outperform the state-of-the-art on a standard speech benchmark.}
}

@inproceedings{liu_hkust_2006,
  title = {{{HKUST}}/{{MTS}}: {{A}} Very Large Scale {{Mandarin}} Telephone Speech Corpus},
  booktitle = {Proceedings of the {{International Conference}} on {{Spoken Language Processing}}},
  author = {Liu, Yi and Fung, Pascale and Yang, Yongsheng and Cieri, Christopher and Huang, Shudong and Graff, David},
  date = {2006},
  eventtitle = {International {{Conference}} on {{Spoken Language Processing}}}
}

@inproceedings{liu_learning_2020,
  title = {Learning Deep Kernels for Non-Parametric Two-Sample Tests},
  booktitle = {International Conference on Machine Learning ({{ICML}})},
  author = {Liu, Feng and Xu, Wenkai and Lu, Jie and Zhang, Guangquan and Gretton, Arthur and Sutherland, Danica J},
  date = {2020},
  publisher = {{PMLR}}
}

@inproceedings{liu_learning_2023,
  title = {Learning Customized Visual Models with Retrieval-Augmented Knowledge},
  booktitle = {{{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition ({{CVPR}})},
  author = {Liu, Haotian and Son, Kilho and Yang, Jianwei and Liu, Ce and Gao, Jianfeng and Lee, Yong Jae and Li, Chunyuan},
  date = {2023},
  pages = {15148--15158},
  publisher = {{IEEE}},
  location = {{Vancouver, BC, Canada}},
  doi = {10.1109/CVPR52729.2023.01454},
  url = {https://doi.org/10.1109/CVPR52729.2023.01454},
  annotation = {12 citations (Semantic Scholar/DOI) [2023-10-10] 3 citations (Crossref) [2023-10-10]}
}

@online{liu_masked_2020,
  title = {Masked {{Pre-Trained Encoder Base}} on {{Joint Ctc-Transformer}}},
  author = {Liu, Lu and Huang, Yiheng},
  date = {2020-08-13},
  eprint = {2005.11978},
  eprinttype = {arxiv},
  abstract = {This study (The work was accomplished during the internship in Tencent AI lab) addresses semi-supervised acoustic modeling, i.e. attaining high-level representations from unsupervised audio data and fine-tuning the parameters of pre-trained model with supervised data. The proposed approach adopts a two-stage training framework, consisting of masked pre-trained encoder (MPE) and Joint CTC-Transformer (JCT). In the MPE framework, part of input frames are masked and reconstructed after the encoder with massive unsupervised data. In JCT framework, compared with original Transformer, acoustic features are applied as input instead of plain text. CTC loss performs as the prediction target on top of the encoder, and decoder blocks remain unchanged. This paper presents a comparison between two-stage training method and the fully supervised JCT. In addition, this paper investigates the our approach's robustness against different volumns of training data. Experiments on the two-stage training method deliver much better performance than fully supervised model. The word error rate (WER) with two-stage training which only exploits 30\textbackslash\% of WSJ labeled data achieves 17\textbackslash\% reduction than which trained by 50\textbackslash\% of WSJ in a fully supervised way. Moreover, increasing unlabeled data for MPE from WSJ (81h) to Librispeech (960h) attains about 22\textbackslash\% WER reduction.},
  pubstate = {preprint}
}

@inproceedings{liu_mockingjay_2020,
  title = {Mockingjay: {{Unsupervised Speech Representation Learning}} with {{Deep Bidirectional Transformer Encoders}}},
  shorttitle = {Mockingjay},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Liu, Andy T. and Yang, Shu-wen and Chi, Po-Han and Hsu, Po-chun and Lee, Hung-yi},
  date = {2020-05},
  pages = {6419--6423},
  issn = {2379-190X},
  doi = {10.1109/ICASSP40776.2020.9054458},
  abstract = {We present Mockingjay as a new speech representation learning approach, where bidirectional Transformer encoders are pre-trained on a large amount of unlabeled speech. Previous speech representation methods learn through conditioning on past frames and predicting information about future frames. Whereas Mockingjay is designed to predict the current frame through jointly conditioning on both past and future contexts. The Mockingjay representation improves performance for a wide range of downstream tasks, including phoneme classification, speaker recognition, and sentiment classification on spoken content, while outperforming other approaches. Mockingjay is empirically powerful and can be fine-tuned with downstream models, with only 2 epochs we further improve performance dramatically. In a low resource setting with only 0.1\% of labeled data, we outperform the result of Mel-features that uses all 100\% labeled data.},
  eventtitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})}
}

@unpublished{liu_nonautoregressive_2020,
  title = {Non-{{Autoregressive Predictive Coding}} for {{Learning Speech Representations}} from {{Local Dependencies}}},
  author = {Liu, Alexander H. and Chung, Yu-An and Glass, James},
  date = {2020-10-31},
  eprint = {2011.00406},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2011.00406},
  urldate = {2021-10-11},
  abstract = {Self-supervised speech representations have been shown to be effective in a variety of speech applications. However, existing representation learning methods generally rely on the autoregressive model and/or observed global dependencies while generating the representation. In this work, we propose Non-Autoregressive Predictive Coding (NPC), a self-supervised method, to learn a speech representation in a non-autoregressive manner by relying only on local dependencies of speech. NPC has a conceptually simple objective and can be implemented easily with the introduced Masked Convolution Blocks. NPC offers a significant speedup for inference since it is parallelizable in time and has a fixed inference time for each time step regardless of the input sequence length. We discuss and verify the effectiveness of NPC by theoretically and empirically comparing it with other methods. We show that the NPC representation is comparable to other methods in speech experiments on phonetic and speaker classification while being more efficient.}
}

@misc{liu_pay_2021,
  title = {Pay {{Attention}} to {{MLPs}}},
  author = {Liu, Hanxiao and Dai, Zihang and So, David R. and Le, Quoc V.},
  date = {2021-05-17},
  eprint = {2105.08050},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2105.08050},
  urldate = {2021-05-19},
  abstract = {Transformers [1] have become one of the most important architectural innovations in deep learning and have enabled many breakthroughs over the past few years. Here we propose a simple attention-free network architecture, gMLP, based solely on MLPs with gating, and show that it can perform as well as Transformers in key language and vision applications. Our comparisons show that self-attention is not critical for Vision Transformers, as gMLP can achieve the same accuracy. For BERT, our model achieves parity with Transformers on pretraining perplexity and is better on some downstream tasks. On finetuning tasks where gMLP performs worse, making the gMLP model substantially larger can close the gap with Transformers. In general, our experiments show that gMLP can scale as well as Transformers over increased data and compute.},
  langid = {english}
}

@online{liu_pretrain_2021,
  title = {Pre-Train, Prompt, and Predict: {{A}} Systematic Survey of Prompting Methods in Natural Language Processing},
  author = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  date = {2021},
  eprint = {2107.13586},
  eprinttype = {arxiv},
  pubstate = {preprint}
}

@misc{liu_roberta_2019,
  title = {{{RoBERTa}}: {{A Robustly Optimized BERT Pretraining Approach}}},
  shorttitle = {{{RoBERTa}}},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  date = {2019-07-26},
  eprint = {1907.11692},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1907.11692},
  urldate = {2019-09-08},
  abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  langid = {english}
}

@article{liu_selfsupervised_2021,
  title = {Self-{{Supervised Learning}}: {{Generative}} or {{Contrastive}}},
  author = {Liu, X. and Zhang, F. and Hou, Z. and Mian, L. and Wang, Z. and Zhang, J. and Tang, J.},
  date = {2021-06},
  journaltitle = {IEEE Transactions on Knowledge \& Data Engineering},
  number = {01},
  pages = {1--1},
  publisher = {{IEEE Computer Society}},
  location = {{Los Alamitos, CA, USA}},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2021.3090866},
  keywords = {computational modeling,computer architecture,context modeling,data models,predictive models,supervised learning,task analysis}
}

@inproceedings{liu_ssd:_2016,
  title = {{{SSD}}: {{Single}} Shot Multibox Detector},
  booktitle = {European {{Conference}} on {{Computer Vision}}},
  author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
  date = {2016},
  pages = {21--37}
}

@unpublished{liu_swin_2021,
  title = {Swin {{Transformer}}: {{Hierarchical Vision Transformer}} Using {{Shifted Windows}}},
  shorttitle = {Swin {{Transformer}}},
  author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  date = {2021-08-17},
  eprint = {2103.14030},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2103.14030},
  urldate = {2022-05-03},
  abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \textbackslash textbf\{S\}hifted \textbackslash textbf\{win\}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at\textasciitilde\textbackslash url\{https://github.com/microsoft/Swin-Transformer\}.},
  keywords = {Unread}
}

@inproceedings{liu_swin_2022,
  title = {Swin Transformer {{V2}}: {{Scaling}} up Capacity and Resolution},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition ({{CVPR}})},
  author = {Liu, Ze and Hu, Han and Lin, Yutong and Yao, Zhuliang and Xie, Zhenda and Wei, Yixuan and Ning, Jia and Cao, Yue and Zhang, Zheng and Dong, Li and Wei, Furu and Guo, Baining},
  date = {2022-06},
  pages = {12009--12019}
}

@article{liu_tera_2021,
  title = {{{TERA}}: {{Self-Supervised Learning}} of {{Transformer Encoder Representation}} for {{Speech}}},
  shorttitle = {{{TERA}}},
  author = {Liu, Andy T. and Li, Shang-Wen and Lee, Hung-yi},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {2351--2366},
  issn = {2329-9304},
  doi = {10.1109/TASLP.2021.3095662},
  abstract = {We introduce a self-supervised speech pre-training method called TERA, which stands for Transformer Encoder Representations from Alteration. Recent approaches often learn by using a single auxiliary task like contrastive prediction, autoregressive prediction, or masked reconstruction. Unlike previous methods, we use alteration along three orthogonal axes to pre-train Transformer Encoders on a large amount of unlabeled speech. The model learns through the reconstruction of acoustic frames from their altered counterpart, where we use a stochastic policy to alter along various dimensions: time, frequency, and magnitude. TERA can be used for speech representations extraction or fine-tuning with downstream models. We evaluate TERA on several downstream tasks, including phoneme classification, keyword spotting, speaker recognition, and speech recognition. We present a large-scale comparison of various self-supervised models. TERA achieves strong performance in the comparison by improving upon surface features and outperforming previous models. In our experiments, we study the effect of applying different alteration techniques, pre-training on more data, and pre-training on various features. We analyze different model sizes and find that smaller models are strong representation learners than larger models, while larger models are more effective for downstream fine-tuning than smaller models. Furthermore, we show the proposed method is transferable to downstream datasets not used in pre-training.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}}
}

@inproceedings{liu_unsupervised_2017,
  title = {Unsupervised Sequence Classification Using Sequential Output Statistics},
  booktitle = {Conference on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Liu, Yu and Chen, Jianshu and Deng, Li},
  date = {2017}
}

@inproceedings{liu_unsupervised_2020,
  title = {Towards {{Unsupervised Speech Recognition}} and {{Synthesis}} with {{Quantized Speech Representation Learning}}},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Liu, Alexander H. and Tu, Tao and Lee, Hung-yi and Lee, Lin-shan},
  date = {2020-05},
  pages = {7259--7263},
  issn = {2379-190X},
  doi = {10.1109/ICASSP40776.2020.9053571},
  abstract = {In this paper we propose a Sequential Representation Quantization AutoEncoder (SeqRQ-AE) to learn from primarily unpaired audio data and produce sequences of representations very close to phoneme sequences of speech utterances. This is achieved by proper temporal segmentation to make the representations phoneme-synchronized, and proper phonetic clustering to have total number of distinct representations close to the number of phonemes. Mapping between the distinct representations and phonemes is learned from a small amount of annotated paired data. Preliminary experiments on LJSpeech demonstrated the learned representations for vowels have relative locations in latent space in good parallel to that shown in the IPA vowel chart defined by linguistics experts. With less than 20 minutes of annotated speech, our method outperformed existing methods on phoneme recognition and is able to synthesize intelligible speech that beats our baseline model.},
  eventtitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})}
}

@inproceedings{liu_unsupervised_2023a,
  title = {Unsupervised Out-of-Distribution Detection with Diffusion Inpainting},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Liu, Zhenzhen and Zhou, Jin Peng and Wang, Yufan and Weinberger, Kilian Q.},
  editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  date = {2023-07-23/2023-07-29},
  series = {Proceedings of Machine Learning Research},
  volume = {202},
  eprint = {2302.10326},
  eprinttype = {arxiv},
  pages = {22528--22538},
  publisher = {{PMLR}},
  location = {{Honolulu, Hawaii, USA}},
  url = {https://proceedings.mlr.press/v202/liu23bd.html},
  eventtitle = {International {{Conference}} on {{Machine Learning}} ({{ICML}})}
}

@misc{liu_variance_2019,
  title = {On the {{Variance}} of the {{Adaptive Learning Rate}} and {{Beyond}}},
  author = {Liu, Liyuan and Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Han, Jiawei},
  date = {2019-08-08},
  eprint = {1908.03265},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1908.03265},
  urldate = {2019-09-11},
  abstract = {The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Here, we study its mechanism in details. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate (i.e., it has problematically large variance in the early stage), suggest warmup works as a variance reduction technique, and provide both empirical and theoretical evidence to verify our hypothesis. We further propose RAdam, a new variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Extensive experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the effectiveness and robustness of our proposed method. All implementations are available at: https://github.com/LiyuanLucasLiu/RAdam.},
  langid = {english}
}

@inproceedings{liuEffectiveConvolutionalAttention2021,
  title = {Effective {{Convolutional Attention Network}} for {{Multi-label Clinical Document Classification}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Liu, Yang and Cheng, Hua and Klopfer, Russell and Gormley, Matthew R. and Schaaf, Thomas},
  date = {2021-11},
  pages = {5941--5953},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-main.481},
  abstract = {Multi-label document classification (MLDC) problems can be challenging, especially for long documents with a large label set and a long-tail distribution over labels. In this paper, we present an effective convolutional attention network for the MLDC problem with a focus on medical code prediction from clinical documents. Our innovations are three-fold: (1) we utilize a deep convolution-based encoder with the squeeze-and-excitation networks and residual networks to aggregate the information across the document and learn meaningful document representations that cover different ranges of texts; (2) we explore multi-layer and sum-pooling attention to extract the most informative features from these multi-scale representations; (3) we combine binary cross entropy loss and focal loss to improve performance for rare labels. We focus our evaluation study on MIMIC-III, a widely used dataset in the medical domain. Our models outperform prior work on medical coding and achieve new state-of-the-art results on multiple metrics. We also demonstrate the language independent nature of our approach by applying it to two non-English datasets. Our model outperforms prior best model and a multilingual Transformer model by a substantial margin.}
}

@article{lloydPhysicianCodingErrors1985,
  title = {Physician and {{Coding Errors}} in {{Patient Records}}},
  author = {Lloyd, Susan S. and Rissing, J. Peter},
  date = {1985-09},
  journaltitle = {JAMA : the journal of the American Medical Association},
  shortjournal = {JAMA},
  volume = {254},
  number = {10},
  pages = {1330--1336},
  issn = {0098-7484},
  doi = {10.1001/jama.1985.03360100080018},
  abstract = {The Veterans Administration's discharge abstract system was studied to identify error frequency, source, and effect in five Veterans Administration hospitals. We reviewed 1,829 medical records from 21 services for concordance with the abstract; sampling provided 95\% confidence for each service. Of these records, 1,499 (82\%) differed from the abstract in at least one item. Of 20,260 items, 4,360 (22\%) were incorrect, with three error sources: physician (62\%), coding (35\%), and keypunch (3\%). We projected 2.14 physician and 0.81 coding errors in the average abstract. Eighty-nine percent of projected physician errors were failures to report a procedure or diagnosis. Coding was subjective and errors were synergistic with physician errors. We projected that correction of errors would change 19\% of the records for diagnosis-related group purposes and substantially increase future resource allocation. This effect varied considerably by service.(JAMA 1985;254:1330-1336)}
}

@misc{loaiza-ganem_continuous_2019,
  title = {The Continuous {{Bernoulli}}: Fixing a Pervasive Error in Variational Autoencoders},
  shorttitle = {The Continuous {{Bernoulli}}},
  author = {Loaiza-Ganem, Gabriel and Cunningham, John P.},
  date = {2019-12-29},
  eprint = {1907.06845},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1907.06845},
  urldate = {2020-01-11},
  abstract = {Variational autoencoders (VAE) have quickly become a central tool in machine learning, applicable to a broad range of data types and latent variable models. By far the most common first step, taken by seminal papers and by core software libraries alike, is to model MNIST data using a deep network parameterizing a Bernoulli likelihood. This practice contains what appears to be and what is often set aside as a minor inconvenience: the pixel data is [0,1] valued, not \{0,1\} as supported by the Bernoulli likelihood. Here we show that, far from being a triviality or nuisance that is convenient to ignore, this error has profound importance to VAE, both qualitative and quantitative. We introduce and fully characterize a new [0,1]-supported, single parameter distribution: the continuous Bernoulli, which patches this pervasive bug in VAE. This distribution is not nitpicking; it produces meaningful performance improvements across a range of metrics and datasets, including sharper image samples, and suggests a broader class of performant VAE.},
  langid = {english}
}

@inproceedings{long_fully_2015,
  title = {Fully {{Convolutional Networks}} for {{Semantic Segmentation}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  date = {2015},
  eprint = {16190471},
  eprinttype = {pmid},
  pages = {3431--3440},
  issn = {10636919},
  doi = {10.1109/CVPR.2015.7298965},
  abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
  isbn = {978-1-4673-6964-0}
}

@misc{loshchilov_fixing_2017,
  title = {Fixing {{Weight Decay Regularization}} in {{Adam}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  date = {2017-11-14},
  eprint = {1711.05101},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1711.05101},
  urldate = {2018-06-03},
  abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \textbackslash emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common deep learning frameworks of these algorithms implement L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by decoupling the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam, and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). We also propose a version of Adam with warm restarts (AdamWR) that has strong anytime performance while achieving state-of-the-art results on CIFAR-10 and ImageNet32x32. Our source code is available at https://github.com/loshchil/AdamW-and-SGDW},
  keywords = {★}
}

@inproceedings{loshchilovDecoupledWeightDecay2022,
  title = {Decoupled {{Weight Decay Regularization}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  date = {2022-02},
  abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \textbackslash emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it “weight decay” in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \textbackslash emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at \textbackslash url\{https://github.com/loshchil/AdamW-and-SGDW\}},
  langid = {english}
}

@online{lu_conditional_2022,
  title = {Conditional {{Diffusion Probabilistic Model}} for {{Speech Enhancement}}},
  author = {Lu, Yen-Ju and Wang, Zhong-Qiu and Watanabe, Shinji and Richard, Alexander and Yu, Cheng and Tsao, Yu},
  date = {2022-02-10},
  eprint = {2202.05256},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2202.05256},
  urldate = {2022-09-21},
  abstract = {Speech enhancement is a critical component of many user-oriented audio applications, yet current systems still suffer from distorted and unnatural outputs. While generative models have shown strong potential in speech synthesis, they are still lagging behind in speech enhancement. This work leverages recent advances in diffusion probabilistic models, and proposes a novel speech enhancement algorithm that incorporates characteristics of the observed noisy speech signal into the diffusion and reverse processes. More specifically, we propose a generalized formulation of the diffusion probabilistic model named conditional diffusion probabilistic model that, in its reverse process, can adapt to non-Gaussian real noises in the estimated speech signal. In our experiments, we demonstrate strong performance of the proposed approach compared to representative generative models, and investigate the generalization capability of our models to other datasets with noise characteristics unseen during training.},
  langid = {english},
  pubstate = {preprint}
}

@online{lu_endpoint_2022,
  title = {Endpoint {{Detection}} for {{Streaming End-to-End Multi-talker ASR}}},
  author = {Lu, Liang and Li, Jinyu and Gong, Yifan},
  date = {2022-01-24},
  eprint = {2201.09979},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2201.09979},
  urldate = {2023-06-29},
  abstract = {Streaming end-to-end multi-talker speech recognition aims at transcribing the overlapped speech from conversations or meetings with an all-neural model in a streaming fashion, which is fundamentally different from a modular-based approach that usually cascades the speech separation and the speech recognition models trained independently. Previously, we proposed the Streaming Unmixing and Recognition Transducer (SURT) model based on recurrent neural network transducer (RNN-T) for this problem and presented promising results. However, for real applications, the speech recognition system is also required to determine the timestamp when a speaker finishes speaking for prompt system response. This problem, known as endpoint (EP) detection, has not been studied previously for multi-talker end-to-end models. In this work, we address the EP detection problem in the SURT framework by introducing an end-of-sentence token as an output unit, following the practice of single-talker end-to-end models. Furthermore, we also present a latency penalty approach that can significantly cut down the EP detection latency. Our experimental results based on the 2-speaker LibrispeechMix dataset show that the SURT model can achieve promising EP detection without significantly degradation of the recognition accuracy.},
  langid = {english},
  pubstate = {preprint}
}

@misc{lucas_analyzing_2021,
  title = {Analyzing {{Monotonic Linear Interpolation}} in {{Neural Network Loss Landscapes}}},
  author = {Lucas, James and Bae, Juhan and Zhang, Michael R. and Fort, Stanislav and Zemel, Richard and Grosse, Roger},
  date = {2021-04-23},
  eprint = {2104.11044},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2104.11044},
  urldate = {2021-05-07},
  abstract = {Linear interpolation between initial neural network parameters and converged parameters after training with stochastic gradient descent (SGD) typically leads to a monotonic decrease in the training objective. This Monotonic Linear Interpolation (MLI) property, first observed by Goodfellow et al. (2014), persists in spite of the non-convex objectives and highly non-linear training dynamics of neural networks. Extending this work, we evaluate several hypotheses for this property that, to our knowledge, have not yet been explored. Using tools from differential geometry, we draw connections between the interpolated paths in function space and the monotonicity of the network — providing sufficient conditions for the MLI property under mean squared error. While the MLI property holds under various settings (e.g. network architectures and learning problems), we show in practice that networks violating the MLI property can be produced systematically, by encouraging the weights to move far from initialization. The MLI property raises important questions about the loss landscape geometry of neural networks and highlights the need to further study their global properties.},
  langid = {english}
}

@article{lucas_dont_nodate,
  title = {Don't {{Blame}} the {{ELBO}}! {{A Linear VAE Perspective}} on {{Posterior Collapse}}},
  author = {Lucas, James and Tucker, George and Grosse, Roger B and Norouzi, Mohammad},
  pages = {11},
  abstract = {Posterior collapse in Variational Autoencoders (VAEs) arises when the variational posterior distribution closely matches the prior for a subset of latent variables. This paper presents a simple and intuitive explanation for posterior collapse through the analysis of linear VAEs and their direct correspondence with Probabilistic PCA (pPCA). We explain how posterior collapse may occur in pPCA due to local maxima in the log marginal likelihood. Unexpectedly, we prove that the ELBO objective for the linear VAE does not introduce additional spurious local maxima relative to log marginal likelihood. We show further that training a linear VAE with exact variational inference recovers an identifiable global maximum corresponding to the principal component directions. Empirically, we find that our linear analysis is predictive even for high-capacity, non-linear VAEs and helps explain the relationship between the observation noise, local maxima, and posterior collapse in deep Gaussian VAEs.},
  langid = {english}
}

@inproceedings{lugosch_speech_2019,
  title = {Speech Model Pre-Training for End-to-End Spoken Language Understanding},
  booktitle = {Annual {{Conference}} of the {{International Speech Communication Association}}},
  author = {Lugosch, Loren and Ravanelli, Mirco and Ignoto, Patrick and Tomar, Vikrant Singh and Bengio, Yoshua},
  date = {2019}
}

@article{lundberg_unified_nodate,
  title = {A {{Unified Approach}} to {{Interpreting Model Predictions}}},
  author = {Lundberg, Scott M and Lee, Su-In},
  pages = {10},
  abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction’s accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
  langid = {english}
}

@article{lundervold_overview_2019,
  title = {An Overview of Deep Learning in Medical Imaging Focusing on {{MRI}}},
  author = {Lundervold, Alexander Selvikvåg and Lundervold, Arvid},
  date = {2019},
  journaltitle = {Zeitschrift für Medizinische Physik},
  volume = {29},
  number = {2},
  pages = {102--127},
  publisher = {{Elsevier}}
}

@article{luo_conv-tasnet_2019,
  title = {Conv-{{TasNet}}: {{Surpassing Ideal Time-Frequency Magnitude Masking}} for {{Speech Separation}}},
  author = {Luo, Yi and Mesgarani, Nima},
  date = {2019},
  journaltitle = {IEEE ACM Trans. Audio Speech Lang. Process.},
  volume = {27},
  number = {8},
  pages = {1256--1266},
  doi = {10.1109/TASLP.2019.2915167},
  url = {https://doi.org/10.1109/TASLP.2019.2915167}
}

@article{luo_dropout_2021,
  title = {Dropout Regularization for Self-Supervised Learning of {{Transformer}} Encoder Speech Representation},
  author = {Luo, Jian and Wang, Jianzong and Cheng, Ning and Xiao, Jing},
  date = {2021},
  journaltitle = {Annual Conference of the International Speech Communication Association}
}

@article{luo_towards_2018,
  title = {Towards {{Understanding Regularization}} in {{Batch Normalization}}},
  author = {Luo, Ping and Wang, Xinjiang and Shao, Wenqi and Peng, Zhanglin},
  date = {2018-09-04},
  url = {https://arxiv.org/abs/1809.00846v2},
  urldate = {2018-09-29},
  abstract = {Batch Normalization (BN) improves both convergence and generalization in training neural networks. This work understands these phenomena theoretically. We analyze BN by using a basic block of neural networks, consisting of a kernel layer, a BN layer, and a nonlinear activation function. This basic network helps us understand the impacts of BN in three aspects. First, by viewing BN as an implicit regularizer, BN can be decomposed into population normalization (PN) and gamma decay as an explicit regularization. Second, learning dynamics of BN and the regularization show that training converged with large maximum and effective learning rate. Third, generalization of BN is explored by using statistical mechanics. Experiments demonstrate that BN in convolutional neural networks share the same traits of regularization as the above analyses.},
  langid = {english}
}

@online{luo_understanding_2022,
  title = {Understanding {{Diffusion Models}}: {{A Unified Perspective}}},
  shorttitle = {Understanding {{Diffusion Models}}},
  author = {Luo, Calvin},
  date = {2022-08-25},
  eprint = {2208.11970},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2208.11970},
  urldate = {2022-12-14},
  abstract = {Diffusion models have shown incredible capabilities as generative models; indeed, they power the current state-of-the-art models on text-conditioned image generation such as Imagen and DALL-E 2. In this work we review, demystify, and unify the understanding of diffusion models across both variational and score-based perspectives. We first derive Variational Diffusion Models (VDM) as a special case of a Markovian Hierarchical Variational Autoencoder, where three key assumptions enable tractable computation and scalable optimization of the ELBO. We then prove that optimizing a VDM boils down to learning a neural network to predict one of three potential objectives: the original source input from any arbitrary noisification of it, the original source noise from any arbitrarily noisified input, or the score function of a noisified input at any arbitrary noise level. We then dive deeper into what it means to learn the score function, and connect the variational perspective of a diffusion model explicitly with the Score-based Generative Modeling perspective through Tweedie's Formula. Lastly, we cover how to learn a conditional distribution using diffusion models via guidance.},
  pubstate = {preprint}
}

@misc{luong_achieving_2016,
  title = {Achieving {{Open Vocabulary Neural Machine Translation}} with {{Hybrid Word-Character Models}}},
  author = {Luong, Minh-Thang and Manning, Christopher D.},
  date = {2016},
  eprint = {1604.00788},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1604.00788},
  urldate = {2018-05-02},
  abstract = {Nearly all previous work on neural machine translation (NMT) has used quite restricted vocabularies, perhaps with a subsequent method to patch in unknown words. This paper presents a novel word-character solution to achieving open vocabulary NMT. We build hybrid systems that translate mostly at the word level and consult the character components for rare words. Our character-level recurrent neural networks compute source word representations and recover unknown target words when needed. The twofold advantage of such a hybrid approach is that it is much faster and easier to train than character-based ones; at the same time, it never produces unknown words as in the case of word-based models. On the WMT'15 English to Czech translation task, this hybrid approach offers an addition boost of +2.1-11.4 BLEU points over models that already handle unknown words. Our best system achieves a new state-of-the-art result with 20.7 BLEU score. We demonstrate that our character models can successfully learn to not only generate well-formed words for Czech, a highly-inflected language with a very complex vocabulary, but also build correct representations for English source words.},
  isbn = {9781510827585}
}

@misc{luong_effective_2015,
  title = {Effective Approaches to Attention-Based Neural Machine Translation},
  author = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D.},
  date = {2015},
  eprint = {1508.04025},
  eprinttype = {arxiv},
  url = {https://arxiv.org/abs/1508.04025}
}

@misc{ly_tutorial_2017,
  title = {A {{Tutorial}} on {{Fisher Information}}},
  author = {Ly, Alexander and Marsman, Maarten and Verhagen, Josine and Grasman, Raoul and Wagenmakers, Eric-Jan},
  date = {2017-10-17},
  eprint = {1705.01064},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1705.01064},
  urldate = {2018-05-13},
  abstract = {In many statistical applications that concern mathematical psychologists, the concept of Fisher information plays an important role. In this tutorial we clarify the concept of Fisher information as it manifests itself across three different statistical paradigms. First, in the frequentist paradigm, Fisher information is used to construct hypothesis tests and confidence intervals using maximum likelihood estimators; second, in the Bayesian paradigm, Fisher information is used to define a default prior; lastly, in the minimum description length paradigm, Fisher information is used to measure model complexity.}
}

@article{lydia_linear_2016,
  title = {Linear and Non-Linear Autoregressive Models for Short-Term Wind Speed Forecasting},
  author = {Lydia, M. and Suresh Kumar, S. and Immanuel Selvakumar, A. and Edwin Prem Kumar, G.},
  date = {2016},
  journaltitle = {Energy Conversion and Management},
  volume = {112},
  pages = {115--124},
  issn = {01968904},
  doi = {10.1016/j.enconman.2016.01.007},
  abstract = {Wind speed forecasting aids in estimating the energy produced from wind farms. The soaring energy demands of the world and minimal availability of conventional energy sources have significantly increased the role of non-conventional sources of energy like solar, wind, etc. Development of models for wind speed forecasting with higher reliability and greater accuracy is the need of the hour. In this paper, models for predicting wind speed at 10-min intervals up to 1 h have been built based on linear and non-linear autoregressive moving average models with and without external variables. The autoregressive moving average models based on wind direction and annual trends have been built using data obtained from Sotavento Galicia Plc. and autoregressive moving average models based on wind direction, wind shear and temperature have been built on data obtained from Centre for Wind Energy Technology, Chennai, India. While the parameters of the linear models are obtained using the Gauss-Newton algorithm, the non-linear autoregressive models are developed using three different data mining algorithms. The accuracy of the models has been measured using three performance metrics namely, the Mean Absolute Error, Root Mean Squared Error and Mean Absolute Percentage Error.}
}

@report{lyudchik_outlier_2016,
  title = {Outlier Detection Using Autoencoders},
  author = {Lyudchik, Olga},
  date = {2016}
}

@inproceedings{ma_eddi_2019,
  title = {{{EDDI}}: {{Efficient}} Dynamic Discovery of High-Value Information with Partial {{VAE}}},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning, ({{ICML}})},
  author = {Ma, Chao and Tschiatschek, Sebastian and Palla, Konstantina and Hernández-Lobato, José Miguel and Nowozin, Sebastian and Zhang, Cheng},
  date = {2019},
  publisher = {{PMLR}}
}

@inproceedings{ma_unsupervised_2006,
  title = {Unsupervised Training on Large Amounts of Broadcast News Data},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Ma, Jeff and Matsoukas, Spyros and Kimball, Owen and Schwartz, Richard},
  date = {2006},
  volume = {3},
  pages = {III--III},
  eventtitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})}
}

@inproceedings{maaloe_auxiliary_2016,
  title = {Auxiliary Deep Generative Models},
  booktitle = {Proceedings of the 33rd {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Maaløe, Lars and Sønderby, Casper Kaae and Sønderby, Søren Kaae and Winther, Ole},
  editor = {Balcan, Maria Florina and Weinberger, Kilian Q.},
  date = {2016-06-20/2016-06-22},
  series = {Proceedings of Machine Learning Research},
  volume = {48},
  pages = {1445--1453},
  publisher = {{PMLR}},
  location = {{New York, New York, USA}},
  url = {http://proceedings.mlr.press/v48/maaloe16.html},
  abstract = {Deep generative models parameterized by neural networks have recently achieved state-of-the-art performance in unsupervised and semi-supervised learning. We extend deep generative models with auxiliary variables which improves the variational approximation. The auxiliary variables leave the generative model unchanged but make the variational distribution more expressive. Inspired by the structure of the auxiliary variable we also propose a model with two stochastic layers and skip connections. Our findings suggest that more expressive and properly specified deep generative models converge faster with better results. We show state-of-the-art performance within semi-supervised learning on MNIST, SVHN and NORB datasets.}
}

@inproceedings{maaloe_biva_2019,
  title = {{{BIVA}}: {{A Very Deep Hierarchy}} of {{Latent Variables}} for {{Generative Modeling}}},
  shorttitle = {{{BIVA}}},
  booktitle = {Proceedings of the 32nd {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Maaløe, Lars and Fraccaro, Marco and Liévin, Valentin and Winther, Ole},
  date = {2019-02-06},
  pages = {6548--6558},
  location = {{Vancouver, Canada}},
  url = {http://arxiv.org/abs/1902.02102},
  urldate = {2019-03-19},
  abstract = {With the introduction of the variational autoencoder (VAE), probabilistic latent variable models have received renewed attention as powerful generative models. However, their performance in terms of test likelihood and quality of generated samples has been surpassed by autoregressive models without stochastic units. Furthermore, flow-based models have recently been shown to be an attractive alternative that scales well to high-dimensional data. In this paper we close the performance gap by constructing VAE models that can effectively utilize a deep hierarchy of stochastic variables and model complex covariance structures. We introduce the Bidirectional-Inference Variational Autoencoder (BIVA), characterized by a skip-connected generative model and an inference network formed by a bidirectional stochastic inference path. We show that BIVA reaches state-of-the-art test likelihoods, generates sharp and coherent natural images, and uses the hierarchy of latent variables to capture different aspects of the data distribution. We observe that BIVA, in contrast to recent results, can be used for anomaly detection. We attribute this to the hierarchy of latent variables which is able to extract high-level semantic features. Finally, we extend BIVA to semi-supervised classification tasks and show that it performs comparably to state-of-the-art results by generative adversarial networks.},
  eventtitle = {Conference on {{Neural Information Processing Systems}}}
}

@misc{maaloe_semisupervised_2017,
  title = {Semi-{{Supervised Generation}} with {{Cluster-aware Generative Models}}},
  author = {Maaløe, Lars and Fraccaro, Marco and Winther, Ole},
  date = {2017-04-03},
  eprint = {1704.00637},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1704.00637},
  urldate = {2021-01-28},
  abstract = {Deep generative models trained with large amounts of unlabelled data have proven to be powerful within the domain of unsupervised learning. Many real life data sets contain a small amount of labelled data points, that are typically disregarded when training generative models. We propose the Cluster-aware Generative Model, that uses unlabelled information to infer a latent representation that models the natural clustering of the data, and additional labelled data points to refine this clustering. The generative performances of the model significantly improve when labelled information is exploited, obtaining a log-likelihood of −79.38 nats on permutation invariant MNIST, while also achieving competitive semi-supervised classification accuracies. The model can also be trained fully unsupervised, and still improve the log-likelihood performance with respect to related methods.},
  langid = {english}
}

@inproceedings{maas_wordlevel_2012,
  title = {Word-Level Acoustic Modeling with Convolutional Vector Regression},
  booktitle = {Procedings of the {{International Conference}} on {{Machine Learning}} ({{ICML}}), {{Workshop}} on {{Representation Learning}}},
  author = {Maas, Andrew L and Miller, Stephen D and O'neil, Tyler M and Ng, Andrew Y and Nguyen, Patrick},
  date = {2012}
}

@book{mackay_information_2003,
  title = {Information Theory, Inference, and Learning Algorithms},
  author = {MacKay, David J. C.},
  date = {2003},
  edition = {1},
  publisher = {{Cambridge University Press}},
  abstract = {Information theory and inference, often taught separately, are here united in one entertaining textbook. These topics lie at the heart of many exciting areas of contemporary science and engineering -- communication, signal processing, data mining, machine learning, pattern recognition, computational neuroscience, bioinformatics, and cryptography. This textbook introduces theory in tandem with applications. Information theory is taught alongside practical communication systems, such as arithmetic coding for data compression and sparse-graph codes for error-correction. A toolbox of inference techniques, including message-passing algorithms, Monte Carlo methods, and variational approximations, are developed alongside applications of these tools to clustering, convolutional codes, independent component analysis, and neural networks. The final part of the book describes the state of the art in error-correcting codes, including low-density parity-check codes, turbo codes, and digital fountain codes -- the twentyfirst-century standards for satellite communications, disk drives, and data broadcast. Richly illustrated, filled with worked examples and over 400 exercises, some with detailed solutions, the book is ideal for self-learning and for undergraduate or graduate courses. Interludes on crosswords, evolution, and sex provide entertainment along the way. The result is a textbook on information, communication, and coding for a new generation of students, and an unparalleled entry point into these subjects for professionals in areas as diverse as computational biology, financial engineering, and machine learning.},
  isbn = {978-0-521-64298-9},
  pagetotal = {640}
}

@article{mackay_practical_1992,
  title = {A {{Practical Bayesian Framework}} for {{Backpropagation Networks}}},
  author = {MacKay, David J. C.},
  date = {1992},
  journaltitle = {Neural Computation},
  volume = {4},
  number = {3},
  pages = {448--472},
  issn = {0899-7667},
  doi = {10.1162/neco.1992.4.3.448},
  abstract = {A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks. The framework makes possible (1) objective comparisons between solutions using alternative network architectures, (2) objective stopping rules for network pruning or growing procedures, (3) objective choice of magnitude and type of weight decay terms or additive regularizers (for penalizing large weights, etc.), (4) a measure of the effective number of well-determined parameters in a model, (5) quantified estimates of the error bars on network parameters and on network output, and (6) objective comparisons with alternative learning and interpolation models such as splines and radial basis functions. The Bayesian "evidence" automatically embodies "Occam's razor," penalizing overflexible and overcomplex models. The Bayesian approach helps detect poor underlying assumptions in learning models. For learning models well matched to a problem, a good correlation between generalization ability and the Bayesian evidence is obtained.},
  isbn = {0899-7667}
}

@inproceedings{maddison_concrete_2017,
  title = {The Concrete Distribution: {{A}} Continuous Relaxation of Discrete Random Variables},
  booktitle = {Proceedings of the 5th {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Maddison, Chris J. and Mnih, Andriy and Teh, Yee Whye},
  date = {2017},
  location = {{Toulon, France}},
  url = {https://openreview.net/forum?id=S1jE5L5gl}
}

@misc{maddison_hamiltonian_2018,
  title = {Hamiltonian {{Descent Methods}}},
  author = {Maddison, Chris J. and Paulin, Daniel and Teh, Yee Whye and O'Donoghue, Brendan and Doucet, Arnaud},
  date = {2018-09-13},
  eprint = {1809.05042},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1809.05042},
  urldate = {2018-09-15},
  abstract = {We propose a family of optimization methods that achieve linear convergence using first-order gradient information and constant step sizes on a class of convex functions much larger than the smooth and strongly convex ones. This larger class includes functions whose second derivatives may be singular or unbounded at their minima. Our methods are discretizations of conformal Hamiltonian dynamics, which generalize the classical momentum method to model the motion of a particle with non-standard kinetic energy exposed to a dissipative force and the gradient field of the function of interest. They are first-order in the sense that they require only gradient computation. Yet, crucially the kinetic gradient map can be designed to incorporate information about the convex conjugate in a fashion that allows for linear convergence on convex functions that may be non-smooth or non-strongly convex. We study in detail one implicit and two explicit methods. For one explicit method, we provide conditions under which it converges to stationary points of non-convex functions. For all, we provide conditions on the convex function and kinetic energy pair that guarantee linear convergence, and show that these conditions can be satisfied by functions with power growth. In sum, these methods expand the class of convex functions on which linear convergence is possible with first-order computation.}
}

@misc{maddox_simple_2019,
  title = {A {{Simple Baseline}} for {{Bayesian Uncertainty}} in {{Deep Learning}}},
  author = {Maddox, Wesley and Garipov, Timur and Izmailov, Pavel and Vetrov, Dmitry and Wilson, Andrew Gordon},
  date = {2019-12-31},
  eprint = {1902.02476},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1902.02476},
  urldate = {2020-09-20},
  abstract = {We propose SWA-Gaussian (SWAG), a simple, scalable, and general purpose approach for uncertainty representation and calibration in deep learning. Stochastic Weight Averaging (SWA), which computes the first moment of stochastic gradient descent (SGD) iterates with a modified learning rate schedule, has recently been shown to improve generalization in deep learning. With SWAG, we fit a Gaussian using the SWA solution as the first moment and a low rank plus diagonal covariance also derived from the SGD iterates, forming an approximate posterior distribution over neural network weights; we then sample from this Gaussian distribution to perform Bayesian model averaging. We empirically find that SWAG approximates the shape of the true posterior, in accordance with results describing the stationary distribution of SGD iterates. Moreover, we demonstrate that SWAG performs well on a wide variety of tasks, including out of sample detection, calibration, and transfer learning, in comparison to many popular alternatives including MC dropout, KFAC Laplace, SGLD, and temperature scaling.},
  langid = {english}
}

@book{madsen_time_2008,
  title = {Time {{Series Analysis}}},
  author = {Madsen, Henrik},
  date = {2008},
  edition = {1},
  publisher = {{Chapman and Hall/CRC}},
  location = {{Copenhagen}},
  isbn = {978-1-4200-5967-0},
  pagetotal = {380}
}

@inproceedings{maekaku_speech_2021,
  title = {Speech Representation Learning Combining Conformer {{CPC}} with Deep Cluster for the {{ZeroSpeech}} Challenge 2021},
  booktitle = {{{IEEE Annual Conference}} of the {{International Speech Communication Association}} ({{Interspeech}})},
  author = {Maekaku, Takashi and Chang, Xuankai and Fujita, Yuya and Chen, Li-Wei and Watanabe, Shinji and Rudnicky, Alexander},
  date = {2021}
}

@article{magdon-ismail_approximating_2010,
  title = {Approximating the {{Covariance Matrix}} with {{Low-rank Perturbations}}},
  author = {Magdon-Ismail, Malik and Purnell, Jonathan T.},
  date = {2010},
  journaltitle = {Intelligent Data Engineering and Automated Learning (IDEAL)},
  pages = {300--307},
  abstract = {Covariance matrices capture correlations that are invaluable in mod-eling real-life datasets. Using all d 2 elements of the covariance (in d dimensions) is costly and could result in over-fitting; and the simple diagonal approximation can be over-restrictive. We present an algorithm that improves upon the diagonal matrix by allowing a low rank perturbation. The efficiency is comparable to the diagonal approximation, yet one can capture correlations among the dimensions. We show that this method outperforms the diagonal when training GMMs on both synthetic and real-world data.}
}

@misc{magicdatatechnologycoltd_magicdata_2019,
  title = {{{MAGICDATA Mandarin Chinese Read Speech Corpus}}},
  author = {{Magic Data Technology Co., Ltd.}},
  date = {2019},
  url = {https://openslr.org/68/}
}

@article{magnus_matrix_1985,
  title = {Matrix Differential Calculus with Applications to Simple, Hadamard, and Kronecker Products},
  author = {Magnus, Jan R. and Neudecker, H.},
  date = {1985-12},
  journaltitle = {Journal of Mathematical Psychology},
  volume = {29},
  number = {4},
  pages = {474--492},
  issn = {00222496},
  doi = {10/fg7r4m},
  url = {https://linkinghub.elsevier.com/retrieve/pii/0022249685900069},
  urldate = {2020-09-16},
  langid = {english}
}

@misc{makhzani_adversarial_2015,
  title = {Adversarial {{Autoencoders}}},
  author = {Makhzani, Alireza and Shlens, Jonathon and Jaitly, Navdeep and Goodfellow, Ian and Frey, Brendan},
  date = {2015-11-17},
  eprint = {1511.05644},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1511.05644},
  urldate = {2018-06-07},
  abstract = {In this paper, we propose the "adversarial autoencoder" (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how the adversarial autoencoder can be used in applications such as semi-supervised classification, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks.},
  isbn = {9781509008063}
}

@article{malykh_robust_2018,
  title = {Robust {{Word Vectors}}: {{Context-Informed Embeddings}} for {{Noisy Texts}}},
  author = {Malykh, Valentin and Khakhulin, Taras and Logacheva, Varvara},
  date = {2018},
  pages = {10},
  abstract = {We suggest a new language-independent architecture of robust word vectors (RoVe). It is designed to alleviate the issue of typos, which are common in almost any user-generated content, and hinder automatic text processing. Our model is morphologically motivated, which allows it to deal with unseen word forms in morphologically rich languages. We present the results on a number of Natural Language Processing (NLP) tasks and languages for the variety of related architectures and show that proposed architecture is typo-proof.},
  langid = {english}
}

@misc{mania_simple_2018,
  title = {Simple Random Search Provides a Competitive Approach to Reinforcement Learning},
  author = {Mania, Horia and Guy, Aurelia and Recht, Benjamin},
  date = {2018},
  eprint = {1803.07055},
  eprinttype = {arxiv},
  url = {https://arxiv.org/abs/1803.07055},
  urldate = {2018-03-22},
  abstract = {A common belief in model-free reinforcement learning is that methods based on random search in the parameter space of policies exhibit significantly worse sample complexity than those that explore the space of actions. We dispel such beliefs by introducing a random search method for training static, linear policies for continuous control problems, matching state-of-the-art sample efficiency on the benchmark MuJoCo locomotion tasks. Our method also finds a nearly optimal controller for a challenging instance of the Linear Quadratic Regulator, a classi-cal problem in control theory, when the dynamics are not known. Computationally, our random search algorithm is at least 15 times more efficient than the fastest competing model-free meth-ods on these benchmarks. We take advantage of this computational efficiency to evaluate the performance of our method over hundreds of random seeds and many different hyperparam-eter configurations for each benchmark task. Our simulations highlight a high variability in performance in these benchmark tasks, suggesting that commonly used estimations of sample efficiency do not adequately evaluate the performance of RL algorithms.}
}

@article{mann_test_1947,
  title = {On a {{Test}} of {{Whether}} One of {{Two Random Variables}} Is {{Stochastically Larger}} than the {{Other}}},
  author = {Mann, H. B. and Whitney, D. R.},
  date = {1947},
  journaltitle = {The Annals of Mathematical Statistics},
  volume = {18},
  number = {1},
  eprint = {10226},
  eprinttype = {pmid},
  pages = {50--60},
  issn = {0003-4851},
  doi = {10.1214/aoms/1177730491},
  url = {http://projecteuclid.org/euclid.aoms/1177730491},
  abstract = {The object of this note is to point out and discuss a simple transformation of an absolutely continuous k-variate distribution in the uniform distribution on the k-dimensional hypercube.}
}

@misc{mao_deep_2014,
  title = {Deep {{Captioning}} with {{Multimodal Recurrent Neural Networks}} (m-{{RNN}})},
  author = {Mao, Junhua and Xu, Wei and Yang, Yi and Wang, Jiang and Huang, Zhiheng and Yuille, Alan L.},
  date = {2014},
  eprint = {1412.6632},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1412.6632},
  abstract = {In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel image captions. It directly models the probability distribution of generating a word given previous words and an image. Image captions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on four benchmark datasets (IAPR TC-12, Flickr 8K, Flickr 30K and MS COCO). Our model outperforms the state-of-the-art methods. In addition, we apply the m-RNN model to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval. The project page of this work is: www.stat.ucla.edu/\textasciitilde junhua.mao/m-RNN.html .},
  isbn = {1558608044}
}

@online{mao_speech_2020,
  title = {Speech {{Recognition}} and {{Multi-Speaker Diarization}} of {{Long Conversations}}},
  author = {Mao, Huanru Henry and Li, Shuyang and McAuley, Julian and Cottrell, Garrison},
  date = {2020-11-04},
  eprint = {2005.08072},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2005.08072},
  urldate = {2023-04-14},
  abstract = {Speech recognition (ASR) and speaker diarization (SD) models have traditionally been trained separately to produce rich conversation transcripts with speaker labels. Recent advances have shown that joint ASR and SD models can learn to leverage audio-lexical inter-dependencies to improve word diarization performance. We introduce a new benchmark of hour-long podcasts collected from the weekly This American Life radio program to better compare these approaches when applied to extended multi-speaker conversations. We find that training separate ASR and SD models perform better when utterance boundaries are known but otherwise joint models can perform better. To handle long conversations with unknown utterance boundaries, we introduce a striding attention decoding algorithm and data augmentation techniques which, combined with model pre-training, improves ASR and SD.},
  pubstate = {preprint}
}

@misc{marcus_deep_2018,
  title = {Deep {{Learning}}: {{A Critical Appraisal}}},
  author = {Marcus, Gary},
  date = {2018},
  eprint = {1801.00631},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1801.00631},
  abstract = {Although deep learning has historical roots going back decades, neither the term "deep learning" nor the approach was popular just over five years ago, when the field was reignited by papers such as Krizhevsky, Sutskever and Hinton's now classic (2012) deep network model of Imagenet. What has the field discovered in the five subsequent years? Against a background of considerable progress in areas such as speech recognition, image recognition, and game playing, and considerable enthusiasm in the popular press, I present ten concerns for deep learning, and suggest that deep learning must be supplemented by other techniques if we are to reach artificial general intelligence.}
}

@article{martens_new_2020,
  title = {New Insights and Perspectives on the Natural Gradient Method},
  author = {Martens, J.},
  date = {2020},
  journaltitle = {Journal of Machine Learning Research}
}

@inproceedings{martens_optimizing_2015,
  title = {Optimizing Neural Networks with Kronecker-Factored Approximate Curvature},
  booktitle = {International Conference on Machine Learning ({{ICML}})},
  author = {Martens, J. and Grosse, R.},
  date = {2015},
  publisher = {{PMLR}}
}

@inproceedings{masegosa_learning_2020,
  title = {Learning under Model Misspecification: {{Applications}} to Variational and Ensemble Methods},
  booktitle = {Advances in Neural Information Processing Systems 33: {{Annual}} Conference on Neural Information Processing Systems 2020, {{NeurIPS}} 2020, December 6-12, 2020, Virtual},
  author = {Masegosa, Andrés R.},
  editor = {Larochelle, Hugo and Ranzato, Marc'Aurelio and Hadsell, Raia and Balcan, Maria-Florina and Lin, Hsuan-Tien},
  date = {2020},
  url = {https://proceedings.neurips.cc/paper/2020/hash/3ac48664b7886cf4e4ab4aba7e6b6bc9-Abstract.html}
}

@misc{masters_revisiting_2018,
  title = {Revisiting {{Small Batch Training}} for {{Deep Neural Networks}}},
  author = {Masters, Dominic and Luschi, Carlo},
  date = {2018},
  eprint = {1804.07612},
  eprinttype = {arxiv},
  abstract = {Modern deep neural network training is typically based on mini-batch stochastic gradient optimization. While the use of large mini-batches increases the avail-able computational parallelism, small batch training has been shown to provide improved generalization performance and allows a significantly smaller memory footprint, which might also be exploited to improve machine throughput. In this paper, we review common assumptions on learning rate scaling and train-ing duration, as a basis for an experimental comparison of test performance for different mini-batch sizes. We adopt a learning rate that corresponds to a constant average weight update per gradient calculation (i.e., per unit cost of computation), and point out that this results in a variance of the weight updates that increases linearly with the mini-batch size m. The collected experimental results for the CIFAR-10, CIFAR-100 and ImageNet datasets show that increasing the mini-batch size progressively reduces the range of learning rates that provide stable convergence and acceptable test performance. On the other hand, small mini-batch sizes provide more up-to-date gradient cal-culations, which yields more stable and reliable training. The best performance has been consistently obtained for mini-batch sizes between m = 2 and m = 32, which contrasts with recent work advocating the use of mini-batch sizes in the thousands.},
  keywords = {★}
}

@inproceedings{masumura_phonemetographeme_2020,
  title = {Phoneme-to-Grapheme Conversion Based Large-Scale Pre-Training for End-to-End Automatic Speech Recognition},
  booktitle = {Proceedings of the {{Annual Conference}} of the {{International Speech Communication Association}} ({{Interspeech}})},
  author = {Masumura, Ryo and Makishima, Naoki and Ihori, Mana and Takashima, Akihiko and Tanaka, Tomohiro and Orihashi, Shota},
  date = {2020}
}

@misc{mathieu_disentangling_2019,
  title = {Disentangling {{Disentanglement}} in {{Variational Autoencoders}}},
  author = {Mathieu, Emile and Rainforth, Tom and Siddharth, N. and Teh, Yee Whye},
  date = {2019-06-12},
  eprint = {1812.02833},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1812.02833},
  urldate = {2021-04-20},
  abstract = {We develop a generalisation of disentanglement in variational autoencoders (VAEs)—decomposition of the latent representation—characterising it as the fulfilment of two factors: a) the latent encodings of the data having an appropriate level of overlap, and b) the aggregate encoding of the data conforming to a desired structure, represented through the prior. Decomposition permits disentanglement, i.e. explicit independence between latents, as a special case, but also allows for a much richer class of properties to be imposed on the learnt representation, such as sparsity, clustering, independent subspaces, or even intricate hierarchical dependency relationships. We show that the β-VAE varies from the standard VAE predominantly in its control of latent overlap and that for the standard choice of an isotropic Gaussian prior, its objective is invariant to rotations of the latent representation. Viewed from the decomposition perspective, breaking this invariance with simple manipulations of the prior can yield better disentanglement with little or no detriment to reconstructions. We further demonstrate how other choices of prior can assist in producing different decompositions and introduce an alternative training objective that allows the control of both decomposition factors in a principled manner.},
  langid = {english}
}

@inproceedings{mattei_leveraging_2018,
  title = {Leveraging the Exact Likelihood of Deep Latent Variable Models},
  booktitle = {Proceedings of the 32nd {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Mattei, Pierre-Alexandre and Frellsen, Jes},
  date = {2018},
  pages = {3859--3870},
  location = {{Montreal, Canada}},
  url = {https://proceedings.neurips.cc/paper/2018/hash/0609154fa35b3194026346c9cac2a248-Abstract.html},
  eventtitle = {Neural {{Information Processing Systems}}}
}

@inproceedings{mattei_miwae_2019,
  title = {{{MIWAE}}: {{Deep}} Generative Modelling and Imputation of Incomplete Data Sets},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Mattei, Pierre-Alexandre and Frellsen, Jes},
  date = {2019},
  pages = {4413--4423},
  location = {{Long Beach, CA, USA}},
  url = {http://proceedings.mlr.press/v97/mattei19a.html},
  eventtitle = {International {{Conference}} on {{Machine Learning}}}
}

@inproceedings{mattei_refit_2018,
  title = {Refit Your Encoder When New Data Comes By},
  booktitle = {3rd {{NeurIPS Workshop}} on {{Bayesian Deep Learning}}},
  author = {Mattei, Pierre-Alexandre and Frellsen, Jes},
  date = {2018}
}

@article{mazidi_infusing_2016,
  title = {Infusing {{NLU}} into {{Automatic Question Generation}}},
  author = {Mazidi, Karen and Tarau, Paul},
  date = {2016},
  journaltitle = {Proceedings of the International Conference on Natural Language Generation (INLG)},
  number = {2011},
  pages = {51--60},
  url = {http://www.aclweb.org/anthology/W16-6609},
  urldate = {2018-06-06},
  abstract = {We present a fresh approach to automatic question generation that significantly in-creases the percentage of acceptable questions compared to prior state-of-the-art systems. In our evaluation of the top 20 questions, our sys-tem generated 71\% more acceptable questions by informing the generation process with Nat-ural Language Understanding techniques. The system also introduces our DeconStructure al-gorithm which creates an intuitive and prac-tical structure for easily accessing sentence functional constituents in NLP applications.}
}

@inproceedings{mcallester_formal_2020,
  title = {Formal Limitations on the Measurement of Mutual Information},
  booktitle = {Proceedings of the 23rd {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}} (Aistats)},
  author = {McAllester, David and Stratos, Karl},
  editor = {Chiappa, Silvia and Calandra, Roberto},
  date = {2020},
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {108},
  pages = {875--884},
  publisher = {{PMLR}},
  location = {{Palermo, Sicily, Italy}},
  url = {http://proceedings.mlr.press/v108/mcallester20a.html}
}

@article{mcauley_image-based_2015,
  title = {Image-Based {{Recommendations}} on {{Styles}} and {{Substitutes}}},
  author = {McAuley, Julian},
  date = {2015}
}

@article{mcauley_inferring_2015,
  title = {Inferring {{Networks}} of {{Substitutable}} and {{Complementary Products}}},
  author = {McAuley, Julian},
  date = {2015}
}

@misc{mccann_learned_2017,
  title = {Learned in {{Translation}}: {{Contextualized Word Vectors}}},
  author = {McCann, Bryan and Bradbury, James and Xiong, Caiming and Socher, Richard},
  date = {2017},
  eprint = {1708.00107},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1708.00107},
  urldate = {2018-05-02},
  abstract = {Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art.}
}

@article{mcclelland_why_1995,
  title = {Why There Are Complementary Learning Systems in the Hippocampus and Neocortex: {{Insights}} from the Successes and Failures of Connectionist Models of Learning and Memory},
  author = {McClelland, James L. and McNaughton, Bruce L. and O'Reilly, Randall C.},
  date = {1995},
  journaltitle = {Psychological Review},
  volume = {102},
  number = {3},
  eprint = {7624455},
  eprinttype = {pmid},
  pages = {419--457},
  issn = {0033295X},
  doi = {10.1037/0033-295X.102.3.419},
  abstract = {Damage to the hippocampal system disrupts recent memory but leaves remote memory intact. The account presented here suggests that memories are first stored via synaptic changes in the hippocampal system, that these changes support reinstatement of recent memories in the neocortex, that neocortical synapses change a little on each reinstatement, and that remote memory is based on accumulated neocortical changes. Models that learn via changes to connections help explain this organization. These models discover the structure in ensembles of items if learning of each item is gradual and interleaved with learning about other items. This suggests that the neocortex learns slowly to discover the structure in ensembles of experiences. The hippocampal system permits rapid learning of new items without disrupting this structure, and reinstatement of new memories interleaves them with others to integrate them into structured neocortical memory systems.},
  isbn = {0033-295X (Print)\textbackslash r0033-295X (Linking)}
}

@article{mcculloch_logical_1943,
  title = {A {{Logical Calculus}} of the {{Idea Immanent}} in {{Nervous Activity}}},
  author = {McCulloch, Warren S. and Pitts, W.},
  date = {1943},
  journaltitle = {Bulletin of Mathematical Biophysics},
  volume = {5},
  number = {4},
  eprint = {2185863},
  eprinttype = {pmid},
  pages = {115--133},
  issn = {0007-4985},
  doi = {10.1007/BF02478259},
  abstract = {Because of the 'all-or-none' character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behaviour of every net can be described in these terms, with the addition of more complicated logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which nehaves under the other and gives the same results, algthough perhaps not in the same time. Various applications of the calculus are discussed.}
}

@article{mcnemar_note_1947,
  title = {Note on the Sampling Error of the Difference between Correlated Proportions or Percentages},
  author = {McNemar, Quinn},
  date = {1947},
  journaltitle = {Psychometrika},
  volume = {12},
  number = {2},
  pages = {153--157},
  publisher = {{Springer-Verlag New York}}
}

@article{mehta_mobilevit_2022,
  title = {{{MobileViT}}: {{Light-Weight}}, {{General-Purpose}}, and {{Mobile-Friendly Vision Transformer}}},
  author = {Mehta, Sachin and Rastegari, Mohammad},
  date = {2022},
  pages = {26},
  abstract = {Light-weight convolutional neural networks (CNNs) are the de-facto for mobile vision tasks. Their spatial inductive biases allow them to learn representations with fewer parameters across different vision tasks. However, these networks are spatially local. To learn global representations, self-attention-based vision transformers (ViTs) have been adopted. Unlike CNNs, ViTs are heavyweight. In this paper, we ask the following question: is it possible to combine the strengths of CNNs and ViTs to build a light-weight and low latency network for mobile vision tasks? Towards this end, we introduce MobileViT, a light-weight and general-purpose vision transformer for mobile devices. MobileViT presents a different perspective for the global processing of information with transformers. Our results show that MobileViT significantly outperforms CNNand ViT-based networks across different tasks and datasets. On the ImageNet-1k dataset, MobileViT achieves top-1 accuracy of 78.4\% with about 6 million parameters, which is 3.2\% and 6.2\% more accurate than MobileNetv3 (CNN-based) and DeIT (ViT-based) for a similar number of parameters. On the MS-COCO object detection task, MobileViT is 5.7\% more accurate than MobileNetv3 for a similar number of parameters. Our source code is open-source and available at: https://github.com/apple/ml-cvnets.},
  langid = {english},
  keywords = {Read}
}

@inproceedings{mei_imputing_2019,
  title = {Imputing {{Missing Events}} in {{Continuous-Time Event Streams}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Mei, Hongyuan and Qin, Guanghui and Eisner, Jason},
  editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  date = {2019-06-09/2019-06-15},
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {97},
  pages = {4475--4485},
  publisher = {{PMLR}},
  location = {{Long Beach, CA, USA}},
  url = {https://proceedings.mlr.press/v97/mei19a.html},
  abstract = {Events in the world may be caused by other, unobserved events. We consider sequences of events in continuous time. Given a probability model of complete sequences, we propose particle smoothing—a form of sequential importance sampling—to impute the missing events in an incomplete sequence. We develop a trainable family of proposal distributions based on a type of bidirectional continuous-time LSTM: Bidirectionality lets the proposals condition on future observations, not just on the past as in particle filtering. Our method can sample an ensemble of possible complete sequences (particles), from which we form a single consensus prediction that has low Bayes risk under our chosen loss metric. We experiment in multiple synthetic and real domains, using different missingness mechanisms, and modeling the complete sequences in each domain with a neural Hawkes process (Mei \&amp; Eisner 2017). On held-out incomplete sequences, our method is effective at inferring the ground-truth unobserved events, with particle smoothing consistently improving upon particle filtering.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}}
}

@misc{melis_state_2017,
  title = {On the {{State}} of the {{Art}} of {{Evaluation}} in {{Neural Language Models}}},
  author = {Melis, Gábor and Dyer, Chris and Blunsom, Phil},
  date = {2017-07-18},
  eprint = {1707.05589},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1707.05589},
  urldate = {2018-06-03},
  abstract = {Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing code bases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.},
  isbn = {9781604562170}
}

@inproceedings{melzer_nonlinear_2001,
  title = {Nonlinear Feature Extraction Using Generalized Canonical Correlation Analysis},
  booktitle = {International Conference on Artificial Neural Networks},
  author = {Melzer, Thomas and Reiter, Michael and Bischof, Horst},
  date = {2001}
}

@inproceedings{meng_adavit_2022,
  title = {{{AdaViT}}: {{Adaptive Vision Transformers}} for {{Efficient Image Recognition}}},
  shorttitle = {{{AdaViT}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Meng, Lingchen and Li, Hengduo and Chen, Bor-Chun and Lan, Shiyi and Wu, Zuxuan and Jiang, Yu-Gang and Lim, Ser-Nam},
  date = {2022-06},
  pages = {12299--12308},
  publisher = {{IEEE}},
  location = {{New Orleans, LA, USA}},
  doi = {10.1109/CVPR52688.2022.01199},
  url = {https://ieeexplore.ieee.org/document/9879366/},
  urldate = {2022-10-08},
  abstract = {Built on top of self-attention mechanisms, vision transformers have demonstrated remarkable performance on a variety of tasks recently. While achieving excellent performance, they still require relatively intensive computational cost that scales up drastically as the numbers of patches, self-attention heads and transformer blocks increase. In this paper, we argue that due to the large variations among images, their need for modeling long-range dependencies between patches differ. To this end, we introduce AdaViT, an adaptive computation framework that learns to derive usage policies on which patches, self-attention heads and transformer blocks to use throughout the backbone on a perinput basis, aiming to improve inference efficiency of vision transformers with a minimal drop of accuracy for image recognition. Optimized jointly with a transformer backbone in an end-to-end manner, a light-weight decision network is attached to the backbone to produce decisions on-the-fly. Extensive experiments on ImageNet demonstrate that our method obtains more than 2× improvement on efficiency compared to state-of-the-art vision transformers with only 0.8\% drop of accuracy, achieving good efficiency/accuracy trade-offs conditioned on different computational budgets. We further conduct quantitative and qualitative analysis on learned usage polices and provide more insights on the redundancy in vision transformers. Code is available at https://github.com/MengLcool/AdaViT.},
  eventtitle = {{{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-66546-946-3},
  langid = {english}
}

@inproceedings{merboldt_analysis_2019,
  title = {An {{Analysis}} of {{Local Monotonic Attention Variants}}},
  booktitle = {Proceedings of the {{Annual Conference}} of the {{International Speech Communication Association}} ({{Interspeech}})},
  author = {Merboldt, André and Zeyer, Albert and Schlüter, Ralf and Ney, Hermann},
  date = {2019-09-15},
  pages = {1398--1402},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2019-2879},
  url = {https://www.isca-speech.org/archive/interspeech_2019/merboldt19_interspeech.html},
  urldate = {2023-04-14},
  abstract = {Speech recognition using attention-based models is an effective approach to transcribing audio directly to text within an integrated end-to-end architecture. Global attention approaches compute a weighting over the complete input sequence, whereas local attention mechanisms are restricted to only a localized window of the sequence. For speech, the latter approach supports the monotonicity property of the speech-text alignment. Therefore, we revise several variants of such models and provide a comprehensive comparison, which has been missing so far in the literature. Additionally, we introduce a simple technique to implement windowed attention. This can be applied on top of an existing global attention model. The goal is to transition into a local attention model, by using a local window for the otherwise unchanged attention mechanism, starting from the temporal position with the most recent most active attention energy. We test this method on Switchboard and LibriSpeech and show that the proposed model can even be trained from random initialization and achieve results comparable to the global attention baseline.},
  langid = {english}
}

@online{meripo_asr_2022,
  title = {{{ASR Error Detection}} via {{Audio-Transcript}} Entailment},
  author = {Meripo, Nimshi Venkat and Konam, Sandeep},
  date = {2022-07-21},
  eprint = {2207.10849},
  eprinttype = {arxiv},
  eprintclass = {cs, eess, stat},
  url = {http://arxiv.org/abs/2207.10849},
  urldate = {2023-10-18},
  abstract = {Despite improved performances of the latest Automatic Speech Recognition (ASR) systems, transcription errors are still unavoidable. These errors can have a considerable impact in critical domains such as healthcare, when used to help with clinical documentation. Therefore, detecting ASR errors is a critical first step in preventing further error propagation to downstream applications. To this end, we propose a novel end-to-end approach for ASR error detection using audio-transcript entailment. To the best of our knowledge, we are the first to frame this problem as an end-to-end entailment task between the audio segment and its corresponding transcript segment. Our intuition is that there should be a bidirectional entailment between audio and transcript when there is no recognition error and vice versa. The proposed model utilizes an acoustic encoder and a linguistic encoder to model the speech and transcript respectively. The encoded representations of both modalities are fused to predict the entailment. Since doctor-patient conversations are used in our experiments, a particular emphasis is placed on medical terms. Our proposed model achieves classification error rates (CER) of 26.2\% on all transcription errors and 23\% on medical errors specifically, leading to improvements upon a strong baseline by 12\% and 15.4\%, respectively.},
  pubstate = {preprint}
}

@article{merkx_language_2019,
  title = {Language Learning Using Speech to Image Retrieval},
  author = {Merkx, Danny and Frank, Stefan L and Ernestus, Mirjam},
  date = {2019},
  journaltitle = {Annual Conference of the International Speech Communication Association}
}

@article{mesaros_detection_2018,
  title = {Detection and Classification of Acoustic Scenes and Events},
  author = {Mesaros, Annamaria and Heittola, Toni and Virtanen, Tuomas},
  date = {2018},
  journaltitle = {Proceedings of the Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE)},
  pages = {9--13}
}

@misc{mescheder_adversarial_2017,
  title = {Adversarial {{Variational Bayes}}: {{Unifying Variational Autoencoders}} and {{Generative Adversarial Networks}}},
  author = {Mescheder, Lars and Nowozin, Sebastian and Geiger, Andreas},
  date = {2017-01-17},
  eprint = {1701.04722},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1701.04722},
  urldate = {2018-06-07},
  abstract = {Variational Autoencoders (VAEs) are expressive latent variable models that can be used to learn complex probability distributions from training data. However, the quality of the resulting model crucially relies on the expressiveness of the inference model. We introduce Adversarial Variational Bayes (AVB), a technique for training Variational Autoencoders with arbitrarily expressive inference models. We achieve this by introducing an auxiliary discriminative network that allows to rephrase the maximum-likelihood-problem as a two-player game, hence establishing a principled connection between VAEs and Generative Adversarial Networks (GANs). We show that in the nonparametric limit our method yields an exact maximum-likelihood assignment for the parameters of the generative model, as well as the exact posterior distribution over the latent variables given an observation. Contrary to competing approaches which combine VAEs with GANs, our approach has a clear theoretical justification, retains most advantages of standard Variational Autoencoders and is easy to implement.}
}

@unpublished{meyer_unsupervised_2017,
  title = {Unsupervised {{Feature Learning}} for {{Audio Analysis}}},
  author = {Meyer, Matthias and Beutel, Jan and Thiele, Lothar},
  date = {2017-12-11},
  eprint = {1712.03835},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1712.03835},
  urldate = {2021-10-22},
  abstract = {Identifying acoustic events from a continuously streaming audio source is of interest for many applications including environmental monitoring for basic research. In this scenario neither different event classes are known nor what distinguishes one class from another. Therefore, an unsupervised feature learning method for exploration of audio data is presented in this paper. It incorporates the two following novel contributions: First, an audio frame predictor based on a Convolutional LSTM autoencoder is demonstrated, which is used for unsupervised feature extraction. Second, a training method for autoencoders is presented, which leads to distinct features by amplifying event similarities. In comparison to standard approaches, the features extracted from the audio frame predictor trained with the novel approach show 13 \% better results when used with a classifier and 36 \% better results when used for clustering.}
}

@misc{miao_language_2016,
  title = {Language as a {{Latent Variable}}: {{Discrete Generative Models}} for {{Sentence Compression}}},
  shorttitle = {Language as a {{Latent Variable}}},
  author = {Miao, Yishu and Blunsom, Phil},
  date = {2016-10-13},
  eprint = {1609.07317},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1609.07317},
  urldate = {2021-05-19},
  abstract = {In this work we explore deep generative models of text in which the latent representation of a document is itself drawn from a discrete language model distribution. We formulate a variational auto-encoder for inference in this model and apply it to the task of compressing sentences. In this application the generative model first draws a latent summary sentence from a background language model, and then subsequently draws the observed sentence conditioned on this latent summary. In our empirical evaluation we show that generative formulations of both abstractive and extractive compression yield state-of-the-art results when trained on a large amount of supervised data. Further, we explore semi-supervised compression scenarios where we show that it is possible to achieve performance competitive with previously proposed supervised models while training on a fraction of the supervised data.},
  langid = {english}
}

@inproceedings{michaeli_nonparametric_2016,
  title = {Nonparametric Canonical Correlation Analysis},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Michaeli, Tomer and Wang, Weiran and Livescu, Karen},
  date = {2016},
  pages = {1967--1976}
}

@inproceedings{michalopoulosICDBigBirdContextualEmbedding2022,
  title = {{{ICDBigBird}}: {{A Contextual Embedding Model}} for {{ICD Code Classification}}},
  shorttitle = {{{ICDBigBird}}},
  booktitle = {Proceedings of the 21st {{Workshop}} on {{Biomedical Language Processing}}},
  author = {Michalopoulos, George and Malyska, Michal and Sahar, Nicola and Wong, Alexander and Chen, Helen},
  date = {2022-05},
  pages = {330--336},
  publisher = {{Association for Computational Linguistics}},
  location = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.bionlp-1.32},
  abstract = {The International Classification of Diseases (ICD) system is the international standard for classifying diseases and procedures during a healthcare encounter and is widely used for healthcare reporting and management purposes. Assigning correct codes for clinical procedures is important for clinical, operational and financial decision-making in healthcare. Contextual word embedding models have achieved state-of-the-art results in multiple NLP tasks. However, these models have yet to achieve state-of-the-art results in the ICD classification task since one of their main disadvantages is that they can only process documents that contain a small number of tokens which is rarely the case with real patient notes. In this paper, we introduce ICDBigBird a BigBird-based model which can integrate a Graph Convolutional Network (GCN), that takes advantage of the relations between ICD codes in order to create `enriched' representations of their embeddings, with a BigBird contextual model that can process larger documents. Our experiments on a real-world clinical dataset demonstrate the effectiveness of our BigBird-based model on the ICD classification task as it outperforms the previous state-of-the-art models.}
}

@article{michel_blind_2017,
  title = {Blind Phoneme Segmentation with Temporal Prediction Errors},
  author = {Michel, Paul and Rasanen, Okko and Thiollière, Roland and Dupoux, Emmanuel},
  date = {2017},
  journaltitle = {Association for Computational Linguistics (ACL) Student Research Workshop}
}

@misc{michelis_linear_2021,
  title = {On {{Linear Interpolation}} in the {{Latent Space}} of {{Deep Generative Models}}},
  author = {Michelis, Mike Yan and Becker, Quentin},
  date = {2021-05-08},
  eprint = {2105.03663},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2105.03663},
  urldate = {2021-05-18},
  abstract = {The underlying geometrical structure of the latent space in deep generative models is in most cases not Euclidean, which may lead to biases when comparing interpolation capabilities of two models. Smoothness and plausibility of linear interpolations in latent space are associated with the quality of the underlying generative model. In this paper, we show that not all such interpolations are comparable as they can deviate arbitrarily from the shortest interpolation curve given by the geodesic. This deviation is revealed by computing curve lengths with the pull-back metric of the generative model, finding shorter curves than the straight line between endpoints, and measuring a non-zero relative length improvement on this straight line. This leads to a strategy to compare linear interpolations across two generative models. We also show the effect and importance of choosing an appropriate output space for computing shorter curves. For this computation we derive an extension of the pull-back metric.},
  langid = {english}
}

@article{mikolov_distributed_2013,
  title = {Distributed {{Representations}} of {{Words}} and {{Phrases}} and Their {{Compositionality}}},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  date = {2013},
  journaltitle = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
  eprint = {903},
  eprinttype = {pmid},
  pages = {3111--3119},
  issn = {10495258},
  doi = {10.1162/jmlr.2003.3.4-5.951},
  abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
  isbn = {2150-8097}
}

@inproceedings{mikolov_efficient_2013,
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  booktitle = {Workshop {{Track Proceedings}} of the {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  date = {2013},
  eprint = {1301.3781},
  eprinttype = {arxiv},
  location = {{Scottsdale, Arizona, USA}},
  url = {https://arxiv.org/abs/1301.3781},
  urldate = {2018-04-24},
  abstract = {We propose two novel model architectures for computing continuous vector repre-sentations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previ-ously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art perfor-mance on our test set for measuring syntactic and semantic word similarities.}
}

@inproceedings{mikolov_recurrent_2010,
  title = {Recurrent Neural Network Based Language Model.},
  booktitle = {Proceedings of the {{Annual Conference}} of the {{International Speech Communication Association}} ({{Interspeech}})},
  author = {Mikolov, Tomas and Karafiát, Martin and Burget, Lukas and Cernockỳ, Jan and Khudanpur, Sanjeev},
  date = {2010},
  volume = {2},
  number = {3},
  pages = {1045--1048},
  publisher = {{Makuhari}}
}

@article{mikolov_subword_nodate,
  title = {Subword {{Language Modelling}} with {{Neural Networks}}},
  author = {Mikolov, Tomasˇ and Sutskever, Ilya and Deoras, Anoop and Le, Hai-Son and Kombrink, Stefan},
  pages = {4},
  abstract = {We explore the performance of several types of language models on the word-level and the character-level language modeling tasks. This includes two recently proposed recurrent neural network architectures, a feedforward neural network model, a maximum entropy model and the usual smoothed n-gram models. We then propose a simple technique for learning sub-word level units from the data, and show that it combines advantages of both character and wordlevel models. Finally, we show that neural network based language models can be order of magnitude smaller than compressed n-gram models, at the same level of performance when applied to a Broadcast news RT04 speech recognition task. By using sub-word units, the size can be reduced even more.},
  langid = {english}
}

@online{milde_unspeech_2018,
  title = {Unspeech: {{Unsupervised Speech Context Embeddings}}},
  shorttitle = {Unspeech},
  author = {Milde, Benjamin and Biemann, Chris},
  date = {2018-08-23},
  eprint = {1804.0677},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1804.06775},
  urldate = {2021-10-21},
  abstract = {We introduce "Unspeech" embeddings, which are based on unsupervised learning of context feature representations for spoken language. The embeddings were trained on up to 9500 hours of crawled English speech data without transcriptions or speaker information, by using a straightforward learning objective based on context and non-context discrimination with negative sampling. We use a Siamese convolutional neural network architecture to train Unspeech embeddings and evaluate them on speaker comparison, utterance clustering and as a context feature in TDNN-HMM acoustic models trained on TED-LIUM, comparing it to i-vector baselines. Particularly decoding out-of-domain speech data from the recently released Common Voice corpus shows consistent WER reductions. We release our source code and pre-trained Unspeech models under a permissive open source license.},
  pubstate = {preprint}
}

@misc{miller_introduction_2016,
  title = {Lecture {{Notes}} for "15-750 {{Graduate Algorithms}}": {{Lecture}} 1: {{Introduction}} and {{Strassen}}’s {{Algorithm}}},
  author = {Miller, Gary},
  date = {2016-01},
  organization = {{Carnegie Mellon University, Department of Computer Science}}
}

@book{minsky_introduction_1969,
  title = {An {{Introduction}} to {{Computational Geometry}}},
  author = {Minsky, Marvin and Papert, Seymour A.},
  date = {1969},
  journaltitle = {MIT Press},
  publisher = {{MIT Press}},
  abstract = {I. Algebraic theory of linear parallel predicates -- 1. Theory of linear Boolean inequalities -- 2. Group invariance of Boolean inequalities -- 3. Parity and one-in-a-box predicates -- 4. The "and/or" theorem -- II. Geometric theory of linear inequalities -- 5. Ψconnected: a geometric property with unbounded order -- 6. Geometric patterns of small order: spectra and context -- 7. Stratification and normalization -- 8. The diameter-limited perceptron -- 9. Geometric predicates and serial algorithms -- III. Learning theory -- 10. Magnitude of the coefficients -- 11. Learning -- 12. Linear separation and learning -- 13. Perceptrons and pattern recognition.},
  isbn = {0-262-63022-2},
  pagetotal = {258}
}

@misc{mishkin_all_2016,
  title = {All You Need Is a Good Init},
  author = {Mishkin, Dmytro and Matas, Jiri},
  date = {2016-02-19},
  eprint = {1511.06422},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1511.06422},
  urldate = {2021-12-19},
  abstract = {Layer-sequential unit-variance (LSUV) initialization – a simple method for weight initialization for deep net learning – is proposed. The method consists of the two steps. First, pre-initialize weights of each convolution or inner-product layer with orthonormal matrices. Second, proceed from the first to the final layer, normalizing the variance of the output of each layer to be equal to one.},
  langid = {english}
}

@misc{mitchell_why_2021,
  title = {Why {{AI}} Is {{Harder Than We Think}}},
  author = {Mitchell, Melanie},
  date = {2021-04-28},
  eprint = {2104.12871},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2104.12871},
  urldate = {2021-05-07},
  abstract = {Since its beginning in the 1950s, the field of artificial intelligence has cycled several times between periods of optimistic predictions and massive investment (“AI spring”) and periods of disappointment, loss of confidence, and reduced funding (“AI winter”). Even with today’s seemingly fast pace of AI breakthroughs, the development of long-promised technologies such as self-driving cars, housekeeping robots, and conversational companions has turned out to be much harder than many people expected. One reason for these repeating cycles is our limited understanding of the nature and complexity of intelligence itself. In this paper I describe four fallacies in common assumptions made by AI researchers, which can lead to overconfident predictions about the field. I conclude by discussing the open questions spurred by these fallacies, including the age-old challenge of imbuing machines with humanlike common sense.},
  langid = {english}
}

@misc{miyato_adversarial_2017,
  title = {{{ADVERSARIAL TRAINING METHODS FOR SEMI-SUPERVISED TEXT CLASSIFICATION}}},
  author = {Miyato, Takeru and Dai, Andrew M. and Goodfellow, Ian J.},
  date = {2017},
  eprint = {1605.07725v3},
  eprinttype = {arxiv},
  abstract = {Adversarial training provides a means of regularizing supervised learning algo-rithms while virtual adversarial training is able to extend supervised learning al-gorithms to the semi-supervised setting. However, both methods require making small perturbations to numerous entries of the input vector, which is inappropri-ate for sparse high-dimensional inputs such as one-hot word representations. We extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself. The proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks. We provide visualizations and analysis showing that the learned word embeddings have im-proved in quality and that while training, the model is less prone to overfitting.}
}

@online{miyazaki_structured_2022,
  title = {Structured {{State Space Decoder}} for {{Speech Recognition}} and {{Synthesis}}},
  author = {Miyazaki, Koichi and Murata, Masato and Koriyama, Tomoki},
  date = {2022-10-31},
  eprint = {2210.17098},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2210.17098},
  urldate = {2022-12-06},
  abstract = {Automatic speech recognition (ASR) systems developed in recent years have shown promising results with self-attention models (e.g., Transformer and Conformer), which are replacing conventional recurrent neural networks. Meanwhile, a structured state space model (S4) has been recently proposed, producing promising results for various long-sequence modeling tasks, including raw speech classification. The S4 model can be trained in parallel, same as the Transformer model. In this study, we applied S4 as a decoder for ASR and text-to-speech (TTS) tasks by comparing it with the Transformer decoder. For the ASR task, our experimental results demonstrate that the proposed model achieves a competitive word error rate (WER) of 1.88\%/4.25\% on LibriSpeech test-clean/test-other set and a character error rate (CER) of 3.80\%/2.63\%/2.98\% on the CSJ eval1/eval2/eval3 set. Furthermore, the proposed model is more robust than the standard Transformer model, particularly for long-form speech on both the datasets. For the TTS task, the proposed method outperforms the Transformer baseline.},
  langid = {english},
  pubstate = {preprint}
}

@article{mnih_asynchronous_2016,
  title = {Asynchronous {{Methods}} for {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Badia, Adrià Puigdomènech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  date = {2016},
  journaltitle = {Proceedings of the International Conference on Machine Learning (ICML)},
  eprint = {1000272564},
  eprinttype = {pmid},
  issn = {1938-7228},
  url = {http://arxiv.org/abs/1602.01783},
  abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
  isbn = {9781510829008}
}

@article{mnih_human-level_2015,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  date = {2015},
  journaltitle = {Nature},
  volume = {518},
  number = {7540},
  eprint = {25719670},
  eprinttype = {pmid},
  pages = {529--533},
  issn = {14764687},
  doi = {10.1038/nature14236},
  abstract = {The theory of reinforcement learning provides a normative account 1 , deeply rooted in psychological 2 and neuroscientific 3 perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory pro-cessing systems 4,5 , the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopa-minergic neurons and temporal difference reinforcement learning algorithms 3 . While reinforcement learning agents have achieved some successes in a variety of domains 6–8 , their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks 9–11 to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games 12}
}

@article{mnih_learning_2014,
  title = {Learning Word Embeddings Efficiently with Noise-Contrastive},
  author = {Mnih, Andriy},
  date = {2014},
  doi = {10.1.1.476.3088},
  url = {http://findit.dtu.dk/en/catalog/273153637},
  urldate = {2017-04-01}
}

@inproceedings{mnih_neural_2014,
  title = {Neural {{Variational Inference}} and {{Learning}} in {{Belief Networks}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Mnih, Andriy and Gregor, Karol},
  date = {2014},
  eprint = {1402.0030v2},
  eprinttype = {arxiv},
  pages = {1791--1799},
  publisher = {{PMLR}},
  abstract = {Highly expressive directed latent variable models , such as sigmoid belief networks, are difficult to train on large datasets because exact inference in them is intractable and none of the approximate inference methods that have been applied to them scale well. We propose a fast non-iterative approximate inference method that uses a feedforward network to implement efficient exact sampling from the variational posterior. The model and this inference network are trained jointly by maximizing a variational lower bound on the log-likelihood. Although the naive estimator of the inference network gradient is too high-variance to be useful, we make it practical by applying several straightforward model-independent variance reduction techniques. Applying our approach to training sigmoid belief networks and deep autoregressive networks, we show that it outperforms the wake-sleep algorithm on MNIST and achieves state-of-the-art results on the Reuters RCV1 document dataset.}
}

@misc{mnih_playing_2013,
  title = {Playing {{Atari}} with {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  date = {2013},
  eprint = {25719670},
  eprinttype = {pmid},
  url = {http://arxiv.org/abs/1312.5602},
  abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  isbn = {1476-4687 (Electronic) 0028-0836 (Linking)}
}

@inproceedings{mnih_variational_2016,
  title = {Variational {{Inference}} for {{Monte Carlo Objectives}}},
  booktitle = {Proceedings of the 33rd {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Mnih, Andriy and Rezende, Danilo Jimenez},
  date = {2016},
  pages = {2188--2196},
  location = {{New York, NY, USA}},
  url = {http://proceedings.mlr.press/v48/mnihb16.html}
}

@misc{mogren_crnngan_2016,
  title = {C-{{RNN-GAN}}: {{Continuous}} Recurrent Neural Networks with Adversarial Training},
  author = {Mogren, Olof},
  date = {2016},
  eprint = {1611.09904},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1611.09904},
  abstract = {Generative adversarial networks have been proposed as a way of efficiently training deep generative neural networks. We propose a generative adversarial model that works on continuous sequential data, and apply it by training it on a collection of classical music. We conclude that it generates music that sounds better and better as the model is trained, report statistics on generated music, and let the reader judge the quality by downloading the generated songs.}
}

@article{mohamed_acoustic_2012,
  title = {Acoustic Modeling Using Deep Belief Networks},
  author = {Mohamed, Abdel-rahman and Dahl, George E. and Hinton, Geoffrey E.},
  date = {2012},
  journaltitle = {IEEE Transactions on Audio, Speech, and Language Processing},
  volume = {20},
  number = {1},
  pages = {14--22},
  publisher = {{IEEE}}
}

@misc{mohamed_monte_2019,
  title = {Monte {{Carlo Gradient Estimation}} in {{Machine Learning}}},
  author = {Mohamed, Shakir and Rosca, Mihaela and Figurnov, Michael and Mnih, Andriy},
  date = {2019-06-25},
  eprint = {1906.10652},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1906.10652},
  urldate = {2019-08-01},
  abstract = {This paper is a broad and accessible survey of the methods we have at our disposal for Monte Carlo gradient estimation in machine learning and across the statistical sciences: the problem of computing the gradient of an expectation of a function with respect to parameters defining the distribution that is integrated; the problem of sensitivity analysis. In machine learning research, this gradient problem lies at the core of many learning problems, in supervised, unsupervised and reinforcement learning. We will generally seek to rewrite such gradients in a form that allows for Monte Carlo estimation, allowing them to be easily and efficiently used and analysed. We explore three strategies—the pathwise, score function, and measure-valued gradient estimators—exploring their historical developments, derivation, and underlying assumptions. We describe their use in other fields, show how they are related and can be combined, and expand on their possible generalisations. Wherever Monte Carlo gradient estimators have been derived and deployed in the past, important advances have followed. A deeper and more widely-held understanding of this problem will lead to further advances, and it is these advances that we wish to support.},
  langid = {english}
}

@article{mohamed_selfsupervised_2022,
  title = {Self-{{Supervised Speech Representation Learning}}: {{A Review}}},
  author = {Mohamed, Abdelrahman and Lee, Hung-yi and Borgholt, Lasse and Havtorn, Jakob D. and Edin, Joakim and Igel, Christian and Kirchhoff, Katrin and Li, Shang-Wen and Livescu, Karen and Maaløe, Lars and Sainath, Tara N. and Watanabe, Shinji},
  date = {2022},
  journaltitle = {IEEE Journal of Selected Topics in Signal Processing (JSTSP)},
  volume = {16},
  number = {6},
  eprint = {2205.10643},
  eprinttype = {arxiv},
  pages = {1179--1210},
  doi = {10.1109/JSTSP.2022.3207050}
}

@book{mohri_foundations_2012,
  title = {Foundations of {{Machine Learning}}},
  author = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
  date = {2012},
  series = {Adaptive Computation and Machine Learning Series},
  publisher = {{MIT Press}},
  location = {{Cambridge, MA}},
  isbn = {978-0-262-01825-8},
  langid = {english},
  pagetotal = {414},
  keywords = {★}
}

@article{mohri_weighted_2002,
  title = {Weighted Finite-State Transducers in Speech Recognition},
  author = {Mohri, Mehryar and Pereira, Fernando and Riley, Michael},
  date = {2002},
  journaltitle = {Computer Speech \& Language},
  volume = {16},
  number = {1},
  pages = {69--88}
}

@misc{molchanov_variational_2017,
  title = {Variational {{Dropout Sparsifies Deep Neural Networks}}},
  author = {Molchanov, Dmitry and Ashukha, Arsenii and Vetrov, Dmitry},
  date = {2017},
  eprint = {1701.05369},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1701.05369},
  urldate = {2018-04-16},
  abstract = {We explore a recently proposed Variational Dropout technique that provided an elegant Bayesian interpretation to Gaussian Dropout. We extend Variational Dropout to the case when dropout rates are unbounded, propose a way to reduce the variance of the gradient estimator and report first experimental results with individual dropout rates per weight. Interestingly, it leads to extremely sparse solutions both in fully-connected and convolutional layers. This effect is similar to automatic relevance determination effect in empirical Bayes but has a number of advantages. We reduce the number of parameters up to 280 times on LeNet architectures and up to 68 times on VGG-like networks with a negligible decrease of accuracy.}
}

@incollection{montavon_practical_2012,
  title = {A {{Practical Guide}} to {{Training Restricted Boltzmann Machines}}},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}},
  author = {Hinton, Geoffrey E.},
  editor = {Montavon, Grégoire and Orr, Geneviève B. and Müller, Klaus-Robert},
  date = {2012},
  volume = {7700},
  pages = {599--619},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-35289-8_32},
  url = {http://link.springer.com/10.1007/978-3-642-35289-8_32},
  urldate = {2020-11-25},
  isbn = {978-3-642-35288-1 978-3-642-35289-8},
  langid = {english}
}

@article{montufar_number_,
  title = {On the {{Number}} of {{Linear Regions}} of {{Deep Neural Networks}}},
  author = {Montúfar, Guido and Cho, Kyunghyun and Bengio, Yoshua},
  abstract = {We study the complexity of functions computable by deep feedforward neural networks with piecewise linear activations in terms of the symmetries and the number of linear regions that they have. Deep networks are able to sequentially map portions of each layer's input-space to the same output. In this way, deep models compute functions that react equally to complicated patterns of different inputs. The compositional structure of these functions enables them to re-use pieces of computation exponentially often in terms of the network's depth. This paper investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piecewise linear activation functions. In particular, our analysis is not specific to a single family of models, and as an example, we employ it for rectifier and maxout networks. We improve complexity bounds from pre-existing work and investigate the behavior of units in higher layers.}
}

@article{moonsComparisonDeepLearning2020,
  title = {A {{Comparison}} of {{Deep Learning Methods}} for {{ICD Coding}} of {{Clinical Records}}},
  author = {Moons, Elias and Khanna, Aditya and Akkasi, Abbas and Moens, Marie-Francine},
  date = {2020-01},
  journaltitle = {Applied Sciences},
  volume = {10},
  number = {15},
  pages = {5262},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2076-3417},
  doi = {10.3390/app10155262},
  abstract = {In this survey, we discuss the task of automatically classifying medical documents into the taxonomy of the International Classification of Diseases (ICD), by the use of deep neural networks. The literature in this domain covers different techniques. We will assess and compare the performance of those techniques in various settings and investigate which combination leverages the best results. Furthermore, we introduce an hierarchical component that exploits the knowledge of the ICD taxonomy. All methods and their combinations are evaluated on two publicly available datasets that represent ICD-9 and ICD-10 coding, respectively. The evaluation leads to a discussion of the advantages and disadvantages of the models.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english}
}

@inproceedings{mor_confidence_2018,
  title = {Confidence Prediction for Lexicon-Free {{OCR}}},
  booktitle = {2018 {{IEEE}} Winter Conference on Applications of Computer Vision ({{WACV}})},
  author = {Mor, Noam and Wolf, Lior},
  date = {2018},
  pages = {218--225},
  publisher = {{IEEE}}
}

@inproceedings{morais_endtoend_2021,
  title = {End-to-End Spoken Language Understanding Using Transformer Networks and Self-Supervised Pre-Trained Features},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Morais, Edmilson and Kuo, Hong-Kwang J and Thomas, Samuel and Tüske, Zoltán and Kingsbury, Brian},
  date = {2021},
  pages = {7483--7487},
  publisher = {{IEEE}}
}

@online{moreno-munoz_masked_2023,
  title = {On {{Masked Pre-training}} and the {{Marginal Likelihood}}},
  author = {Moreno-Muñoz, Pablo and Recasens, Pol G. and Hauberg, Søren},
  date = {2023-06-01},
  eprint = {2306.00520},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2306.00520},
  url = {http://arxiv.org/abs/2306.00520},
  urldate = {2023-10-18},
  abstract = {Masked pre-training removes random input dimensions and learns a model that can predict the missing values. Empirical results indicate that this intuitive form of self-supervised learning yields models that generalize very well to new domains. A theoretical understanding is, however, lacking. This paper shows that masked pre-training with a suitable cumulative scoring function corresponds to maximizing the model's marginal likelihood, which is de facto the Bayesian model selection measure of generalization. Beyond shedding light on the success of masked pre-training, this insight also suggests that Bayesian models can be trained with appropriately designed self-supervision. Empirically, we confirm the developed theory and explore the main learning principles of masked pre-training in large language models.},
  pubstate = {preprint}
}

@inproceedings{morningstar_density_2021,
  title = {Density of {{States Estimation}} for {{Out}} of {{Distribution Detection}}},
  booktitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}} ({{AISTATS}})},
  author = {Morningstar, Warren and Ham, Cusuh and Gallagher, Andrew and Lakshminarayanan, Balaji and Alemi, Alex and Dillon, Joshua},
  date = {2021},
  pages = {3232--3240},
  publisher = {{PMLR}}
}

@article{morokoff_quasi-monte_1995,
  title = {Quasi-{{Monte Carlo Integration}}},
  author = {Morokoff, William J. and Caflisch, Russel E.},
  date = {1995-12-01},
  journaltitle = {Journal of Computational Physics},
  volume = {122},
  number = {2},
  pages = {218--230},
  publisher = {{Academic Press}},
  issn = {00219991},
  doi = {10.1006/jcph.1995.1209},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S0021999185712090},
  urldate = {2018-06-26},
  abstract = {The standard Monte Carlo approach to evaluating multidimensional integrals using (pseudo)-random integration nodes is frequently used when quadrature methods are too difficult or expensive to implement. As an alternative to the random methods, it has been suggested that lower error and improved convergence may be obtained by replacing the pseudo-random sequences with more uniformly distributed sequences known as quasi-random. In this paper quasi-random (Halton, Sobol', and Faure) and pseudo-random sequences are compared in computational experiments designed to determine the effects on convergence of certain properties of the integrand, including variance, variation, smoothness, and dimension. The results show that variation, which plays an important role in the theoretical upper bound given by the Koksma-Hlawka inequality, does not affect convergence, while variance, the determining factor in random Monte Carlo, is shown to provide a rough upper bound, but does not accurately predict performance. In general, quasi-Monte Carlo methods are superior to random Monte Carlo, but the advantage may be slight, particularly in high dimensions or for integrands that are not smooth. For discontinuous integrands, we derive a bound which shows that the exponent for algebraic decay of the integration error from quasi-Monte Carlo is only slightly larger than 1/2 in high dimensions. © 1995 Academic Press. All rights reserved.}
}

@inproceedings{morse_simple_2016,
  title = {Simple {{Evolutionary Optimization Can Rival Stochastic Gradient Descent}} in {{Neural Networks}}},
  booktitle = {Proceedings of the {{Conference}} on {{Genetic}} and {{Evolutionary Computation}} ({{GECCO}})},
  author = {Morse, Gregory and Stanley, Kenneth O.},
  date = {2016},
  pages = {477--484},
  doi = {10.1145/2908812.2908916},
  url = {http://dl.acm.org/citation.cfm?doid=2908812.2908916},
  abstract = {While evolutionary algorithms (EAs) have long offered an alternative approach to optimization, in recent years backpropagation through stochastic gradient descent (SGD) has come to dominate the fi elds of neural network optimization and deep learning. One hypothesis for the absence of EAs in deep learning is that modern neural networks have become so high dimensional that evolution with its inexact gradient cannot match the exact gradient calculations of backpropagation. Furthermore, the evaluation of a single individual in evolution on the big data sets now prevalent in deep learning would present a prohibitive obstacle towards effi cient optimization. This paper challenges these views, suggesting that EAs can be made to run signi cantly faster than previously thought by evaluating individuals only on a small number of training examples per generation. Surprisingly, using this approach with only a simple EA (called the limited evaluation EA or LEEA) is competitive with the performance of the state-of-the-art SGD variant RMSProp on several benchmarks with neural networks with over 1,000 weights. More investigation is warranted, but these initial results suggest the possibility that EAs could be the fi rst viable training alternative for deep learning outside of SGD, thereby opening up deep learning to all the tools of evolutionary computation.},
  isbn = {978-1-4503-4206-3}
}

@misc{mostafazadeh_generating_2016,
  title = {Generating {{Natural Questions About}} an {{Image}}},
  author = {Mostafazadeh, Nasrin and Misra, Ishan and Devlin, Jacob and Mitchell, Margaret and He, Xiaodong and Vanderwende, Lucy},
  date = {2016-03-19},
  eprint = {1603.06059},
  eprinttype = {arxiv},
  url = {https://arxiv.org/abs/1603.06059},
  urldate = {2018-06-06},
  abstract = {There has been an explosion of work in the vision \& language community during the past few years from image captioning to video transcription, and answering questions about images. These tasks have focused on literal descriptions of the image. To move beyond the literal, we choose to explore how questions about an image are often directed at commonsense inference and the abstract events evoked by objects in the image. In this paper, we introduce the novel task of Visual Question Generation (VQG), where the system is tasked with asking a natural and engaging question when shown an image. We provide three datasets which cover a variety of images from object-centric to event-centric, with considerably more abstract training data than provided to state-of-the-art captioning systems thus far. We train and test several generative and retrieval models to tackle the task of VQG. Evaluation results show that while such models ask reasonable questions for a variety of images, there is still a wide gap with human performance which motivates further work on connecting images with commonsense knowledge and pragmatics. Our proposed task offers a new challenge to the community which we hope furthers interest in exploring deeper connections between vision \& language.},
  isbn = {9781510827585}
}

@misc{mozannar_consistent_2020,
  title = {Consistent {{Estimators}} for {{Learning}} to {{Defer}} to an {{Expert}}},
  author = {Mozannar, Hussein and Sontag, David},
  date = {2020-06-02},
  eprint = {2006.01862},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2006.01862},
  urldate = {2020-07-15},
  abstract = {Learning algorithms are often used in conjunction with expert decision makers in practical scenarios, however this fact is largely ignored when designing these algorithms. In this paper we explore how to learn predictors that can either predict or choose to defer the decision to a downstream expert. Given only samples of the expert’s decisions, we give a procedure based on learning a classifier and a rejector and analyze it theoretically. Our approach is based on a novel reduction to cost sensitive learning where we give a consistent surrogate loss for cost sensitive learning that generalizes the cross entropy loss. We show the effectiveness of our approach on a variety of experimental tasks.},
  langid = {english}
}

@inproceedings{mrksic_fully_2018,
  title = {Fully {{Statistical Neural Belief Tracking}}},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  author = {Mrkšić, Nikola and Vulić, Ivan},
  date = {2018},
  pages = {108--113},
  publisher = {{Association for Computational Linguistics}},
  location = {{Melbourne, Australia}},
  doi = {10/ggmjgv},
  url = {http://aclweb.org/anthology/P18-2018},
  urldate = {2020-02-25},
  abstract = {This paper proposes an improvement to the existing data-driven Neural Belief Tracking (NBT) framework for Dialogue State Tracking (DST). The existing NBT model uses a hand-crafted belief state update mechanism which involves an expensive manual retuning step whenever the model is deployed to a new dialogue domain. We show that this update mechanism can be learned jointly with the semantic decoding and context modelling parts of the NBT model, eliminating the last rule-based module from this DST framework. We propose two different statistical update mechanisms and show that dialogue dynamics can be modelled with a very small number of additional model parameters. In our DST evaluation over three languages, we show that this model achieves competitive performance and provides a robust framework for building resource-light DST models.},
  eventtitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  langid = {english}
}

@inproceedings{mrksic_neural_2017,
  title = {Neural {{Belief Tracker}}: {{Data-Driven Dialogue State Tracking}}},
  shorttitle = {Neural {{Belief Tracker}}},
  booktitle = {Proceedings of the 55th {{Annual Meeting}} of the {{Association}} for           {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Mrkšić, Nikola and Ó Séaghdha, Diarmuid and Wen, Tsung-Hsien and Thomson, Blaise and Young, Steve},
  date = {2017},
  pages = {1777--1788},
  publisher = {{Association for Computational Linguistics}},
  location = {{Vancouver, Canada}},
  doi = {10/gf6j2w},
  url = {http://aclweb.org/anthology/P17-1163},
  urldate = {2020-02-19},
  abstract = {One of the core components of modern spoken dialogue systems is the belief tracker, which estimates the user’s goal at every step of the dialogue. However, most current approaches have difficulty scaling to larger, more complex dialogue domains. This is due to their dependency on either: a) Spoken Language Understanding models that require large amounts of annotated training data; or b) hand-crafted lexicons for capturing some of the linguistic variation in users’ language. We propose a novel Neural Belief Tracking (NBT) framework which overcomes these problems by building on recent advances in representation learning. NBT models reason over pre-trained word vectors, learning to compose them into distributed representations of user utterances and dialogue context. Our evaluation on two datasets shows that this approach surpasses past limitations, matching the performance of state-of-the-art models which rely on hand-crafted semantic lexicons and outperforming them when such lexicons are not provided.},
  eventtitle = {Proceedings of the 55th {{Annual Meeting}} of the {{Association}} for           {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  langid = {english}
}

@book{muirhead_aspects_2009,
  title = {Aspects of Multivariate Statistical Theory},
  author = {Muirhead, Robb J},
  date = {2009},
  volume = {197},
  publisher = {{John Wiley \& Sons}}
}

@inproceedings{mullenbachExplainablePredictionMedical2018,
  title = {Explainable {{Prediction}} of {{Medical Codes}} from {{Clinical Text}}},
  booktitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long Papers}})},
  author = {Mullenbach, James and Wiegreffe, Sarah and Duke, Jon and Sun, Jimeng and Eisenstein, Jacob},
  date = {2018-06},
  pages = {1101--1111},
  publisher = {{Association for Computational Linguistics}},
  location = {{New Orleans, Louisiana}},
  doi = {10.18653/v1/N18-1100},
  abstract = {Clinical notes are text documents that are created by clinicians for each patient encounter. They are typically accompanied by medical codes, which describe the diagnosis and treatment. Annotating these codes is labor intensive and error prone; furthermore, the connection between the codes and the text is not annotated, obscuring the reasons and details behind specific diagnoses and treatments. We present an attentional convolutional network that predicts medical codes from clinical text. Our method aggregates information across the document using a convolutional neural network, and uses an attention mechanism to select the most relevant segments for each of the thousands of possible codes. The method is accurate, achieving precision@8 of 0.71 and a Micro-F1 of 0.54, which are both better than the prior state of the art. Furthermore, through an interpretability evaluation by a physician, we show that the attention mechanism identifies meaningful explanations for each code assignment.}
}

@book{murphy_machine_2012,
  title = {Machine {{Learning}}},
  author = {Murphy, Kevin P.},
  date = {2012},
  edition = {1},
  eprint = {20236947},
  eprinttype = {pmid},
  publisher = {{MIT Press}},
  issn = {0262018020},
  url = {http://www.springerreference.com/index/doi/10.1007/SpringerReference_35834},
  urldate = {2017-02-28},
  abstract = {Some of the most remarkable issues related to interharmonics observed from a probabilistic perspective are presented. Attention is firstly devoted to interharmonic frequency and amplitude variability. Starting from the basic mathematical and computational aspects of probabilistic harmonic models, the difficulties to include interharmonics are discussed with particular attention to the problem of the frequency resolution and of the computational burden. Then, simulation and measurement aspects are discussed, also showing some numerical and experimental results.},
  isbn = {978-0-262-01802-9},
  keywords = {★}
}

@article{nadarajah_making_2011,
  title = {Making the {{Cauchy}} Work},
  author = {Nadarajah, Saralees},
  date = {2011},
  journaltitle = {Brazilian Journal of Probability and Statistics},
  volume = {25},
  number = {1},
  pages = {99--120},
  issn = {01030752},
  doi = {10.1214/09-BJPS112},
  abstract = {A truncated version of the Cauchy distribution is introduced. Un-like the Cauchy distribution, this possesses finite moments of all orders and could therefore be a better model for certain practical situations. More than 10 practical situations where the truncated distribution could be applied are dis-cussed. Explicit expressions are derived for the moments, L moments, mean deviations, moment generating function, characteristic function, convolution properties, Bonferroni curve, Lorenz curve, entropies, order statistics and the asymptotic distribution of the extreme order statistics. Estimation procedures are detailed by the method of moments and the method of maximum likeli-hood and expressions derived for the associated Fisher information matrix. Simulation issues are discussed. Finally, an application is illustrated for con-sumer price indices from the six major economics.}
}

@inproceedings{naeini_obtaining_2015,
  title = {Obtaining Well Calibrated Probabilities Using Bayesian Binning},
  booktitle = {Proceedings of the {{AAAI}} Conference on Artificial Intelligence},
  author = {Naeini, Mahdi Pakdaman and Cooper, Gregory and Hauskrecht, Milos},
  date = {2015},
  volume = {29},
  number = {1}
}

@misc{nagano_wrapped_2019,
  title = {A {{Wrapped Normal Distribution}} on {{Hyperbolic Space}} for {{Gradient-Based Learning}}},
  author = {Nagano, Yoshihiro and Yamaguchi, Shoichiro and Fujita, Yasuhiro and Koyama, Masanori},
  date = {2019-05-09},
  eprint = {1902.02992},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1902.02992},
  urldate = {2020-07-20},
  abstract = {Hyperbolic space is a geometry that is known to be well-suited for representation learning of data with an underlying hierarchical structure. In this paper, we present a novel hyperbolic distribution called pseudo-hyperbolic Gaussian, a Gaussianlike distribution on hyperbolic space whose density can be evaluated analytically and differentiated with respect to the parameters. Our distribution enables the gradient-based learning of the probabilistic models on hyperbolic space that could never have been considered before. Also, we can sample from this hyperbolic probability distribution without resorting to auxiliary means like rejection sampling. As applications of our distribution, we develop a hyperbolic-analog of variational autoencoder and a method of probabilistic word embedding on hyperbolic space. We demonstrate the efficacy of our distribution on various datasets including MNIST, Atari 2600 Breakout, and WordNet.},
  langid = {english}
}

@misc{nagel_adaptive_2020,
  title = {Up or {{Down}}? {{Adaptive Rounding}} for {{Post-Training Quantization}}},
  shorttitle = {Up or {{Down}}?},
  author = {Nagel, Markus and Amjad, Rana Ali and family=Baalen, given=Mart, prefix=van, useprefix=true and Louizos, Christos and Blankevoort, Tijmen},
  date = {2020-06-30},
  eprint = {2004.10568},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2004.10568},
  urldate = {2022-02-26},
  abstract = {When quantizing neural networks, assigning each floating-point weight to its nearest fixed-point value is the predominant approach. We find that, perhaps surprisingly, this is not the best we can do. In this paper, we propose AdaRound, a better weight-rounding mechanism for post-training quantization that adapts to the data and the task loss. AdaRound is fast, does not require fine-tuning of the network, and only uses a small amount of unlabelled data. We start by theoretically analyzing the rounding problem for a pre-trained neural network. By approximating the task loss with a Taylor series expansion, the rounding task is posed as a quadratic unconstrained binary optimization problem. We simplify this to a layer-wise local loss and propose to optimize this loss with a soft relaxation. AdaRound not only outperforms rounding-to-nearest by a significant margin but also establishes a new state-of-the-art for post-training quantization on several networks and tasks. Without fine-tuning, we can quantize the weights of Resnet18 and Resnet50 to 4 bits while staying within an accuracy loss of 1\%.}
}

@inproceedings{nagesh_robust_2016,
  title = {A Robust Speech Rate Estimation Based on the Activation Profile from the Selected Acoustic Unit Dictionary},
  booktitle = {2016 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Nagesh, S. and Yarra, C. and Deshmukh, O. D. and Ghosh, P. K.},
  date = {2016-03},
  pages = {5400--5404},
  doi = {10.1109/ICASSP.2016.7472709},
  abstract = {A typical solution for the speech rate estimation consists of two stages, which involves first computing a short-time feature contour such that most of peaks of the contour correspond to the syllable nuclei followed by the detection of the peaks of the contour corresponding to the syllable nuclei. Temporal correlation selected subband correlation (TCSSBC) is often used as a feature contour for the speech rate estimation in which correlation within and across a few selected sub-band energies are computed. In this work, instead of a fixed set of sub-bands, we learn them in a data-driven manner using a dictionary learning approach. Similarly, instead of the energy contours, we use the activation profile from the learned dictionary elements. We found that the peaks detected from the data-driven approach significantly improve the speech rate estimation when combined with the traditional TCSSBC approach using a proposed peak-merging strategy. Experiments are performed separately using Switchboard, TIMIT and CTIMIT corpora. Except Switchboard, the correlation coefficient for the speech rate estimation using the proposed approach is found to be higher than those by the TCSSBC technique - 3.1\% and 5.2\% (relative) improvements for TIMIT and CTIMIT respectively.},
  eventtitle = {2016 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})}
}

@inproceedings{nagrani_voxceleb_2017,
  title = {{{VoxCeleb}}: {{A}} Large-Scale Speaker Identification Dataset},
  booktitle = {Annual {{Conference}} of the {{International Speech Communication Association}}},
  author = {Nagrani, Arsha and Chung, Joon Son and Zisserman, Andrew},
  date = {2017}
}

@inproceedings{nair_rectified_2010,
  title = {Rectified Linear Units Improve Restricted Boltzmann Machines},
  booktitle = {Proceedings of the 27th {{International Conference}} on {{Machine Learning}} ({{ICML}} 2021)},
  author = {Nair, Vinod and Hinton, Geoffrey E.},
  date = {2010},
  pages = {807--814},
  location = {{Haifa, Israel}},
  url = {https://icml.cc/Conferences/2010/papers/432.pdf},
  eventtitle = {International {{Conference}} on {{Machine Learning}}}
}

@misc{nakkiran_deep_2019,
  title = {Deep {{Double Descent}}: {{Where Bigger Models}} and {{More Data Hurt}}},
  shorttitle = {Deep {{Double Descent}}},
  author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
  date = {2019-12-04},
  eprint = {1912.02292},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1912.02292},
  urldate = {2019-12-08},
  abstract = {We show that a variety of modern deep learning tasks exhibit a “double-descent” phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.},
  langid = {english}
}

@inproceedings{nalisnick_deep_2019,
  title = {Do {{Deep Generative Models Know What They Don}}'t {{Know}}?},
  booktitle = {Proceedings of the 7th {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Nalisnick, Eric and Matsukawa, Akihiro and Teh, Yee Whye and Gorur, Dilan and Lakshminarayanan, Balaji},
  date = {2019},
  eprint = {1810.09136},
  eprinttype = {arxiv},
  location = {{New Orleans, LA, USA}},
  url = {http://arxiv.org/abs/1810.09136},
  urldate = {2019-10-02},
  abstract = {A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data. A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong. Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel, out-of-distribution inputs. In this paper we challenge this assumption. We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former. Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN. To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood. We find such behavior persists even when we restrict the flows to constant-volume transformations. These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english}
}

@misc{nalisnick_detecting_2019,
  title = {Detecting {{Out-of-Distribution Inputs}} to {{Deep Generative Models Using Typicality}}},
  author = {Nalisnick, Eric and Matsukawa, Akihiro and Teh, Yee Whye and Lakshminarayanan, Balaji},
  date = {2019},
  eprint = {1906.02994},
  eprinttype = {arxiv},
  url = {https://arxiv.org/abs/1906.02994},
  langid = {english}
}

@inproceedings{narayanan_multimodal_2011,
  title = {A Multimodal Real-Time {{MRI}} Articulatory Corpus for Speech Research},
  booktitle = {Annual {{Conference}} of the {{International Speech Communication Association}}},
  author = {Narayanan, Shrikanth and Bresch, Erik and Ghosh, Prasanta Kumar and Goldstein, Louis and Katsamanis, Athanasios and Kim, Yoon and Lammert, Adam and Proctor, Michael and Ramanarayanan, Vikram and Zhu, Yinghua},
  date = {2011}
}

@inproceedings{narayanan_speech_2005,
  title = {Speech {{Rate Estimation}} via {{Temporal Correlation}} and {{Selected Sub-Band Correlation}}},
  booktitle = {Proceedings. ({{ICASSP}} '05). {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}, 2005.},
  author = {Narayanan, S. and Wang, D.},
  date = {2005},
  volume = {1},
  pages = {413--416},
  publisher = {{IEEE}},
  location = {{Philadelphia, Pennsylvania, USA}},
  doi = {10.1109/ICASSP.2005.1415138},
  url = {http://ieeexplore.ieee.org/document/1415138/},
  urldate = {2018-11-23},
  abstract = {In this paper, we propose a novel method for speech rate estimation without requiring automatic speech recognition. It extends the methods of spectral subband correlation by including temporal correlation and the use of selecting prominent spectral subbands for correlation. Further more, to address some of the practical issues in previously published methods, we introduce some novel components into the algorithm such as the use of pitch confidence, magnifying window, relative peak measure and relative threshold. By selecting the parameters and thresholds from realistic development sets, this method achieves a 0.972 correlation coefficient on syllable number estimation and a 0.706 correlation on speech rate estimation. This result is about 6.9\% improvement than current best single estimator and 3.5\% improvement than current multi-estimator evaluated on the same switchboard database.},
  eventtitle = {({{ICASSP}} '05). {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}, 2005.},
  isbn = {978-0-7803-8874-1},
  langid = {english}
}

@unpublished{naseer_intriguing_2021,
  title = {Intriguing {{Properties}} of {{Vision Transformers}}},
  author = {Naseer, Muzammal and Ranasinghe, Kanchana and Khan, Salman and Hayat, Munawar and Khan, Fahad Shahbaz and Yang, Ming-Hsuan},
  date = {2021-11-25},
  eprint = {2105.10497},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2105.10497},
  urldate = {2022-04-26},
  abstract = {Vision transformers (ViT) have demonstrated impressive performance across various machine vision problems. These models are based on multi-head self-attention mechanisms that can flexibly attend to a sequence of image patches to encode contextual cues. An important question is how such flexibility in attending image-wide context conditioned on a given patch can facilitate handling nuisances in natural images e.g., severe occlusions, domain shifts, spatial permutations, adversarial and natural perturbations. We systematically study this question via an extensive set of experiments encompassing three ViT families and comparisons with a high-performing convolutional neural network (CNN). We show and analyze the following intriguing properties of ViT: (a) Transformers are highly robust to severe occlusions, perturbations and domain shifts, e.g., retain as high as 60\% top-1 accuracy on ImageNet even after randomly occluding 80\% of the image content. (b) The robust performance to occlusions is not due to a bias towards local textures, and ViTs are significantly less biased towards textures compared to CNNs. When properly trained to encode shape-based features, ViTs demonstrate shape recognition capability comparable to that of human visual system, previously unmatched in the literature. (c) Using ViTs to encode shape representation leads to an interesting consequence of accurate semantic segmentation without pixel-level supervision. (d) Off-the-shelf features from a single ViT model can be combined to create a feature ensemble, leading to high accuracy rates across a range of classification datasets in both traditional and few-shot learning paradigms. We show effective features of ViTs are due to flexible and dynamic receptive fields possible via the self-attention mechanism.},
  keywords = {Unread}
}

@report{nationaltransportationsafetyboardnhsa_collision_2019,
  type = {Highway Accident Report},
  title = {Collision {{Between Vehicle Controlled}} by {{Developmental Automated Driving System}} and {{Pedestrian}}, {{Tempe}}, {{Arizona}}, {{March}} 18, 2018},
  author = {{National Transportation Safety Board (NHSA)}},
  date = {2019-11-19},
  number = {NTSB/HAR-19/03 PB2019-101402},
  pages = {66},
  location = {{Washington D.C.}},
  url = {https://www.ntsb.gov/investigations/AccidentReports/Reports/HAR1903.pdf},
  urldate = {2023-08-25},
  langid = {english}
}

@article{nava_stateconsistency_2021,
  title = {State-Consistency Loss for Learning Spatial Perception Tasks from Partial Labels},
  author = {Nava, Mirko and Gambardella, Luca Maria and Giusti, Alessandro},
  date = {2021},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {6},
  number = {2},
  pages = {1112--1119},
  doi = {10.1109/LRA.2021.3056378}
}

@article{nava_uncertaintyaware_2021,
  title = {Uncertainty-Aware Self-Supervised Learning of Spatial Perception Tasks},
  author = {Nava, Mirko and Paolillo, Antonio and Guzzi, Jérôme and Gambardella, Luca Maria and Giusti, Alessandro},
  date = {2021},
  journaltitle = {IEEE Robotics and Automation Letters},
  volume = {6},
  number = {4},
  pages = {6693--6700}
}

@article{nazabal_handling_2020,
  title = {Handling Incomplete Heterogeneous Data Using {{VAEs}}},
  author = {Nazábal, Alfredo and Olmos, Pablo M. and Ghahramani, Zoubin and Valera, Isabel},
  date = {2020},
  journaltitle = {Pattern Recognition},
  volume = {107},
  pages = {107501}
}

@online{ndiour_outofdistribution_2020,
  title = {Out-of-Distribution Detection with Subspace Techniques and Probabilistic Modeling of Features},
  author = {Ndiour, Ibrahima and Ahuja, Nilesh and Tickoo, Omesh},
  date = {2020},
  number = {2012.04250},
  eprint = {2012.04250},
  eprinttype = {arxiv},
  pubstate = {preprint}
}

@article{neal_bayesian_1993,
  title = {Bayesian Learning via Stochastic Dynamics},
  author = {Neal, Radford M.},
  date = {1993},
  journaltitle = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
  pages = {475},
  abstract = {The attempt to find a single "optimal" weight vector in conven-tional network training can lead to overfitting and poor generaliza-tion. Bayesian methods avoid this, without the need for a valida-tion set, by averaging the outputs of many networks with weights sampled from the posterior distribution given the training data. This sample can be obtained by simulating a stochastic dynamical system that has the posterior as its stationary distribution.}
}

@thesis{neal_bayesian_1995,
  title = {Bayesian {{Learning}} for {{Neural Networks}}},
  author = {Neal, Radford M.},
  date = {1995},
  institution = {{University of Toronto}},
  abstract = {Artificial ``neural networks'' are now widely used as flexible models for regression and classification applications, but questions remain regarding what these models mean, and how they can safely be used when training data is limited. Bayesian Learning for Neural Networks shows that Bayesian methods allow complex neural network models to be used without fear of the ``overfitting'' that can occur with traditional neural network learning methods. Insight into the nature of these complex Bayesian models is provided by a theoretical investigation of the priors over functions that underlie them. Use of these models in practice is made possible using Markov chain Monte Carlo techniques. Both the theoretical and computational aspects of this work are of wider statistical interest, as they contribute to a better understanding of how Bayesian methods can be applied to complex problems. Presupposing only basic knowledge of probability and statistics, this book should be of interest to many researchers in Statistics, Engineering, and Artificial Intelligence. Software for Unix systems that implements the methods described is freely available over the Internet.}
}

@article{neal_connectionist_1992,
  title = {Connectionist Learning of Belief Networks},
  author = {Neal, Radford M.},
  date = {1992},
  journaltitle = {Artificial Intelligence},
  volume = {56},
  number = {1},
  pages = {71--113},
  issn = {00043702},
  doi = {10.1016/0004-3702(92)90065-6},
  abstract = {Connectionist learning procedures are presented for "sigmoid" and "noisy-OR" varieties of probabilistic belief networks. These networks have previously been seen primarily as a means of representing knowledge derived from experts. Here it is shown that the "Gibbs sampling" simulation procedure for such networks can support maximum-likelihood learning from empirical data through local gradient ascent. This learning procedure resembles that used for "Boltzmann machines", and like it, allows the use of "hidden" variables to model correlations between visible variables. Due to the directed nature of the connections in a belief network, however, the "negative phase" of Boltzmann machine learning is unnecessary. Experimental results show that, as a result, learning in a sigmoid belief network can be faster than in a Boltzmann machine. These networks have other advantages over Boltzmann machines in pattern classification and decision making applications, are naturally applicable to unsupervised learning problems, and provide a link between work on connectionist learning and work on the representation of expert knowledge. © 1992.},
  isbn = {0004-3702}
}

@misc{neklyudov_involutive_2020,
  title = {Involutive {{MCMC}}: A {{Unifying Framework}}},
  shorttitle = {Involutive {{MCMC}}},
  author = {Neklyudov, Kirill and Welling, Max and Egorov, Evgenii and Vetrov, Dmitry},
  date = {2020-06-30},
  eprint = {2006.16653},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2006.16653},
  urldate = {2020-07-14},
  abstract = {Markov Chain Monte Carlo (MCMC) is a computational approach to fundamental problems such as inference, integration, optimization, and simulation. The field has developed a broad spectrum of algorithms, varying in the way they are motivated, the way they are applied and how efficiently they sample. Despite all the differences, many of them share the same core principle, which we unify as the Involutive MCMC (iMCMC) framework. Building upon this, we describe a wide range of MCMC algorithms in terms of iMCMC, and formulate a number of “tricks” which one can use as design principles for developing new MCMC algorithms. Thus, iMCMC provides a unified view of many known MCMC algorithms, which facilitates the derivation of powerful extensions. We demonstrate the latter with two examples where we transform known reversible MCMC algorithms into more efficient irreversible ones.},
  langid = {english}
}

@misc{neklyudov_orbital_2020,
  title = {Orbital {{MCMC}}},
  author = {Neklyudov, Kirill and Welling, Max},
  date = {2020-10-15},
  eprint = {2010.08047},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2010.08047},
  urldate = {2020-10-24},
  abstract = {Markov Chain Monte Carlo (MCMC) is a computational approach to fundamental problems such as inference, integration, optimization, and simulation. Recently, the framework (Involutive MCMC) was proposed describing a large body of MCMC algorithms via two components: a stochastic acceptance test and an involutive deterministic function. This paper demonstrates that this framework is a special case of a larger family of algorithms operating on orbits of continuous deterministic bijections. We describe this family by deriving a novel MCMC kernel, which we call orbital MCMC (oMCMC). We provide a theoretical analysis and illustrate its utility using simple examples.},
  langid = {english}
}

@article{nesterov_method_1983,
  title = {A {{Method}} of {{Solving A Convex Programming Problem With Convergence}} Rate {{O}}(1/K\^2)},
  author = {Nesterov, Yurii},
  date = {1983},
  journaltitle = {Soviet Mathematics Doklady},
  volume = {27},
  number = {2},
  pages = {372--376}
}

@article{nesterov_random_2017,
  title = {Random {{Gradient-Free Minimization}} of {{Convex Functions}}},
  author = {Nesterov, Yurii and Spokoiny, Vladimir},
  date = {2017},
  journaltitle = {Foundations of Computational Mathematics},
  volume = {17},
  number = {2},
  pages = {527--566},
  issn = {16153383},
  doi = {10.1007/s10208-015-9296-2},
  url = {http://production.datastore.cvt.dk/filestore?oid=58d660db5010df4e490ca70c&targetid=58d660db5010df4e490ca70e},
  urldate = {2018-05-08},
  abstract = {In this paper, we prove new complexity bounds for methods of convex optimization based only on computation of the function value. The search directions of our schemes are normally distributed random Gaussian vectors. It appears that such methods usually need at most n times more iterations than the standard gradient methods, where n is the dimension of the space of variables. This conclusion is true for both nonsmooth and smooth problems. For the latter class, we present also an accelerated scheme with the expected rate of convergence O(n\^2/k\^2), where k is the iteration counter. For stochastic optimization, we propose a zero-order scheme and justify its expected rate of convergence O(n/k\^1/2).We give also some bounds for the rate of convergence of the random gradient-free methods to stationary points of nonconvex functions, for both smooth and nonsmooth cases. Our theoretical results are supported by preliminary computational experiments.}
}

@inproceedings{netzer_reading_2011,
  title = {Reading {{Digits}} in {{Natural Images}} with {{Unsupervised Feature Learning}}},
  booktitle = {{{NeurIPS Workshop}} on {{Deep Learning}} and {{Unsupervised Feature Learning}} 2011},
  author = {Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew Y.},
  date = {2011},
  url = {http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf},
  abstract = {Detecting and reading text from natural images is a hard computer vision task that is central to a variety of emerging applications. Related problems like document character recognition have been widely studied by computer vision and machine learning researchers and are virtually solved for practical applications like reading handwritten digits. Reliably recognizing characters in more complex scenes like photographs, however, is far more difficult: the best existing methods lag well behind human performance on the same tasks. In this paper we attack the problem of recognizing digits in a real application using unsupervised feature learning methods: reading house numbers from street level photos. To this end, we introduce a new benchmark dataset for research use containing over 600,000 labeled digits cropped from Street View images. We then demonstrate the difficulty of recognizing these digits when the problem is approached with hand-designed features. Finally, we employ variants of two recently proposed unsupervised feature learning methods and find that they are convincingly superior on our benchmarks.}
}

@book{neugebauer_mathematical_1945,
  title = {Mathematical Cuneiform Texts},
  author = {Neugebauer, O. and Sachs, Abraham and Götze, Albrecht},
  date = {1945},
  publisher = {{The American Oriental Society and the American Schools of Oriental Research}},
  location = {{New Haven, Conn.}},
  langid = {english}
}

@article{neyman_use_1928,
  title = {On the Use and Interpretation of Certain Test Criteria for Purposes of Statistical Inference: {{Part I}}},
  author = {Neyman, Jerzy and Pearson, Egon S},
  date = {1928},
  journaltitle = {Biometrika},
  pages = {175--240},
  publisher = {{JSTOR}}
}

@inproceedings{ngiam_multimodal_2011,
  title = {Multimodal Deep Learning},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Ngiam, Jiquan and Khosla, Aditya and Kim, Mingyu and Nam, Juhan and Lee, Honglak and Ng, Andrew Y},
  date = {2011}
}

@article{nguyen_are_2022,
  title = {Are Discrete Units Necessary for Spoken Language Modeling?},
  author = {Nguyen, Tu Anh and Sagot, Benoît and Dupoux, Emmanuel},
  date = {2022},
  journaltitle = {IEEE Journal on Selected Topics in Signal Processing (JSTSP)},
  volume = {16},
  number = {6},
  pages = {1415--1423}
}

@inproceedings{nguyen_deep_2015,
  title = {Deep Neural Networks Are Easily Fooled: {{High}} Confidence Predictions for Unrecognizable Images},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
  date = {2015},
  volume = {07-12-June},
  pages = {427--436},
  doi = {10.1109/CVPR.2015.7298640},
  abstract = {Deep neural networks (DNNs) have recently been achieving state-of-the-art performance on a variety of pattern-recognition tasks, most notably visual classification problems. Given that DNNs are now able to classify objects in images with near-human-level performance, questions naturally arise as to what differences remain between computer and human vision. A recent study revealed that changing an image (e.g. of a lion) in a way imperceptible to humans can cause a DNN to label the image as something else entirely (e.g. mislabeling a lion a library). Here we show a related result: it is easy to produce images that are completely unrecognizable to humans, but that state-of-the-art DNNs believe to be recognizable objects with 99.99\% confidence (e.g. labeling with certainty that white noise static is a lion). Specifically, we take convolutional neural networks trained to perform well on either the ImageNet or MNIST datasets and then find images with evolutionary algorithms or gradient ascent that DNNs label with high confidence as belonging to each dataset class. It is possible to produce images totally unrecognizable to human eyes that DNNs believe with near certainty are familiar objects, which we call "fooling images" (more generally, fooling examples). Our results shed light on interesting differences between human vision and current DNNs, and raise questions about the generality of DNN computer vision.},
  isbn = {978-1-4673-6964-0}
}

@online{nguyen_generative_2022,
  title = {Generative Spoken Dialogue Language Modeling},
  author = {Nguyen, Tu Anh and Kharitonov, Eugene and Copet, Jade and Adi, Yossi and Hsu, Wei-Ning and Elkahky, Ali and Tomasello, Paden and Algayres, Robin and Sagot, Benoit and Mohamed, Abdelrahman and Dupoux, Emmanuel},
  date = {2022},
  eprint = {2203.16502},
  eprinttype = {arxiv},
  pubstate = {preprint}
}

@misc{nguyen_loss_2017,
  title = {The Loss Surface and Expressivity of Deep Convolutional Neural Networks},
  author = {Nguyen, Quynh and Hein, Matthias},
  date = {2017-10-30},
  eprint = {1710.10928},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1710.10928},
  urldate = {2018-04-24},
  abstract = {We analyze the expressiveness and loss surface of practical deep convolutional neural networks (CNNs) with shared weights and max pooling layers. We show that such CNNs produce linearly independent features at a "wide" layer which has more neurons than the number of training samples. This condition holds e.g. for the VGG network. Furthermore, we provide for such wide CNNs necessary and sufficient conditions for global minima with zero training error. For the case where the wide layer is followed by a fully connected layer, we show that almost every critical point of the empirical loss is a global minimum with zero training error. Our analysis suggests that both depth and width are very important in deep learning. While depth brings more representational power and allows the network to learn high level features, width smoothes the optimization landscape of the loss function in the sense that a sufficiently wide network has a well-behaved loss surface with potentially no bad local minima.}
}

@inproceedings{nguyen_out_2022,
  title = {Out of Distribution Data Detection Using Dropout Bayesian Neural Networks},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Nguyen, Andre T and Lu, Fred and Munoz, Gary Lopez and Raff, Edward and Nicholas, Charles and Holt, James},
  date = {2022},
  volume = {36},
  number = {7},
  pages = {7877--7885}
}

@article{nguyen_structured_,
  title = {Structured {{Dropout Variational Inference}} for {{Bayesian Neural Networks}}},
  author = {Nguyen, Son and Nguyen, Duong and Nguyen, Khai and Than, Khoat and Bui, Hung and Ho, Nhat},
  abstract = {Approximate inference in Bayesian deep networks exhibits a dilemma of how to yield high fidelity posterior approximations while maintaining computational efficiency and scalability. We tackle this challenge by introducing a novel variational structured approximation inspired by the Bayesian interpretation of Dropout regularization. Concretely, we focus on the inflexibility of the factorized structure in Dropout posterior and then propose an improved method called Variational Structured Dropout (VSD). VSD employs an orthogonal transformation to learn a structured representation on the variational Gaussian noise with plausible complexity, and consequently induces statistical dependencies in the approximate posterior. Theoretically, VSD successfully addresses the pathologies of previous Variational Dropout methods and thus offers a standard Bayesian justification. We further show that VSD induces an adaptive regularization term with several desirable properties which contribute to better generalization. Finally, we conduct extensive experiments on standard benchmarks to demonstrate the effectiveness of VSD over state-of-the-art variational methods on predictive accuracy, uncertainty estimation, and out-of-distribution detection.},
  langid = {english}
}

@online{nguyen_zero_2020,
  title = {The {{Zero Resource Speech Benchmark}} 2021: {{Metrics}} and Baselines for Unsupervised Spoken Language Modeling},
  shorttitle = {The {{Zero Resource Speech Benchmark}} 2021},
  author = {Nguyen, Tu Anh and family=Seyssel, given=Maureen, prefix=de, useprefix=true and Rozé, Patricia and Rivière, Morgane and Kharitonov, Evgeny and Baevski, Alexei and Dunbar, Ewan and Dupoux, Emmanuel},
  date = {2020-12-01},
  eprint = {2011.11588},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2011.11588},
  urldate = {2023-04-12},
  abstract = {We introduce a new unsupervised task, spoken language modeling: the learning of linguistic representations from raw audio signals without any labels, along with the Zero Resource Speech Benchmark 2021: a suite of 4 black-box, zero-shot metrics probing for the quality of the learned models at 4 linguistic levels: phonetics, lexicon, syntax and semantics. We present the results and analyses of a composite baseline made of the concatenation of three unsupervised systems: self-supervised contrastive representation learning (CPC), clustering (k-means) and language modeling (LSTM or BERT). The language models learn on the basis of the pseudo-text derived from clustering the learned representations. This simple pipeline shows better than chance performance on all four metrics, demonstrating the feasibility of spoken language modeling from raw speech. It also yields worse performance compared to text-based 'topline' systems trained on the same data, delineating the space to be explored by more sophisticated end-to-end models.},
  pubstate = {preprint}
}

@misc{niculae_sparsemap_2018,
  title = {{{SparseMAP}}: {{Differentiable Sparse Structured Inference}}},
  shorttitle = {{{SparseMAP}}},
  author = {Niculae, Vlad and Martins, André F. T. and Blondel, Mathieu and Cardie, Claire},
  date = {2018-06-20},
  eprint = {1802.04223},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1802.04223},
  urldate = {2021-04-19},
  abstract = {Structured prediction requires searching over a combinatorial number of structures. To tackle it, we introduce SparseMAP: a new method for sparse structured inference, and its natural loss function. SparseMAP automatically selects only a few global structures: it is situated between MAP inference, which picks a single structure, and marginal inference, which assigns nonzero probability to all structures, including implausible ones. SparseMAP can be computed using only calls to a MAP oracle, making it applicable to problems with intractable marginal inference, e.g., linear assignment. Sparsity makes gradient backpropagation efficient regardless of the structure, enabling us to augment deep neural networks with generic and sparse structured hidden layers. Experiments in dependency parsing and natural language inference reveal competitive accuracy, improved interpretability, and the ability to capture natural language ambiguities, which is attractive for pipeline systems.},
  langid = {english}
}

@inproceedings{niculescu-mizil_predicting_2005,
  title = {Predicting Good Probabilities with Supervised Learning},
  booktitle = {Proceedings of the International Conference on Machine Learning ({{ICML}})},
  author = {Niculescu-Mizil, Alexandru and Caruana, Rich},
  date = {2005},
  pages = {625--632},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/1102351.1102430},
  url = {https://doi.org/10.1145/1102351.1102430},
  abstract = {We examine the relationship between the predictions made by different learning algorithms and true posterior probabilities. We show that maximum margin methods such as boosted trees and boosted stumps push probability mass away from 0 and 1 yielding a characteristic sigmoid shaped distortion in the predicted probabilities. Models such as Naive Bayes, which make unrealistic independence assumptions, push probabilities toward 0 and 1. Other models such as neural nets and bagged trees do not have these biases and predict well calibrated probabilities. We experiment with two ways of correcting the biased probabilities predicted by some learning methods: Platt Scaling and Isotonic Regression. We qualitatively examine what kinds of distortions these calibration methods are suitable for and quantitatively examine how much data they need to be effective. The empirical results show that after calibration boosted trees, random forests, and SVMs predict the best probabilities.},
  isbn = {1-59593-180-5}
}

@online{nielsen_neural_2015,
  title = {Neural {{Networks}} and {{Deep Learning}}},
  author = {Nielsen, Michael},
  date = {2015},
  url = {http://neuralnetworksanddeeplearning.com/index.html},
  urldate = {2018-05-28},
  abstract = {Neural Networks and Deep Learning is a free online book. The book will teach you about: Neural networks, a beautiful biologically-inspired programming paradigm which enables a computer to learn from observational data Deep learning, a powerful set of techniques for learning in neural networks Neural networks and deep learning currently provide the best solutions to many problems in image recognition, speech recognition, and natural language processing. This book will teach you many of the core concepts behind neural networks and deep learning.},
  organization = {{Determination Press}}
}

@inproceedings{nielsen_survae_2020,
  title = {{{SurVAE Flows}}: {{Surjections}} to {{Bridge}} the {{Gap}} between {{VAEs}} and {{Flows}}},
  booktitle = {34th {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}} 2020)},
  author = {Nielsen, Didrik and Jaini, Priyank and Hoogeboom, Emiel and Winther, Ole and Welling, Max},
  date = {2020},
  pages = {12},
  location = {{Vancouver, Canada}},
  abstract = {Normalizing flows and variational autoencoders are powerful generative models that can represent complicated density functions. However, they both impose constraints on the models: Normalizing flows use bijective transformations to model densities whereas VAEs learn stochastic transformations that are non-invertible and thus typically do not provide tractable estimates of the marginal likelihood. In this paper, we introduce SurVAE Flows: A modular framework of composable transformations that encompasses VAEs and normalizing flows. SurVAE Flows bridge the gap between normalizing flows and VAEs with surjective transformations, wherein the transformations are deterministic in one direction – thereby allowing exact likelihood computation, and stochastic in the reverse direction – hence providing a lower bound on the corresponding likelihood. We show that several recently proposed methods, including dequantization and augmented normalizing flows, can be expressed as SurVAE Flows. Finally, we introduce common operations such as the max value, the absolute value, sorting and stochastic permutation as composable layers in SurVAE Flows.},
  langid = {english}
}

@article{nilsen_epistemic_2022,
  title = {Epistemic {{Uncertainty Quantification}} in {{Deep Learning Classification}} by the {{Delta Method}}},
  author = {Nilsen, Geir K. and Munthe-Kaas, Antonella Z. and Skaug, Hans J. and Brun, Morten},
  date = {2022},
  journaltitle = {Neural Networks},
  volume = {145},
  pages = {164--176},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2021.10.014},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608021004056},
  abstract = {The Delta method is a classical procedure for quantifying epistemic uncertainty in statistical models, but its direct application to deep neural networks is prevented by the large number of parameters P. We propose a low cost approximation of the Delta method applicable to L2-regularized deep neural networks based on the top K eigenpairs of the Fisher information matrix. We address efficient computation of full-rank approximate eigendecompositions in terms of the exact inverse Hessian, the inverse outer-products of gradients approximation and the so-called Sandwich estimator. Moreover, we provide bounds on the approximation error for the uncertainty of the predictive class probabilities. We show that when the smallest computed eigenvalue of the Fisher information matrix is near the L2-regularization rate, the approximation error will be close to zero even when K≪P. A demonstration of the methodology is presented using a TensorFlow implementation, and we show that meaningful rankings of images based on predictive uncertainty can be obtained for two LeNet and ResNet-based neural networks using the MNIST and CIFAR-10 datasets. Further, we observe that false positives have on average a higher predictive epistemic uncertainty than true positives. This suggests that there is supplementing information in the uncertainty measure not captured by the classification alone.},
  keywords = {Deep learning,Fisher information,Hessian,Neural networks,Predictive epistemic uncertainty,Uncertainty quantification}
}

@article{ning_spectral_,
  title = {A {{Spectral Clustering Approach}} to {{Speaker Diarization}}},
  author = {Ning, Huazhong and Liu, Ming and Tang, Hao and Huang, Thomas},
  pages = {4},
  abstract = {In this paper, we present a spectral clustering approach to explore the possibility of discovering structure from audio data. To apply the Ng-Jordan-Weiss (NJW) spectral clustering algorithm to speaker diarization, we propose some domain specific solutions to the open issues of this algorithm: choice of metric; selection of scaling parameter; estimation of the number of clusters. Then, a postprocessing step – “Cross EM refinement” – is conducted to further improve the performance of spectral learning. In experiments, this approach has performance very similar to the traditional hierarchical clustering on the audio data of Japanese Parliament Panel Discussions, but it runs much faster than the latter.},
  langid = {english}
}

@inproceedings{NIPS2014_2b38c2df,
  title = {Tree-Structured Gaussian Process Approximations},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bui, Thang D and Turner, Richard E},
  editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. and Weinberger, K. Q.},
  date = {2014},
  volume = {27},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2014/file/2b38c2df6a49b97f706ec9148ce48d86-Paper.pdf}
}

@book{nissen_archaic_1993,
  title = {Archaic Bookkeeping: Early Writing and Techniques of Economic Administration in the Ancient {{Near East}}},
  author = {Nissen, Hans J and Damerow, Peter and Englund, Robert K},
  date = {1993},
  publisher = {{University of Chicago Press}}
}

@inproceedings{nix_estimating_1994,
  title = {Estimating the Mean and Variance of the Target Probability Distribution},
  booktitle = {Proceedings of 1994 {{IEEE}} International Conference on Neural Networks ({{ICNN}}'94)},
  author = {Nix, D.A. and Weigend, A.S.},
  date = {1994},
  volume = {1},
  pages = {55-60 vol.1},
  doi = {10.1109/ICNN.1994.374138}
}

@online{noauthor_adversarial_nodate,
  title = {Adversarial {{Multi-task Learning}} of {{Deep Neural Networks}} for {{Robust Speech Recognition}} - {{Google-søgning}}},
  url = {https://www.google.dk/search?q=Adversarial+Multi-task+Learning+of+Deep+Neural+Networks+for+Robust+Speech+Recognition&oq=Adversarial+Multi-task+Learning+of+Deep+Neural+Networks+for+Robust+Speech+Recognition&aqs=chrome..69i57.144j0j7&sourceid=chrome&ie=UTF-8},
  urldate = {2018-10-31}
}

@inproceedings{noauthor_mask-based_2022,
  title = {Mask-Based {{Latent Reconstruction}} for {{Reinforcement Learning}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  date = {2022},
  abstract = {For deep reinforcement learning (RL) from pixels, learning effective state representations is crucial for achieving high performance. However, in practice, limited experience and high-dimensional input prevent effective representation learning. To address this, motivated by the success of masked modeling in other research fields, we introduce mask-based reconstruction to promote state representation learning in RL. Specifically, we propose a simple yet effective self-supervised method, Mask-based Latent Reconstruction (MLR), to predict the complete state representations in the latent space from the observations with spatially and temporally masked pixels. MLR enables the better use of context information when learning state representations to make them more informative, which facilitates RL agent training. Extensive experiments show that our MLR significantly improves the sample efficiency in RL and outperforms the state-of-the-art sample-efficient RL methods on multiple continuous benchmark environments.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}}
}

@online{noauthor_roberts_nodate,
  title = {Roberts , {{Gelman}} , {{Gilks}} : {{Weak}} Convergence and Optimal Scaling of Random Walk {{Metropolis}} Algorithms},
  url = {https://projecteuclid.org/euclid.aoap/1034625254},
  urldate = {2020-10-24}
}

@online{noauthor_twelve-factor_nodate,
  title = {The {{Twelve-Factor App}}},
  url = {https://12factor.net/},
  urldate = {2020-10-21}
}

@book{nocedal_numerical_2006,
  title = {Numerical Optimization},
  author = {Nocedal, Jorge and Wright, Stephen J.},
  date = {2006},
  publisher = {{Springer}},
  isbn = {0-387-30303-0},
  keywords = {★}
}

@article{noh_simple_,
  title = {Simple and {{Effective Out-of-Distribution Detection}} via {{Cosine-based Softmax Loss}}},
  author = {Noh, SoonCheol and Jeong, DongEon and Lee, Jee-Hyong},
  abstract = {Deep learning models need to detect out-of-distribution (OOD) data in the inference stage because they are trained to estimate the train distribution and infer the data sampled from the distribution. Many methods have been proposed, but they have some limitations, such as requiring additional data, input processing, or high computational cost. Moreover, most methods have hyperparameters to be set by users, which have a significant impact on the detection rate. We propose a simple and effective OOD detection method by combining the feature norm and the Mahalanobis distance obtained from classification models trained with the cosinebased softmax loss. Our method is practical because it does not use additional data for training, is about three times faster when inferencing than the methods using the input processing, and is easy to apply because it does not have any hyperparameters for OOD detection. We confirm that our method is superior to or at least comparable to stateof-the-art OOD detection methods through the experiments.},
  langid = {english}
}

@article{norouzi_exemplar_nodate,
  title = {Exemplar {{VAE}}: {{Linking Generative Models}}, {{Nearest Neighbor Retrieval}}, and {{Data Augmentation}}},
  author = {Norouzi, Sajad and Fleet, David J and Norouzi, Mohamamd},
  pages = {12},
  abstract = {We introduce Exemplar VAEs, a family of generative models that bridge the gap between parametric and non-parametric, exemplar based generative models. Exemplar VAE is a variant of VAE with a non-parametric prior in the latent space based on a Parzen window estimator. To sample from it, one first draws a random exemplar from a training set, then stochastically transforms that exemplar into a latent code and a new observation. We propose retrieval augmented training (RAT) as a way to speed up Exemplar VAE training by using approximate nearest neighbor search in the latent space to define a lower bound on log marginal likelihood. To enhance generalization, model parameters are learned using exemplar leave-oneout and subsampling. Experiments demonstrate the effectiveness of Exemplar VAEs on density estimation and representation learning. Importantly, generative data augmentation using Exemplar VAEs on permutation invariant MNIST and Fashion MNIST reduces classification error from 1.17\% to 0.69\% and from 8.56\% to 8.16\%. Code is available at https://github.com/sajadn/Exemplar-VAE.},
  langid = {english}
}

@article{nouraeiAuditNatureImpact2013,
  title = {An Audit of the Nature and Impact of Clinical Coding Subjectivity Variability and Error in Otolaryngology},
  author = {Nouraei, S.a.r. and Hudovsky, A. and Virk, J.s. and Chatrath, P. and Sandhu, G.s.},
  date = {2013},
  journaltitle = {Clinical Otolaryngology},
  volume = {38},
  number = {6},
  pages = {512--524},
  issn = {1749-4486},
  doi = {10.1111/coa.12153},
  abstract = {Objectives To audit the accuracy of clinical coding in otolaryngology, assess the effectiveness of previously implemented interventions, and determine ways in which it can be further improved. Design Prospective clinician–auditor multidisciplinary audit of clinical coding accuracy. Participants Elective and emergency ENT admissions and day-case activity. Main outcome measures Concordance between initial coding and the clinician–auditor multi-disciplinary teams (MDT) coding in respect of primary and secondary diagnoses and procedures, health resource groupings health resource groupings (HRGs) and tariffs. Results The audit of 3131 randomly selected otolaryngology patients between 2010 and 2012 resulted in 420 instances of change to the primary diagnosis (13\%) and 417 changes to the primary procedure (13\%). In 1420 cases (44\%), there was at least one change to the initial coding and 514 (16\%) health resource groupings changed. There was an income variance of £343,169 or £109.46 per patient. The highest rates of health resource groupings change were observed in head and neck surgery and in particular skull-based surgery, laryngology and within that tracheostomy, and emergency admissions, and specially, epistaxis management. A randomly selected sample of 235 patients from the audit were subjected to a second audit by a second clinician–auditor multi-disciplinary team. There were 12 further health resource groupings changes (5\%) and at least one further coding change occurred in 57 patients (24\%). These changes were significantly lower than those observed in the pre-audit sample, but were also significantly greater than zero. Asking surgeons to `code in theatre' and applying these codes without further quality assurance to activity resulted in an health resource groupings error rate of 45\%. The full audit sample was regrouped under health resource groupings 3.5 and was compared with a previous audit of 1250 patients performed between 2007 and 2008. This comparison showed a reduction in the baseline rate of health resource groupings change from 16\% during the first audit cycle to 9\% in the current audit cycle (P {$<$} 0.001). Conclusions Otolaryngology coding is complex and susceptible to subjectivity, variability and error. Coding variability can be improved, but not eliminated through regular education supported by an audit programme.},
  langid = {english}
}

@article{olah_attention_2016,
  title = {Attention and {{Augmented Recurrent Neural Networks}}},
  author = {Olah, Chris and Carter, Shan},
  date = {2016-09-08},
  journaltitle = {Distill},
  issn = {2476-0757},
  doi = {10.23915/distill.00001},
  url = {http://distill.pub/2016/augmented-rnns},
  urldate = {2018-04-18},
  abstract = {Recurrent neural networks are one of the staples of deep learning, allowing neural networks to work with sequences of data like text, audio and video. They can be used to boil a sequence down into a high-level understanding, to annotate sequences, and even to generate new sequences from scratch!}
}

@online{olah_understanding_2015,
  title = {Understanding {{LSTM Networks}}},
  author = {Olah, Chris},
  date = {2015},
  url = {http://colah.github.io/posts/2015-08-Understanding-LSTMs/},
  urldate = {2018-05-26},
  abstract = {Humans don’t start their thinking from scratch every second. As you read this essay, you understand each word based on your understanding of previous words. You don’t throw everything away and start thinking from scratch again. Your thoughts have persistence. Traditional neural networks can’t do this, and it seems like a major shortcoming. For example, imagine you want to classify what kind of event is happening at every point in a movie. It’s unclear how a traditional neural network could use its reasoning about previous events in the film to inform later ones. Recurrent neural networks address this issue. They are networks with loops in them, allowing information to persist.},
  organization = {{colah.github.io}}
}

@article{olshausen_emergence_1996,
  title = {Emergence of Simple-Cell Receptive Field Properties by Learning a Sparse Code for Natural Images},
  author = {Olshausen, Bruno A. and Field, David J.},
  date = {1996-06},
  journaltitle = {Nature},
  volume = {381},
  number = {6583},
  eprint = {8637596},
  eprinttype = {pmid},
  pages = {607--609},
  issn = {00280836},
  doi = {10.1038/381607a0},
  abstract = {The receptive fields of simple cells in mammalian primary visual cortex can be characterized as being spatially localized, oriented1–4 and bandpass (selective to structure at different spatial scales), comparable to the basis functions of wavelet transforms5,6. One approach to understanding such response properties of visual neurons has been to consider their relationship to the statistical structure of natural images in terms of efficient coding7–12. Along these lines, a number of studies have attempted to train unsupervised learning algorithms on natural images in the hope of developing receptive fields with similar properties13–18, but none has succeeded in producing a full set that spans the image space and contains all three of the above properties. Here we investigate the proposal8,12 that a coding strategy that maximizes sparseness is sufficient to account for these properties. We show that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex. The resulting sparse image code provides a more efficient representation for later stages of processing because it possesses a higher degree of statistical independence among its outputs.},
  isbn = {9781612849379}
}

@article{omalleyMeasuringDiagnosesICD2005,
  title = {Measuring {{Diagnoses}}: {{ICD Code Accuracy}}},
  shorttitle = {Measuring {{Diagnoses}}},
  author = {O'Malley, Kimberly J and Cook, Karon F and Price, Matt D and Wildes, Kimberly Raiford and Hurdle, John F and Ashton, Carol M},
  date = {2005-10},
  journaltitle = {Health Services Research},
  volume = {40},
  eprint = {16178999},
  eprinttype = {pmid},
  pages = {1620--1639},
  issn = {0017-9124},
  doi = {10.1111/j.1475-6773.2005.00444.x},
  abstract = {Objective To examine potential sources of errors at each step of the described inpatient International Classification of Diseases (ICD) coding process. Data Sources/Study Setting The use of disease codes from the ICD has expanded from classifying morbidity and mortality information for statistical purposes to diverse sets of applications in research, health care policy, and health care finance. By describing a brief history of ICD coding, detailing the process for assigning codes, identifying where errors can be introduced into the process, and reviewing methods for examining code accuracy, we help code users more systematically evaluate code accuracy for their particular applications. Study Design/Methods We summarize the inpatient ICD diagnostic coding process from patient admission to diagnostic code assignment. We examine potential sources of errors at each step and offer code users a tool for systematically evaluating code accuracy. Principle Findings Main error sources along the “patient trajectory” include amount and quality of information at admission, communication among patients and providers, the clinician's knowledge and experience with the illness, and the clinician's attention to detail. Main error sources along the “paper trail” include variance in the electronic and written records, coder training and experience, facility quality-control efforts, and unintentional and intentional coder errors, such as misspecification, unbundling, and upcoding. Conclusions By clearly specifying the code assignment process and heightening their awareness of potential error sources, code users can better evaluate the applicability and limitations of codes for their particular situations. ICD codes can then be used in the most appropriate ways.},
  issue = {5 Pt 2},
  pmcid = {PMC1361216}
}

@unpublished{ondel_bayesian_2019,
  title = {Bayesian {{Subspace Hidden Markov Model}} for {{Acoustic Unit Discovery}}},
  author = {Ondel, Lucas and Vydana, Hari Krishna and Burget, Lukáš and Černocký, Jan},
  date = {2019-07-02},
  eprint = {1904.03876},
  eprinttype = {arxiv},
  eprintclass = {cs, eess, stat},
  url = {http://arxiv.org/abs/1904.03876},
  urldate = {2021-10-26},
  abstract = {This work tackles the problem of learning a set of language specific acoustic units from unlabeled speech recordings given a set of labeled recordings from other languages. Our approach may be described by the following two steps procedure: first the model learns the notion of acoustic units from the labelled data and then the model uses its knowledge to find new acoustic units on the target language. We implement this process with the Bayesian Subspace Hidden Markov Model (SHMM), a model akin to the Subspace Gaussian Mixture Model (SGMM) where each low dimensional embedding represents an acoustic unit rather than just a HMM's state. The subspace is trained on 3 languages from the GlobalPhone corpus (German, Polish and Spanish) and the AUs are discovered on the TIMIT corpus. Results, measured in equivalent Phone Error Rate, show that this approach significantly outperforms previous HMM based acoustic units discovery systems and compares favorably with the Variational Auto Encoder-HMM.}
}

@article{ondel_variational_2016,
  title = {Variational {{Inference}} for {{Acoustic Unit Discovery}}},
  author = {Ondel, Lucas and Burget, Lukaš and Černocký, Jan},
  date = {2016-01-01},
  journaltitle = {Procedia Computer Science},
  shortjournal = {Procedia Computer Science},
  series = {{{SLTU-2016}} 5th {{Workshop}} on {{Spoken Language Technologies}} for {{Under-resourced}} Languages 09-12 {{May}} 2016 {{Yogyakarta}}, {{Indonesia}}},
  volume = {81},
  pages = {80--86},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2016.04.033},
  url = {https://www.sciencedirect.com/science/article/pii/S1877050916300473},
  urldate = {2021-10-14},
  abstract = {Recently, several nonparametric Bayesian models have been proposed to automatically discover acoustic units in unlabeled data. Most of them are trained using various versions of the Gibbs Sampling (GS) method. In this work, we consider Variational Bayes (VB) as alternative inference process. Even though VB yields an approximate solution of the posterior distribution it can be easily parallelized which makes it more suitable for large database. Results show that, notwithstanding VB inference is an order of magnitude faster, it outperforms GS in terms of accuracy.},
  langid = {english}
}

@online{oneata_evaluation_2021,
  title = {An Evaluation of Word-Level Confidence Estimation for End-to-End Automatic Speech Recognition},
  author = {Oneata, Dan and Caranica, Alexandru and Stan, Adriana and Cucu, Horia},
  date = {2021-01-14},
  eprint = {2101.05525},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2101.05525},
  urldate = {2022-12-12},
  abstract = {Quantifying the confidence (or conversely the uncertainty) of a prediction is a highly desirable trait of an automatic system, as it improves the robustness and usefulness in downstream tasks. In this paper we investigate confidence estimation for end-to-end automatic speech recognition (ASR). Previous work has addressed confidence measures for lattice-based ASR, while current machine learning research mostly focuses on confidence measures for unstructured deep learning. However, as the ASR systems are increasingly being built upon deep end-to-end methods, there is little work that tries to develop confidence measures in this context. We fill this gap by providing an extensive benchmark of popular confidence methods on four well-known speech datasets. There are two challenges we overcome in adapting existing methods: working on structured data (sequences) and obtaining confidences at a coarser level than the predictions (words instead of tokens). Our results suggest that a strong baseline can be obtained by scaling the logits by a learnt temperature, followed by estimating the confidence as the negative entropy of the predictive distribution and, finally, sum pooling to aggregate at word level.},
  pubstate = {preprint}
}

@inproceedings{oord_conditional_2016,
  title = {Conditional Image Generation with {{PixelCNN}} Decoders},
  booktitle = {Proceedings of the 29th {{International Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {family=Oord, given=Aäron, prefix=van den, useprefix=false and Kalchbrenner, Nal and Espeholt, Lasse and Kavukcuoglu, Koray and Vinyals, Oriol and Graves, Alex},
  date = {2016},
  pages = {4790--4798},
  location = {{Barcelona, Spain}},
  url = {https://proceedings.neurips.cc/paper/2016/hash/b1301141feffabac455e1f90a7de2054-Abstract.html}
}

@inproceedings{oord_neural_2018,
  title = {Neural {{Discrete Representation Learning}}},
  booktitle = {Proceedings of the 30th {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {family=Oord, given=Aäron, prefix=van den, useprefix=false and Vinyals, Oriol and Kavukcuoglu, Koray},
  date = {2018},
  location = {{Long Beach, CA, USA}},
  url = {http://arxiv.org/abs/1711.00937},
  abstract = {Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector QuantisedVariational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of “posterior collapse” -— where the latents are ignored when they are paired with a powerful autoregressive decoder -— typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.},
  eventtitle = {Conference on {{Neural Information Processing Systems}}},
  langid = {english}
}

@inproceedings{oord_parallel_2018,
  title = {Parallel {{WaveNet}}: {{Fast}} High-Fidelity Speech Synthesis},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {family=Oord, given=Aäron, prefix=van den, useprefix=false and Li, Yazhe and Babuschkin, Igor and Simonyan, Karen and Vinyals, Oriol and Kavukcuoglu, Koray and family=Driessche, given=George, prefix=van den, useprefix=true and Lockhart, Edward and Cobo, Luis C. and Stimberg, Florian and Casagrande, Norman and Grewe, Dominik and Noury, Seb and Dieleman, Sander and Elsen, Erich and Kalchbrenner, Nal and Zen, Heiga and Graves, Alex and King, Helen and Walters, Tom and Belov, Dan and Hassabis, Demis},
  date = {2018},
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {80},
  eprint = {1711.10433},
  eprinttype = {arxiv},
  pages = {3915--3923},
  publisher = {{PMLR}},
  location = {{Stockholm, Sweden}},
  url = {http://proceedings.mlr.press/v80/oord18a.html}
}

@inproceedings{oord_pixel_2016,
  title = {Pixel {{Recurrent Neural Networks}}},
  booktitle = {Proceedings of the 33rd {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {family=Oord, given=Aäron, prefix=van den, useprefix=false and Kalchbrenner, Nal and Kavukcuoglu, Koray},
  date = {2016-08-19},
  publisher = {{Journal of Machine Learning}},
  location = {{New York, NY, USA}},
  url = {http://arxiv.org/abs/1601.06759},
  urldate = {2019-12-14},
  abstract = {Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast twodimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english}
}

@misc{oord_representation_2018,
  title = {Representation {{Learning}} with {{Contrastive Predictive Coding}}},
  author = {family=Oord, given=Aäron, prefix=van den, useprefix=false and Li, Yazhe and Vinyals, Oriol},
  date = {2018-07-10},
  eprint = {1807.03748},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1807.03748},
  urldate = {2019-05-24},
  abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.}
}

@inproceedings{oord_wavenet_2016,
  title = {{{WaveNet}}: {{A Generative Model}} for {{Raw Audio}}},
  shorttitle = {{{WaveNet}}},
  booktitle = {Proceedings of the 9th {{ISCA Speech Synthesis Workshop}}},
  author = {family=Oord, given=Aäron, prefix=van den, useprefix=false and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  date = {2016-09-12},
  location = {{Sunnyvale, CA, USA}},
  url = {http://arxiv.org/abs/1609.03499},
  urldate = {2018-05-22},
  abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.}
}

@online{opencv_development_team_opencv_2018,
  title = {{{OpenCV}} Documentation},
  author = {{OpenCV Development Team}},
  date = {2018},
  url = {https://docs.opencv.org/master/},
  organization = {{Documentation}}
}

@misc{opitzMacroF1Macro2021,
  title = {Macro {{F1}} and {{Macro F1}}},
  author = {Opitz, Juri and Burst, Sebastian},
  date = {2021-02},
  eprint = {1911.03347},
  eprinttype = {arxiv},
  abstract = {The 'macro F1' metric is frequently used to evaluate binary, multi-class and multi-label classification problems. Yet, we find that there exist two different formulas to calculate this quantity. In this note, we show that only under rare circumstances the two computations can be considered equivalent. More specifically, one formula well 'rewards' classifiers which produce a skewed error type distribution. In fact, the difference in outcome of the two computations can be as high as 0.5. The two computations may not only diverge in their scalar result but can also lead to different classifier rankings.},
  issue = {arXiv:1911.03347},
  organization = {{arXiv}}
}

@article{oponowicz_spoken_2018,
  title = {Spoken {{Language Identification}}},
  author = {Oponowicz, Tomasz},
  date = {2018},
  url = {https://www.kaggle.com/toponowicz/spoken-language-identification}
}

@inproceedings{oquab_learning_2014,
  title = {Learning and Transferring Mid-Level Image Representations Using Convolutional Neural Networks},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Oquab, Maxime and Bottou, Léon and Laptev, Ivan and Sivic, Josef},
  date = {2014},
  pages = {1717--1724}
}

@article{ormerod_explaining_2010,
  title = {Explaining {{Variational Approximations}}},
  author = {Ormerod, J. T. and Wand, M. P.},
  date = {2010-05},
  journaltitle = {The American Statistician},
  shortjournal = {The American Statistician},
  volume = {64},
  number = {2},
  pages = {140--153},
  issn = {0003-1305, 1537-2731},
  doi = {10.1198/tast.2010.09058},
  url = {http://www.tandfonline.com/doi/abs/10.1198/tast.2010.09058},
  urldate = {2021-02-08},
  langid = {english}
}

@article{oron_centered_2017,
  title = {Centered Isotonic Regression: Point and Interval Estimation for Dose–Response Studies},
  author = {Oron, Assaf P and Flournoy, Nancy},
  date = {2017},
  journaltitle = {Statistics in Biopharmaceutical Research},
  volume = {9},
  number = {3},
  pages = {258--267}
}

@article{oshaughnessy_linear_1988,
  title = {Linear Predictive Coding},
  author = {O'Shaughnessy, D.},
  date = {1988},
  journaltitle = {IEEE Potentials},
  volume = {7},
  number = {1},
  pages = {29--32},
  doi = {10.1109/45.1890}
}

@article{osterwalder_business_2012,
  title = {Business {{Model Generation20124Alexander Osterwalder}} and {{Yves Pigneur}}. {{Business Model Generation}} . {{New York}}, {{NY}}: {{Wiley}} 2011. 288 Pp. £23.99, {{ISBN}}: 9780470876411},
  author = {Osterwalder, Alexander and Pigneur, Yves},
  date = {2012},
  journaltitle = {Kybernetes},
  volume = {41},
  number = {5/6},
  pages = {823--824},
  publisher = {{Wiley,}},
  issn = {0368-492X},
  doi = {10.1108/03684921211261761},
  url = {http://findit.dtu.dk/en/catalog?utf8=%E2%9C%93&locale=en&search_field=all_fields&q=business+model+generation},
  urldate = {2017-03-29},
  isbn = {978-84-234-2841}
}

@unpublished{ouali_overview_2020,
  title = {An {{Overview}} of {{Deep Semi-Supervised Learning}}},
  author = {Ouali, Yassine and Hudelot, Céline and Tami, Myriam},
  date = {2020-07-06},
  eprint = {2006.05278},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.05278},
  urldate = {2021-10-26},
  abstract = {Deep neural networks demonstrated their ability to provide remarkable performances on a wide range of supervised learning tasks (e.g., image classification) when trained on extensive collections of labeled data (e.g., ImageNet). However, creating such large datasets requires a considerable amount of resources, time, and effort. Such resources may not be available in many practical cases, limiting the adoption and the application of many deep learning methods. In a search for more data-efficient deep learning methods to overcome the need for large annotated datasets, there is a rising research interest in semi-supervised learning and its applications to deep neural networks to reduce the amount of labeled data required, by either developing novel methods or adopting existing semi-supervised learning frameworks for a deep learning setting. In this paper, we provide a comprehensive overview of deep semi-supervised learning, starting with an introduction to the field, followed by a summarization of the dominant semi-supervised approaches in deep learning.}
}

@article{ovinnikov_poincare_,
  title = {Poincaré {{Wasserstein Autoencoder}}},
  author = {Ovinnikov, Ivan},
  pages = {8},
  abstract = {This work presents a reformulation of the recently proposed Wasserstein autoencoder framework on a non-Euclidean manifold, the Poincaré ball model of the hyperbolic space Hn. By assuming the latent space to be hyperbolic, we can use its intrinsic hierarchy to impose structure on the learned latent space representations. We demonstrate the model in the visual domain to analyze some of its properties and show competitive results on a graph link prediction task.},
  langid = {english}
}

@article{paisley_variational_,
  title = {Variational {{Bayesian Inference}} with {{Stochastic Search}}},
  author = {Paisley, John and Blei, David M and Jordan, Michael I},
  pages = {8},
  abstract = {Mean-field variational inference is a method for approximate Bayesian posterior inference. It approximates a full posterior distribution with a factorized set of distributions by maximizing a lower bound on the marginal likelihood. This requires the ability to integrate a sum of terms in the log joint likelihood using this factorized distribution. Often not all integrals are in closed form, which is typically handled by using a lower bound. We present an alternative algorithm based on stochastic optimization that allows for direct optimization of the variational lower bound. This method uses control variates to reduce the variance of the stochastic search gradient, in which existing lower bounds can play an important role. We demonstrate the approach on two non-conjugate models: logistic regression and an approximation to the HDP.},
  langid = {english}
}

@online{pan_fast_2022,
  title = {Fast {{Vision Transformers}} with {{HiLo Attention}}},
  author = {Pan, Zizheng and Cai, Jianfei and Zhuang, Bohan},
  date = {2022-09-17},
  eprint = {2205.13213},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2205.13213},
  urldate = {2022-10-11},
  abstract = {Vision Transformers (ViTs) have triggered the most recent and significant breakthroughs in computer vision. Their efficient designs are mostly guided by the indirect metric of computational complexity, i.e., FLOPs, which however has a clear gap with the direct metric such as throughput. Thus, we propose to use the direct speed evaluation on the target platform as the design principle for efficient ViTs. Particularly, we introduce LITv2, a simple and effective ViT which performs favourably against the existing state-of-the-art methods across a spectrum of different model sizes with faster speed. At the core of LITv2 is a novel self-attention mechanism, which we dub HiLo. HiLo is inspired by the insight that high frequencies in an image capture local fine details and low frequencies focus on global structures, whereas a multi-head self-attention layer neglects the characteristic of different frequencies. Therefore, we propose to disentangle the high/low frequency patterns in an attention layer by separating the heads into two groups, where one group encodes high frequencies via self-attention within each local window, and another group performs the attention to model the global relationship between the average-pooled low-frequency keys from each window and each query position in the input feature map. Benefiting from the efficient design for both groups, we show that HiLo is superior to the existing attention mechanisms by comprehensively benchmarking FLOPs, speed and memory consumption on GPUs. Powered by HiLo, LITv2 serves as a strong backbone for mainstream vision tasks including image classification, dense detection and segmentation. Code is available at https://github.com/ziplab/LITv2.},
  pubstate = {preprint}
}

@online{pan_iared_2021,
  title = {{{IA-RED}}\$\^2\$: {{Interpretability-Aware Redundancy Reduction}} for {{Vision Transformers}}},
  shorttitle = {{{IA-RED}}\$\^2\$},
  author = {Pan, Bowen and Panda, Rameswar and Jiang, Yifan and Wang, Zhangyang and Feris, Rogerio and Oliva, Aude},
  date = {2021-10-26},
  eprint = {2106.12620},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2106.12620},
  urldate = {2022-10-08},
  abstract = {The self-attention-based model, transformer, is recently becoming the leading backbone in the field of computer vision. In spite of the impressive success made by transformers in a variety of vision tasks, it still suffers from heavy computation and intensive memory costs. To address this limitation, this paper presents an Interpretability-Aware REDundancy REDuction framework (IA-RED\$\^2\$). We start by observing a large amount of redundant computation, mainly spent on uncorrelated input patches, and then introduce an interpretable module to dynamically and gracefully drop these redundant patches. This novel framework is then extended to a hierarchical structure, where uncorrelated tokens at different stages are gradually removed, resulting in a considerable shrinkage of computational cost. We include extensive experiments on both image and video tasks, where our method could deliver up to 1.4x speed-up for state-of-the-art models like DeiT and TimeSformer, by only sacrificing less than 0.7\% accuracy. More importantly, contrary to other acceleration approaches, our method is inherently interpretable with substantial visual evidence, making vision transformer closer to a more human-understandable architecture while being lighter. We demonstrate that the interpretability that naturally emerged in our framework can outperform the raw attention learned by the original visual transformer, as well as those generated by off-the-shelf interpretation methods, with both qualitative and quantitative results. Project Page: http://people.csail.mit.edu/bpan/ia-red/.},
  pubstate = {preprint}
}

@inproceedings{panaredabusto_open_2017,
  title = {Open Set Domain Adaptation},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Panareda Busto, Pau and Gall, Juergen},
  date = {2017},
  pages = {754--763}
}

@inproceedings{panayotov_librispeech_2015,
  title = {Librispeech: {{An ASR}} Corpus Based on Public Domain Audio Books},
  booktitle = {Proceedings of the {{International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
  date = {2015},
  pages = {5206--5210},
  location = {{Brisbane, Australia}},
  eventtitle = {International {{Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}}
}

@inproceedings{papamakarios_masked_2017,
  title = {Masked {{Autoregressive Flow}} for {{Density Estimation}}},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Papamakarios, George and Pavlakou, Theo and Murray, Iain},
  date = {2017},
  eprint = {1705.07057},
  eprinttype = {arxiv},
  location = {{Long Beach, CA, USA}},
  url = {http://arxiv.org/abs/1705.07057},
  urldate = {2020-02-12},
  abstract = {Autoregressive models are among the best performing neural density estimators. We describe an approach for increasing the flexibility of an autoregressive model, based on modelling the random numbers that the model uses internally when generating data. By constructing a stack of autoregressive models, each modelling the random numbers of the next model in the stack, we obtain a type of normalizing flow suitable for density estimation, which we call Masked Autoregressive Flow. This type of flow is closely related to Inverse Autoregressive Flow and is a generalization of Real NVP. Masked Autoregressive Flow achieves state-of-the-art performance in a range of general-purpose density estimation tasks.},
  eventtitle = {International {{Conference}} on {{Neural Information Processing Systems}} ({{NIPS}})},
  langid = {english}
}

@misc{papamakarios_normalizing_2019,
  title = {Normalizing {{Flows}} for {{Probabilistic Modeling}} and {{Inference}}},
  author = {Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
  date = {2019-12-05},
  eprint = {1912.02762},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1912.02762},
  urldate = {2020-12-08},
  abstract = {Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.},
  langid = {english}
}

@article{papineni_bleu_2002,
  title = {{{BLEU}}: A Method for Automatic Evaluation of Machine Translation},
  author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  date = {2002},
  journaltitle = {Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics},
  eprint = {1702.00764},
  eprinttype = {arxiv},
  pages = {311--318},
  issn = {00134686},
  doi = {10.3115/1073083.1073135},
  url = {http://dl.acm.org/citation.cfm?id=1073135},
  abstract = {Human evaluations of machine translation are extensive but expensive. Human eval- uations can take months to finish and in- volve human labor that can not be reused. We propose a method of automatic ma- chine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evalu- ation, and that has little marginal cost per run. We present this method as an auto- mated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.},
  isbn = {1-55860-883-4},
  issue = {July}
}

@inproceedings{park_analysis_2017,
  title = {Analysis on the {{Dropout Effect}} in {{Convolutional Neural Networks}}},
  booktitle = {Lecture {{Notes}} in {{Computer Science}} (Including Subseries {{Lecture Notes}} in {{Artificial Intelligence}} and {{Lecture Notes}} in {{Bioinformatics}})},
  author = {Park, Sungheon and Kwak, Nojun},
  date = {2017},
  volume = {10112 LNCS},
  pages = {189--204},
  issn = {16113349},
  doi = {10.1007/978-3-319-54184-6_12},
  abstract = {Regularizing neural networks is an important task to reduce overfitting. Dropout [1] has been a widelyused regularization trick for neural networks. In convolutional neural networks (CNNs), dropout is usually applied to the fully connected layers. Meanwhile, the regularization effect of dropout in the convolutional layers has not been thoroughly analyzed in the literature. In this paper, we analyze the effect of dropout in the convolutional layers, which is indeed proved as a powerful generalization method. We observed that dropout in CNNs regularizes the networks by adding noise to the output feature maps of each layer, yielding robustness to variations of images. Based on this observation, we propose a stochastic dropout whose drop ratio varies for each iteration. Furthermore, we propose a new regularization method which is inspired by behaviors of image filters. Rather than randomly drop the activation, we selectively drop the activations which have high values across the feature map or across the channels. Experimental results validate the regularization performance of selective max-drop and stochastic dropout is competitive to the dropout or spatial dropout [2]. © Springer International Publishing AG 2017.},
  isbn = {978-3-319-54183-9}
}

@article{park_efficient_2013,
  title = {An Efficient {{MapReduce}} Algorithm for Counting Triangles in a Very Large Graph},
  author = {Park, Ha-Myung and Chung, Chin-Wan},
  date = {2013},
  journaltitle = {CIKM},
  doi = {10.1145/2505515.2505563},
  abstract = {Triangle counting problem is one of the fundamental prob-lem in various domains. The problem can be utilized for computation of clustering coefficient, transitivity, trianglu-lar connectivity, trusses, etc. The problem have been exten-sively studied in internal memory but the algorithms are not scalable for enormous graphs. In recent years, the MapRe-duce has emerged as a de facto standard framework for pro-cessing large data through parallel computing. A MapRe-duce algorithm was proposed for the problem based on graph partitioning. However, the algorithm redundantly generates a large number of intermediate data that cause network over-load and prolong the processing time. In this paper, we pro-pose a new algorithm based on graph partitioning with a novel idea of triangle classification to count the number of triangles in a graph. The algorithm substantially reduces the duplication by classifying triangles into three types and processing each triangle differently according to its type. In the experiments, we compare the proposed algorithm with recent existing algorithms using both synthetic datasets and real-world datasets that are composed of millions of nodes and billions of edges. The proposed algorithm outperforms other algorithms in most cases. Especially, for a twitter dataset, the proposed algorithm is more than twice as fast as existing MapReduce algorithms. Moreover, the performance gap increases as the graph becomes larger and denser.},
  isbn = {9781450322638}
}

@online{park_generative_2023,
  title = {Generative {{Agents}}: {{Interactive Simulacra}} of {{Human Behavior}}},
  shorttitle = {Generative {{Agents}}},
  author = {Park, Joon Sung and O'Brien, Joseph C. and Cai, Carrie J. and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
  date = {2023-04-06},
  eprint = {2304.03442},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2304.03442},
  urldate = {2023-04-11},
  abstract = {Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents--computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors: for example, starting with only a single user-specified notion that one agent wants to throw a Valentine's Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture--observation, planning, and reflection--each contribute critically to the believability of agent behavior. By fusing large language models with computational, interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.},
  pubstate = {preprint}
}

@misc{park_hierarchical_2018,
  title = {A {{Hierarchical Latent Structure}} for {{Variational Conversation Modeling}}},
  author = {Park, Yookoon and Cho, Jaemin and Kim, Gunhee},
  date = {2018-04-10},
  eprint = {1804.03424},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1804.03424},
  urldate = {2019-09-15},
  abstract = {Variational autoencoders (VAE) combined with hierarchical RNNs have emerged as a powerful framework for conversation modeling. However, they suffer from the notorious degeneration problem, where the decoders learn to ignore latent variables and reduce to vanilla RNNs. We empirically show that this degeneracy occurs mostly due to two reasons. First, the expressive power of hierarchical RNN decoders is often high enough to model the data using only its decoding distributions without relying on the latent variables. Second, the conditional VAE structure whose generation process is conditioned on a context, makes the range of training targets very sparse; that is, the RNN decoders can easily overfit to the training data ignoring the latent variables. To solve the degeneration problem, we propose a novel model named Variational Hierarchical Conversation RNNs (VHCR), involving two key ideas of (1) using a hierarchical structure of latent variables, and (2) exploiting an utterance drop regularization. With evaluations on two datasets of Cornell Movie Dialog and Ubuntu Dialog Corpus, we show that our VHCR successfully utilizes latent variables and outperforms state-of-the-art models for conversation generation. Moreover, it can perform several new utterance control tasks, thanks to its hierarchical latent structure.},
  langid = {english}
}

@inproceedings{park_improved_2020,
  title = {Improved Noisy Student Training for Automatic Speech Recognition},
  booktitle = {Annual Conference of the International Speech Communication Association},
  author = {Park, Daniel S. and Zhang, Yu and Jia, Ye and Han, Wei and Chiu, Chung-Cheng and Li, Bo and Wu, Yonghui and Le, Quoc V.},
  date = {2020}
}

@online{park_review_2021,
  title = {A {{Review}} of {{Speaker Diarization}}: {{Recent Advances}} with {{Deep Learning}}},
  shorttitle = {A {{Review}} of {{Speaker Diarization}}},
  author = {Park, Tae Jin and Kanda, Naoyuki and Dimitriadis, Dimitrios and Han, Kyu J. and Watanabe, Shinji and Narayanan, Shrikanth},
  date = {2021-11-26},
  eprint = {2101.09624},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2101.09624},
  urldate = {2023-04-14},
  abstract = {Speaker diarization is a task to label audio or video recordings with classes that correspond to speaker identity, or in short, a task to identify "who spoke when". In the early years, speaker diarization algorithms were developed for speech recognition on multispeaker audio recordings to enable speaker adaptive processing. These algorithms also gained their own value as a standalone application over time to provide speaker-specific metainformation for downstream tasks such as audio retrieval. More recently, with the emergence of deep learning technology, which has driven revolutionary changes in research and practices across speech application domains, rapid advancements have been made for speaker diarization. In this paper, we review not only the historical development of speaker diarization technology but also the recent advancements in neural speaker diarization approaches. Furthermore, we discuss how speaker diarization systems have been integrated with speech recognition applications and how the recent surge of deep learning is leading the way of jointly modeling these two components to be complementary to each other. By considering such exciting technical trends, we believe that this paper is a valuable contribution to the community to provide a survey work by consolidating the recent developments with neural methods and thus facilitating further progress toward a more efficient speaker diarization.},
  pubstate = {preprint}
}

@misc{park_specaugment_2019,
  title = {{{SpecAugment}}: {{A Simple Data Augmentation Method}} for {{Automatic Speech Recognition}}},
  shorttitle = {{{SpecAugment}}},
  author = {Park, Daniel S. and Chan, William and Zhang, Yu and Chiu, Chung-Cheng and Zoph, Barret and Cubuk, Ekin D. and Le, Quoc V.},
  date = {2019-04-18},
  eprint = {1904.08779},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1904.08779},
  urldate = {2019-05-13},
  abstract = {We present SpecAugment, a simple data augmentation method for speech recognition. SpecAugment is applied directly to the feature inputs of a neural network (i.e., filter bank coefficients). The augmentation policy consists of warping the features, masking blocks of frequency channels, and masking blocks of time steps. We apply SpecAugment on Listen, Attend and Spell networks for end-to-end speech recognition tasks. We achieve state-of-the-art performance on the LibriSpeech 960h and Swichboard 300h tasks, outperforming all prior work. On LibriSpeech, we achieve 6.8\% WER on test-other without the use of a language model, and 5.8\% WER with shallow fusion with a language model. This compares to the previous state-of-the-art hybrid system of 7.5\% WER. For Switchboard, we achieve 7.2\%/14.6\% on the Switchboard/CallHome portion of the Hub5'00 test set without the use of a language model, and 6.8\%/14.1\% with shallow fusion, which compares to the previous state-of-the-art hybrid system at 8.3\%/17.3\% WER.}
}

@article{parkInformationRetrievalApproach2019,
  title = {An {{Information Retrieval Approach}} to {{ICD-10 Classification}}},
  author = {Park, Hee and Castaño, José and Ávila, Pilar and Pérez, David and Berinsky, Hernán and Gambarte, Laura and Luna, Daniel and Otero, Carlos},
  date = {2019-08},
  journaltitle = {Studies in Health Technology and Informatics},
  volume = {264},
  eprint = {31438233},
  eprinttype = {pmid},
  pages = {1564--1565},
  issn = {1879-8365},
  doi = {10.3233/SHTI190536},
  abstract = {ICD-10 (International Classification of Diseases 10th revision) is a classification code for diseases, signs and symptoms, abnormal findings, complaints, social circumstances, and external causes of injury or diseases. This paper describes an automatic information retrieval approach to map free-text disease descriptions to ICD-10 codes. We use the Hospital Italiano de Buenos Aires (HIBA) terminology data mapped to ICD-10 codes as indexed data to find an appropriate ICD-10 code using search engine similarity metrics.},
  langid = {english}
}

@article{partaourides_asymmetric_2017,
  title = {Asymmetric Deep Generative Models},
  author = {Partaourides, Harris and Chatzis, Sotirios P.},
  date = {2017-06},
  journaltitle = {Neurocomputing},
  volume = {241},
  pages = {90--96},
  doi = {10.1016},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231217302989},
  urldate = {2018-09-02},
  abstract = {Amortized variational inference, whereby the inferred latent variable posterior distributions are parameterized by means of neural network functions, has invigorated a new wave of innovation in the field of generative latent variable modeling, giving rise to the family of deep generative models (DGMs). Existing DGM formulations are based on the assumption of a symmetric Gaussian posterior over the model latent variables. This assumption, although mathematically convenient, can be well-expected to undermine the eventually obtained representation power, as it imposes apparent expressiveness limitations. Indeed, it has been recently shown that even some moderate increase in the latent variable posterior expressiveness, obtained by introducing an additional level of dependencies upon auxiliary (Gaussian) latent variables, can result in significant performance improvements in the context of semi-supervised learning tasks. Inspired from these advances, in this paper we examine whether a more potent increase in the expressiveness and representation power of modern DGMs can be achieved by completely relaxing their typical symmetric (Gaussian) latent variable posterior assumptions: Specifically, we consider DGMs with asymmetric posteriors, formulated as restricted multivariate skew-Normal (rMSN) distributions. We derive an efficient amortized variational inference algorithm for the proposed model, and exhibit its superiority over the current state-of-the-art in several semi-supervised learning benchmarks.},
  langid = {english}
}

@online{parthasarathi_lessons_2019,
  title = {Lessons from Building Acoustic Models with a Million Hours of Speech},
  author = {Parthasarathi, Sree Hari Krishnan and Strom, Nikko},
  date = {2019},
  eprint = {1904.01624},
  eprinttype = {arxiv},
  pubstate = {preprint}
}

@article{parzen_estimation_1962,
  title = {On Estimation of a Probability Density Function and Mode},
  author = {Parzen, Emanuel},
  date = {1962},
  journaltitle = {The Annals of Mathematical Statistics},
  volume = {33},
  number = {3},
  pages = {1065--1076},
  publisher = {{JSTOR}}
}

@article{pasad_contributions_2019,
  title = {On the Contributions of Visual and Textual Supervision in Low-Resource Semantic Speech Retrieval},
  author = {Pasad, A and Shi, B and Kamper, H and Livescu, K},
  date = {2019},
  journaltitle = {Annual Conference of the International Speech Communication Association}
}

@inproceedings{pasad_layerwise_2021,
  title = {Layer-Wise Analysis of a Self-Supervised Speech Representation Model},
  booktitle = {{{IEEE Workshop}} on {{Automatic Speech Recognition}} and {{Understanding}} ({{ASRU}})},
  author = {Pasad, Ankita and Chou, Ju-Chieh and Livescu, Karen},
  date = {2021}
}

@online{pasad_use_2022,
  title = {On the {{Use}} of {{External Data}} for {{Spoken Named Entity Recognition}}},
  author = {Pasad, Ankita and Wu, Felix and Shon, Suwon and Livescu, Karen and Han, Kyu J.},
  date = {2022-07-08},
  eprint = {2112.07648},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2112.07648},
  urldate = {2023-09-23},
  abstract = {Spoken language understanding (SLU) tasks involve mapping from speech audio signals to semantic labels. Given the complexity of such tasks, good performance might be expected to require large labeled datasets, which are difficult to collect for each new task and domain. However, recent advances in self-supervised speech representations have made it feasible to consider learning SLU models with limited labeled data. In this work we focus on low-resource spoken named entity recognition (NER) and address the question: Beyond self-supervised pre-training, how can we use external speech and/or text data that are not annotated for the task? We draw on a variety of approaches, including self-training, knowledge distillation, and transfer learning, and consider their applicability to both end-to-end models and pipeline (speech recognition followed by text NER model) approaches. We find that several of these approaches improve performance in resource-constrained settings beyond the benefits from pre-trained representations alone. Compared to prior work, we find improved F1 scores of up to 16\%. While the best baseline model is a pipeline approach, the best performance when using external data is ultimately achieved by an end-to-end model. We provide detailed comparisons and analyses, showing for example that end-to-end models are able to focus on the more NER-specific words.},
  pubstate = {preprint}
}

@article{pascanu_difficulty_,
  title = {On the Difficulty of Training Recurrent Neural Networks},
  author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  pages = {9},
  abstract = {There are two widely known issues with properly training recurrent neural networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
  langid = {english}
}

@misc{pascanu_how_2013,
  title = {How to {{Construct Deep Recurrent Neural Networks}}},
  author = {Pascanu, Razvan and Gülçehre, Çaglar and Cho, Kyunghyun and Bengio, Yoshua and Gulcehre, Caglar and Cho, Kyunghyun and Bengio, Yoshua},
  date = {2013-12-20},
  eprint = {1312.6026},
  eprinttype = {arxiv},
  url = {https://arxiv.org/abs/1312.6026},
  urldate = {2018-06-06},
  abstract = {In this paper, we propose a novel way to extend a recurrent neural network (RNN) to a deep RNN. We start by arguing that the concept of the depth in an RNN is not as clear as it is in feedforward neural networks. By carefully analyzing and understanding the architecture of an RNN, we define three points which may be made deeper; (1) input-to-hidden function, (2) hidden-to-hidden transition and (3) hidden-to-output function. This can be considered in addition to stacking multiple recurrent layers proposed earlier by Schmidhuber (1992). Based on this observation, we propose two novel architectures of a deep RNN and provide an alternative interpretation of these deep RNN's using a novel framework based on neural operators. The proposed deep RNN's are empirically evaluated on the tasks of polyphonic music prediction and language modeling. The experimental result supports our claim that the proposed deep RNN's benefit from the depth and outperform the conventional, shallow RNN.}
}

@inproceedings{pascual_learning_2019,
  title = {Learning Problem-Agnostic Speech Representations from Multiple Self-Supervised Tasks},
  booktitle = {Annual {{Conference}} of the {{International Speech Communication Association}}},
  author = {Pascual, Santiago and Ravanelli, Mirco and Serrà, Joan and Bonafonte, Antonio and Bengio, Yoshua},
  date = {2019},
  eprint = {1904.03416},
  eprinttype = {arxiv},
  abstract = {Learning good representations without supervision is still an open issue in machine learning, and is particularly challenging for speech signals, which are often characterized by long sequences with a complex hierarchical structure. Some recent works, however, have shown that it is possible to derive useful speech representations by employing a self-supervised encoder-discriminator approach. This paper proposes an improved self-supervised method, where a single neural encoder is followed by multiple workers that jointly solve different self-supervised tasks. The needed consensus across different tasks naturally imposes meaningful constraints to the encoder, contributing to discover general representations and to minimize the risk of learning superficial ones. Experiments show that the proposed approach can learn transferable, robust, and problem-agnostic features that carry on relevant information from the speech signal, such as speaker identity, phonemes, and even higher-level features such as emotional cues. In addition, a number of design choices make the encoder easily exportable, facilitating its direct usage or adaptation to different problems.}
}

@inproceedings{pascualBERTbasedAutomaticICD2021,
  title = {Towards {{BERT-based Automatic ICD Coding}}: {{Limitations}} and {{Opportunities}}},
  shorttitle = {Towards {{BERT-based Automatic ICD Coding}}},
  booktitle = {Proceedings of the 20th {{Workshop}} on {{Biomedical Language Processing}}},
  author = {Pascual, Damian and Luck, Sandro and Wattenhofer, Roger},
  date = {2021},
  pages = {54--63},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2021.bionlp-1.6},
  langid = {english}
}

@inproceedings{paszke_automatic_2017,
  title = {Automatic Differentiation in {{PyTorch}}},
  booktitle = {Proceedings of the 31st {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Paszke, Adam and Chanan, Gregory and Lin, Zeming and Gross, Sam and Yang, Edward and Antiga, Luca and Devito, Zachary},
  date = {2017},
  url = {https://pytorch.org/},
  abstract = {In this article, we describe an automatic differentiation module of PyTorch — a library designed to enable rapid research on machine learning models. It builds upon a few projects, most notably Lua Torch, Chainer, and HIPS Autograd, and provides a high performance environment with easy access to automatic differentiation of models executed on different devices (CPU and GPU). To make prototyping easier, PyTorch does not follow the symbolic approach used in many other deep learning frameworks, but focuses on differentiation of purely imperative programs, with a focus on extensibility and low overhead. Note that this preprint is a draft of certain sections from an upcoming paper covering all PyTorch features.}
}

@online{patacchiola_estimating_2021,
  title = {Estimating the Gradient of the {{ELBO}}},
  author = {Patacchiola, Massimiliano},
  date = {2021-02-08},
  url = {https://mpatacchiola.github.io/blog/2021/02/08/intro-variational-inference-2.html},
  urldate = {2023-10-23},
  abstract = {This is the second post of the series on variational inference. In the previous post I have introduced the variational framework, and the three main characters at play: the evidence, the Kullback-Leibler (KL) divergence, and the Evidence Lower BOund (ELBO). I have showed how the ELBO can be considered as a surrogate objective for finding the posterior distribution  p ( z | x )  when the evidence  p ( x )  is intractable. By maximizing the ELBO we can find the parameters  θ  of a variational distribution  q θ ( z )  that better fit the posterior. However, I did not mention how the ELBO itself can by maximized in practice. One approach is to estimate the gradient of the ELBO w.r.t.  θ  and then update  θ  in the direction of steepest ascent. Depending on the particular problem at hand and the choice of variational distribution, this could be challenging. In this post I will focus on this particular problem, showing how we can estimate the gradients of the ELBO by using two techniques: the score function estimator (a.k.a. REINFORCE) and the pathwise estimator (a.k.a. reparametrization trick).},
  langid = {english},
  organization = {{mpatacchiola’s blog}}
}

@inproceedings{pathak_context_2016,
  title = {Context {{Encoders}}: {{Feature Learning}} by {{Inpainting}}},
  shorttitle = {Context {{Encoders}}},
  author = {Pathak, Deepak and Krahenbuhl, Philipp and Donahue, Jeff and Darrell, Trevor and Efros, Alexei A.},
  date = {2016},
  pages = {2536--2544},
  url = {https://openaccess.thecvf.com/content_cvpr_2016/html/Pathak_Context_Encoders_Feature_CVPR_2016_paper.html},
  urldate = {2021-11-11},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}}
}

@inproceedings{pathak_curiosity-driven_2017,
  title = {Curiosity-{{Driven Exploration}} by {{Self-Supervised Prediction}}},
  booktitle = {{{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}}},
  author = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A. and Darrell, Trevor},
  date = {2017},
  volume = {2017-July},
  eprint = {1705.05363},
  eprinttype = {arxiv},
  pages = {488--489},
  issn = {21607516},
  doi = {10.1109/CVPRW.2017.70},
  url = {https://arxiv.org/abs/1705.05363},
  urldate = {2017-10-06},
  abstract = {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch. Demo video and code available at https://pathak22.github.io/noreward-rl/},
  isbn = {978-1-5386-0733-6}
}

@inproceedings{paul_design_1992,
  title = {The Design for the {{Wall Street Journal-based CSR}} Corpus},
  booktitle = {Proceedings of the {{Worhshop}} on {{Speech}} and {{Natural Language}}:},
  author = {Paul, Douglas B and Baker, Janet},
  date = {1992-02},
  location = {{Harriman, New York}}
}

@misc{paulus_deep_2017,
  title = {A {{Deep Reinforced Model}} for {{Abstractive Summarization}}},
  author = {Paulus, Romain and Xiong, Caiming and Socher, Richard},
  date = {2017},
  eprint = {23459267},
  eprinttype = {pmid},
  url = {http://arxiv.org/abs/1705.04304},
  urldate = {2018-04-24},
  abstract = {Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents and summaries however these models often include repetitive and incoherent phrases. We introduce a neural network model with a novel intra-attention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL). Models trained only with supervised learning often exhibit "exposure bias" - they assume ground truth is provided at each step during training. However, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable. We evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, an improvement over previous state-of-the-art models. Human evaluation also shows that our model produces higher quality summaries.},
  isbn = {2004012439}
}

@misc{paulus_gradient_2021,
  title = {Gradient {{Estimation}} with {{Stochastic Softmax Tricks}}},
  author = {Paulus, Max B. and Choi, Dami and Tarlow, Daniel and Krause, Andreas and Maddison, Chris J.},
  date = {2021-02-28},
  eprint = {2006.08063},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2006.08063},
  urldate = {2021-05-07},
  abstract = {The Gumbel-Max trick is the basis of many relaxed gradient estimators. These estimators are easy to implement and low variance, but the goal of scaling them comprehensively to large combinatorial distributions is still outstanding. Working within the perturbation model framework, we introduce stochastic softmax tricks, which generalize the Gumbel-Softmax trick to combinatorial spaces. Our framework is a unified perspective on existing relaxed estimators for perturbation models, and it contains many novel relaxations. We design structured relaxations for subset selection, spanning trees, arborescences, and others. When compared to less structured baselines, we find that stochastic softmax tricks can be used to train latent variable models that perform better and discover more latent structure.},
  langid = {english}
}

@article{pearl_causal_2009,
  title = {Causal Inference in Statistics: {{An}} Overview},
  shorttitle = {Causal Inference in Statistics},
  author = {Pearl, Judea},
  date = {2009},
  journaltitle = {Statistics Surveys},
  shortjournal = {Statist. Surv.},
  volume = {3},
  number = {0},
  pages = {96--146},
  issn = {1935-7516},
  doi = {10.1214/09-SS057},
  url = {http://projecteuclid.org/euclid.ssu/1255440554},
  urldate = {2020-10-05},
  abstract = {This review presents empirical researchers with recent advances in causal inference, and stresses the paradigmatic shifts that must be undertaken in moving from traditional statistical analysis to causal analysis of multivariate data. Special emphasis is placed on the assumptions that underly all causal inferences, the languages used in formulating those assumptions, the conditional nature of all causal and counterfactual claims, and the methods that have been developed for the assessment of such claims. These advances are illustrated using a general theory of causation based on the Structural Causal Model (SCM) described in Pearl (2000a), which subsumes and unifies other approaches to causation, and provides a coherent mathematical foundation for the analysis of causes and counterfactuals. In particular, the paper surveys the development of mathematical tools for inferring (from a combination of data and assumptions) answers to three types of causal queries: (1) queries about the effects of potential interventions, (also called “causal effects” or “policy evaluation”) (2) queries about probabilities of counterfactuals, (including assessment of “regret,” “attribution” or “causes of effects”) and (3) queries about direct and indirect effects (also known as “mediation”). Finally, the paper defines the formal and conceptual relationships between the structural and potential-outcome frameworks and presents tools for a symbiotic analysis that uses the strong features of both.},
  langid = {english},
  keywords = {★}
}

@book{pearl_causality_2009,
  title = {Causality: {{Models}}, {{Reasoning}}, and {{Inference}}},
  shorttitle = {Causality},
  author = {Pearl, Judea},
  date = {2009},
  edition = {2. ed},
  publisher = {{Cambridge Univ. Press}},
  location = {{Cambridge}},
  isbn = {978-0-511-80316-1 978-0-521-89560-6 978-0-521-77362-1},
  langid = {english},
  pagetotal = {464}
}

@inproceedings{peng_correspondence_2020,
  title = {A Correspondence Variational Autoencoder for Unsupervised Acoustic Word Embeddings},
  booktitle = {Procedings of the {{NeurIPS Workshop}} on {{Self-Supervised Learning}} for {{Speech}} and {{Audio Processing}}},
  author = {Peng, Puyuan and Kamper, Herman and Livescu, Karen},
  date = {2020}
}

@inproceedings{peng_fastslow_2022,
  title = {Fast-Slow Transformer for Visually Grounding Speech},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Peng, Puyuan and Harwath, David},
  date = {2022}
}

@online{peng_random_2021,
  title = {Random {{Feature Attention}}},
  author = {Peng, Hao and Pappas, Nikolaos and Yogatama, Dani and Schwartz, Roy and Smith, Noah A. and Kong, Lingpeng},
  date = {2021-03-19},
  eprint = {2103.02143},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2103.02143},
  urldate = {2022-10-10},
  abstract = {Transformers are state-of-the-art models for a variety of sequence modeling tasks. At their core is an attention function which models pairwise interactions between the inputs at every timestep. While attention is powerful, it does not scale efficiently to long sequences due to its quadratic time and space complexity in the sequence length. We propose RFA, a linear time and space attention that uses random feature methods to approximate the softmax function, and explore its application in transformers. RFA can be used as a drop-in replacement for conventional softmax attention and offers a straightforward way of learning with recency bias through an optional gating mechanism. Experiments on language modeling and machine translation demonstrate that RFA achieves similar or better performance compared to strong transformer baselines. In the machine translation experiment, RFA decodes twice as fast as a vanilla transformer. Compared to existing efficient transformer variants, RFA is competitive in terms of both accuracy and efficiency on three long text classification datasets. Our analysis shows that RFA's efficiency gains are especially notable on long sequences, suggesting that RFA will be particularly useful in tasks that require working with large inputs, fast decoding speed, or low memory footprints.},
  pubstate = {preprint}
}

@inproceedings{peng_selfsupervised_2022,
  title = {Self-Supervised Representation Learning for Speech Using Visual Grounding and Masked Language Modeling},
  booktitle = {{{AAAI SAS}} Workshop},
  author = {Peng, Puyuan and Harwath, David},
  date = {2022}
}

@online{peng_word_2022,
  title = {Word Discovery in Visually Grounded, Self-Supervised Speech Models},
  author = {Peng, Puyuan and Harwath, David},
  date = {2022},
  eprint = {2203.15081},
  eprinttype = {arxiv},
  pubstate = {preprint}
}

@inproceedings{pennington_glove:_2014,
  title = {Glove: {{Global Vectors}} for {{Word Representation}}},
  booktitle = {Proceedings of the {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D.},
  date = {2014},
  volume = {14},
  eprint = {1710995},
  eprinttype = {pmid},
  pages = {1532--1543},
  issn = {10495258},
  doi = {10.3115/v1/D14-1162},
  url = {http://aclweb.org/anthology/D14-1162},
  abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arith- metic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global log- bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co- occurrence matrix, rather than on the en- tire sparse matrix or on individual context windows in a large corpus. On a recent word analogy task our model obtains 75\% accuracy, an improvement of 11\% over Mikolov et al. (2013). It also outperforms related word vector models on similarity tasks and named entity recognition.},
  isbn = {978-1-937284-96-1}
}

@inproceedings{perronnin_improving_2010,
  title = {Improving the Fisher Kernel for Large-Scale Image Classification},
  booktitle = {Computer {{Vision}}–{{ECCV}} 2010: 11th European Conference on Computer Vision, Heraklion, Crete, Greece, September 5-11, 2010, Proceedings, Part {{IV}} 11},
  author = {Perronnin, Florent and Sánchez, Jorge and Mensink, Thomas},
  date = {2010},
  pages = {143--156},
  publisher = {{Springer}}
}

@article{pervez_spectral_nodate,
  title = {Spectral {{Smoothing Unveils Phase Transitions}} in {{Hierarchical Variational Autoencoders}}},
  author = {Pervez, Adeel and Gavves, Efstratios},
  pages = {10},
  abstract = {Variational autoencoders with deep stochastic hierarchies are known to suffer from the problem of posterior collapse, where the top layers fall back to the prior and become independent of input. We suggest that the hierarchical VAE objective explicitly includes the variance of the function parameterizing the mean and variance of the latent Gaussian distribution which itself is often a high variance function. Building on this we generalize VAE neural networks by incorporating a smoothing parameter motivated by Gaussian analysis to reduce higher frequency components and consequently the variance in parameterizing functions. We show this helps to solve the problem of posterior collapse. We further show that under such smoothing the VAE loss exhibits a phase transition, where the top layer KL divergence sharply drops to zero at a critical value of the smoothing parameter that is similar for the same model across datasets. We validate the phenomenon across model configurations and datasets.},
  langid = {english}
}

@thesis{petajan_automatic_1984,
  type = {phdthesis},
  title = {Automatic Lipreading to Enhance Speech Recognition (Speech Reading)},
  author = {Petajan, Eric David},
  date = {1984},
  institution = {{University of Illinois at Urbana-Champaign, USA}}
}

@online{peternorvig_chomsky_2012,
  title = {On {{Chomsky}} and the {{Two Cultures}} of {{Statistical Learning}}},
  author = {{Peter Norvig}},
  date = {2012},
  url = {https://norvig.com/chomsky.html},
  urldate = {2023-10-14}
}

@misc{peters_deep_2018,
  title = {Deep Contextualized Word Representations},
  author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  date = {2018},
  eprint = {1802.05365},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1802.05365},
  urldate = {2018-05-02},
  abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.}
}

@article{peters_interpretable_nodate,
  title = {Interpretable {{Structure Induction}} via {{Sparse Attention}}},
  author = {Peters, Ben and Niculae, Vlad and Martins, André F T},
  pages = {3},
  langid = {english}
}

@thesis{peters_machine_2007,
  title = {Machine {{Learning}} of Motor {{Skills}} for {{Robotics}}},
  author = {Peters, Jan},
  date = {2007},
  journaltitle = {University of Southern California},
  institution = {{University of Southern California}},
  abstract = {Autonomous robots that can assist humans in situations of daily life have been a long standing vision of robotics, artificial intelligence, and cognitive sciences. A first step towards this goal is to create robots that can accomplish a multitude of different tasks, triggered by environmental context or higher level instruction. Early approaches to this goal during the heydays of artificial intelligence research in the late 1980ies, however, made it clear that an approach purely based on reasoning and human insights would not be able to model all the perceptuomotor tasks that a robot should fulfill. Instead, new hope was put in the growing wake of machine learning that promised fully adaptive control algorithms which learn both by observation and trial-and-error. However, to date, learning techniques have yet to fulfill this promise as only few methods manage to scale into the high-dimensional domains of manipulator robotics, or even the new upcoming trend of humanoid robotics, and usually scaling was only achieved in precisely pre-structured domains. In this thesis, we investigate the ingredients for a general approach to motor skill learning in order to get one step closer towards human-like performance. For doing so, we study two major components for such an approach, i.e., firstly, a theoretically well-founded general approach to representing the required control structures for task representation and execution and, secondly, appropriate learning algorithms which can be applied in this setting. As a theoretical foundation, we first study a general framework to generate control laws for real robots with a particular focus on skills represented as dynamical systems in differential constraint form. We present a point-wise optimal control framework resulting from a generalization of Gauss’ principle and show how various well-known robot control laws can be derived by modifying the metric of the employed cost function. The framework has been successfully applied to task space tracking control for holonomic systems for several different metrics on the anthropomorphic SARCOS Master Arm. In order to overcome the limiting requirement of accurate robot models, we first employ learning methods to find learning controllers for task space control. However, when learning to execute a redundant control problem, we face the general problem of the non-convexity of the solution space which can force the robot to steer into physically impossible configurations if supervised learning methods are employed without further consideration. This problem can be resolved using two major insights, i.e., the learning problem can be treated as locally convex and the cost function of the analytical framework can be used to ensure global consistency. Thus, we derive an immediate reinforcement learning algorithm from the expectation-maximization point of view which results in a reward-weighted regression technique. This method can be used both for operational space control as well as general immediate reward reinforcement learning problems. We demonstrate the feasibility of the resulting framework on the problem of redundant end-effector tracking for both a simulated 3 degrees of freedom robot arm as well as for a simulated anthropomorphic SARCOS Master Arm. While learning to execute tasks in task space is an essential component to a general framework to motor skill learning, learning the actual task is of even higher importance, particularly as this issue is more frequently beyond the abilities of analytical approaches than execution. We focus on the learning of elemental tasks which can serve as the “building blocks of movement generation”, called motor primitives. Motor primitives are parameterized task representations based on splines or nonlinear differential equations with desired attractor properties. While imitation learning of parameterized motor primitives is a relatively well-understood problem, the self-improvement by interaction of the system with the environment remains a challenging problem, tackled in the fourth chapter of this thesis. For pursuing this goal, we highlight the difficulties with current reinforcement learning methods, and outline both established and novel algorithms for the gradient-based improvement of parameterized policies. We compare these algorithms in the context of motor primitive learning, and show that our most modern algorithm, the Episodic Natural Actor-Critic outperforms previous algorithms by at least an order of magnitude. We demonstrate the efficiency of this reinforcement learning method in the application of learning to hit a baseball with an anthropomorphic robot arm. In conclusion, in this thesis, we have contributed a general framework for analytically computing robot control laws which can be used for deriving various previous control approaches and serves as foundation as well as inspiration for our learning algorithms. We have introduced two classes of novel reinforcement learning methods, i.e., the Natural Actor-Critic and the Reward-Weighted Regression algorithm. These algorithms have been used in order to replace the analytical components of the theoretical framework by learned representations. Evaluations have been performed on both simulated and real robot arms.},
  issue = {April}
}

@misc{petersen_matrix_2012,
  title = {The {{Matrix Cookbook}}},
  author = {Petersen, Kaare Breandt and Pedersen, Michael Syskind},
  date = {2012},
  location = {{Kongens Lyngby, Denmark}},
  abstract = {These pages are a collection of facts (identities, approxima- tions, inequalities, relations, ...) about matrices and matters relating to them. It is collected in this form for the convenience of anyone who wants a quick desktop reference .},
  organization = {{Technical University of Denmark}},
  keywords = {★}
}

@online{peyre_computational_2020,
  title = {Computational {{Optimal Transport}}},
  author = {Peyré, Gabriel and Cuturi, Marco},
  date = {2020-03-18},
  eprint = {1803.00567},
  eprinttype = {arxiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.1803.00567},
  url = {http://arxiv.org/abs/1803.00567},
  urldate = {2023-10-18},
  abstract = {Optimal transport (OT) theory can be informally described using the words of the French mathematician Gaspard Monge (1746-1818): A worker with a shovel in hand has to move a large pile of sand lying on a construction site. The goal of the worker is to erect with all that sand a target pile with a prescribed shape (for example, that of a giant sand castle). Naturally, the worker wishes to minimize her total effort, quantified for instance as the total distance or time spent carrying shovelfuls of sand. Mathematicians interested in OT cast that problem as that of comparing two probability distributions, two different piles of sand of the same volume. They consider all of the many possible ways to morph, transport or reshape the first pile into the second, and associate a "global" cost to every such transport, using the "local" consideration of how much it costs to move a grain of sand from one place to another. Recent years have witnessed the spread of OT in several fields, thanks to the emergence of approximate solvers that can scale to sizes and dimensions that are relevant to data sciences. Thanks to this newfound scalability, OT is being increasingly used to unlock various problems in imaging sciences (such as color or texture processing), computer vision and graphics (for shape manipulation) or machine learning (for regression, classification and density fitting). This short book reviews OT with a bias toward numerical methods and their applications in data sciences, and sheds lights on the theoretical properties of OT that make it particularly useful for some of these applications.},
  pubstate = {preprint}
}

@inproceedings{pham_dropout_2014,
  title = {Dropout {{Improves Recurrent Neural Networks}} for {{Handwriting Recognition}}},
  booktitle = {Proceedings of {{International Conference}} on {{Frontiers}} in {{Handwriting Recognition}} ({{ICFHR}})},
  author = {Pham, Vu and Bluche, Theodore and Kermorvant, Christopher and Louradour, Jerome},
  date = {2014},
  eprint = {1312.4569},
  eprinttype = {arxiv},
  pages = {285--290},
  issn = {21676453},
  doi = {10.1109/ICFHR.2014.55},
  abstract = {Recurrent neural networks (RNNs) with Long Short-Term memory cells currently hold the best known results in unconstrained handwriting recognition. We show that their performance can be greatly improved using dropout - a recently proposed regularization method for deep architectures. While previous works showed that dropout gave superior performance in the context of convolutional networks, it had never been applied to RNNs. In our approach, dropout is carefully used in the network so that it does not affect the recurrent connections, hence the power of RNNs in modeling sequence is preserved. Extensive experiments on a broad range of handwritten databases confirm the effectiveness of dropout on deep architectures even when the network mainly consists of recurrent and shared connections.},
  isbn = {978-1-4799-4334-0}
}

@article{phuong_mutual_2018,
  title = {The Mutual Autoencoder: {{Controlling}} Information in Latent Code Representations},
  author = {Phuong, Mary and Welling, Max and Kushman, Nate and Tomioka, Ryota and Nowozin, Sebastian},
  date = {2018},
  url = {https://openreview.net/forum?id=HkbmWqxCZ}
}

@article{pimentel_review_2014,
  title = {A Review of Novelty Detection},
  author = {Pimentel, Marco AF and Clifton, David A and Clifton, Lei and Tarassenko, Lionel},
  date = {2014},
  journaltitle = {Signal processing},
  volume = {99},
  pages = {215--249},
  publisher = {{Elsevier}}
}

@article{platt_probabilistic_1999,
  title = {Probabilistic Outputs for Support Vector Machines and Comparisons to Regularized Likelihood Methods},
  author = {Platt, John and others},
  date = {1999},
  journaltitle = {Advances in large margin classifiers},
  volume = {10},
  number = {3},
  pages = {61--74},
  publisher = {{Cambridge, MA}}
}

@online{poli_hyena_2023,
  title = {Hyena {{Hierarchy}}: {{Towards Larger Convolutional Language Models}}},
  shorttitle = {Hyena {{Hierarchy}}},
  author = {Poli, Michael and Massaroli, Stefano and Nguyen, Eric and Fu, Daniel Y. and Dao, Tri and Baccus, Stephen and Bengio, Yoshua and Ermon, Stefano and Ré, Christopher},
  date = {2023-04-19},
  eprint = {2302.10866},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2302.10866},
  urldate = {2023-06-01},
  abstract = {Recent advances in deep learning have relied heavily on the use of large Transformers due to their ability to learn at scale. However, the core building block of Transformers, the attention operator, exhibits quadratic cost in sequence length, limiting the amount of context accessible. Existing subquadratic methods based on low-rank and sparse approximations need to be combined with dense attention layers to match Transformers, indicating a gap in capability. In this work, we propose Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long convolutions and data-controlled gating. In recall and reasoning tasks on sequences of thousands to hundreds of thousands of tokens, Hyena improves accuracy by more than 50 points over operators relying on state-spaces and other implicit and explicit methods, matching attention-based models. We set a new state-of-the-art for dense-attention-free architectures on language modeling in standard datasets (WikiText103 and The Pile), reaching Transformer quality with a 20\% reduction in training compute required at sequence length 2K. Hyena operators are twice as fast as highly optimized attention at sequence length 8K, and 100x faster at sequence length 64K.},
  pubstate = {preprint}
}

@article{polyak_acceleration_1992,
  title = {Acceleration of {{Stochastic Approximation}} by {{Averaging}}},
  author = {Polyak, B. T. and Juditsky, A. B.},
  date = {1992-07},
  journaltitle = {SIAM Journal on Control and Optimization},
  volume = {30},
  number = {4},
  pages = {838--855},
  issn = {0363-0129, 1095-7138},
  doi = {10.1137/0330046},
  url = {http://epubs.siam.org/doi/10.1137/0330046},
  urldate = {2018-09-10},
  abstract = {A new recursive algorithm of stochastic approximation type with the averaging of trajectories is investigated. Convergence with probability one is proved for a variety of classical optimization and identification problems. It is also demonstrated for these problems that the proposed algorithm achieves the highest possible rate of convergence.},
  langid = {english}
}

@inproceedings{polyak_speech_2021,
  title = {Speech Resynthesis from Discrete Disentangled Self-Supervised Representations},
  booktitle = {Annual {{Conference}} of the {{International Speech Communication Association}}},
  author = {Polyak, Adam and Adi, Yossi and Copet, Jade and Kharitonov, Eugene and Lakhotia, Kushal and Hsu, Wei-Ning and Mohamed, Abdelrahman and Dupoux, Emmanuel},
  date = {2021}
}

@inproceedings{poole_variational_2019,
  title = {On {{Variational Bounds}} of {{Mutual Information}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Poole, Ben and Ozair, Sherjil},
  date = {2019-06},
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {97},
  pages = {5171--5180},
  publisher = {{PMLR}},
  location = {{Long Beach, CA, USA}},
  abstract = {Estimating and optimizing Mutual Information (MI) is core to many problems in machine learning; however, bounding MI in high dimensions is challenging. To establish tractable and scalable objectives, recent work has turned to variational bounds parameterized by neural networks, but the relationships and tradeoffs between these bounds remains unclear. In this work, we unify these recent developments in a single framework. We find that the existing variational lower bounds degrade when the MI is large, exhibiting either high bias or high variance. To address this problem, we introduce a continuum of lower bounds that encompasses previous bounds and flexibly trades off bias and variance. On high-dimensional, controlled problems, we empirically characterize the bias and variance of the bounds and their gradients and demonstrate the effectiveness of our new bounds for estimation and representation learning.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english}
}

@article{potamianos_recent_2003,
  title = {Recent Advances in the Automatic Recognition of Audiovisual Speech},
  author = {Potamianos, Gerasimos and Neti, Chalapathy and Gravier, Guillaume and Garg, Ashutosh and Senior, Andrew W},
  date = {2003},
  journaltitle = {Proceedings of the IEEE},
  volume = {91},
  number = {9},
  pages = {1306--1326}
}

@inproceedings{pratap_mls_2020,
  title = {{{MLS}}: {{A}} Large-Scale Multilingual Dataset for Speech Research},
  booktitle = {Annual {{Conference}} of the {{International Speech Communication Association}}},
  author = {Pratap, Vineel and Xu, Qiantong and Sriram, Anuroop and Synnaeve, Gabriel and Collobert, Ronan},
  date = {2020}
}

@misc{pratap_wav2letter_2018,
  title = {Wav2letter++: {{The Fastest Open-source Speech Recognition System}}},
  shorttitle = {Wav2letter++},
  author = {Pratap, Vineel and Hannun, Awni and Xu, Qiantong and Cai, Jeff and Kahn, Jacob and Synnaeve, Gabriel and Liptchinsky, Vitaliy and Collobert, Ronan},
  date = {2018-12-18},
  eprint = {1812.07625},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1812.07625},
  urldate = {2019-01-25},
  abstract = {This paper introduces wav2letter++, the fastest open-source deep learning speech recognition framework. wav2letter++ is written entirely in C++, and uses the ArrayFire tensor library for maximum efficiency. Here we explain the architecture and design of the wav2letter++ system and compare it to other major open-source speech recognition systems. In some cases wav2letter++ is more than 2x faster than other optimized frameworks for training end-to-end neural networks for speech recognition. We also show that wav2letter++'s training times scale linearly to 64 GPUs, the highest we tested, for models with 100 million parameters. High-performance frameworks enable fast iteration, which is often a crucial factor in successful research and model tuning on new datasets and tasks.}
}

@inproceedings{prenger_waveglow_2019,
  title = {Waveglow: {{A}} Flow-Based Generative Network for Speech Synthesis},
  booktitle = {{{IEEE}} International Conference on Acoustics, Speech and Signal Processing, {{ICASSP}} 2019, Brighton, United Kingdom, May 12-17, 2019},
  author = {Prenger, Ryan and Valle, Rafael and Catanzaro, Bryan},
  date = {2019},
  pages = {3617--3621},
  publisher = {{IEEE}},
  doi = {10.1109/ICASSP.2019.8683143},
  url = {https://doi.org/10.1109/ICASSP.2019.8683143}
}

@book{press_numerical_2007,
  title = {Numerical Recipes},
  author = {Press, William H. and family=Teukolsky, given=Saul, prefix=a., useprefix=false and Vetterling, William T. and Flannery, Brian P.},
  date = {2007},
  journaltitle = {Cambridge University Press},
  volume = {29},
  number = {4},
  url = {numerical.recipes},
  abstract = {From Preface: “I was just going to say, when I was interrupted:::” begins Oliver Wendell Holmes in the second series of his famous essays, The Autocrat of the Breakfast Table. The interruption referred towas a gap of 25 years. In our case, as the autocrats of Numerical Recipes, the gap between our second and third editions has been “only” 15 years. Scientific computing has changed enormously in that time. The first edition of Numerical Recipes was roughly coincident with the first commercial success of the personal computer. The second edition came at about the time that the Internet, as we know it today, was created. Now, as we launch the third edition, the practice of science and engineering, and thus scientific computing, has been profoundly altered by the mature Internet and Web. It is no longer difficult to find somebody’s algorithm, and usually free code, for almost any conceivable scien- tific application. The critical questions have instead become, “How does it work?” and “Is it any good?” Correspondingly, the second edition of Numerical Recipes has come to be valued more and more for its text explanations, concise mathematical derivations, critical judgments, and advice, and less for its code implementations per se. Recognizing the change, we have expanded and improved the text in many places in this edition and addedmany completely newsections. We seriously consid- ered leaving the code out entirely, or making it available only on theWeb. However, in the end, we decided that without code, it wouldn’t be Numerical Recipes.That is, without code you, the reader, could never know whether our advice was in fact hon- est, implementable, and practical. Many discussions of algorithms in the literature and on the Web omit crucial details that can only be uncovered by actually coding (our job) or reading compilable code (your job). Also, we needed actual code to teach and illustrate the large number of lessons about object-oriented programming that are implicit and explicit in this edition.},
  isbn = {0-521-88068-8},
  pagetotal = {1256}
}

@misc{primewordsinformationtechnologycoltd_primewords_2018,
  title = {Primewords {{Chinese Corpus Set}} 1},
  author = {{Primewords Information Technology Co., Ltd.}},
  date = {2018}
}

@inproceedings{pu_scaling_2021,
  title = {Scaling Effect of Self-Supervised Models},
  booktitle = {Proceedings of the 22nd {{Annual Conference}} of the {{International Speech Communication Association}} ({{Interspeech}})},
  author = {Pu, Jie and Yang, Yuguang and Li, Ruirui and Elibol, Oguz and Droppo, Jasha},
  date = {2021},
  publisher = {{ISCA}}
}

@inproceedings{qian_contentvec_2022,
  title = {{{ContentVec}}: {{An Improved Self-Supervised Speech Representation}} by {{Disentangling Speakers}}},
  booktitle = {International {{Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Qian, Kaizhi and Zhang, Yang and Gao, Heting and Ni, Junrui and Lai, Cheng-I and Cox, David and Hasegawa-Johnson, Mark and Chang, Shiyu},
  date = {2022}
}

@inproceedings{qian_momentum_1999,
  title = {On the Momentum Term in Gradient Descent Learning Algorithms},
  booktitle = {Proceedings of the {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Qian, Ning},
  date = {1999},
  volume = {12},
  number = {1},
  eprint = {12662723},
  eprinttype = {pmid},
  pages = {145--151},
  issn = {08936080},
  doi = {10.1016/S0893-6080(98)00116-6},
  abstract = {A momentum term is usually included in the simulations of connectionist learning algorithms. Although it is well known that such a term greatly improves the speed of learning, there have been few rigorous studies of its mechanisms. In this paper, I show that in the limit of continuous time, the momentum parameter is analogous to the mass of Newtonian particles that move through a viscous medium in a conservative force field. The behavior of the system near a local minimum is equivalent to a set of coupled and damped harmonic oscillators. The momentum term improves the speed of convergence by bringing some eigen components of the system closer to critical damping. Similar results can be obtained for the discrete time case used in computer simulations. In particular, I derive the bounds for convergence on learning- rate and momentum parameters, and demonstrate that the momentum term can increase the range of learning rate over which the system converges. The optimal condition for convergence is also analyzed.},
  isbn = {1-212-54352-1}
}

@article{qiu_pretrained_2020,
  title = {Pre-Trained Models for Natural Language Processing: {{A}} Survey},
  author = {Qiu, XiPeng and Sun, TianXiang and Xu, YiGe and Shao, YunFan and Dai, Ning and Huang, XuanJing},
  date = {2020-09},
  journaltitle = {Science China Technological Sciences},
  volume = {63},
  number = {10},
  pages = {1872--1897},
  issn = {1869-1900},
  doi = {10.1007/s11431-020-1647-3},
  url = {http://dx.doi.org/10.1007/s11431-020-1647-3}
}

@unpublished{quitry_learning_2019,
  title = {Learning Audio Representations via Phase Prediction},
  author = {Quitry, Félix de Chaumont and Tagliasacchi, Marco and Roblek, Dominik},
  date = {2019-10-25},
  eprint = {1910.11910},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/1910.11910},
  urldate = {2021-10-13},
  abstract = {We learn audio representations by solving a novel self-supervised learning task, which consists of predicting the phase of the short-time Fourier transform from its magnitude. A convolutional encoder is used to map the magnitude spectrum of the input waveform to a lower dimensional embedding. A convolutional decoder is then used to predict the instantaneous frequency (i.e., the temporal rate of change of the phase) from such embedding. To evaluate the quality of the learned representations, we evaluate how they transfer to a wide variety of downstream audio tasks. Our experiments reveal that the phase prediction task leads to representations that generalize across different tasks, partially bridging the gap with fully-supervised models. In addition, we show that the predicted phase can be used as initialization of the Griffin-Lim algorithm, thus reducing the number of iterations needed to reconstruct the waveform in the time domain.}
}

@inproceedings{rabiner_considerations_1979,
  title = {Considerations in Applying Clustering Techniques to Speaker Independent Word Recognition},
  booktitle = {Ieee {{International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}},
  author = {Rabiner, L. and Wilpon, J.},
  date = {1979},
  volume = {4},
  pages = {578--581}
}

@article{rabiner_tutorial_1989,
  title = {A Tutorial on Hidden {{Markov}} Models and Selected Applications in Speech Recognition},
  author = {Rabiner, Lawrence R},
  date = {1989},
  journaltitle = {Proceedings of the IEEE},
  volume = {77},
  number = {2},
  pages = {257--286},
  publisher = {{IEEE}}
}

@article{radford_improving_2018,
  title = {Improving Language Understanding by Generative Pre-Training},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  date = {2018},
  publisher = {{OpenAI}}
}

@article{radford_language_2019,
  title = {Language Models Are Unsupervised Multitask Learners},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  date = {2019},
  journaltitle = {OpenAI blog},
  volume = {1},
  number = {8},
  pages = {9}
}

@misc{radford_learning_2021,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  date = {2021-02-26},
  eprint = {2103.00020},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2103.00020},
  urldate = {2022-02-03},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.}
}

@inproceedings{radford_robust_2023,
  title = {Robust Speech Recognition via Large-Scale Weak Supervision},
  booktitle = {Proceedings of the International Conference on Machine Learning},
  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and Mcleavey, Christine and Sutskever, Ilya},
  editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  date = {2023-07-23/2023-07-29},
  series = {Proceedings of Machine Learning Research},
  volume = {202},
  pages = {28492--28518},
  publisher = {{PMLR}},
  url = {https://proceedings.mlr.press/v202/radford23a.html},
  abstract = {We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results without the need for any dataset specific fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.},
  keywords = {Read}
}

@misc{radford_unsupervised_2015,
  title = {Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks},
  author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
  date = {2015},
  eprint = {1511.06434},
  eprinttype = {arxiv},
  url = {https://arxiv.org/abs/1511.06434}
}

@misc{raffel_feedforward_2015,
  title = {Feed-{{Forward Networks}} with {{Attention Can Solve Some Long-Term Memory Problems}}},
  author = {Raffel, Colin and Ellis, Daniel P. W.},
  date = {2015-12-29},
  eprint = {1512.08756},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1512.08756},
  urldate = {2019-06-19},
  abstract = {We propose a simplified model of attention which is applicable to feed-forward neural networks and demonstrate that the resulting model can solve the synthetic “addition” and “multiplication” long-term memory problems for sequence lengths which are both longer and more widely varying than the best published results for these tasks.},
  langid = {english}
}

@unpublished{raghu_vision_2022,
  title = {Do {{Vision Transformers See Like Convolutional Neural Networks}}?},
  author = {Raghu, Maithra and Unterthiner, Thomas and Kornblith, Simon and Zhang, Chiyuan and Dosovitskiy, Alexey},
  date = {2022-03-03},
  eprint = {2108.08810},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2108.08810},
  urldate = {2022-04-26},
  abstract = {Convolutional neural networks (CNNs) have so far been the de-facto model for visual data. Recent work has shown that (Vision) Transformer models (ViT) can achieve comparable or even superior performance on image classification tasks. This raises a central question: how are Vision Transformers solving these tasks? Are they acting like convolutional networks, or learning entirely different visual representations? Analyzing the internal representation structure of ViTs and CNNs on image classification benchmarks, we find striking differences between the two architectures, such as ViT having more uniform representations across all layers. We explore how these differences arise, finding crucial roles played by self-attention, which enables early aggregation of global information, and ViT residual connections, which strongly propagate features from lower to higher layers. We study the ramifications for spatial localization, demonstrating ViTs successfully preserve input spatial information, with noticeable effects from different classification methods. Finally, we study the effect of (pretraining) dataset scale on intermediate features and transfer learning, and conclude with a discussion on connections to new architectures such as the MLP-Mixer.},
  keywords = {Unread}
}

@misc{rainforth_tighter_2019,
  title = {Tighter {{Variational Bounds}} Are {{Not Necessarily Better}}},
  author = {Rainforth, Tom and Kosiorek, Adam R. and Le, Tuan Anh and Maddison, Chris J. and Igl, Maximilian and Wood, Frank and Teh, Yee Whye},
  date = {2019-03-05},
  eprint = {1802.04537},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1802.04537},
  urldate = {2020-10-01},
  abstract = {We provide theoretical and empirical evidence that using tighter evidence lower bounds (ELBOs) can be detrimental to the process of learning an inference network by reducing the signal-to-noise ratio of the gradient estimator. Our results call into question common implicit assumptions that tighter ELBOs are better variational objectives for simultaneous model learning and inference amortization schemes. Based on our insights, we introduce three new algorithms: the partially importance weighted auto-encoder (PIWAE), the multiply importance weighted auto-encoder (MIWAE), and the combination importance weighted autoencoder (CIWAE), each of which includes the standard importance weighted auto-encoder (IWAE) as a special case. We show that each can deliver improvements over IWAE, even when performance is measured by the IWAE target itself. Furthermore, our results suggest that PIWAE may be able to deliver simultaneous improvements in the training of both the inference and generative networks.},
  langid = {english}
}

@misc{rajani_explain_2019,
  title = {Explain {{Yourself}}! {{Leveraging Language Models}} for {{Commonsense Reasoning}}},
  author = {Rajani, Nazneen Fatema and McCann, Bryan and Xiong, Caiming and Socher, Richard},
  date = {2019-06-05},
  eprint = {1906.02361},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1906.02361},
  urldate = {2019-08-01},
  abstract = {Deep learning models perform poorly on tasks that require commonsense reasoning, which often necessitates some form of worldknowledge or reasoning over information not immediately present in the input. We collect human explanations for commonsense reasoning in the form of natural language sequences and highlighted annotations in a new dataset called Common Sense Explanations (CoS-E). We use CoS-E to train language models to automatically generate explanations that can be used during training and inference in a novel Commonsense Auto-Generated Explanation (CAGE) framework. CAGE improves the state-of-the-art by 10\% on the challenging CommonsenseQA task. We further study commonsense reasoning in DNNs using both human and auto-generated explanations including transfer to out-of-domain tasks. Empirical results indicate that we can effectively leverage language models for commonsense reasoning.},
  langid = {english}
}

@misc{rajpurkar_know_2018,
  title = {Know {{What You Don}}'t {{Know}}: {{Unanswerable Questions}} for {{SQuAD}}},
  shorttitle = {Know {{What You Don}}'t {{Know}}},
  author = {Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
  date = {2018-06-11},
  eprint = {1806.03822},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1806.03822},
  urldate = {2019-06-14},
  abstract = {Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuAD 2.0, the latest version of the Stanford Question Answering Dataset (SQuAD). SQuAD 2.0 combines existing SQuAD data with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD 2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuAD 2.0 is a challenging natural language understanding task for existing models: a strong neural system that gets 86\% F1 on SQuAD 1.1 achieves only 66\% F1 on SQuAD 2.0.},
  langid = {english}
}

@misc{rajpurkar_squad_2016,
  title = {{{SQuAD}}: 100,000+ {{Questions}} for {{Machine Comprehension}} of {{Text}}},
  shorttitle = {{{SQuAD}}},
  author = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  date = {2016-06-16},
  eprint = {1606.05250},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1606.05250},
  urldate = {2019-06-14},
  abstract = {We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0\%, a significant improvement over a simple baseline (20\%). However, human performance (86.8\%) is much higher, indicating that the dataset presents a good challenge problem for future research. The dataset is freely available at https://stanford-qa.com.},
  langid = {english}
}

@misc{ramachandran_searching_2017,
  title = {Searching for {{Activation Functions}}},
  author = {Ramachandran, Prajit and Zoph, Barret and Le, Quoc V.},
  date = {2017-10-27},
  eprint = {1710.05941},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1710.05941},
  urldate = {2020-09-25},
  abstract = {The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance. Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU). Although various hand-designed alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains. In this work, we propose to leverage automatic search techniques to discover new activation functions. Using a combination of exhaustive and reinforcement learning-based search, we discover multiple novel activation functions. We verify the effectiveness of the searches by conducting an empirical evaluation with the best discovered activation function. Our experiments show that the best discovered activation function, f (x) = x · sigmoid(βx), which we name Swish, tends to work better than ReLU on deeper models across a number of challenging datasets. For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9\% for Mobile NASNet-A and 0.6\% for Inception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network.},
  langid = {english}
}

@unpublished{ramachandran_standalone_2019,
  title = {Stand-{{Alone Self-Attention}} in {{Vision Models}}},
  author = {Ramachandran, Prajit and Parmar, Niki and Vaswani, Ashish and Bello, Irwan and Levskaya, Anselm and Shlens, Jonathon},
  date = {2019-06-13},
  eprint = {1906.05909},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1906.05909},
  urldate = {2022-05-03},
  abstract = {Convolutions are a fundamental building block of modern computer vision systems. Recent approaches have argued for going beyond convolutions in order to capture long-range dependencies. These efforts focus on augmenting convolutional models with content-based interactions, such as self-attention and non-local means, to achieve gains on a number of vision tasks. The natural question that arises is whether attention can be a stand-alone primitive for vision models instead of serving as just an augmentation on top of convolutions. In developing and testing a pure self-attention vision model, we verify that self-attention can indeed be an effective stand-alone layer. A simple procedure of replacing all instances of spatial convolutions with a form of self-attention applied to ResNet model produces a fully self-attentional model that outperforms the baseline on ImageNet classification with 12\% fewer FLOPS and 29\% fewer parameters. On COCO object detection, a pure self-attention model matches the mAP of a baseline RetinaNet while having 39\% fewer FLOPS and 34\% fewer parameters. Detailed ablation studies demonstrate that self-attention is especially impactful when used in later layers. These results establish that stand-alone self-attention is an important addition to the vision practitioner's toolbox.},
  keywords = {Unread}
}

@misc{ramanujan_what_2020,
  title = {What's {{Hidden}} in a {{Randomly Weighted Neural Network}}?},
  author = {Ramanujan, Vivek and Wortsman, Mitchell and Kembhavi, Aniruddha and Farhadi, Ali and Rastegari, Mohammad},
  date = {2020-03-30},
  eprint = {1911.13299},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1911.13299},
  urldate = {2021-04-23},
  abstract = {Training a neural network is synonymous with learning the values of the weights. In contrast, we demonstrate that randomly weighted neural networks contain subnetworks which achieve impressive performance without ever modifying the weight values. Hidden in a randomly weighted Wide ResNet-50 [32] we find a subnetwork (with random weights) that is smaller than, but matches the performance of a ResNet-34 [9] trained on ImageNet [4]. Not only do these “untrained subnetworks” exist, but we provide an algorithm to effectively find them. We empirically show that as randomly weighted neural networks with fixed weights grow wider and deeper, an “untrained subnetwork” approaches a network with learned weights in accuracy. Our code and pretrained models are available at: https://github.com/allenai/hidden-networks.},
  langid = {english}
}

@misc{ramesh_zeroshot_2021,
  title = {Zero-{{Shot Text-to-Image Generation}}},
  author = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  date = {2021-02-26},
  eprint = {2102.12092},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2102.12092},
  urldate = {2021-03-05},
  abstract = {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.},
  langid = {english}
}

@misc{ramsauer_hopfield_2020,
  title = {Hopfield {{Networks}} Is {{All You Need}}},
  author = {Ramsauer, Hubert and Schäfl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Gruber, Lukas and Holzleitner, Markus and Pavlović, Milena and Sandve, Geir Kjetil and Greiff, Victor and Kreil, David and Kopp, Michael and Klambauer, Günter and Brandstetter, Johannes and Hochreiter, Sepp},
  date = {2020-07-16},
  eprint = {2008.02217},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2008.02217},
  urldate = {2020-08-20},
  abstract = {We show that the transformer attention mechanism is the update rule of a modern Hopfield network with continuous states. This new Hopfield network can store exponentially (with the dimension) many patterns, converges with one update, and has exponentially small retrieval errors. The number of stored patterns is traded off against convergence speed and retrieval error. The new Hopfield network has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. Transformer and BERT models operate in their first layers preferably in the global averaging regime, while they operate in higher layers in metastable states. The gradient in transformers is maximal for metastable states, is uniformly distributed for global averaging, and vanishes for a fixed point near a stored pattern. Using the Hopfield network interpretation, we analyzed learning of transformer and BERT models. Learning starts with attention heads that average and then most of them switch to metastable states. However, the majority of heads in the first layers still averages and can be replaced by averaging, e.g. our proposed Gaussian weighting. In contrast, heads in the last layers steadily learn and seem to use metastable states to collect information created in lower layers. These heads seem to be a promising target for improving transformers. Neural networks with Hopfield networks outperform other methods on immune repertoire classification, where the Hopfield net stores several hundreds of thousands of patterns. We provide a new PyTorch layer called "Hopfield", which allows to equip deep learning architectures with modern Hopfield networks as a new powerful concept comprising pooling, memory, and attention. GitHub: https://github.com/ml-jku/hopfield-layers},
  langid = {english}
}

@misc{ranganath_hierarchical_2016,
  title = {Hierarchical {{Variational Models}}},
  author = {Ranganath, Rajesh and Tran, Dustin and Blei, David M.},
  date = {2016-05-30},
  eprint = {1511.02386},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1511.02386},
  urldate = {2020-02-08},
  abstract = {Black box variational inference allows researchers to easily prototype and evaluate an array of models. Recent advances allow such algorithms to scale to high dimensions. However, a central question remains: How to specify an expressive variational distribution that maintains efficient computation? To address this, we develop hierarchical variational models (HVMs). HVMs augment a variational approximation with a prior on its parameters, which allows it to capture complex structure for both discrete and continuous latent variables. The algorithm we develop is black box, can be used for any HVM, and has the same computational efficiency as the original approximation. We study HVMs on a variety of deep discrete latent variable models. HVMs generalize other expressive variational distributions and maintains higher fidelity to the posterior.},
  langid = {english}
}

@inproceedings{ranzato_unified_2007,
  title = {A Unified Energy-Based Framework for Unsupervised Learning},
  booktitle = {Procedings of the 11th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}} ({{AISTATS}})},
  author = {Ranzato, Marc'Aurelio and Boureau, Y-Lan and Chopra, Sumit and LeCun, Yann},
  date = {2007},
  pages = {371--379}
}

@inproceedings{ranzato_unsupervised_2007,
  title = {Unsupervised {{Learning}} of {{Invariant Feature Hierarchies}} with {{Applications}} to {{Object Recognition}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Ranzato, Marc'Aurelio and Huang, Fu Jie and Boureau, Y-Lan and LeCun, Yann A.},
  date = {2007-06},
  eprint = {4270182},
  eprinttype = {pmid},
  pages = {1--8},
  issn = {10636919},
  doi = {10.1109/CVPR.2007.383157},
  url = {http://ieeexplore.ieee.org/document/4270182/},
  urldate = {2018-04-05},
  abstract = {We present an unsupervised method for learning a hi-erarchy of sparse feature detectors that are invariant to small shifts and distortions. The resulting feature extrac-tor consists of multiple convolution filters, followed by a feature-pooling layer that computes the max of each fil-ter output within adjacent windows, and a point-wise sig-moid non-linearity. A second level of larger and more in-variant features is obtained by training the same algorithm on patches of features from the first level. Training a su-pervised classifier on these features yields 0.64\% error on MNIST, and 54\% average recognition rate on Caltech 101 with 30 training samples per category. While the result-ing architecture is similar to convolutional networks, the layer-wise unsupervised training procedure alleviates the over-parameterization problems that plague purely super-vised learning procedures, and yields good performance with very few labeled training samples.},
  isbn = {1-4244-1180-7}
}

@unpublished{rao_dynamicvit_2021,
  title = {{{DynamicViT}}: {{Efficient Vision Transformers}} with {{Dynamic Token Sparsification}}},
  shorttitle = {{{DynamicViT}}},
  author = {Rao, Yongming and Zhao, Wenliang and Liu, Benlin and Lu, Jiwen and Zhou, Jie and Hsieh, Cho-Jui},
  date = {2021-10-26},
  eprint = {2106.02034},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2106.02034},
  urldate = {2022-04-19},
  abstract = {Attention is sparse in vision transformers. We observe the final prediction in vision transformers is only based on a subset of most informative tokens, which is sufficient for accurate image recognition. Based on this observation, we propose a dynamic token sparsification framework to prune redundant tokens progressively and dynamically based on the input. Specifically, we devise a lightweight prediction module to estimate the importance score of each token given the current features. The module is added to different layers to prune redundant tokens hierarchically. To optimize the prediction module in an end-to-end manner, we propose an attention masking strategy to differentiably prune a token by blocking its interactions with other tokens. Benefiting from the nature of self-attention, the unstructured sparse tokens are still hardware friendly, which makes our framework easy to achieve actual speed-up. By hierarchically pruning 66\% of the input tokens, our method greatly reduces 31\%\textasciitilde 37\% FLOPs and improves the throughput by over 40\% while the drop of accuracy is within 0.5\% for various vision transformers. Equipped with the dynamic token sparsification framework, DynamicViT models can achieve very competitive complexity/accuracy trade-offs compared to state-of-the-art CNNs and vision transformers on ImageNet. Code is available at https://github.com/raoyongming/DynamicViT},
  keywords = {Unread}
}

@inproceedings{rao_large_1948,
  title = {Large Sample Tests of Statistical Hypotheses Concerning Several Parameters with Applications to Problems of Estimation},
  booktitle = {Mathematical Proceedings of the Cambridge Philosophical Society},
  author = {Rao, C Radhakrishna},
  date = {1948},
  volume = {44},
  number = {1},
  pages = {50--57},
  publisher = {{Cambridge University Press}}
}

@article{rao_score_2005,
  title = {Score {{Test}}: {{Historical Review}} and {{Recent Developments}}},
  author = {family=Rao, given=CR, given-i=CR},
  date = {2005},
  journaltitle = {Advances in ranking and selection, multiple comparisons, and reliability: methodology and applications},
  pages = {3--20},
  publisher = {{Springer}}
}

@book{rasmussen_gaussian_2006,
  title = {Gaussian Processes for Machine Learning},
  author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  date = {2006},
  series = {Adaptive Computation and Machine Learning},
  publisher = {{MIT Press}},
  location = {{Cambridge, Mass}},
  isbn = {978-0-262-18253-9},
  langid = {english},
  pagetotal = {248},
  annotation = {OCLC: ocm61285753}
}

@inproceedings{ravanelli_dirhaenglish_2015,
  title = {The {{DIRHA-English}} Corpus and Related Tasks for Distant-Speech Recognition in Domestic Environments},
  booktitle = {{{IEEE Workshop}} on {{Automatic Speech Recognition}} and {{Understanding}}},
  author = {Ravanelli, Mirco and Cristoforetti, Luca and Gretter, Roberto and Pellin, Marco and Sosi, Alessandro and Omologo, Maurizio},
  date = {2015}
}

@online{ravanelli_learning_2018,
  title = {Learning Speaker Representations with Mutual Information},
  author = {Ravanelli, Mirco and Bengio, Yoshua},
  date = {2018},
  eprint = {1812.00271},
  eprinttype = {arxiv},
  pubstate = {preprint}
}

@inproceedings{ravanelli_multitask_2020,
  title = {Multi-{{Task Self-Supervised Learning}} for {{Robust Speech Recognition}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Ravanelli, Mirco and Zhong, Jianyuan and Pascual, Santiago and Swietojanski, Pawel and Monteiro, Joao and Trmal, Jan and Bengio, Yoshua},
  date = {2020-05},
  pages = {6989--6993},
  issn = {2379-190X},
  doi = {10.1109/ICASSP40776.2020.9053569},
  abstract = {Despite the growing interest in unsupervised learning, extracting meaningful knowledge from unlabelled audio remains an open challenge. To take a step in this direction, we recently proposed a problem-agnostic speech encoder (PASE), that combines a convolutional encoder followed by multiple neural networks, called workers, tasked to solve self-supervised problems (i.e., ones that do not require manual annotations as ground truth). PASE was shown to capture relevant speech information, including speaker voice-print and phonemes. This paper proposes PASE+, an improved version of PASE for robust speech recognition in noisy and reverberant environments. To this end, we employ an online speech distortion module, that contaminates the input signals with a variety of random disturbances. We then propose a revised encoder that better learns short- and long-term speech dynamics with an efficient combination of recurrent and convolutional networks. Finally, we refine the set of workers used in self-supervision to encourage better cooperation. Results on TIMIT, DIRHA and CHiME-5 show that PASE+ significantly outperforms both the previous version of PASE as well as common acoustic features. Interestingly, PASE+ learns transferable representations suitable for highly mismatched acoustic conditions.}
}

@inproceedings{ravi_deciphering_2011,
  title = {Deciphering Foreign Language},
  booktitle = {Proceedings of the 49th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}} ({{ACL}})},
  author = {Ravi, Sujith and Knight, Kevin},
  date = {2011-06},
  pages = {12--21},
  publisher = {{Association for Computational Linguistics}},
  location = {{Portland, Oregon, USA}},
  url = {https://aclanthology.org/P11-1002}
}

@article{razali_power_2011,
  title = {Power Comparisons of {{Shapiro-Wilk}}, {{Kolmogorov-Smirnov}}, {{Lilliefors}} and {{Anderson-Darling}} Tests},
  author = {Razali, Nornadiah Mohd and Wah, Yap Bee},
  date = {2011},
  journaltitle = {Journal ofStatistical Modeling and Analytics},
  volume = {2},
  number = {1},
  pages = {21--33}
}

@misc{razavi_generating_2019,
  title = {Generating {{Diverse High-Fidelity Images}} with {{VQ-VAE-2}}},
  author = {Razavi, Ali and family=Oord, given=Aäron, prefix=van den, useprefix=false and Vinyals, Oriol},
  date = {2019-06-02},
  eprint = {1906.00446},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1906.00446},
  urldate = {2020-09-29},
  abstract = {We explore the use of Vector Quantized Variational AutoEncoder (VQ-VAE) models for large scale image generation. To this end, we scale and enhance the autoregressive priors used in VQ-VAE to generate synthetic samples of much higher coherence and fidelity than possible before. We use simple feed-forward encoder and decoder networks, making our model an attractive candidate for applications where the encoding and/or decoding speed is critical. Additionally, VQ-VAE requires sampling an autoregressive model only in the compressed latent space, which is an order of magnitude faster than sampling in the pixel space, especially for large images. We demonstrate that a multi-scale hierarchical organization of VQ-VAE, augmented with powerful priors over the latent codes, is able to generate samples with quality that rivals that of state of the art Generative Adversarial Networks on multifaceted datasets such as ImageNet, while not suffering from GAN’s known shortcomings such as mode collapse and lack of diversity.},
  langid = {english}
}

@misc{razavi_preventing_2019,
  title = {Preventing {{Posterior Collapse}} with Delta-{{VAEs}}},
  author = {Razavi, Ali and family=Oord, given=Aäron, prefix=van den, useprefix=false and Poole, Ben and Vinyals, Oriol},
  date = {2019-01-10},
  eprint = {1901.03416},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1901.03416},
  urldate = {2021-01-18},
  abstract = {Due to the phenomenon of “posterior collapse,” current latent variable generative models pose a challenging design choice that either weakens the capacity of the decoder or requires augmenting the objective so it does not only maximize the likelihood of the data. In this paper, we propose an alternative that utilizes the most powerful generative models as decoders, whilst optimising the variational lower bound all while ensuring that the latent variables preserve and encode useful information. Our proposed δ-VAEs achieve this by constraining the variational family for the posterior to have a minimum distance to the prior. For sequential latent variable models, our approach resembles the classic representation learning approach of slow feature analysis. We demonstrate the efficacy of our approach at modeling text on LM1B and modeling images: learning representations, improving sample quality, and achieving state of the art log-likelihood on CIFAR-10 and ImageNet 32 × 32.},
  langid = {english}
}

@article{recht_valley_2012,
  title = {Beneath the Valley of the Noncommutative Arithmetic-Geometric Mean Inequality: Conjectures, Case-Studies, and Consequences},
  shorttitle = {Beneath the Valley of the Noncommutative Arithmetic-Geometric Mean Inequality},
  author = {Recht, Benjamin and Re, Christopher},
  date = {2012-02-19},
  url = {https://arxiv.org/abs/1202.4184v1},
  urldate = {2018-10-16},
  abstract = {Randomized algorithms that base iteration-level decisions on samples from some pool are ubiquitous in machine learning and optimization. Examples include stochastic gradient descent and randomized coordinate descent. This paper makes progress at theoretically evaluating the difference in performance between sampling with- and without-replacement in such algorithms. Focusing on least means squares optimization, we formulate a noncommutative arithmetic-geometric mean inequality that would prove that the expected convergence rate of without-replacement sampling is faster than that of with-replacement sampling. We demonstrate that this inequality holds for many classes of random matrices and for some pathological examples as well. We provide a deterministic worst-case bound on the gap between the discrepancy between the two sampling models, and explore some of the impediments to proving this inequality in full generality. We detail the consequences of this inequality for stochastic gradient descent and the randomized Kaczmarz algorithm for solving linear systems.},
  langid = {english}
}

@inproceedings{reddi_convergence_2018,
  title = {On the {{Convergence}} of {{Adam}} and {{Beyond}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Reddi, S. J. and Kale, S. and Kumar, S.},
  date = {2018},
  pages = {1--23},
  abstract = {Several recently proposed stochastic optimization methods that have been suc-cessfully used in training deep networks such as RMSPROP, ADAM, ADADELTA, NADAM, etc are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. It has been empirically observed that sometimes these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where ADAM does not converge to the optimal solution, and describe the precise problems with the previous analysis of ADAM algorithm. Our analysis suggests that the convergence issues may be fixed by endowing such algorithms with "long-term memory" of past gradients, and propose new variants of the ADAM algorithm which not only fix the convergence issues but often also lead to improved empirical performance.},
  keywords = {★}
}

@article{redmon_you_2015,
  title = {You {{Only Look Once}}: {{Unified}}, {{Real-Time Object Detection}}},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  date = {2015},
  journaltitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  eprint = {27295650},
  eprinttype = {pmid},
  pages = {779--788},
  issn = {01689002},
  doi = {10.1109/CVPR.2016.91},
  url = {http://arxiv.org/abs/1506.02640},
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
  isbn = {978-1-4673-8851-1}
}

@online{reiss_no_2023,
  title = {No {{Free Lunch}}: {{The Hazards}} of {{Over-Expressive Representations}} in {{Anomaly Detection}}},
  shorttitle = {No {{Free Lunch}}},
  author = {Reiss, Tal and Cohen, Niv and Hoshen, Yedid},
  date = {2023-06-12},
  eprint = {2306.07284},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2306.07284},
  urldate = {2023-06-13},
  abstract = {Anomaly detection methods, powered by deep learning, have recently been making significant progress, mostly due to improved representations. It is tempting to hypothesize that anomaly detection can improve indefinitely by increasing the scale of our networks, making their representations more expressive. In this paper, we provide theoretical and empirical evidence to the contrary. In fact, we empirically show cases where very expressive representations fail to detect even simple anomalies when evaluated beyond the well-studied object-centric datasets. To investigate this phenomenon, we begin by introducing a novel theoretical toy model for anomaly detection performance. The model uncovers a fundamental trade-off between representation sufficiency and over-expressivity. It provides evidence for a no-free-lunch theorem in anomaly detection stating that increasing representation expressivity will eventually result in performance degradation. Instead, guidance must be provided to focus the representation on the attributes relevant to the anomalies of interest. We conduct an extensive empirical investigation demonstrating that state-of-the-art representations often suffer from over-expressivity, failing to detect many types of anomalies. Our investigation demonstrates how this over-expressivity impairs image anomaly detection in practical settings. We conclude with future directions for mitigating this issue.},
  pubstate = {preprint}
}

@article{ren_faster_2017,
  title = {Faster {{R-CNN}}: {{Towards Real-Time Object Detection}} With},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  date = {2017},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {39},
  number = {6},
  pages = {1137--1149}
}

@inproceedings{ren_likelihood_2019,
  title = {Likelihood {{Ratios}} for {{Out-of-Distribution Detection}}},
  booktitle = {Proceedings of the 33rd {{International Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Ren, Jie and Liu, Peter J and Fertig, Emily and Snoek, Jasper and Poplin, Ryan and Depristo, Mark and Dillon, Joshua and Lakshminarayanan, Balaji},
  date = {2019},
  pages = {12},
  location = {{Vancouver, Canada}},
  url = {https://papers.nips.cc/paper/2019/file/1e79596878b2320cac26dd792a6c51c9-Paper.pdf},
  abstract = {Discriminative neural networks offer little or no performance guarantees when deployed on data not generated by the same process as the training distribution. On such out-of-distribution (OOD) inputs, the prediction may not only be erroneous, but confidently so, limiting the safe deployment of classifiers in real-world applications. One such challenging application is bacteria identification based on genomic sequences, which holds the promise of early detection of diseases, but requires a model that can output low confidence predictions on OOD genomic sequences from new bacteria that were not present in the training data. We introduce a genomics dataset for OOD detection that allows other researchers to benchmark progress on this important problem. We investigate deep generative model based approaches for OOD detection and observe that the likelihood score is heavily affected by population level background statistics. We propose a likelihood ratio method for deep generative models which effectively corrects for these confounding background statistics. We benchmark the OOD detection performance of the proposed method against existing approaches on the genomics dataset and show that our method achieves state-of-the-art performance. We demonstrate the generality of the proposed method by showing that it significantly improves OOD detection when applied to deep generative models of images.},
  eventtitle = {International {{Conference}} on {{Neural Information Processing Systems}}},
  langid = {english}
}

@inproceedings{renduchintala_multimodal_2018,
  title = {Multi-Modal Data Augmentation for End-to-End {{ASR}}},
  booktitle = {Proceedings of the {{Annual Conference}} of the {{International Speech Communication Association}} ({{Interspeech}})},
  author = {Renduchintala, Adithya and Ding, Shuoyang and Wiesner, Matthew and Watanabe, Shinji},
  editor = {Yegnanarayana, B.},
  date = {2018},
  pages = {2394--2398},
  publisher = {{ISCA}},
  location = {{Hyderabad, India}},
  doi = {10.21437/Interspeech.2018-2456},
  url = {https://doi.org/10.21437/Interspeech.2018-2456}
}

@unpublished{renggli_learning_2022,
  title = {Learning to {{Merge Tokens}} in {{Vision Transformers}}},
  author = {Renggli, Cedric and Pinto, André Susano and Houlsby, Neil and Mustafa, Basil and Puigcerver, Joan and Riquelme, Carlos},
  date = {2022-02-24},
  eprint = {2202.12015},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2202.12015},
  urldate = {2022-04-19},
  abstract = {Transformers are widely applied to solve natural language understanding and computer vision tasks. While scaling up these architectures leads to improved performance, it often comes at the expense of much higher computational costs. In order for large-scale models to remain practical in real-world systems, there is a need for reducing their computational overhead. In this work, we present the PatchMerger, a simple module that reduces the number of patches or tokens the network has to process by merging them between two consecutive intermediate layers. We show that the PatchMerger achieves a significant speedup across various model sizes while matching the original performance both upstream and downstream after fine-tuning.},
  keywords = {Medium,Unread}
}

@article{renshaw_comparison_2015,
  title = {A Comparison of Neural Network Methods for Unsupervised Representation Learning on the {{Zero Resource Speech Challenge}}},
  author = {Renshaw, Daniel and Kamper, Herman and Jansen, Aren and Goldwater, Sharon},
  date = {2015},
  journaltitle = {Annual Conference of the International Speech Communication Association}
}

@misc{rezende_normalizing_2020,
  title = {Normalizing {{Flows}} on {{Tori}} and {{Spheres}}},
  author = {Rezende, Danilo Jimenez and Papamakarios, George and Racanière, Sébastien and Albergo, Michael S. and Kanwar, Gurtej and Shanahan, Phiala E. and Cranmer, Kyle},
  date = {2020-02-06},
  eprint = {2002.02428},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2002.02428},
  urldate = {2020-02-11},
  abstract = {Normalizing flows are a powerful tool for building expressive distributions in high dimensions. So far, most of the literature has concentrated on learning flows on Euclidean spaces. Some problems however, such as those involving angles, are defined on spaces with more complex geometries, such as tori or spheres. In this paper, we propose and compare expressive and numerically stable flows on such spaces. Our flows are built recursively on the dimension of the space, starting from flows on circles, closed intervals or spheres.},
  langid = {english}
}

@inproceedings{rezende_stochastic_2014,
  title = {Stochastic {{Backpropagation}} and {{Approximate Inference}} in {{Deep Generative Models}}},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
  date = {2014-01-16},
  volume = {32},
  pages = {1278--1286},
  publisher = {{PMLR}},
  location = {{Beijing, China}},
  url = {http://proceedings.mlr.press/v32/rezende14.pdf},
  urldate = {2018-08-12},
  abstract = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic back-propagation -- rules for back-propagation through stochastic variables -- and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}}
}

@inproceedings{rezende_variational_2015,
  title = {Variational {{Inference}} with {{Normalizing Flows}}},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
  date = {2015},
  location = {{Lille, France}},
  url = {http://arxiv.org/abs/1505.05770},
  urldate = {2020-02-11},
  abstract = {The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english}
}

@inproceedings{ribeiro_why_2016,
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  shorttitle = {"{{Why Should I Trust You}}?},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  date = {2016-08-09},
  eprint = {1602.04938},
  eprinttype = {arxiv},
  pages = {1135--1144},
  publisher = {{ACM}},
  location = {{San Francisco, California, USA}},
  doi = {10.1145/2939672.2939778},
  url = {http://arxiv.org/abs/1602.04938},
  urldate = {2023-04-20},
  abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.}
}

@misc{riedmiller_learning_2018,
  title = {Learning by {{Playing}} - {{Solving Sparse Reward Tasks}} from {{Scratch}}},
  author = {Riedmiller, Martin and Hafner, Roland and Lampe, Thomas and Neunert, Michael and Degrave, Jonas and Van de Wiele, Tom and Mnih, Volodymyr and Heess, Nicolas and Springenberg, Jost Tobias},
  date = {2018},
  eprint = {1802.10567},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1802.10567},
  urldate = {2018-04-01},
  abstract = {We propose Scheduled Auxiliary Control (SAC-X), a new learning paradigm in the context of Reinforcement Learning (RL). SAC-X enables learning of complex behaviors - from scratch - in the presence of multiple sparse reward signals. To this end, the agent is equipped with a set of general auxiliary tasks, that it attempts to learn simultaneously via off-policy RL. The key idea behind our method is that active (learned) scheduling and execution of auxiliary policies allows the agent to efficiently explore its environment - enabling it to excel at sparse reward RL. Our experiments in several challenging robotic manipulation settings demonstrate the power of our approach.}
}

@online{rieger_bayesspeech_2023,
  title = {{{BayesSpeech}}: {{A Bayesian Transformer Network}} for {{Automatic Speech Recognition}}},
  shorttitle = {{{BayesSpeech}}},
  author = {Rieger, Will},
  date = {2023-01-16},
  eprint = {2301.11276},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2301.11276},
  urldate = {2023-09-09},
  abstract = {Recent developments using End-to-End Deep Learning models have been shown to have near or better performance than state of the art Recurrent Neural Networks (RNNs) on Automatic Speech Recognition tasks. These models tend to be lighter weight and require less training time than traditional RNN-based approaches. However, these models take frequentist approach to weight training. In theory, network weights are drawn from a latent, intractable probability distribution. We introduce BayesSpeech for end-to-end Automatic Speech Recognition. BayesSpeech is a Bayesian Transformer Network where these intractable posteriors are learned through variational inference and the local reparameterization trick without recurrence. We show how the introduction of variance in the weights leads to faster training time and near state-of-the-art performance on LibriSpeech-960.},
  pubstate = {preprint}
}

@article{rieser_natural_2009,
  title = {Natural {{Language Generation}} as {{Planning Under Uncertainty}} for {{Spoken Dialogue Systems}}},
  author = {Rieser, Verena and Lemon, Oliver},
  date = {2009},
  pages = {9},
  abstract = {We present and evaluate a new model for Natural Language Generation (NLG) in Spoken Dialogue Systems, based on statistical planning, given noisy feedback from the current generation context (e.g. a user and a surface realiser). We study its use in a standard NLG problem: how to present information (in this case a set of search results) to users, given the complex tradeoffs between utterance length, amount of information conveyed, and cognitive load. We set these trade-offs by analysing existing MATCH data. We then train a NLG policy using Reinforcement Learning (RL), which adapts its behaviour to noisy feedback from the current generation context. This policy is compared to several baselines derived from previous work in this area. The learned policy significantly outperforms all the prior approaches.},
  langid = {english}
}

@inproceedings{rifai_contractive_2011,
  title = {Contractive {{Auto-Encoders}}: {{Explicit Invariance}} during {{Feature Extraction}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{International Conference}} on {{Machine Learning}}},
  author = {Rifai, Salah and Vincent, Pascal and Muller, Xavier and Glorot, Xavier and Bengio, Yoshua},
  date = {2011},
  series = {{{ICML}}'11},
  pages = {833--840},
  publisher = {{Omnipress}},
  location = {{Madison, WI, USA}},
  abstract = {We present in this paper a novel approach for training deterministic auto-encoders. We show that by adding a well chosen penalty term to the classical reconstruction cost function, we can achieve results that equal or surpass those attained by other regularized auto-encoders as well as denoising auto-encoders on a range of datasets. This penalty term corresponds to the Frobenius norm of the Jacobian matrix of the encoder activations with respect to the input. We show that this penalty term results in a localized space contraction which in turn yields robust features on the activation layer. Furthermore, we show how this penalty term is related to both regularized auto-encoders and denoising auto-encoders and how it can be seen as a link between deterministic and non-deterministic auto-encoders. We find empirically that this penalty helps to carve a representation that better captures the local directions of variation dictated by the data, corresponding to a lower-dimensional non-linear manifold, while being more invariant to the vast majority of directions orthogonal to the manifold. Finally, we show that by using the learned features to initialize a MLP, we achieve state of the art classification error on a range of datasets, surpassing other methods of pretraining.},
  isbn = {978-1-4503-0619-5},
  venue = {Bellevue, Washington, USA}
}

@inproceedings{rifai_higher_2011,
  title = {Higher {{Order Contractive Auto-Encoder}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Rifai, Salah and Mesnil, Grégoire and Vincent, Pascal and Muller, Xavier and Bengio, Yoshua and Dauphin, Yann and Glorot, Xavier},
  editor = {Gunopulos, Dimitrios and Hofmann, Thomas and Malerba, Donato and Vazirgiannis, Michalis},
  date = {2011},
  pages = {645--660},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  abstract = {We propose a novel regularizer when training an auto-encoder for unsupervised feature extraction. We explicitly encourage the latent representation to contract the input space by regularizing the norm of the Jacobian (analytically) and the Hessian (stochastically) of the encoder's output with respect to its input, at the training points. While the penalty on the Jacobian's norm ensures robustness to tiny corruption of samples in the input space, constraining the norm of the Hessian extends this robustness when moving further away from the sample. From a manifold learning perspective, balancing this regularization with the auto-encoder's reconstruction objective yields a representation that varies most when moving along the data manifold in input space, and is most insensitive in directions orthogonal to the manifold. The second order regularization, using the Hessian, penalizes curvature, and thus favors smooth manifold. We show that our proposed technique, while remaining computationally efficient, yields representations that are significantly better suited for initializing deep architectures than previously proposed approaches, beating state-of-the-art performance on a number of datasets.},
  isbn = {978-3-642-23783-6}
}

@article{risi_neuroevolution_2017,
  title = {Neuroevolution in {{Games}}: {{State}} of the {{Art}} and {{Open Challenges}}},
  author = {Risi, Sebastian and Togelius, Julian},
  date = {2017},
  journaltitle = {IEEE Transactions on Computational Intelligence and AI in Games},
  volume = {9},
  number = {1},
  eprint = {1410.7326},
  eprinttype = {arxiv},
  issn = {1943068X},
  doi = {10.1109/TCIAIG.2015.2494596},
  url = {https://arxiv.org/abs/1410.7326},
  urldate = {2018-05-27},
  abstract = {This paper surveys research on applying neuroevolution (NE) to games. In neuroevolution, artificial neural networks are trained through evolutionary algorithms, taking inspiration from the way biological brains evolved. We analyse the application of NE in games along five different axes, which are the role NE is chosen to play in a game, the different types of neural networks used, the way these networks are evolved, how the fitness is determined and what type of input the network receives. The article also highlights important open research challenges in the field.}
}

@online{riviere_unsupervised_2020,
  title = {Unsupervised Pretraining Transfers Well across Languages},
  author = {Rivière, Morgane and Joulin, Armand and Mazaré, Pierre-Emmanuel and Dupoux, Emmanuel},
  date = {2020-02-07},
  eprint = {2002.02848},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2002.02848},
  urldate = {2021-10-29},
  abstract = {Cross-lingual and multi-lingual training of Automatic Speech Recognition (ASR) has been extensively investigated in the supervised setting. This assumes the existence of a parallel corpus of speech and orthographic transcriptions. Recently, contrastive predictive coding (CPC) algorithms have been proposed to pretrain ASR systems with unlabelled data. In this work, we investigate whether unsupervised pretraining transfers well across languages. We show that a slight modification of the CPC pretraining extracts features that transfer well to other languages, being on par or even outperforming supervised pretraining. This shows the potential of unsupervised methods for languages with few linguistic resources.},
  pubstate = {preprint}
}

@incollection{rizzoICDCodeRetrieval2015,
  title = {{{ICD Code Retrieval}}: {{Novel Approach}} for {{Assisted Disease Classification}}},
  shorttitle = {{{ICD Code Retrieval}}},
  booktitle = {Data {{Integration}} in the {{Life Sciences}}},
  author = {Rizzo, Stefano Giovanni and Montesi, Danilo and Fabbri, Andrea and Marchesini, Giulio},
  editor = {Ashish, Naveen and Ambite, Jose-Luis},
  date = {2015},
  volume = {9162},
  pages = {147--161},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-21843-4_12},
  abstract = {The task of assigning classification codes to short medical text is a hard text classification problem, especially when the set of possible codes is as big as the ICD-9-CM set. The problem, which has been only partially tamed for a subset of ICD-9-CM, becomes even harder in real world applications, where the labeled data are scarce and noisy.},
  isbn = {978-3-319-21842-7 978-3-319-21843-4},
  langid = {english}
}

@inproceedings{roberts_hierarchical_2018,
  title = {A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Roberts, Adam and Engel, Jesse H. and Raffel, Colin and Hawthorne, Curtis and Eck, Douglas},
  date = {2018},
  volume = {80},
  pages = {4361--4370},
  publisher = {{PMLR}},
  location = {{Stockholm, Sweden}},
  url = {http://proceedings.mlr.press/v80/roberts18a.html},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/icml/RobertsERHE18.bib},
  timestamp = {Mon, 22 Jul 2019 13:51:23 +0200}
}

@article{roberts_weak_1997,
  title = {Weak Convergence and Optimal Scaling of Random Walk {{Metropolis}} Algorithms},
  author = {Roberts, G. O. and Gelman, A. and Gilks, W. R.},
  date = {1997-02},
  journaltitle = {The Annals of Applied Probability},
  volume = {7},
  number = {1},
  pages = {110--120},
  doi = {10.1214/aoap/1034625254},
  url = {http://projecteuclid.org/euclid.aoap/1034625254},
  urldate = {2020-10-24},
  langid = {english}
}

@online{rochette_efficient_2019,
  title = {Efficient Per-Example Gradient Computations in Convolutional Neural Networks},
  author = {Rochette, G. and Manoel, A. and Tramel, E. W.},
  date = {2019},
  number = {1912.06015},
  eprint = {1912.06015},
  eprinttype = {arxiv},
  pubstate = {preprint}
}

@inproceedings{rodriguez-fuentes_gttsehu_2014,
  title = {{{GTTS-EHU}} Systems for {{QUESST}} at {{MediaEval}} 2014},
  booktitle = {{{MediaEval}}},
  author = {Rodríguez-Fuentes, Luis Javier and Varona, Amparo and Penagarikano, Mikel and Bordel, Germán and Diez, Mireia},
  date = {2014}
}

@misc{roeder_sticking_2017,
  title = {Sticking the {{Landing}}: {{Simple}}, {{Lower-Variance Gradient Estimators}} for {{Variational Inference}}},
  shorttitle = {Sticking the {{Landing}}},
  author = {Roeder, Geoffrey and Wu, Yuhuai and Duvenaud, David},
  date = {2017-05-28},
  eprint = {1703.09194},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1703.09194},
  urldate = {2022-02-10},
  abstract = {We propose a simple and general variant of the standard reparameterized gradient estimator for the variational evidence lower bound. Specifically, we remove a part of the total derivative with respect to the variational parameters that corresponds to the score function. Removing this term produces an unbiased gradient estimator whose variance approaches zero as the approximate posterior approaches the exact posterior. We analyze the behavior of this gradient estimator theoretically and empirically, and generalize it to more complex variational distributions such as mixtures and importance-weighted posteriors.}
}

@article{rogers_primer_2020,
  title = {A Primer in {{BERTology}}: {{What}} We Know about How {{BERT}} Works},
  author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
  date = {2020},
  journaltitle = {Transactions of the Association for Computational Linguistics (ACL)},
  volume = {8},
  pages = {842--866},
  doi = {10.1162/tacl_a_00349},
  url = {https://aclanthology.org/2020.tacl-1.54}
}

@incollection{rognvaldsson_brownian_1993,
  title = {Brownian {{Motion Updating}} of {{Multi-layered Perceptrons}}},
  booktitle = {{{ICANN}} ’93},
  author = {Rögnvaldsson, Thorsteinn S.},
  date = {1993},
  pages = {527--532},
  publisher = {{Springer London}},
  location = {{London}},
  doi = {10.1007/978-1-4471-2063-6_147},
  url = {http://link.springer.com/10.1007/978-1-4471-2063-6_147},
  urldate = {2018-03-25}
}

@misc{rolfe_discrete_2016,
  title = {Discrete {{Variational Autoencoders}}},
  author = {Rolfe, Jason Tyler},
  date = {2016-09-07},
  eprint = {1609.02200},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1609.02200},
  urldate = {2018-05-27},
  abstract = {Probabilistic models with discrete latent variables naturally capture datasets composed of discrete classes. However, they are difficult to train efficiently, since backpropagation through discrete variables is generally not possible. We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables. The associated class of probabilistic models comprises an undirected discrete component and a directed hierarchical continuous component. The discrete component captures the distribution over the disconnected smooth manifolds induced by the continuous component. As a result, this class of models efficiently learns both the class of objects in an image, and their specific realization in pixels, from unsupervised data, and outperforms state-of-the-art methods on the permutation-invariant MNIST, Omniglot, and Caltech-101 Silhouettes datasets.},
  isbn = {1511.02386}
}

@misc{roller_hash_2021,
  title = {Hash {{Layers For Large Sparse Models}}},
  author = {Roller, Stephen and Sukhbaatar, Sainbayar and Szlam, Arthur and Weston, Jason},
  date = {2021-06-08},
  eprint = {2106.04426},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2106.04426},
  urldate = {2021-06-15},
  abstract = {We investigate the training of sparse layers that use different parameters for different inputs based on hashing in large Transformer models. Specifically, we modify the feedforward layer to hash to different sets of weights depending on the current token, over all tokens in the sequence. We show that this procedure either outperforms or is competitive with learning-to-route mixture-of-expert methods such as Switch Transformers and BASE Layers, while requiring no routing parameters or extra terms in the objective function such as a load balancing loss, and no sophisticated assignment algorithm. We study the performance of different hashing techniques, hash sizes and input features, and show that balanced and random hashes focused on the most local features work best, compared to either learning clusters or using longer-range context. We show our approach works well both on large language modeling and dialogue tasks, and on downstream fine-tuning tasks.},
  langid = {english}
}

@online{romano_permutation_2020,
  title = {Permutation {{Testing}} for {{Dependence}} in {{Time Series}}},
  author = {Romano, Joseph P. and Tirlea, Marius A.},
  date = {2020-09-07},
  eprint = {2009.03170},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2009.03170},
  urldate = {2023-04-05},
  abstract = {Given observations from a stationary time series, permutation tests allow one to construct exactly level \$\textbackslash alpha\$ tests under the null hypothesis of an i.i.d. (or, more generally, exchangeable) distribution. On the other hand, when the null hypothesis of interest is that the underlying process is an uncorrelated sequence, permutation tests are not necessarily level \$\textbackslash alpha\$, nor are they approximately level \$\textbackslash alpha\$ in large samples. In addition, permutation tests may have large Type 3, or directional, errors, in which a two-sided test rejects the null hypothesis and concludes that the first-order autocorrelation is larger than 0, when in fact it is less than 0. In this paper, under weak assumptions on the mixing coefficients and moments of the sequence, we provide a test procedure for which the asymptotic validity of the permutation test holds, while retaining the exact rejection probability \$\textbackslash alpha\$ in finite samples when the observations are independent and identically distributed. A Monte Carlo simulation study, comparing the permutation test to other tests of autocorrelation, is also performed, along with an empirical example of application to financial data.},
  pubstate = {preprint}
}

@inproceedings{ros_simple_2008,
  title = {A Simple Modification in {{CMA-ES}} Achieving Linear Time and Space Complexity},
  booktitle = {Lecture {{Notes}} in {{Computer Science}} (Including Subseries {{Lecture Notes}} in {{Artificial Intelligence}} and {{Lecture Notes}} in {{Bioinformatics}})},
  author = {Ros, Raymond and Hansen, Nikolaus},
  date = {2008},
  volume = {5199 LNCS},
  pages = {296--305},
  issn = {03029743},
  doi = {10.1007/978-3-540-87700-4_30},
  url = {http://link.springer.com/10.1007/978-3-540-87700-4_30},
  urldate = {2018-02-24},
  abstract = {This paper proposes a simple modification of the Covariance Matrix\textbackslash nAdaptation Evolution Strategy (CMA-ES) for high dimensional objective\textbackslash nfunctions, reducing the internal time and space complexity from quadratic\textbackslash nto linear. The covariance matrix is constrained to be diagonal and\textbackslash nthe resulting algorithm, sep-CMA-ES, samples each coordinate independently.\textbackslash nBecause the model complexity is reduced, the learning rate for the\textbackslash ncovariance matrix can be increased. Consequently, on essentially\textbackslash nseparable functions, sep-CMA-ES significantly outperforms CMA-ES\textbackslash n. For dimensions larger than a hundred, even on the non-separable\textbackslash nRosenbrock function, the sep-CMA-ES needs fewer function evaluations\textbackslash nthan CMA-ES .},
  isbn = {3-540-87699-5}
}

@misc{rosca_distribution_2018,
  title = {Distribution {{Matching}} in {{Variational Inference}}},
  author = {Rosca, Mihaela and Lakshminarayanan, Balaji and Mohamed, Shakir},
  date = {2018-02-19},
  eprint = {1802.06847},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1802.06847},
  urldate = {2020-01-09},
  abstract = {With the increasingly widespread deployment of generative models, there is a mounting need for a deeper understanding of their behaviors and limitations. In this paper, we expose the limitations of Variational Autoencoders (VAEs), which consistently fail to learn marginal distributions in both latent and visible spaces. We show this to be a consequence of learning by matching conditional distributions, and the limitations of explicit model and posterior distributions. It is popular to consider Generative Adversarial Networks (GANs) as a means of overcoming these limitations, leading to hybrids of VAEs and GANs. We perform a large-scale evaluation of several VAE-GAN hybrids and analyze the implications of class probability estimation for learning distributions. While promising, we conclude that at present, VAE-GAN hybrids have limited applicability: they are harder to scale, evaluate, and use for inference compared to VAEs; and they do not improve over the generation quality of GANs.},
  langid = {english}
}

@article{rosca_measurevalued_,
  title = {Measure-{{Valued Derivatives}} for {{Approximate Bayesian Inference}}},
  author = {Rosca, Mihaela and Figurnov, Michael and Mohamed, Shakir and Mnih, Andriy},
  pages = {7},
  langid = {english}
}

@inproceedings{rosenberg_speech_2019,
  title = {Speech Recognition with Augmented Synthesized Speech},
  booktitle = {{{IEEE Workshop}} on {{Automatic Speech Recognition}} and {{Understanding}} ({{ASRU}})},
  author = {Rosenberg, Andrew and Zhang, Yu and Ramabhadran, Bhuvana and Jia, Ye and Moreno, Pedro and Wu, Yonghui and Wu, Zelin},
  date = {2019},
  pages = {996--1002}
}

@article{rosenblatt_analytic_1962,
  title = {Analytic {{Techniques}} for the {{Study}} of {{Neural Nets}}},
  author = {Rosenblatt, Frank},
  date = {1962},
  journaltitle = {Proceedings of AIEE Joint Automatic Control Conference},
  volume = {401},
  pages = {285--292}
}

@report{rosenblatt_perceptron_1957,
  title = {The {{Perceptron}} - {{A Perceiving}} and {{Recognizing Automaton}}},
  author = {Rosenblatt, Frank},
  date = {1957},
  pages = {Report 85-460-1},
  institution = {{Cornell Aeronautical Laboratory}},
  doi = {85-460-1}
}

@article{rosenblatt_perceptron_1958,
  title = {The Perceptron: {{A}} Probabilistic Model for Information Storage and Organization in the Brain.},
  author = {Rosenblatt, F.},
  date = {1958},
  journaltitle = {Psychological Review},
  volume = {65},
  pages = {386--408},
  publisher = {{American Psychological Association}},
  location = {{US}},
  doi = {10.1037/h0042519},
  abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {*Brain,*Cognition,*Memory,Nervous System}
}

@inproceedings{rouditchenko_avlnet_2021,
  title = {{{AVLnet}}: {{Learning}} Audio-Visual Language Representations from Instructional Videos},
  booktitle = {Annual {{Conference}} of the {{International Speech Communication Association}}},
  author = {Rouditchenko, Andrew and Boggust, Angie and Harwath, David and Chen, Brian and Joshi, Dhiraj and Thomas, Samuel and Audhkhasi, Kartik and Kuehne, Hilde and Panda, Rameswar and Feris, Rogerio and others},
  date = {2021}
}

@inproceedings{rousseau_tedlium_2012,
  title = {{{TED-LIUM}}: {{An}} Automatic Speech Recognition Dedicated Corpus},
  booktitle = {Proceedings of the {{International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}})},
  author = {Rousseau, Anthony and Deléglise, Paul and Esteve, Yannick},
  date = {2012},
  pages = {125--129},
  eventtitle = {International {{Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}})}
}

@misc{roy_does_2021,
  title = {Does {{Your Dermatology Classifier Know What It Doesn}}'t {{Know}}? {{Detecting}} the {{Long-Tail}} of {{Unseen Conditions}}},
  shorttitle = {Does {{Your Dermatology Classifier Know What It Doesn}}'t {{Know}}?},
  author = {Roy, Abhijit Guha and Ren, Jie and Azizi, Shekoofeh and Loh, Aaron and Natarajan, Vivek and Mustafa, Basil and Pawlowski, Nick and Freyberg, Jan and Liu, Yuan and Beaver, Zach and Vo, Nam and Bui, Peggy and Winter, Samantha and MacWilliams, Patricia and Corrado, Greg S. and Telang, Umesh and Liu, Yun and Cemgil, Taylan and Karthikesalingam, Alan and Lakshminarayanan, Balaji and Winkens, Jim},
  date = {2021-04-08},
  eprint = {2104.03829},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2104.03829},
  urldate = {2021-05-07},
  abstract = {Supervised deep learning models have proven to be highly effective in classification of dermatological conditions. These models rely on the availability of abundant labeled training examples. However, in the real-world, many dermatological conditions are individually too infrequent for per-condition classification with supervised learning. Although individually infrequent, these conditions may collectively be common and therefore are clinically significant in aggregate. To prevent models from generating erroneous outputs on such examples, there remains a considerable unmet need for deep learning systems that can better detect such infrequent conditions. These infrequent ‘outlier’ conditions are seen very rarely (or not at all) during training. In this paper, we frame this task as an out-of-distribution (OOD) detection problem. We set up a benchmark ensuring that outlier conditions are disjoint between the model training, validation, and test sets. Unlike traditional OOD detection benchmarks where the task is to detect dataset distribution shift, we aim at the more challenging task of detecting subtle semantic differences. We propose a novel hierarchical outlier detection (HOD) loss, which assigns multiple abstention classes corresponding to each training outlier class and jointly performs a coarse classification of inliers vs. outliers, along with fine-grained classification of the individual classes. We demonstrate that the proposed HOD loss based approach outperforms leading methods that leverage outlier data during training. Further, performance is significantly boosted by using recent representation learning methods (BiT, SimCLR, MICLe). Further, we explore ensembling strategies for OOD detection and propose a diverse ensemble selection process for the best result. We also perform a subgroup analysis over conditions of varying risk levels and different skin types to investigate how OOD performance changes over each subgroup and demonstrate the gains of our framework in comparison to baseline. Furthermore, we go beyond traditional performance metrics and introduce a cost matrix for model trust analysis to approximate downstream clinical impact. We use this cost matrix to compare the proposed method against the baseline, thereby making a stronger case for its effectiveness in real-world scenarios.},
  langid = {english}
}

@thesis{roy_learning_1999,
  type = {phdthesis},
  title = {Learning from Sights and Sounds: {{A}} Computational Model},
  author = {Roy, Deb},
  date = {1999},
  location = {{MIT Media Laboratory, USA}}
}

@misc{ruan_improving_2021,
  title = {Improving {{Lossless Compression Rates}} via {{Monte Carlo Bits-Back Coding}}},
  author = {Ruan, Yangjun and Ullrich, Karen and Severo, Daniel and Townsend, James and Khisti, Ashish and Doucet, Arnaud and Makhzani, Alireza and Maddison, Chris J.},
  date = {2021-02-22},
  eprint = {2102.11086},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2102.11086},
  urldate = {2021-05-18},
  abstract = {Latent variable models have been successfully applied in lossless compression with the bits-back coding algorithm. However, bits-back suffers from an increase in the bitrate equal to the KL divergence between the approximate posterior and the true posterior. In this paper, we show how to remove this gap asymptotically by deriving bits-back coding algorithms from tighter variational bounds. The key idea is to exploit extended space representations of Monte Carlo estimators of the marginal likelihood. Naively applied, our schemes would require more initial bits than the standard bits-back coder, but we show how to drastically reduce this additional cost with couplings in the latent space. When parallel architectures can be exploited, our coders can achieve better rates than bits-back with little additional cost. We demonstrate improved lossless compression rates in a variety of settings, including entropy coding for lossy compression.},
  langid = {english}
}

@online{ruff_deep_2020,
  title = {Deep {{Semi-Supervised Anomaly Detection}}},
  author = {Ruff, Lukas and Vandermeulen, Robert A. and Görnitz, Nico and Binder, Alexander and Müller, Emmanuel and Müller, Klaus-Robert and Kloft, Marius},
  date = {2020-02-14},
  eprint = {1906.02694},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1906.02694},
  urldate = {2023-09-12},
  abstract = {Deep approaches to anomaly detection have recently shown promising results over shallow methods on large and complex datasets. Typically anomaly detection is treated as an unsupervised learning problem. In practice however, one may have---in addition to a large set of unlabeled samples---access to a small pool of labeled samples, e.g. a subset verified by some domain expert as being normal or anomalous. Semi-supervised approaches to anomaly detection aim to utilize such labeled samples, but most proposed methods are limited to merely including labeled normal samples. Only a few methods take advantage of labeled anomalies, with existing deep approaches being domain-specific. In this work we present Deep SAD, an end-to-end deep methodology for general semi-supervised anomaly detection. We further introduce an information-theoretic framework for deep anomaly detection based on the idea that the entropy of the latent distribution for normal data should be lower than the entropy of the anomalous distribution, which can serve as a theoretical interpretation for our method. In extensive experiments on MNIST, Fashion-MNIST, and CIFAR-10, along with other anomaly detection benchmark datasets, we demonstrate that our method is on par or outperforms shallow, hybrid, and deep competitors, yielding appreciable performance improvements even when provided with only little labeled data.},
  pubstate = {preprint}
}

@article{rumelhart_learning_1986,
  title = {Learning Representations by Back-Propagating Errors},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  date = {1986},
  journaltitle = {Nature},
  volume = {323},
  pages = {533--536},
  abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure}
}

@incollection{rumelhart_learning_2013,
  title = {Learning {{Internal Representations}} by {{Error Propagation}}},
  booktitle = {Readings in {{Cognitive Science}}: {{A Perspective}} from {{Psychology}} and {{Artificial Intelligence}}},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  date = {2013},
  eprint = {25246403},
  eprinttype = {pmid},
  pages = {399--421},
  issn = {1-55860-013-2},
  doi = {10.1016/B978-1-4832-1446-7.50035-2},
  abstract = {This paper presents a generalization of the perception learning procedure for learning the correct sets of connections for arbitrary networks. The rule, falled the generalized delta rule, is a simple scheme for implementing a gradient descent method for finding weights that minimize the sum squared error of the sytem's performance. The major theoretical contribution of the work is the procedure called error propagation, whereby the gradient can be determined by individual units of the network based only on locally available information. The major empirical contribution of the work is to show that the problem of local minima not serious in this application of gradient descent.},
  isbn = {1-55860-013-2}
}

@inproceedings{sabour_dynamic_2017,
  title = {Dynamic {{Routing Between Capsules}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Neural Information Processing Systems}} ({{NIPS}})},
  author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E.},
  date = {2017-10-26},
  eprint = {1710.09829},
  eprinttype = {arxiv},
  location = {{Long Beach, CA, USA}},
  url = {http://arxiv.org/abs/1710.09829},
  urldate = {2017-11-01},
  abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
  keywords = {★}
}

@misc{sadhu_wav2vecc_2021,
  title = {Wav2vec-{{C}}: {{A Self-supervised Model}} for {{Speech Representation Learning}}.},
  shorttitle = {Wav2vec-{{C}}},
  author = {Sadhu, Samik and He, Di and Huang, Che-Wei and Mallidi, Sri Harish and Wu, Minhua and Rastrow, Ariya and Stolcke, Andreas and Droppo, Jasha and Maas, Roland},
  date = {2021-06-23},
  eprint = {2103.08393},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2103.08393},
  urldate = {2022-02-01},
  abstract = {Wav2vec-C introduces a novel representation learning technique combining elements from wav2vec 2.0 and VQ-VAE. Our model learns to reproduce quantized representations from partially masked speech encoding using a contrastive loss in a way similar to Wav2vec 2.0. However, the quantization process is regularized by an additional consistency network that learns to reconstruct the input features to the wav2vec 2.0 network from the quantized representations in a way similar to a VQ-VAE model. The proposed self-supervised model is trained on 10k hours of unlabeled data and subsequently used as the speech encoder in a RNN-T ASR model and fine-tuned with 1k hours of labeled data. This work is one of only a few studies of self-supervised learning on speech tasks with a large volume of real far-field labeled data. The Wav2vec-C encoded representations achieves, on average, twice the error reduction over baseline and a higher codebook utilization in comparison to wav2vec 2.0}
}

@inproceedings{sajjadi_enhancenet_2017,
  title = {{{EnhanceNet}}: {{Single Image Super-Resolution Through Automated Texture Synthesis}}},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Sajjadi, Mehdi S.M. and Scholkopf, Bernhard and Hirsch, Michael},
  date = {2017},
  eprint = {1612.07919},
  eprinttype = {arxiv},
  pages = {4501--4510},
  issn = {15505499},
  doi = {10.1109/ICCV.2017.481},
  abstract = {Single image super-resolution is the task of inferring a high-resolution image from a single low-resolution input. Traditionally, the performance of algorithms for this task is measured using pixel-wise reconstruction measures such as peak signal-to-noise ratio (PSNR) which have been shown to correlate poorly with the human perception of image quality. As a result, algorithms minimizing these metrics tend to produce over-smoothed images that lack high-frequency textures and do not look natural despite yielding high PSNR values. We propose a novel application of automated texture synthesis in combination with a perceptual loss focusing on creating realistic textures rather than optimizing for a pixel-accurate reproduction of ground truth images during training. By using feed-forward fully convolutional neural networks in an adversarial training setting, we achieve a significant boost in image quality at high magnification ratios. Extensive experiments on a number of datasets show the effectiveness of our approach, yielding state-of-the-art results in both quantitative and qualitative benchmarks.},
  isbn = {978-1-5386-1032-9}
}

@inproceedings{sakurada_anomaly_2014,
  title = {Anomaly Detection Using Autoencoders with Nonlinear Dimensionality Reduction},
  booktitle = {Proceedings of the {{MLSDA}} 2014 2nd {{Workshop}} on {{Machine Learning}} for {{Sensory Data Analysis}} ({{MLSDA}})},
  author = {Sakurada, Mayu and Yairi, Takehisa},
  date = {2014},
  pages = {4--11},
  location = {{Gold Coast, QLD, Austrailia}}
}

@inproceedings{salakhutdinov_efficient_2010,
  title = {Efficient Learning of Deep {{Boltzmann}} Machines},
  booktitle = {Proceedings of the {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}} ({{AISTATS}})},
  author = {Salakhutdinov, Ruslan and Larochelle, Hugo},
  date = {2010},
  pages = {693--700},
  publisher = {{JMLR Workshop and Conference Proceedings}}
}

@inproceedings{salakhutdinov_restricted_2007,
  title = {Restricted {{Boltzmann Machines}} for {{Collaborative Filtering}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Salakhutdinov, Ruslan and Mnih, Andriy and Hinton, Geoffrey E.},
  date = {2007},
  volume = {227},
  eprint = {19932002},
  eprinttype = {pmid},
  pages = {791--798},
  issn = {1468-1218},
  doi = {10.1145/1273496.1273596},
  url = {http://dx.doi.org/10.1145/1273496.1273596},
  abstract = {Most of the existing approaches to collaborative filtering cannot handle very large data sets. In this paper we show how a class of two-layer undirected graphical models, called Restricted Boltzmann Machines (RBM's), can be used to model tabular data, such as user's ratings of movies. We present efficient learning and inference procedures for this class of models and demonstrate that RBM's can be successfully applied to the Netflix data set, containing over 100 million user/movie ratings. We also show that RBM's slightly outperform carefully-tuned SVD models. When the predictions of multiple RBM models and multiple SVD models are linearly combined, we achieve an error rate that is well over 6\% better than the score of Netflix's own system.},
  isbn = {978-1-59593-793-3}
}

@misc{salimans_evolution_2017,
  title = {Evolution {{Strategies}} as a {{Scalable Alternative}} to {{Reinforcement Learning}}},
  author = {Salimans, Tim and Ho, Jonathan and Chen, Xi and Sidor, Szymon and Sutskever, Ilya},
  date = {2017},
  eprint = {1703.03864},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1703.03864},
  urldate = {2017-10-02},
  abstract = {We explore the use of Evolution Strategies (ES), a class of black box optimization algorithms, as an alternative to popular MDP-based RL techniques such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using a novel communication strategy based on common random numbers, our ES implementation only needs to communicate scalars, making it possible to scale to over a thousand parallel workers. This allows us to solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.}
}

@misc{salimans_improved_2016,
  title = {Improved {{Techniques}} for {{Training GANs}}},
  author = {Salimans, Tim and Goodfellow, Ian J. and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
  date = {2016-06-10},
  eprint = {23259955},
  eprinttype = {pmid},
  url = {http://arxiv.org/abs/1606.03498},
  urldate = {2018-05-22},
  abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3\%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.},
  isbn = {0924-6495}
}

@inproceedings{salimans_pixelcnn_2017,
  title = {{{PixelCNN}}++: {{Improving}} the {{PixelCNN}} with {{Discretized Logistic Mixture Likelihood}} and {{Other Modifications}}},
  shorttitle = {{{PixelCNN}}++},
  booktitle = {Proceedings of the 5th {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Salimans, Tim and Karpathy, Andrej and Chen, Xi and Kingma, Diederik P.},
  date = {2017-04},
  location = {{Toulon, France}},
  url = {http://arxiv.org/abs/1701.05517},
  urldate = {2021-01-19},
  abstract = {PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available at https://github.com/openai/pixel-cnn. Our implementation contains a number of modifications to the original model that both simplify its structure and improve its performance. 1) We use a discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which we find to speed up training. 2) We condition on whole pixels, rather than R/G/B sub-pixels, simplifying the model structure. 3) We use downsampling to efficiently capture structure at multiple resolutions. 4) We introduce additional short-cut connections to further speed up optimization. 5) We regularize the model using dropout. Finally, we present state-of-the-art log likelihood results on CIFAR-10 to demonstrate the usefulness of these modifications.},
  langid = {english}
}

@inproceedings{salimans_weight_2016,
  title = {Weight {{Normalization}}: {{A Simple Reparameterization}} to {{Accelerate Training}} of {{Deep Neural Networks}}},
  booktitle = {Proceedings of the 30th {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Salimans, Tim and Kingma, Diederik P.},
  date = {2016-02-25},
  location = {{Barcelona, Spain}},
  url = {http://arxiv.org/abs/1602.07868},
  urldate = {2017-11-27},
  abstract = {We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.}
}

@article{samuel_studies_1959,
  title = {Some {{Studies}} in {{Machine Learning}} Using the {{Game}} of {{Checkers}}},
  author = {Samuel, Artur L},
  date = {1959},
  journaltitle = {IBM Journal of research and development},
  volume = {3},
  number = {3},
  pages = {210--229},
  doi = {10.1147/rd.33.0210},
  abstract = {Two machine-learning procedures have been investigated 1 in some detail using the game of checkers. Enough work has been done to verify the fact that a computer can be programmed so that it will learn to play a better game of checkers than can be played by the person who wrote the program. Further- more, it can learn to do this in a remarkably short period of time (8 or 10 hours of machine-playing time) when given only the rules of the game, a sense of direction, and a redundant and incomplete list of parameters which are thought to have something to do with the game, but whose correct signs and relative weights are unknown and unspecified. The principles of machine learning verified by these experiments are, of course, applicable to many other situations.}
}

@inproceedings{sanabria_talk_2021,
  title = {Talk, Don't Write: {{A}} Study of Direct Speech-Based Image Retrieval},
  booktitle = {Annual {{Conference}} of the {{International Speech Communication Association}}},
  author = {Sanabria, Ramon and Waters, Austin and Baldridge, Jason},
  date = {2021}
}

@article{sanchez_image_2013,
  title = {Image Classification with the Fisher Vector: {{Theory}} and Practice},
  author = {Sánchez, Jorge and Perronnin, Florent and Mensink, Thomas and Verbeek, Jakob},
  date = {2013},
  journaltitle = {International journal of computer vision},
  volume = {105},
  pages = {222--245},
  publisher = {{Springer}}
}

@inproceedings{saul_exploiting_1996,
  title = {Exploiting Tractable Substructures in Intractable Networks},
  booktitle = {Advances in {{Neural Processing Systems}}},
  author = {Saul, Lawrence and Jordan, Michael},
  editor = {Touretzky, D. and Mozer, M. C. and Hasselmo, M.},
  date = {1996},
  volume = {8},
  publisher = {{MIT Press}},
  url = {https://proceedings.neurips.cc/paper/1995/file/285f89b802bcb2651801455c86d78f2a-Paper.pdf}
}

@misc{savinov_stepunrolled_2021,
  title = {Step-Unrolled {{Denoising Autoencoders}} for {{Text Generation}}},
  author = {Savinov, Nikolay and Chung, Junyoung and Binkowski, Mikolaj and Elsen, Erich and family=Oord, given=Aaron, prefix=van den, useprefix=false},
  date = {2021-12-13},
  eprint = {2112.06749},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2112.06749},
  urldate = {2022-01-17},
  abstract = {In this paper we propose a new generative model of text, Step-unrolled Denoising Autoencoder (SUNDAE), that does not rely on autoregressive models. Similarly to denoising diffusion techniques, SUNDAE is repeatedly applied on a sequence of tokens, starting from random inputs and improving them each time until convergence. We present a simple new improvement operator that converges in fewer iterations than diffusion methods, while qualitatively producing better samples on natural language datasets. SUNDAE achieves state-of-the-art results (among non-autoregressive methods) on the WMT’14 English-to-German translation task and good qualitative results on unconditional language modeling on the Colossal Cleaned Common Crawl dataset and a dataset of Python code from GitHub. The non-autoregressive nature of SUNDAE opens up possibilities beyond left-to-right prompted generation, by filling in arbitrary blank patterns in a template.},
  langid = {english}
}

@article{saxe_exact_2013,
  title = {Exact Solutions to the Nonlinear Dynamics of Learning in Deep Linear Neural Networks},
  author = {Saxe, Andrew M and McClelland, James L. and Ganguli, Surya},
  date = {2013},
  journaltitle = {CoRR},
  volume = {abs/1312.6},
  eprint = {1312.6120},
  eprinttype = {arxiv},
  pages = {1--22},
  url = {http://arxiv.org/abs/1312.6120},
  urldate = {2018-06-15},
  abstract = {Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos.},
  isbn = {1312.6120}
}

@inproceedings{saxena_clockwork_2021,
  title = {Clockwork {{Variational Autoencoders}}},
  booktitle = {Proceedings of the 34th {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Saxena, Vaibhav and Ba, Jimmy and Hafner, Danijar},
  date = {2021-02-20},
  eprint = {2102.09532},
  eprinttype = {arxiv},
  location = {{Virtual}},
  url = {http://arxiv.org/abs/2102.09532},
  urldate = {2021-03-05},
  abstract = {Deep learning has enabled algorithms to generate realistic images. However, accurately predicting long video sequences requires understanding longterm dependencies and remains an open challenge. While existing video prediction models succeed at generating sharp images, they tend to fail at accurately predicting far into the future. We introduce the Clockwork VAE (CW-VAE), a video prediction model that leverages a hierarchy of latent sequences, where higher levels tick at slower intervals. We demonstrate the benefits of both hierarchical latents and temporal abstraction on 4 diverse video prediction datasets with sequences of up to 1000 frames, where CWVAE outperforms top video prediction models. Additionally, we propose a Minecraft benchmark for long-term video prediction. We conduct several experiments to gain insights into CW-VAE and confirm that slower levels learn to represent objects that change more slowly in the video, and faster levels learn to represent faster objects.},
  eventtitle = {Conference on {{Neural Information Processing Systems}}},
  langid = {english}
}

@inproceedings{scharenborg_linguistic_2018,
  title = {Linguistic Unit Discovery from Multi-Modal Inputs in Unwritten Languages: {{Summary}} of the “{{Speaking Rosetta}}” {{JSALT}} 2017 {{Workshop}}},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Scharenborg, Odette and others},
  date = {2018}
}

@article{schatz_evaluating_2013,
  title = {Evaluating Speech Features with the Minimal-Pair {{ABX}} Task: Analysis of the Classical {{MFC}}/{{PLP}} Pipeline},
  author = {Schatz, Thomas and Peddinti, Vijayaditya and Bach, Francis and Jansen, Aren and Hermansky, Hynek and Dupoux, Emmanuel},
  date = {2013},
  journaltitle = {Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech)}
}

@article{schatz_evaluating_2014,
  title = {Evaluating Speech Features with the Minimal-Pair {{ABX}} Task ({{II}}): Resistance to Noise},
  author = {Schatz, Thomas and Peddinti, Vijayaditya and Cao, Xuan-Nga and Bach, Francis and Hermansky, Hynek and Dupoux, Emmanuel},
  date = {2014},
  journaltitle = {Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech)}
}

@inproceedings{schaul_high_2011,
  title = {High {{Dimensions}} and {{Heavy Tails}} for {{Natural Evolution Strategies}}},
  booktitle = {Proceedings of the {{Conference}} on {{Genetic}} and {{Evolutionary Computation}} ({{GECCO}})},
  author = {Schaul, Tom and Glasmachers, Tobias and Schmidhuber, Jürgen},
  date = {2011},
  doi = {10.1145/2001576.2001692},
  abstract = {The family of natural evolution strategies (NES) offers a principled approach to real-valued evolutionary optimization. NES follows the natural gradient of the expected fitness on the parameters of its search distribution. While general in its formulation, previous research has focused on multivariate Gaussian search distributions. Here we exhibit problem classes for which other search distributions are more appropriate, and then derive corresponding NES-variants. First, for separable distributions we obtain SNES, whose complexity is only O(d) instead of O(d(3)). We apply SNES to problems of previously unattainable dimensionality, recovering lowest-energy structures on the Lennard-Jones atom clusters, and obtaining state-of-the-art results on neuro-evolution benchmarks. Second, we develop a new, equivalent formulation based on invariances. This allows for generalizing NES to heavy-tailed distributions, even those with undefined variance, which aids in overcoming deceptive local optima.},
  isbn = {978-1-4503-0557-0}
}

@article{schaul_investigating_2012,
  title = {Investigating the {{Impact}} of {{Adaptation Sampling}} in {{Natural Evolution Strategies}} on {{Black-box Optimization Testbeds}}},
  author = {Schaul, Tom},
  date = {2012},
  journaltitle = {Proceedings of the Conference on Genetic and Evolutionary Computation (GECCO)},
  pages = {221},
  doi = {10.1145/2330784.2330817},
  abstract = {Natural Evolution Strategies (NES) are a recent member of the class of real-valued optimization algorithms that are based on adapting search distributions. Exponential NES (xNES) are the most common instantiation of NES, and particularly appropriate for the BBOB 2012 benchmarks, given that many are non-separable, and their relatively small problem dimensions. The technique of adaptation sampling, which adapts learning rates online further improves the al-gorithm's performance. This report provides an extensive empirical comparison to study the impact of adaptation sampling in xNES, both on the noise-free and noisy BBOB testbeds.},
  isbn = {9781450311786}
}

@thesis{schaul_studies_2011,
  type = {phdthesis},
  title = {Studies in {{Continuous Black-box Optimization}}},
  author = {Schaul, Tom and Brügge, B.},
  date = {2011},
  institution = {{Technical University of Munich}},
  abstract = {Optimization is the research field that studies that studies the design of algorithms for finding the best solutions to problems we humans throw at them. While the whole domain is of important practical utility, the present thesis will focus on the subfield of continuous black-box optimization, presenting a collection of novel, state-of-the-art algorithms for solving problems in that class. First, we introduce a general-purpose algorithm called Natural Evolution Strategies (NES). In contrast to typical evolutionary algorithms which search in the vicinity of the fittest individuals in a population, evolution strategies aim at repeating the type of mutations that led to those individuals. We can characterize those mutations by a search distribution. The key idea of NES is to ascend the gradient on the parameters of that distribution towards higher expected fitness. We show how plain gradient ascent is destined to fail, and provide a viable alternative that instead descends along the natural gradient to adapt the search distribution, which appropriately normalizes the update step with respect to its uncertainty. Being derived from first principles, the NES approach can be extended to all types of search distributions that allow a parametric form, not just the classical multivariate Gaussian one. We derive a number of NES variants for different distributions, and show how they are useful on different problem classes. In addition, we rein in the computational cost, avoiding costly matrix inversions through an incremental change of coordinates. Two additional, novel techniques, importance mixing and adaptation sampling, allow us to automatically tune the learning rate and batch size to the problem, and thereby further reduce the average number of required fitness evaluations. A third technique, restart strategies, provides the algorithm with additional robustness in the presence of multiple local optima, or noise. Second, we introduce a new approach to costly black-box optimization, when fitness evaluations are very expensive. Here, we model the fitness function using state-of-the-art Gaussian process regression, and use the principle of artificial curiosity to direct exploration towards the most informative next evaluation candidate. Both the expected fitness improvement and the expected information gain can be derived explicitly from the Gaussian process model, and our method constructs a front of Pareto-optimal points according to these two criteria. This makes the exploration-exploitation trade-off explicit, and permits maximally informed candidate selection. In summary, this dissertation presents a collection of novel algorithms, for the general problem of continuous black-box optimization as well as a number of special cases, each validated empirically.}
}

@misc{schaul_unit_2013,
  title = {Unit {{Tests}} for {{Stochastic Optimization}}},
  author = {Schaul, Tom and Antonoglou, Ioannis and Silver, David},
  date = {2013},
  eprint = {15003161},
  eprinttype = {pmid},
  url = {http://arxiv.org/abs/1312.6055},
  urldate = {2018-05-27},
  abstract = {Optimization by stochastic gradient descent is an important component of many large-scale machine learning algorithms. A wide variety of such optimization algorithms have been devised; however, it is unclear whether these algorithms are robust and widely applicable across many different optimization landscapes. In this paper we develop a collection of unit tests for stochastic optimization. Each unit test rapidly evaluates an optimization algorithm on a small-scale, isolated, and well-understood difficulty, rather than in real-world scenarios where many such issues are entangled. Passing these unit tests is not sufficient, but absolutely necessary for any algorithms with claims to generality or robustness. We give initial quantitative and qualitative results on numerous established algorithms. The testing framework is open-source, extensible, and easy to apply to new algorithms.},
  isbn = {9783642352881}
}

@inproceedings{scherer_evaluation_2010,
  title = {Evaluation of {{Pooling Operations}} in {{Convolutional Architectures}} for {{Object Recognition}}},
  booktitle = {Lecture {{Notes}} in {{Computer Science}} (Including Subseries {{Lecture Notes}} in {{Artificial Intelligence}} and {{Lecture Notes}} in {{Bioinformatics}})},
  author = {Scherer, Dominik and Müller, Andreas and Behnke, Sven},
  date = {2010},
  volume = {6354 LNCS},
  pages = {92--101},
  issn = {03029743},
  doi = {10.1007/978-3-642-15825-4_10},
  url = {http://www.ais.uni-bonn.de},
  urldate = {2018-04-05},
  abstract = {A common practice to gain invariant features in object recog- nition models is to aggregate multiple low-level features over a small neighborhood. However, the differences between those models makes a comparison of the properties of different aggregation functions hard. Our aim is to gain insight into different functions by directly comparing them on a fixed architecture for several common object recognition tasks. Empirical results show that a maximum pooling operation significantly outperforms subsampling operations. Despite their shift-invariant proper- ties, overlapping pooling windows are no significant improvement over non-overlapping pooling windows. By applying this knowledge, we achieve state-of-the-art error rates of 4.57\% on the NORB normalized-uniform dataset and 5.6\% on the NORB jittered-cluttered dataset.},
  isbn = {3-642-15824-2},
  issue = {PART 3}
}

@article{schiel_rhythm_2010,
  title = {Rhythm and {{Formant Features}} for {{Automatic Alcohol Detection}}},
  author = {Schiel, Florian and Heinrich, Christian and Neumeyer, Veronika},
  date = {2010},
  pages = {4},
  abstract = {Two speech feature sets, RMS rhythmicity and formant frequencies F1-F4, are analyzed for their ability to distinguish alcoholized from sober speech. We describe the statistical framework based on the Alcohol Language Corpus (ALC), including other factors such as gender, age and speaking style, and its application to our case. Rhythm features are calculated using a new method based solely on the short-time energy function; formant features are derived using the standard formant tracker SNACK. Our findings indicate that 3 rhythm and 3 formant features have a high potential to detect intoxication within the speech data of a subject. We also tested the hypothesis that vowels are more centralized in the F1/F2 space for alcoholized speech, but found that, on the contrary, subjects tend to hyperarticulate when being tested for intoxication.},
  langid = {english}
}

@inproceedings{schirrmeister_understanding_2020,
  title = {Understanding Anomaly Detection with Deep Invertible Networks through Hierarchies of Distributions and Features},
  booktitle = {Proceedings of the 33rd {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Schirrmeister, Robin and Zhou, Yuxuan and Ball, Tonio and Zhang, Dan},
  date = {2020},
  location = {{Virtual}},
  url = {https://proceedings.neurips.cc/paper/2020/hash/f106b7f99d2cb30c3db1c3cc0fde9ccb-Abstract.html}
}

@inproceedings{schlegl_unsupervised_2017,
  title = {Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery},
  booktitle = {International {{Conference}} on {{Information Processing}} in {{Medical Imaging}} ({{IPMI}})},
  author = {Schlegl, Thomas and Seeböck, Philipp and Waldstein, Sebastian M and Schmidt-Erfurth, Ursula and Langs, Georg},
  date = {2017},
  pages = {146--157},
  publisher = {{Springer}},
  location = {{Boone, NC, USA}}
}

@article{schmidhuber_deep_2015,
  title = {Deep {{Learning}} in {{Neural Networks}}: {{An Overview}}},
  author = {Schmidhuber, Jürgen},
  date = {2015},
  journaltitle = {Proceedings of the International Joint Conference on Neural Networks (IJCNN)},
  volume = {61},
  eprint = {25462637},
  eprinttype = {pmid},
  pages = {85--117},
  issn = {18792782},
  doi = {10.1016/j.neunet.2014.09.003},
  url = {http://www.idsia.ch},
  urldate = {2018-04-05},
  abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.}
}

@report{schmidhuber_direct_1999,
  title = {Direct {{Policy Search}} and {{Uncertain Policy Evaluation}}},
  author = {Schmidhuber, Jürgen and Zhao, Jieyu},
  date = {1999},
  journaltitle = {AAAI Technical Report SS-99-07},
  pages = {119--124},
  abstract = {Reinforcement learning based on direct search in policy space requires few assumptions about the environment. Hence it is applicable in certain situations where most traditional reinforcement learning algorithms based on dynamic programming are not, especially in partially observable, deterministic worlds. In realistic settings, however, reliable policy evaluations are complicated by numerous sources of uncertainty, such as stochasticity in policy and environment. Given a limited life-time, how much time should a direct policy searcher spend on policy evaluations to obtain reliable statistics? De-spite the fundamental nature of this question it has not received much attention yet. Our efficient approach based on the success-story algorithm (SSA) is radical in the sense that it never stops evaluating any pre-vious policy modification except those it undoes for lack of empirical evidence that they have contributed to lifelong reward accelerations. Here we identify SSA's fundamental advantages over traditional direct policy search (such as stochastic hill-climbing) on problems involving several sources of stochasticity and uncer-taint),. INTRODUCTION In this paper a learner's modifiable parameters that determine its behavior are called its policy. An al-gorithm that modifies the policy is called a learn-ing algorithm. In the context of reinforcement learn-ing (RL) there are two broad classes of learning algo-rithms: (1) methods based on value functions (VFs), and (2) direct search in policy space. VF-based algo-rithms learn a mapping from input-action pairs to ex-pected discounted future reward and use online vari-ants of dynamic programming (DP) (Bellman 1961) for constructing rewarding policies,}
}

@article{schmidhuber_making_nodate,
  title = {Making the {{World Differentiable}}: {{On Using Self-Supervised Fully Recurrent Neu}}�al {{Networks}} for {{Dynamic Reinforcement Learning}} and {{Planning}} in {{Non-Stationary Environm}}�nts},
  author = {Schmidhuber, Jiirgen},
  pages = {26},
  langid = {english}
}

@inproceedings{schneider_wav2vec_2019,
  title = {Wav2vec: {{Unsupervised Pre-training}} for {{Speech Recognition}}},
  shorttitle = {Wav2vec},
  booktitle = {Proceedings of the 20th {{Annual Conference}} of the {{International Speech Communication Association}} ({{Interspeech}})},
  author = {Schneider, Steffen and Baevski, Alexei and Collobert, Ronan and Auli, Michael},
  date = {2019-04-11},
  eprint = {1904.05862},
  eprinttype = {arxiv},
  publisher = {{ISCA}},
  location = {{Graz, Austria}},
  url = {http://arxiv.org/abs/1904.05862},
  urldate = {2019-05-03},
  abstract = {We explore unsupervised pre-training for speech recognition by learning representations of raw audio. wav2vec is trained on large amounts of unlabeled audio data and the resulting representations are then used to improve acoustic model training. We pre-train a simple multi-layer convolutional neural network optimized via a noise contrastive binary classification task. Our experiments on WSJ reduce WER of a strong character-based log-mel filterbank baseline by up to 32\% when only a few hours of transcribed data is available. Our approach achieves 2.78\% WER on the nov92 test set. This outperforms Deep Speech 2, the best reported character-based system in the literature while using three orders of magnitude less labeled training data.},
  langid = {english}
}

@article{scholkopf_artificial_2015,
  title = {Artificial Intelligence: {{Learning}} to See and Act},
  author = {Schölkopf, Bernhard},
  date = {2015},
  journaltitle = {Nature},
  volume = {518},
  number = {7540},
  eprint = {25719660},
  eprinttype = {pmid},
  pages = {486--487},
  issn = {14764687},
  doi = {10.1038/518486a},
  abstract = {An artificial-intelligence system uses machine learning from massive training sets to teach itself to play 49 classic computer games, demonstrating that it can adapt to a variety of tasks. See Letter p.529},
  isbn = {doi:10.1038/518486a}
}

@article{scholkopf_estimating_2001,
  title = {Estimating the Support of a High-Dimensional Distribution},
  author = {Schölkopf, Bernhard and Platt, John C and Shawe-Taylor, John and Smola, Alex J and Williamson, Robert C},
  date = {2001},
  journaltitle = {Neural Computation},
  volume = {13},
  number = {7},
  pages = {1443--1471},
  publisher = {{MIT Press One Rogers Street, Cambridge, MA 02142-1209}}
}

@inproceedings{schroeder_codeexcited_1985,
  title = {Code-Excited Linear Prediction({{CELP}}): {{High-quality}} Speech at Very Low Bit Rates},
  shorttitle = {Code-Excited Linear Prediction({{CELP}})},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Schroeder, M. and Atal, B.},
  date = {1985-04},
  volume = {10},
  pages = {937--940},
  doi = {10.1109/ICASSP.1985.1168147},
  abstract = {We describe in this paper a code-excited linear predictive coder in which the optimum innovation sequence is selected from a code book of stored sequences to optimize a given fidelity criterion. Each sample of the innovation sequence is filtered sequentially through two time-varying linear recursive filters, one with a long-delay (related to pitch period) predictor in the feedback loop and the other with a short-delay predictor (related to spectral envelope) in the feedback loop. We code speech, sampled at 8 kHz, in blocks of 5-msec duration. Each block consisting of 40 samples is produced from one of 1024 possible innovation sequences. The bit rate for the innovation sequence is thus 1/4 bit per sample. We compare in this paper several different random and deterministic code books for their effectiveness in providing the optimum innovation sequence in each block. Our results indicate that a random code book has a slight speech quality advantage at low bit rates. Examples of speech produced by the above method will be played at the conference.},
  eventtitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})}
}

@misc{schulman_gradient_2015,
  title = {Gradient {{Estimation Using Stochastic Computation Graphs}}},
  author = {Schulman, John and Heess, Nicolas and Weber, Theophane and Abbeel, Pieter},
  date = {2015},
  eprint = {1506.05254},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1506.05254},
  urldate = {2017-10-04},
  abstract = {In a variety of problems originating in supervised, unsupervised, and reinforcement learning, the loss function is defined by an expectation over a collection of random variables, which might be part of a probabilistic model or the external world. Estimating the gradient of this loss function, using samples, lies at the core of gradient-based learning algorithms for these problems. We introduce the formalism of stochastic computation graphs---directed acyclic graphs that include both deterministic functions and conditional probability distributions---and describe how to easily and automatically derive an unbiased estimator of the loss function's gradient. The resulting algorithm for computing the gradient estimator is a simple modification of the standard backpropagation algorithm. The generic scheme we propose unifies estimators derived in variety of prior work, along with variance-reduction techniques therein. It could assist researchers in developing intricate models involving a combination of stochastic and deterministic operations, enabling, for example, attention, memory, and control actions.}
}

@misc{schulman_highdimensional_2015,
  title = {High-{{Dimensional Continuous Control Using Generalized Advantage Estimation}}},
  author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael I. and Abbeel, Pieter},
  date = {2015},
  eprint = {1506.02438},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1506.02438},
  urldate = {2017-10-04},
  abstract = {Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.}
}

@misc{schulman_proximal_2017,
  title = {Proximal {{Policy Optimization Algorithms}}},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  date = {2017},
  eprint = {1707.06347},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1707.06347},
  urldate = {2018-06-03},
  abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.}
}

@inproceedings{schulman_trust_2015,
  title = {Trust {{Region Policy Optimization}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
  date = {2015},
  eprint = {1502.05477},
  eprinttype = {arxiv},
  location = {{Lille, France}},
  url = {http://arxiv.org/abs/1502.05477},
  abstract = {We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.}
}

@inproceedings{schultz_learning_2003,
  title = {Learning a Distance Metric from Relative Comparisons},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Schultz, Matthew and Joachims, Thorsten},
  editor = {Thrun, S. and Saul, L. and Schölkopf, B.},
  date = {2003}
}

@incollection{schurer_adaptive_2004,
  title = {Adaptive {{Quasi-Monte Carlo Integration Based}} on {{MISER}} and {{VEGAS}}},
  booktitle = {Monte {{Carlo}} and {{Quasi-Monte Carlo Methods}} 2002},
  author = {Schürer, Rudolf},
  date = {2004},
  pages = {393-406 TS - CrossRef},
  doi = {10.1007/978-3-642-18743-8_25},
  abstract = {Quasi-Monte Carlo (QMC) routines are one of the most common tech-niques for solving integration problems in high dimensions. However, their efficiency degrades if the variation of the integrand is concentrated in small areas of the inte-gration domain. Adaptive algorithms cope with this situation by adjusting the flow of computation based on previous integrand evaluations. We explore ways to modify the Monte Carlo based adaptive algorithms MISER and VEGAS such that low-discrepancy point sets are used instead of random sam-ples. Experimental results show that the proposed algorithms outperform plain QMC as well as the original adaptive integration routine for certain classes of test cases.},
  isbn = {978-3-540-20466-4}
}

@online{scott_not_,
  title = {I {{Do Not Think It Means What You Think It Means}}: {{Artificial Intelligence}}, {{Cognitive Work}} \& {{Scale}}},
  shorttitle = {I {{Do Not Think It Means What You Think It Means}}},
  author = {Scott, Kevin},
  url = {https://www.amacad.org/publication/i-do-not-think-it-means-what-you-think-it-means-artificial-intelligence-cognitive-work},
  urldate = {2022-04-20},
  abstract = {Over the past decade, AI technologies have advanced by leaps and bounds. Progress has been so fast, voluminous, and varied that it can be a challenge even for experts to make sense of it all. In this essay, I propose a framework for thinking about AI systems, specifically the idea that they are ultimately tools developed by humans to help other humans perform an increasing breadth of their cognitive work. Our AI systems for assisting us with our cognitive work have become more capable and general over the past few years. This is in part due to a confluence of novel AI algorithms and the availability of massive amounts of data and compute. From this, researchers and engineers have been able to construct large, general models that serve as flexible and powerful building blocks that can be composed with other software to drive breakthroughs in the natural and physical sciences, to solve hard optimization and strategy problems, to perform perception tasks, and even to assist with complex cognitive tasks like coding.},
  langid = {english},
  organization = {{American Academy of Arts \& Sciences, Dædalus}}
}

@inproceedings{sculley_webscale_2010,
  title = {Web-Scale k-Means Clustering},
  booktitle = {Proceedings of the 19th International Conference on {{World}} Wide Web - {{WWW}} '10},
  author = {Sculley, D.},
  date = {2010},
  pages = {1177},
  publisher = {{ACM Press}},
  location = {{Raleigh, North Carolina, USA}},
  doi = {10.1145/1772690.1772862},
  url = {http://portal.acm.org/citation.cfm?doid=1772690.1772862},
  urldate = {2022-03-03},
  eventtitle = {The 19th International Conference},
  isbn = {978-1-60558-799-8},
  langid = {english}
}

@inproceedings{seabold_statsmodels_2010,
  title = {Statsmodels: {{Econometric}} and {{Statistical Modeling}} with {{Python}}},
  booktitle = {9th Python in Science Conference},
  author = {Seabold, Skipper and Perktold, Josef},
  date = {2010}
}

@inproceedings{searleExperimentalEvaluationDevelopment2020,
  title = {Experimental {{Evaluation}} and {{Development}} of a {{Silver-Standard}} for the {{MIMIC-III Clinical Coding Dataset}}},
  booktitle = {Proceedings of the 19th {{SIGBioMed Workshop}} on {{Biomedical Language Processing}}},
  author = {Searle, Thomas and Ibrahim, Zina and Dobson, Richard},
  date = {2020-07},
  pages = {76--85},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.bionlp-1.8},
  abstract = {Clinical coding is currently a labour-intensive, error-prone, but a critical administrative process whereby hospital patient episodes are manually assigned codes by qualified staff from large, standardised taxonomic hierarchies of codes. Automating clinical coding has a long history in NLP research and has recently seen novel developments setting new benchmark results. A popular dataset used in this task is MIMIC-III, a large database of clinical free text notes and their associated codes amongst other data. We argue for the reconsideration of the validity MIMIC-III's assigned codes, as MIMIC-III has not undergone secondary validation. This work presents an open-source, reproducible experimental methodology for assessing the validity of EHR discharge summaries. We exemplify the methodology with MIMIC-III discharge summaries and show the most frequently assigned codes in MIMIC-III are undercoded up to 35\%.}
}

@inproceedings{sechidisStratificationMultilabelData2011,
  title = {On the {{Stratification}} of {{Multi-label Data}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Sechidis, Konstantinos and Tsoumakas, Grigorios and Vlahavas, Ioannis},
  editor = {Gunopulos, Dimitrios and Hofmann, Thomas and Malerba, Donato and Vazirgiannis, Michalis},
  date = {2011},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {145--158},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-23808-6_10},
  abstract = {Stratified sampling is a sampling method that takes into account the existence of disjoint groups within a population and produces samples where the proportion of these groups is maintained. In single-label classification tasks, groups are differentiated based on the value of the target variable. In multi-label learning tasks, however, where there are multiple target variables, it is not clear how stratified sampling could/should be performed. This paper investigates stratification in the multi-label data context. It considers two stratification methods for multi-label data and empirically compares them along with random sampling on a number of datasets and based on a number of evaluation criteria. The results reveal some interesting conclusions with respect to the utility of each method for particular types of multi-label datasets.},
  isbn = {978-3-642-23808-6},
  langid = {english}
}

@inproceedings{sedighizadeh_adaptive_2008,
  title = {Adaptive {{PID Controller}} Based on {{Reinforcement Learning}} for {{Wind Turbine Control}}},
  booktitle = {{{PROCEEDINGS OF WORLD ACADEMY OF SCIENCE}}, {{ENGINEERING AND TECHNOLOGY}}},
  author = {Sedighizadeh, M. and Rezazadeh, A.},
  date = {2008},
  pages = {257--262},
  abstract = {A self tuning PID control strategy using reinforcement learning is proposed in this paper to deal with the control of wind energy conversion systems (WECS). Actor-Critic learning is used to tune PID parameters in an adaptive way by taking advantage of the model-free and on-line learning properties of reinforcement learning effectively. In order to reduce the demand of storage space and to improve the learning efficiency, a single RBF neural network is used to approximate the policy function of Actor and the value function of Critic simultaneously. The inputs of RBF network are the system error, as well as the first and the second-order differences of error. The Actor can realize the mapping from the system state to PID parameters, while the Critic evaluates the outputs of the Actor and produces TD error. Based on TD error performance index and gradient descent method, the updating rules of RBF kernel function and network weights were given. Simulation results show that the proposed controller is efficient for WECS and it is perfectly adaptable and strongly robust, which is better than that of a conventional PID controller.}
}

@article{see_get_nodate,
  title = {Get {{To The Point}}: {{Summarization}} with {{Pointer-Generator Networks}}},
  author = {See, Abigail and Liu, Peter J. and Brain, Google and Manning, Christopher D.},
  abstract = {Neural sequence-to-sequence models have provided a viable new approach for ab-stractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the origi-nal text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we pro-pose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate repro-duction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail sum-marization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.}
}

@online{segonne_robust_2022,
  title = {Robust Uncertainty Estimates with Out-of-Distribution Pseudo-Inputs Training},
  author = {Segonne, Pierre and Zainchkovskyy, Yevgen and Hauberg, Søren},
  date = {2022-01-15},
  number = {2201.05890},
  eprint = {2201.05890},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2201.05890},
  urldate = {2022-03-22},
  abstract = {Probabilistic models often use neural networks to control their predictive uncertainty. However, when making out-of-distribution (OOD)\vphantom\{\} predictions, the often-uncontrollable extrapolation properties of neural networks yield poor uncertainty predictions. Such models then don't know what they don't know, which directly limits their robustness w.r.t unexpected inputs. To counter this, we propose to explicitly train the uncertainty predictor where we are not given data to make it reliable. As one cannot train without data, we provide mechanisms for generating pseudo-inputs in informative low-density regions of the input space, and show how to leverage these in a practical Bayesian framework that casts a prior distribution over the model uncertainty. With a holistic evaluation, we demonstrate that this yields robust and interpretable predictions of uncertainty while retaining state-of-the-art performance on diverse tasks such as regression and generative modelling},
  pubstate = {preprint}
}

@article{sehnke_parameter-exploring_2010,
  title = {Parameter-Exploring Policy Gradients},
  author = {Sehnke, Frank and Osendorfer, Christian and Rückstieß, Thomas and Graves, Alex and Peters, Jan and Schmidhuber, Jürgen},
  date = {2010-05},
  journaltitle = {Proceedings of the International Joint Conference on Neural Networks (IJCNN)},
  volume = {23},
  number = {4},
  eprint = {20061118},
  eprinttype = {pmid},
  pages = {551--559},
  issn = {08936080},
  doi = {10.1016/j.neunet.2009.12.004},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S0893608009003220},
  urldate = {2018-04-06},
  abstract = {We present a model-free reinforcement learning method for partially observable Markov decision problems. Our method estimates a likelihood gradi-ent by sampling directly in parameter space, which leads to lower variance gradient estimates than obtained by regular policy gradient methods. We show that for several complex control tasks, including robust standing with a humanoid robot, this method outperforms well-known algorithms from the fields of standard policy gradients, finite difference methods and population based heuristics. We also show that the improvement is largest when the pa-rameter samples are drawn symmetrically. Lastly we analyse the importance of the individual components of our method by incrementally incorporating them into the other algorithms, and measuring the gain in performance after each step.},
  isbn = {9780769543000}
}

@inproceedings{sehwag_ssd_2021,
  title = {{{SSD}}: {{A}} Unified Framework for Self-Supervised Outlier Detection},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Sehwag, Vikash and Chiang, Mung and Mittal, Prateek},
  date = {2021-05},
  location = {{Virtual}},
  url = {https://openreview.net/forum?id=v5gjXpmR8J},
  eventtitle = {Proceedings of the {{International Conference}} on {{Learning Representations}} ({{ICLR}})}
}

@misc{semeniuta_hybrid_2017,
  title = {A {{Hybrid Convolutional Variational Autoencoder}} for {{Text Generation}}},
  author = {Semeniuta, Stanislau and Severyn, Aliaksei and Barth, Erhardt},
  date = {2017-02-08},
  eprint = {1702.02390},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1702.02390},
  urldate = {2021-03-04},
  abstract = {In this paper we explore the effect of architectural choices on learning a Variational Autoencoder (VAE) for text generation. In contrast to the previously introduced VAE model for text where both the encoder and decoder are RNNs, we propose a novel hybrid architecture that blends fully feed-forward convolutional and deconvolutional components with a recurrent language model. Our architecture exhibits several attractive properties such as faster run time and convergence, ability to better handle long sequences and, more importantly, it helps to avoid some of the major difficulties posed by training VAE models on textual data.},
  langid = {english}
}

@article{sener_multi-task_2018,
  title = {Multi-{{Task Learning}} as {{Multi-Objective Optimization}}},
  author = {Sener, Ozan and Koltun, Vladlen},
  date = {2018-10-10},
  url = {https://arxiv.org/abs/1810.04650},
  urldate = {2018-10-12},
  langid = {english}
}

@inproceedings{sennrich_improving_2016,
  title = {Improving Neural Machine Translation Models with Monolingual Data},
  booktitle = {Proceedings of the 54th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{ACL}})},
  author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  date = {2016},
  pages = {86--96}
}

@misc{sermanet_overfeat_2013,
  title = {{{OverFeat}}: {{Integrated Recognition}}, {{Localization}} and {{Detection}} Using {{Convolutional Networks}}},
  author = {Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Michael and Fergus, Rob and LeCun, Yann A.},
  date = {2013-12-21},
  eprint = {1312.6229},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1312.6229},
  abstract = {We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.}
}

@inproceedings{serra_input_2020,
  title = {Input Complexity and Out-of-Distribution Detection with Likelihood-Based Generative Models},
  booktitle = {Proceedings of the 8th {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Serrà, Joan and Álvarez, David and Gómez, Vicenç and Slizovskaia, Olga and Núñez, José F. and Luque, Jordi},
  date = {2020},
  location = {{Addis Ababa, Ethiopia}},
  url = {https://openreview.net/forum?id=SyxIWpVYvr},
  eventtitle = {International {{Conference}} on {{Learning Representations}}}
}

@inproceedings{settle_acoustically_2019,
  title = {Acoustically Grounded Word Embeddings for Improved Acoustics-to-Word Speech Recognition},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Settle, S. and Audhkhasi, K. and Livescu, K. and Picheny, M.},
  date = {2019}
}

@unpublished{settle_discriminative_2016,
  title = {Discriminative {{Acoustic Word Embeddings}}: {{Recurrent Neural Network-Based Approaches}}},
  shorttitle = {Discriminative {{Acoustic Word Embeddings}}},
  author = {Settle, Shane and Livescu, Karen},
  date = {2016-11-08},
  eprint = {1611.02550},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1611.02550},
  urldate = {2021-10-20},
  abstract = {Acoustic word embeddings --- fixed-dimensional vector representations of variable-length spoken word segments --- have begun to be considered for tasks such as speech recognition and query-by-example search. Such embeddings can be learned discriminatively so that they are similar for speech segments corresponding to the same word, while being dissimilar for segments corresponding to different words. Recent work has found that acoustic word embeddings can outperform dynamic time warping on query-by-example search and related word discrimination tasks. However, the space of embedding models and training approaches is still relatively unexplored. In this paper we present new discriminative embedding models based on recurrent neural networks (RNNs). We consider training losses that have been successful in prior work, in particular a cross entropy loss for word classification and a contrastive loss that explicitly aims to separate same-word and different-word pairs in a "Siamese network" training setting. We find that both classifier-based and Siamese RNN embeddings improve over previously reported results on a word discrimination task, with Siamese RNNs outperforming classification models. In addition, we present analyses of the learned embeddings and the effects of variables such as dimensionality and network structure.}
}

@inproceedings{settle_querybyexample_2017,
  title = {Query-by-Example Search with Discriminative Neural Acoustic Word Embeddings},
  booktitle = {Annual {{Conference}} of the {{International Speech Communication Association}}},
  author = {Settle, S. and Levin, K. and Kamper, H. and Livescu, K.},
  date = {2017}
}

@online{shafey_joint_2019,
  title = {Joint {{Speech Recognition}} and {{Speaker Diarization}} via {{Sequence Transduction}}},
  author = {Shafey, Laurent El and Soltau, Hagen and Shafran, Izhak},
  date = {2019-07-08},
  eprint = {1907.05337},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1907.05337},
  urldate = {2023-04-14},
  abstract = {Speech applications dealing with conversations require not only recognizing the spoken words, but also determining who spoke when. The task of assigning words to speakers is typically addressed by merging the outputs of two separate systems, namely, an automatic speech recognition (ASR) system and a speaker diarization (SD) system. The two systems are trained independently with different objective functions. Often the SD systems operate directly on the acoustics and are not constrained to respect word boundaries and this deficiency is overcome in an ad hoc manner. Motivated by recent advances in sequence to sequence learning, we propose a novel approach to tackle the two tasks by a joint ASR and SD system using a recurrent neural network transducer. Our approach utilizes both linguistic and acoustic cues to infer speaker roles, as opposed to typical SD systems, which only use acoustic cues. We evaluated the performance of our approach on a large corpus of medical conversations between physicians and patients. Compared to a competitive conventional baseline, our approach improves word-level diarization error rate from 15.8\% to 2.2\%.},
  pubstate = {preprint}
}

@article{shahriari_taking_2016,
  title = {Taking the {{Human Out}} of the {{Loop}}: {{A Review}} of {{Bayesian Optimization}}},
  shorttitle = {Taking the {{Human Out}} of the {{Loop}}},
  author = {Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P. and family=Freitas, given=Nando, prefix=de, useprefix=true},
  date = {2016-01},
  journaltitle = {Proceedings of the IEEE},
  volume = {104},
  number = {1},
  pages = {148--175},
  issn = {0018-9219, 1558-2256},
  doi = {10/f75n9c},
  url = {https://ieeexplore.ieee.org/document/7352306/},
  urldate = {2018-12-20},
  abstract = {Big data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures. The construction of such systems involves many distributed design choices. The end products (e.g., recommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involves many tunable configuration parameters. These parameters are often specified and hard-coded into the software by various developers or teams. If optimized jointly, these parameters can result in significant improvements. Bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years. It promises greater automation so as to increase both product quality and human productivity. This review paper introduces Bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.},
  langid = {english}
}

@inproceedings{shailaja_machine_2018,
  title = {Machine Learning in Healthcare: {{A}} Review},
  booktitle = {2018 {{Second}} International Conference on Electronics, Communication and Aerospace Technology ({{ICECA}})},
  author = {Shailaja, K and Seetharamulu, Banoth and family=Jabbar, given=MA, given-i=MA},
  date = {2018},
  pages = {910--914},
  publisher = {{IEEE}}
}

@article{shamir_withoutreplacement_2016,
  title = {Without-{{Replacement Sampling}} for {{Stochastic Gradient Methods}}: {{Convergence Results}} and {{Application}} to {{Distributed Optimization}}},
  shorttitle = {Without-{{Replacement Sampling}} for {{Stochastic Gradient Methods}}},
  author = {Shamir, Ohad},
  date = {2016-03-02},
  url = {https://arxiv.org/abs/1603.00570},
  urldate = {2018-10-16},
  abstract = {Stochastic gradient methods for machine learning and optimization problems are usually analyzed assuming data points are sampled with replacement. In practice, however, sampling without replacement is very common, easier to implement in many cases, and often performs better. In this paper, we provide competitive convergence guarantees for without-replacement sampling, under various scenarios, for three types of algorithms: Any algorithm with online regret guarantees, stochastic gradient descent, and SVRG. A useful application of our SVRG analysis is a nearly-optimal algorithm for regularized least squares in a distributed setting, in terms of both communication complexity and runtime complexity, when the data is randomly partitioned and the condition number can be as large as the data size per machine (up to logarithmic factors). Our proof techniques combine ideas from stochastic optimization, adversarial online learning, and transductive learning theory, and can potentially be applied to other stochastic optimization and learning problems.},
  langid = {english}
}

@article{shannon_mathematical_1948,
  title = {A {{Mathematical Theory}} of {{Communication}}},
  author = {Shannon, Claude E},
  date = {1948},
  journaltitle = {The Bell System Technical Journal},
  volume = {27},
  eprint = {9230594},
  eprinttype = {pmid},
  pages = {379--423},
  issn = {07246811},
  doi = {10.1145/584091.584093},
  abstract = {The recent development of various methods of modulation such as PCM and PPM which exchange bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A basis for such a theory is contained in the important papers of Nyquist and Hartley on this subject. In the present paper we will extend the theory to include a number of new factors, in particular the effect of noise in the channel, and the savings possible due to the statistical structure of the original message and due to the nature of the final destination of the information. The fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point. Frequently the messages have meaning; that is they refer to or are correlated according to some system with certain physical or conceptual entities. These semantic aspects of communication are irrelevant to the engineering problem. The significant aspect is that the actual message is one selected from a set of possible messages. The system must be designed to operate for each possible selection, not just the one which will actually be chosen since this is unknown at the time of design. If the number of messages in the set is finite then this number or any monotonic function of this number can be regarded as a measure of the information produced when one message is chosen from the set, all choices being equally likely. As was pointed out by Hartley the most natural choice is the logarithmic function. Although this definition must be generalized considerably when we consider the influence of the statistics of the message and when we have a continuous range of messages, we will in all cases use an essentially logarithmic measure.},
  isbn = {0252725484},
  issue = {July 1948}
}

@misc{shannon_optimizing_2017,
  title = {Optimizing Expected Word Error Rate via Sampling for Speech Recognition},
  author = {Shannon, Matt},
  date = {2017-06-08},
  eprint = {1706.02776},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1706.02776},
  urldate = {2018-10-31},
  abstract = {State-level minimum Bayes risk (sMBR) training has become the de facto standard for sequence-level training of speech recognition acoustic models. It has an elegant formulation using the expectation semiring, and gives large improvements in word error rate (WER) over models trained solely using cross-entropy (CE) or connectionist temporal classification (CTC). sMBR training optimizes the expected number of frames at which the reference and hypothesized acoustic states differ. It may be preferable to optimize the expected WER, but WER does not interact well with the expectation semiring, and previous approaches based on computing expected WER exactly involve expanding the lattices used during training. In this paper we show how to perform optimization of the expected WER by sampling paths from the lattices used during conventional sMBR training. The gradient of the expected WER is itself an expectation, and so may be approximated using Monte Carlo sampling. We show experimentally that optimizing WER during acoustic model training gives 5\% relative improvement in WER over a well-tuned sMBR baseline on a 2-channel query recognition task (Google Home).}
}

@article{shapiro_analysis_1965,
  title = {An Analysis of Variance Test for Normality (Complete Samples)},
  author = {Shapiro, Samuel Sanford and Wilk, Martin B},
  date = {1965},
  journaltitle = {Biometrika},
  volume = {52},
  number = {3/4},
  pages = {591--611},
  publisher = {{JSTOR}}
}

@inproceedings{sharifrazavian_cnn_2014,
  title = {Cnn {{Features Off-the-Shelf}}: {{An Astounding Baseline}} for {{Recognition}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Sharif Razavian, Ali and Azizpour, Hossein and Sullivan, Josephine and Carlsson, Stefan},
  date = {2014},
  pages = {806--813}
}

@article{shehab_generic_2013,
  title = {A {{Generic Feature Extraction Model}} Using {{Learnable Evolution Models}} ({{LEM}}+ {{ID3}}).},
  author = {Shehab, M. E. and Badran, K. and Salama, G. I.},
  date = {2013},
  journaltitle = {International Journal of Computer Applications},
  volume = {64},
  number = {11},
  pages = {27--32},
  doi = {10.1.1.278.8123},
  url = {http://findit.dtu.dk/en/catalog/2353090475},
  urldate = {2017-12-13}
}

@misc{shen_natural_2018,
  title = {Natural {{TTS Synthesis}} by {{Conditioning WaveNet}} on {{Mel Spectrogram Predictions}}},
  author = {Shen, Jonathan and Pang, Ruoming and Weiss, Ron J. and Schuster, Mike and Jaitly, Navdeep and Yang, Zongheng and Chen, Zhifeng and Zhang, Yu and Wang, Yuxuan and Skerry-Ryan, R. J. and Saurous, Rif A. and Agiomyrgiannakis, Yannis and Wu, Yonghui},
  date = {2018-02-15},
  eprint = {1712.05884},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1712.05884},
  urldate = {2021-05-17},
  abstract = {This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the conditioning input to WaveNet instead of linguistic, duration, and F0 features. We further show that using this compact acoustic intermediate representation allows for a significant reduction in the size of the WaveNet architecture.},
  langid = {english}
}

@report{shewchuk_introduction_1994,
  title = {An {{Introduction}} to the {{Conjugate Gradient Method Without}} the {{Agonizing Pain}}},
  author = {Shewchuk, Jonathan Richard},
  date = {1994-08-04},
  institution = {{Carnegie Mellon University}},
  url = {https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf},
  urldate = {2018-09-05},
  abstract = {The Conjugate Gradient Method is the most prominent iterative method for solving sparse systems of linear equations. Unfortunately, many textbook treatments of the topic are written with neither illustrations nor intuition, and their victims can be found to this day babbling senselessly in the corners of dusty libraries. For this reason, a deep, geometric understanding of the method has been reserved for the elite brilliant few who have painstakingly decoded the mumblings of their forebears. Nevertheless, the Conjugate Gradient Method is a composite of simple, elegant ideas that almost anyone can understand. Of course, a reader as intelligent as yourself will learn them almost effortlessly. The idea of quadratic forms is introduced and used to derive the methods of Steepest Descent, Conjugate Directions, and Conjugate Gradients. Eigenvectors are explained and used to examine the convergence of the Jacobi Method, Steepest Descent, and Conjugate Gradients. Other topics include preconditioning and the nonlinear Conjugate Gradient Method. I have taken pains to make this article easy to read. Sixty-six illustrations are provided. Dense prose is avoided. Concepts are explained in several different ways. Most equations are coupled with an intuitive interpretation.}
}

@misc{shi_convolutional_2015,
  title = {Convolutional {{LSTM Network}}: {{A Machine Learning Approach}} for {{Precipitation Nowcasting}}},
  shorttitle = {Convolutional {{LSTM Network}}},
  author = {Shi, Xingjian and Chen, Zhourong and Wang, Hao and Yeung, Dit-Yan and Wong, Wai-kin and Woo, Wang-chun},
  date = {2015-06-12},
  eprint = {1506.04214},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1506.04214},
  urldate = {2019-06-17},
  abstract = {The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-theart operational ROVER algorithm for precipitation nowcasting.},
  langid = {english}
}

@online{shi_discretization_2022,
  title = {Discretization and {{Re-Synthesis}}: {{An Alternative Method}} to {{Solve}} the {{Cocktail Party Problem}}},
  shorttitle = {Discretization and {{Re-Synthesis}}},
  author = {Shi, Jing and Chang, Xuankai and Hayashi, Tomoki and Lu, Yen-Ju and Watanabe, Shinji and Xu, Bo},
  date = {2022-01-09},
  eprint = {2112.09382},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2112.09382},
  urldate = {2023-04-12},
  abstract = {Deep learning based models have significantly improved the performance of speech separation with input mixtures like the cocktail party. Prominent methods (e.g., frequency-domain and time-domain speech separation) usually build regression models to predict the ground-truth speech from the mixture, using the masking-based design and the signal-level loss criterion (e.g., MSE or SI-SNR). This study demonstrates, for the first time, that the synthesis-based approach can also perform well on this problem, with great flexibility and strong potential. Specifically, we propose a novel speech separation/enhancement model based on the recognition of discrete symbols, and convert the paradigm of the speech separation/enhancement related tasks from regression to classification. By utilizing the synthesis model with the input of discrete symbols, after the prediction of discrete symbol sequence, each target speech could be re-synthesized. Evaluation results based on the WSJ0-2mix and VCTK-noisy corpora in various settings show that our proposed method can steadily synthesize the separated speech with high speech quality and without any interference, which is difficult to avoid in regression-based methods. In addition, with negligible loss of listening quality, the speaker conversion of enhanced/separated speech could be easily realized through our method.},
  pubstate = {preprint}
}

@online{shi_emformer_2020,
  title = {Emformer: {{Efficient Memory Transformer Based Acoustic Model For Low Latency Streaming Speech Recognition}}},
  shorttitle = {Emformer},
  author = {Shi, Yangyang and Wang, Yongqiang and Wu, Chunyang and Yeh, Ching-Feng and Chan, Julian and Zhang, Frank and Le, Duc and Seltzer, Mike},
  date = {2020-12-30},
  eprint = {2010.10759},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2010.10759},
  urldate = {2022-10-03},
  abstract = {This paper proposes an efficient memory transformer Emformer for low latency streaming speech recognition. In Emformer, the long-range history context is distilled into an augmented memory bank to reduce self-attention's computation complexity. A cache mechanism saves the computation for the key and value in self-attention for the left context. Emformer applies a parallelized block processing in training to support low latency models. We carry out experiments on benchmark LibriSpeech data. Under average latency of 960 ms, Emformer gets WER \$2.50\textbackslash\%\$ on test-clean and \$5.62\textbackslash\%\$ on test-other. Comparing with a strong baseline augmented memory transformer (AM-TRF), Emformer gets \$4.6\$ folds training speedup and \$18\textbackslash\%\$ relative real-time factor (RTF) reduction in decoding with relative WER reduction \$17\textbackslash\%\$ on test-clean and \$9\textbackslash\%\$ on test-other. For a low latency scenario with an average latency of 80 ms, Emformer achieves WER \$3.01\textbackslash\%\$ on test-clean and \$7.09\textbackslash\%\$ on test-other. Comparing with the LSTM baseline with the same latency and model size, Emformer gets relative WER reduction \$9\textbackslash\%\$ and \$16\textbackslash\%\$ on test-clean and test-other, respectively.},
  pubstate = {preprint},
  keywords = {Read}
}

@online{shi_how_2020,
  title = {How {{I}} Learned to Stop Worrying and Write {{ELBO}} (and Its Gradients) in a Billion Ways},
  author = {Shi, Yuge},
  date = {2020-06-19},
  url = {https://yugeten.github.io/posts/2020/06/elbo/},
  urldate = {2021-01-27}
}

@misc{shi_learning_2022,
  title = {Learning {{Audio-Visual Speech Representation}} by {{Masked Multimodal Cluster Prediction}}},
  author = {Shi, Bowen and Hsu, Wei-Ning and Lakhotia, Kushal and Mohamed, Abdelrahman},
  date = {2022-01-05},
  eprint = {2201.02184},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2201.02184},
  urldate = {2022-02-02},
  abstract = {Video recordings of speech contain correlated audio and visual information, providing a strong signal for speech representation learning from the speaker's lip movements and the produced sound. We introduce Audio-Visual Hidden Unit BERT (AV-HuBERT), a self-supervised representation learning framework for audio-visual speech, which masks multi-stream video input and predicts automatically discovered and iteratively refined multimodal hidden units. AV-HuBERT learns powerful audio-visual speech representation benefiting both lip-reading and automatic speech recognition. On the largest public lip-reading benchmark LRS3 (433 hours), AV-HuBERT achieves 32.5\% WER with only 30 hours of labeled data, outperforming the former state-of-the-art approach (33.6\%) trained with a thousand times more transcribed video data (31K hours). The lip-reading WER is further reduced to 26.9\% when using all 433 hours of labeled data from LRS3 and combined with self-training. Using our audio-visual representation on the same benchmark for audio-only speech recognition leads to a 40\% relative WER reduction over the state-of-the-art performance (1.3\% vs 2.3\%). Our code and models are available at https://github.com/facebookresearch/av\_hubert},
  langid = {english}
}

@misc{shi_robust_2022,
  title = {Robust {{Self-Supervised Audio-Visual Speech Recognition}}},
  author = {Shi, Bowen and Hsu, Wei-Ning and Mohamed, Abdelrahman},
  date = {2022-01-05},
  eprint = {2201.01763},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2201.01763},
  urldate = {2022-02-02},
  abstract = {Audio-based automatic speech recognition (ASR) degrades significantly in noisy environments and is particularly vulnerable to interfering speech, as the model cannot determine which speaker to transcribe. Audio-visual speech recognition (AVSR) systems improve robustness by complementing the audio stream with the visual information that is invariant to noise and helps the model focus on the desired speaker. However, previous AVSR work focused solely on the supervised learning setup; hence the progress was hindered by the amount of labeled data available. In this work, we present a self-supervised AVSR framework built upon Audio-Visual HuBERT (AV-HuBERT), a state-of-the-art audio-visual speech representation learning model. On the largest available AVSR benchmark dataset LRS3, our approach outperforms prior state-of-the-art by \textasciitilde 50\% (28.0\% vs. 14.1\%) using less than 10\% of labeled data (433hr vs. 30hr) in the presence of babble noise, while reducing the WER of an audio-based model by over 75\% (25.8\% vs. 5.8\%) on average.},
  langid = {english}
}

@inproceedings{shiAutomatedICDCoding2018,
  title = {Towards {{Automated ICD Coding Using Deep Learning}}},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Shi, Haoran and Xie, Pengtao and Hu, Zhiting and Zhang, Ming and Xing, Eric P.},
  date = {2018},
  eprint = {1711.04075},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {1066--1076},
  doi = {10.18653/v1/P18-1098},
  abstract = {International Classification of Diseases(ICD) is an authoritative health care classification system of different diseases and conditions for clinical and management purposes. Considering the complicated and dedicated process to assign correct codes to each patient admission based on overall diagnosis, we propose a hierarchical deep learning model with attention mechanism which can automatically assign ICD diagnostic codes given written diagnosis. We utilize character-aware neural language models to generate hidden representations of written diagnosis descriptions and ICD codes, and design an attention mechanism to address the mismatch between the numbers of descriptions and corresponding codes. Our experimental results show the strong potential of automated ICD coding from diagnosis descriptions. Our best model achieves 0.53 and 0.90 of F1 score and area under curve of receiver operating characteristic respectively. The result outperforms those achieved using character-unaware encoding method or without attention mechanism. It indicates that our proposed deep learning model can code automatically in a reasonable way and provide a framework for computer-auxiliary ICD coding.}
}

@inproceedings{shinohara_adversarial_2016,
  title = {Adversarial {{Multitask Learning}} of {{Deep Neural Networks}} for {{Robust Speech Recognition}}},
  author = {Shinohara, Yusuke},
  date = {2016},
  abstract = {A method of learning deep neural networks (DNNs) for noise robust speech recognition is proposed. It is widely known that representations (activations) of well-trained DNNs are highly invariant to noise, especially in higher layers, and such invariance leads to the noise robustness of DNNs. However, little is known about how to enhance such invariance of representations, which is a key for improving robustness. In this paper, we propose adversarial multi-task learning of DNNs for explicitly enhancing the invariance of representations. Specifically, a primary task of senone classification and a secondary task of domain (noise condition) classification are jointly solved. What is different from the standard multi-task learning is that the representation is learned adversarially to the secondary task, so that representation with low domain-classification accuracy is induced. As a result, senone-discriminative and domain-invariant representation is obtained, which leads to an improved robustness of DNNs. Experimental results on a noise-corrupted Wall Street Journal data set show the effectiveness of the proposed method.}
}

@online{shoeybi_megatronlm_2020,
  title = {Megatron-{{LM}}: {{Training}} Multi-Billion Parameter Language Models Using Model Parallelism},
  author = {Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  date = {2020},
  eprint = {1909.08053},
  eprinttype = {arxiv},
  pubstate = {preprint}
}

@unpublished{shon_slue_2021,
  title = {{{SLUE}}: {{New Benchmark Tasks}} for {{Spoken Language Understanding Evaluation}} on {{Natural Speech}}},
  shorttitle = {{{SLUE}}},
  author = {Shon, Suwon and Pasad, Ankita and Wu, Felix and Brusco, Pablo and Artzi, Yoav and Livescu, Karen and Han, Kyu J.},
  date = {2021-11-19},
  eprint = {2111.10367},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2111.10367},
  urldate = {2021-12-04},
  abstract = {Progress in speech processing has been facilitated by shared datasets and benchmarks. Historically these have focused on automatic speech recognition (ASR), speaker identification, or other lower-level tasks. Interest has been growing in higher-level spoken language understanding tasks, including using end-to-end models, but there are fewer annotated datasets for such tasks. At the same time, recent work shows the possibility of pre-training generic representations and then fine-tuning for several tasks using relatively little labeled data. We propose to create a suite of benchmark tasks for Spoken Language Understanding Evaluation (SLUE) consisting of limited-size labeled training sets and corresponding evaluation sets. This resource would allow the research community to track progress, evaluate pre-trained representations for higher-level tasks, and study open questions such as the utility of pipeline versus end-to-end approaches. We present the first phase of the SLUE benchmark suite, consisting of named entity recognition, sentiment analysis, and ASR on the corresponding datasets. We focus on naturally produced (not read or synthesized) speech, and freely available datasets. We provide new transcriptions and annotations on subsets of the VoxCeleb and VoxPopuli datasets, evaluation metrics and results for baseline models, and an open-source toolkit to reproduce the baselines and evaluate new models.}
}

@misc{shon_unsupervised_2018,
  title = {Unsupervised {{Representation Learning}} of {{Speech}} for {{Dialect Identification}}},
  author = {Shon, Suwon and Hsu, Wei-Ning and Glass, James},
  date = {2018-09-12},
  eprint = {1809.04458},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1809.04458},
  urldate = {2019-10-15},
  abstract = {In this paper, we explore the use of a factorized hierarchical variational autoencoder (FHVAE) model to learn an unsupervised latent representation for dialect identification (DID). An FHVAE can learn a latent space that separates the more static attributes within an utterance from the more dynamic attributes by encoding them into two different sets of latent variables. Useful factors for dialect identification, such as phonetic or linguistic content, are encoded by a segmental latent variable, while irrelevant factors that are relatively constant within a sequence, such as a channel or a speaker information, are encoded by a sequential latent variable. The disentanglement property makes the segmental latent variable less susceptible to channel and speaker variation, and thus reduces degradation from channel domain mismatch. We demonstrate that on fully-supervised DID tasks, an end-toend model trained on the features extracted from the FHVAE model achieves the best performance, compared to the same model trained on conventional acoustic features and an i-vector based system. Moreover, we also show that the proposed approach can leverage a large amount of unlabeled data for FHVAE training to learn domain-invariant features for DID, and significantly improve the performance in a lowresource condition, where the labels for the in-domain data are not available.},
  langid = {english}
}

@inproceedings{shor_learning_2020,
  title = {Towards Learning a Universal Non-Semantic Representation of Speech},
  booktitle = {Annual {{Conference}} of the {{International Speech Communication Association}}},
  author = {Shor, Joel and Jansen, Aren and Maor, Ronnie and Lang, Oran and Tuval, Omry and family=Chaumont Quitry, given=Félix, prefix=de, useprefix=true and Tagliasacchi, Marco and Shavitt, Ira and Emanuel, Dotan and Haviv, Yinnon},
  date = {2020}
}

@misc{shrikumar_learning_2017,
  title = {Learning {{Important Features Through Propagating Activation Differences}}},
  author = {Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
  date = {2017-04-09},
  eprint = {1704.02685},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1704.02685},
  urldate = {2019-08-01},
  abstract = {The purported “black box” nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its ‘reference activation’ and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. A detailed video tutorial on the method is at http://goo.gl/qKb7pL and code is at http://goo.gl/RM8jvH.},
  langid = {english}
}

@inproceedings{siddharth_learning_2017,
  title = {Learning Disentangled Representations with Semi-Supervised Deep Generative Models},
  booktitle = {Proceedings Og the 31st {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Siddharth, Narayanaswamy and Paige, Brooks and family=Meent, given=Jan-Willem, prefix=van de, useprefix=true and Desmaison, Alban and Goodman, Noah D. and Kohli, Pushmeet and Wood, Frank D. and Torr, Philip H. S.},
  editor = {Guyon, Isabelle and family=Luxburg, given=Ulrike, prefix=von, useprefix=true and Bengio, Samy and Wallach, Hanna M. and Fergus, Rob and Vishwanathan, S. V. N. and Garnett, Roman},
  date = {2017},
  pages = {5925--5935},
  location = {{Long Beach, CA, USA}},
  url = {https://proceedings.neurips.cc/paper/2017/hash/9cb9ed4f35cf7c2f295cc2bc6f732a84-Abstract.html}
}

@article{siddiqui_17_2023,
  entrysubtype = {newspaper},
  title = {17 Fatalities, 736 Crashes; {{The}} Shocking Toll of {{Tesla}}'s {{Autopilot}}},
  author = {Siddiqui, Faiz and Merril, Jeremy B.},
  date = {2023-06-10},
  journaltitle = {The Washington Post},
  url = {https://www.washingtonpost.com/technology/2023/06/10/tesla-autopilot-crashes-elon-musk/},
  urldate = {2023-09-18},
  abstract = {Tesla's driver-assistance system, known as Autopilot, has been involved in far more crashes than previously reported.},
  journalsubtitle = {Tech},
  langid = {english}
}

@article{silver_deterministic_2014,
  title = {Deterministic {{Policy Gradient Algorithms}}},
  author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  date = {2014},
  journaltitle = {Proceedings of the International Conference on Machine Learning (ICML)},
  pages = {387--395},
  issn = {1938-7228},
  abstract = {In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic pol- icy gradient has a particularly appealing form: it is the expected gradient of the action-value function. This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. We demonstrate that deterministic policy gradient algorithms can significantly outperform their stochastic counterparts in high-dimensional action spaces.},
  isbn = {9781634393973}
}

@article{silver_mastering_2016,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and family=Driessche, given=George, prefix=van den, useprefix=true and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc},
  date = {2016},
  journaltitle = {Nature},
  volume = {529},
  number = {7587},
  pages = {484--489},
  publisher = {{Nature Publishing Group}},
  abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.}
}

@online{silver_mastering_2017,
  title = {Mastering {{Chess}} and {{Shogi}} by {{Self-Play}} with a {{General Reinforcement Learning Algorithm}}},
  author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
  date = {2017-12-05},
  eprint = {1712.01815},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1712.01815},
  urldate = {2023-07-11},
  abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
  pubstate = {preprint}
}

@article{simard_best_2003,
  title = {Best Practices for Convolutional Neural Networks Applied to Visual Document Analysis},
  author = {Simard, Patrice Y. and Steinkraus, Dave and Platt, J. C.},
  date = {2003},
  journaltitle = {Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings.},
  volume = {1},
  eprint = {25246403},
  eprinttype = {pmid},
  pages = {958--963},
  issn = {15205363},
  doi = {10.1109/ICDAR.2003.1227801},
  url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1227801},
  urldate = {2018-05-21},
  abstract = {Neural Networks are a powerful technology for classification of visual inputs arising from documents. However, there is a confusing plethora of different neural network methods that are used in the literature and in industry. This paper describes a set of concrete best practices that document analysis researchers can use to get good results with neural networks. The most important practice is that convolutional neural networks are better suited for visual document tasks than fully connected networks. We propose that a simple "do-it-yourself" implementation of convolution neural networks does not require complex methods, such as momentum, weight decay, structure-dependent learning rates, averaging layers, tangent prop, or even finely-tuning the architecture. The end result is a very simple yet general architecture which can yield state-of-the-art performance for document analysis. We illustrate our claims on the MNIST set of English digit images.},
  isbn = {0-7695-1960-1},
  issue = {Icdar}
}

@online{simonnet_asr_2017,
  title = {{{ASR}} Error Management for Improving Spoken Language Understanding},
  author = {Simonnet, Edwin and Ghannay, Sahar and Camelin, Nathalie and Estève, Yannick and De Mori, Renato},
  date = {2017-05-26},
  eprint = {1705.09515},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1705.09515},
  urldate = {2023-10-18},
  abstract = {This paper addresses the problem of automatic speech recognition (ASR) error detection and their use for improving spoken language understanding (SLU) systems. In this study, the SLU task consists in automatically extracting, from ASR transcriptions , semantic concepts and concept/values pairs in a e.g touristic information system. An approach is proposed for enriching the set of semantic labels with error specific labels and by using a recently proposed neural approach based on word embeddings to compute well calibrated ASR confidence measures. Experimental results are reported showing that it is possible to decrease significantly the Concept/Value Error Rate with a state of the art system, outperforming previously published results performance on the same experimental data. It also shown that combining an SLU approach based on conditional random fields with a neural encoder/decoder attention based architecture , it is possible to effectively identifying confidence islands and uncertain semantic output segments useful for deciding appropriate error handling actions by the dialogue manager strategy .},
  pubstate = {preprint}
}

@article{simonyan_two-stream_2014,
  title = {Two-{{Stream Convolutional Networks}} for {{Action Recognition}} in {{Videos}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  date = {2014},
  journaltitle = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
  eprint = {25246403},
  eprinttype = {pmid},
  pages = {568--576},
  issn = {1098-6596},
  doi = {10.1017/CBO9781107415324.004},
  url = {http://arxiv.org/abs/1406.2199},
  abstract = {We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to generalise the best performing hand-crafted features within a data-driven learning framework. Our contribution is three-fold. First, we propose a two-stream ConvNet architecture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multi-task learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both. Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it is competitive with the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification.},
  isbn = {9788578110796}
}

@misc{simonyan_very_2014,
  title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  author = {Simonyan, Karen and Zisserman, Andrew},
  date = {2014},
  eprint = {1409.1556},
  eprinttype = {arxiv},
  url = {https://arxiv.org/abs/1409.1556}
}

@inproceedings{simsekli_tail-index_2019,
  title = {A {{Tail-Index Analysis}} of {{Stochastic Gradient Noise}} in {{Deep Neural Networks}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Simsekli, Umut and Sagun, Levent and Gürbüzbalaban, Mert},
  date = {2019},
  pages = {11},
  location = {{Long Beach, CA, USA}},
  abstract = {The gradient noise (GN) in the stochastic gradient descent (SGD) algorithm is often considered to be Gaussian in the large data regime by assuming that the classical central limit theorem (CLT) kicks in. This assumption is often made for mathematical convenience, since it enables SGD to be analyzed as a stochastic differential equation (SDE) driven by a Brownian motion. We argue that the Gaussianity assumption might fail to hold in deep learning settings and hence render the Brownian motion-based analyses inappropriate. Inspired by non-Gaussian natural phenomena, we consider the GN in a more general context and invoke the generalized CLT (GCLT), which suggests that the GN converges to a heavy-tailed α-stable random variable. Accordingly, we propose to analyze SGD as an SDE driven by a Le´vy motion. Such SDEs can incur ‘jumps’, which force the SDE transition from narrow minima to wider minima, as proven by existing metastability theory. To validate the αstable assumption, we conduct experiments on common deep learning scenarios and show that in all settings, the GN is highly non-Gaussian and admits heavy-tails. We investigate the tail behavior in varying network architectures and sizes, loss functions, and datasets. Our results open up a different perspective and shed more light on the belief that SGD prefers wide minima.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english}
}

@article{singer_active_1982,
  title = {Active {{Comprehension}}: {{Problem-Solving Schema}} with {{Question Generation}} for {{Comprehension}} of {{Complex Short Stories}}},
  author = {Singer, Harry and Donlan, Dan},
  date = {1982},
  journaltitle = {Reading Research Quarterly},
  volume = {17},
  number = {2},
  eprint = {747482},
  eprinttype = {jstor},
  pages = {166},
  publisher = {{WileyInternational Literacy Association}},
  issn = {00340553},
  doi = {10.2307/747482},
  url = {https://www.jstor.org/stable/747482?origin=crossref},
  urldate = {2018-06-06},
  abstract = {A PROBLEM-SOLVING SCHEMA for comprehending short stories was augmented by construction of schema-general questions for each story element. Fifteen eleventh-grade students, randomly assigned to the experimental group, were taught to derive story-specific ques- tions from the schema-general questions as they read complex short stories. The control group read to answer questions posed before- hand by the teacher. Each group read six short stories over a three- week period. Criterion-referenced tests administered after each improve in reader-based processing of text and (2) that story gram- mar structures acquired prior to or during elementary school may be adequate for processing simple fables, but more adequate and more appropriate cognitive structures with strategies for making schema- general questions story-specific are necessary for processing, story resulted in statistically significant differences between the two groups. This evidence implies (1) that instruction can help students storing, and retrieving information derived from reading complex short stories.},
  isbn = {0034-0553}
}

@misc{singh_flava_2021,
  title = {{{FLAVA}}: {{A Foundational Language And Vision Alignment Model}}},
  shorttitle = {{{FLAVA}}},
  author = {Singh, Amanpreet and Hu, Ronghang and Goswami, Vedanuj and Couairon, Guillaume and Galuba, Wojciech and Rohrbach, Marcus and Kiela, Douwe},
  date = {2021-12-08},
  eprint = {2112.04482},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2112.04482},
  urldate = {2022-02-03},
  abstract = {State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety of downstream tasks. Generally, such models are often either cross-modal (contrastive) or multi-modal (with earlier fusion) but not both; and they often only target specific modalities or tasks. A promising direction would be to use a single holistic universal model, as a "foundation", that targets all modalities at once -- a true vision and language foundation model should be good at vision tasks, language tasks, and cross- and multi-modal vision and language tasks. We introduce FLAVA as such a model and demonstrate impressive performance on a wide range of 35 tasks spanning these target modalities.}
}

@misc{sinha_consistency_2021,
  title = {Consistency {{Regularization}} for {{Variational Auto-Encoders}}},
  author = {Sinha, Samarth and Dieng, Adji B.},
  date = {2021-05-31},
  eprint = {2105.14859},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2105.14859},
  urldate = {2021-06-02},
  abstract = {Variational auto-encoders (VAEs) are a powerful approach to unsupervised learning. They enable scalable approximate posterior inference in latent-variable models using variational inference (VI). A VAE posits a variational family parameterized by a deep neural network called an encoder that takes data as input. This encoder is shared across all the observations, which amortizes the cost of inference. However the encoder of a VAE has the undesirable property that it maps a given observation and a semantics-preserving transformation of it to different latent representations. This "inconsistency" of the encoder lowers the quality of the learned representations, especially for downstream tasks, and also negatively affects generalization. In this paper, we propose a regularization method to enforce consistency in VAEs. The idea is to minimize the Kullback-Leibler (KL) divergence between the variational distribution when conditioning on the observation and the variational distribution when conditioning on a random semantic-preserving transformation of this observation. This regularization is applicable to any VAE. In our experiments we apply it to four different VAE variants on several benchmark datasets and found it always improves the quality of the learned representations but also leads to better generalization. In particular, when applied to the Nouveau Variational Auto-Encoder (NVAE), our regularization method yields state-of-the-art performance on MNIST and CIFAR-10. We also applied our method to 3D data and found it learns representations of superior quality as measured by accuracy on a downstream classification task.},
  langid = {english}
}

@inproceedings{sivaram_sparse_2010,
  title = {Sparse Coding for Speech Recognition},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Sivaram, Garimella SVS and Nemala, Sridhar Krishna and Elhilali, Mounya and Tran, Trac D and Hermansky, Hynek},
  date = {2010},
  pages = {4346--4349}
}

@incollection{skiena_matrix_1998,
  title = {"§8.2.3 {{Matrix}} Multiplication"},
  booktitle = {The {{Algorithm Design Manual}}},
  author = {Skiena, Steven S.},
  date = {1998},
  publisher = {{Springer-Verlag}},
  location = {{Berlin, New York}},
  isbn = {978-0-387-94860-7}
}

@incollection{slaney_importance_1993,
  title = {On the Importance of Time- {{A}} Temporal Representatin of Sound},
  booktitle = {Visual {{Representations}} of {{Speech Signals}}},
  author = {Slaney, Malcolm and Lyon, Richard F.},
  date = {1993},
  eprint = {25246403},
  eprinttype = {pmid},
  pages = {95--116},
  issn = {1098-6596},
  doi = {10.1017/CBO9781107415324.004},
  abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-α-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 Å for the interface backbone atoms) increased from 21\% with default Glide SP settings to 58\% with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63\% success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40\% of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
  isbn = {978-85-7811-079-6}
}

@article{slaney_learning_nodate,
  title = {Learning a {{Metric}} for {{Music Similarity}}},
  author = {Slaney, Malcolm and Weinberger, Kilian and White, William},
  pages = {6},
  abstract = {This paper describe five different principled ways to embed songs into a Euclidean metric space. In particular, we learn embeddings so that the pairwise Euclidean distance between two songs reflects semantic dissimilarity. This allows distance-based analysis, such as for example straightforward nearest-neighbor classification, to detect and potentially suggest similar songs within a collection. Each of the six approaches (baseline, whitening, LDA, NCA, LMNN and RCA) rotate and scale the raw feature space with a linear transform. We tune the parameters of these models using a song-classification task with content-based features.},
  langid = {english}
}

@inproceedings{smith_speech_2001,
  title = {Speech Recognition Using {{SVMs}}},
  booktitle = {Conference on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Smith, Nathan and Gales, Mark},
  date = {2001}
}

@incollection{smolensky_chapter_1986,
  title = {Chapter 6: {{Information}} Processing in Dynamical Systems: {{Foundations}} of Harmony Theory},
  booktitle = {Parallel Distributed Processing: {{Explorations}} in the Microstructure of Cognition: {{Foundations}}},
  author = {Smolensky, Paul},
  editor = {D.E. Rumelhart, J.L. McClelland},
  date = {1986},
  pages = {194--281},
  publisher = {{MIT Press}}
}

@inproceedings{snoek_can_2019,
  title = {Can You Trust Your Model's Uncertainty?  {{Evaluating}} Predictive Uncertainty under Dataset Shift},
  booktitle = {Proceedings of the 33rd {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Snoek, Jasper and Ovadia, Yaniv and Fertig, Emily and Lakshminarayanan, Balaji and Nowozin, Sebastian and Sculley, D and Dillon, Joshua and Ren, Jie and Nado, Zachary},
  date = {2019},
  pages = {12},
  location = {{Vancouver, Canada}},
  abstract = {Modern machine learning methods including deep learning have achieved great success in predictive accuracy for supervised learning tasks, but may still fall short in giving useful estimates of their predictive uncertainty. Quantifying uncertainty is especially critical in real-world settings, which often involve input distributions that are shifted from the training distribution due to a variety of factors including sample bias and non-stationarity. In such settings, well calibrated uncertainty estimates convey information about when a model’s output should (or should not) be trusted. Many probabilistic deep learning methods, including Bayesian-and nonBayesian methods, have been proposed in the literature for quantifying predictive uncertainty, but to our knowledge there has not previously been a rigorous largescale empirical comparison of these methods under dataset shift. We present a largescale benchmark of existing state-of-the-art methods on classification problems and investigate the effect of dataset shift on accuracy and calibration. We find that traditional post-hoc calibration does indeed fall short, as do several other previous methods. However, some methods that marginalize over models give surprisingly strong results across a broad spectrum of tasks.},
  eventtitle = {Conference on {{Neural Information Processing Systems}}},
  langid = {english}
}

@inproceedings{snyder_deep_2017,
  title = {Deep {{Neural Network Embeddings}} for {{Text-Independent Speaker Verification}}},
  booktitle = {Proceedings of the 18th {{Annual Conference}} of the {{International Speech Communication Association}} ({{Interspeech}})},
  author = {Snyder, David and Garcia-Romero, Daniel and Povey, Daniel and Khudanpur, Sanjeev},
  date = {2017-08-20},
  pages = {999--1003},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2017-620},
  url = {https://www.isca-speech.org/archive/interspeech_2017/snyder17_interspeech.html},
  urldate = {2021-10-25},
  abstract = {This paper investigates replacing i-vectors for text-independent speaker verification with embeddings extracted from a feedforward deep neural network. Long-term speaker characteristics are captured in the network by a temporal pooling layer that aggregates over the input speech. This enables the network to be trained to discriminate between speakers from variablelength speech segments. After training, utterances are mapped directly to fixed-dimensional speaker embeddings and pairs of embeddings are scored using a PLDA-based backend. We compare performance with a traditional i-vector baseline on NIST SRE 2010 and 2016. We find that the embeddings outperform i-vectors for short speech segments and are competitive on long duration test conditions. Moreover, the two representations are complementary, and their fusion improves on the baseline at all operating points. Similar systems have recently shown promising results when trained on very large proprietary datasets, but to the best of our knowledge, these are the best results reported for speaker-discriminative neural networks when trained and tested on publicly available corpora.},
  langid = {english}
}

@online{snyder_musan_2015,
  title = {{{MUSAN}}: {{A}} Music, Speech, and Noise Corpus},
  author = {Snyder, David and Chen, Guoguo and Povey, Daniel},
  date = {2015},
  eprint = {1510.08484},
  eprinttype = {arxiv},
  pubstate = {preprint}
}

@article{socher_recursive_2013,
  title = {Recursive Deep Models for Semantic Compositionality over a Sentiment Treebank},
  author = {Socher, Richard and Perelygin, Alex and Wu, Jy},
  date = {2013},
  journaltitle = {Proceedings of the …},
  eprint = {24086296},
  eprinttype = {pmid},
  pages = {1631--1642},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0073791},
  url = {http://nlp.stanford.edu/},
  urldate = {2018-07-16},
  abstract = {Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80\% up to 85.4\%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7\%, an improvement of 9.7\% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.},
  isbn = {9781937284978}
}

@inproceedings{sohl-dickstein_deep_2015,
  title = {Deep Unsupervised Learning Using Nonequilibrium Thermodynamics},
  booktitle = {International {{Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  date = {2015},
  pages = {2256--2265}
}

@inproceedings{sohn_learning_2015,
  title = {Learning {{Structured Output Representation}} Using {{Deep Conditional Generative Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 28: {{Annual Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Sohn, Kihyuk and Lee, Honglak and Yan, Xinchen},
  date = {2015},
  pages = {9},
  location = {{Montreal, Quebec, Canada}},
  abstract = {Supervised deep learning has been successfully applied to many recognition problems. Although it can approximate a complex many-to-one function well when a large amount of training data is provided, it is still challenging to model complex structured output representations that effectively perform probabilistic inference and make diverse predictions. In this work, we develop a deep conditional generative model for structured output prediction using Gaussian latent variables. The model is trained efficiently in the framework of stochastic gradient variational Bayes, and allows for fast prediction using stochastic feed-forward inference. In addition, we provide novel strategies to build robust structured prediction algorithms, such as input noise-injection and multi-scale prediction objective at training. In experiments, we demonstrate the effectiveness of our proposed algorithm in comparison to the deterministic deep neural network counterparts in generating diverse but realistic structured output predictions using stochastic inference. Furthermore, the proposed training methods are complimentary, which leads to strong pixel-level object segmentation and semantic labeling performance on Caltech-UCSD Birds 200 and the subset of Labeled Faces in the Wild dataset.},
  langid = {english}
}

@inproceedings{sonderby_amortised_2017,
  title = {Amortised {{MAP}} Inference for Image Super-Resolution},
  booktitle = {Proceedings of the International Conference on Learning Representations ({{ICLR}})},
  author = {Sønderby, Casper Kaae and Caballero, Jose and Theis, Lucas and Shi, Wenzhe and Huszár, Ferenc},
  date = {2017},
  location = {{Toulon, France}},
  url = {https://openreview.net/forum?id=S1RP6GLle}
}

@inproceedings{sonderby_ladder_2016,
  title = {Ladder {{Variational Autoencoders}}},
  booktitle = {Proceedings of the 29th {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Sønderby, Casper Kaae and Raiko, Tapani and Maaløe, Lars and Sønderby, Søren Kaae and Winther, Ole},
  date = {2016-12},
  location = {{Barcelona, Spain}},
  url = {http://arxiv.org/abs/1602.02282},
  urldate = {2019-11-15},
  abstract = {Variational autoencoders are powerful models for unsupervised learning. However deep models with several layers of dependent stochastic variables are difficult to train which limits the improvements obtained using these highly expressive models. We propose a new inference model, the Ladder Variational Autoencoder, that recursively corrects the generative distribution by a data dependent approximate likelihood in a process resembling the recently proposed Ladder Network. We show that this model provides state of the art predictive log-likelihood and tighter log-likelihood lower bound compared to the purely bottom-up inference in layered Variational Autoencoders and other generative models. We provide a detailed analysis of the learned hierarchical latent representation and show that our new inference model is qualitatively different and utilizes a deeper more distributed hierarchy of latent variables. Finally, we observe that batch normalization and deterministic warm-up (gradually turning on the KL-term) are crucial for training variational models with many stochastic layers.},
  eventtitle = {Conference on {{Neural Information Processing Systems}}},
  langid = {english}
}

@article{song_dynamic_,
  title = {Dynamic {{Grained Encoder}} for {{Vision Transformers}}},
  author = {Song, Lin and Zhang, Songyang and Liu, Songtao and Li, Zeming and He, Xuming and Sun, Hongbin and Sun, Jian and Zheng, Nanning},
  pages = {14},
  abstract = {Transformers, the de-facto standard for language modeling, have been recently applied for vision tasks. This paper introduces sparse queries for vision transformers to exploit the intrinsic spatial redundancy of natural images and save computational costs. Specifically, we propose a Dynamic Grained Encoder for vision transformers, which can adaptively assign a suitable number of queries to each spatial region. Thus it achieves a fine-grained representation in discriminative regions while keeping high efficiency. Besides, the dynamic grained encoder is compatible with most vision transformer frameworks. Without bells and whistles, our encoder allows the state-of-the-art vision transformers to reduce computational complexity by 40\%-60\% while maintaining comparable performance on image classification. Extensive experiments on object detection and segmentation further demonstrate the generalizability of our approach. Code is available at https://github.com/StevenGrove/vtpack.},
  langid = {english}
}

@inproceedings{song_generative_2019,
  title = {Generative Modeling by Estimating Gradients of the Data Distribution},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32: {{Annual Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Song, Yang and Ermon, Stefano},
  editor = {Wallach, Hanna M. and Larochelle, Hugo and Beygelzimer, Alina and family=Buc, given=Florence, prefix=d'Alché-, useprefix=true and Fox, Emily B. and Garnett, Roman},
  date = {2019},
  pages = {11895--11907},
  location = {{Vancouver, BC, Canada}},
  url = {https://proceedings.neurips.cc/paper/2019/hash/3001ef257407d5a371a96dcd947c7d93-Abstract.html}
}

@misc{song_how_2021,
  title = {How to {{Train Your Energy-Based Models}}},
  author = {Song, Yang and Kingma, Diederik P.},
  date = {2021-02-17},
  eprint = {2101.03288},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2101.03288},
  urldate = {2021-08-25},
  abstract = {Energy-Based Models (EBMs), also known as non-normalized probabilistic models, specify probability density or mass functions up to an unknown normalizing constant. Unlike most other probabilistic models, EBMs do not place a restriction on the tractability of the normalizing constant, thus are more flexible to parameterize and can model a more expressive family of probability distributions. However, the unknown normalizing constant of EBMs makes training particularly difficult. Our goal is to provide a friendly introduction to modern approaches for EBM training. We start by explaining maximum likelihood training with Markov chain Monte Carlo (MCMC), and proceed to elaborate on MCMC-free approaches, including Score Matching (SM) and Noise Constrastive Estimation (NCE). We highlight theoretical connections among these three approaches, and end with a brief survey on alternative training methods, which are still under active research. Our tutorial is targeted at an audience with basic understanding of generative models who want to apply EBMs or start a research project in this direction.},
  langid = {english}
}

@misc{song_nonlinear_2020,
  title = {Nonlinear {{Equation Solving}}: {{A Faster Alternative}} to {{Feedforward Computation}}},
  shorttitle = {Nonlinear {{Equation Solving}}},
  author = {Song, Yang and Meng, Chenlin and Liao, Renjie and Ermon, Stefano},
  date = {2020-02-10},
  eprint = {2002.03629},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2002.03629},
  urldate = {2020-02-14},
  abstract = {Feedforward computations, such as evaluating a neural network or sampling from an autoregressive model, are ubiquitous in machine learning. The sequential nature of feedforward computation, however, requires a strict order of execution and cannot be easily accelerated with parallel computing. To enable parrallelization, we frame the task of feedforward computation as solving a system of nonlinear equations. We then propose to find the solution using a Jacobi or Gauss-Seidel fixed-point iteration method, as well as hybrid methods of both. Crucially, Jacobi updates operate independently on each equation and can be executed in parallel. Our method is guaranteed to give exactly the same values as the original feedforward computation with a reduced (or equal) number of parallel iterations. Experimentally, we demonstrate the effectiveness of our approach in accelerating 1) the evaluation of DenseNets on ImageNet and 2) autoregressive sampling of MADE and PixelCNN. We are able to achieve between 1.2 and 33 speedup factors under various conditions and computation models.},
  langid = {english}
}

@article{song_scorebased_2021,
  title = {Score-{{Based Generative Modeling Through Stochastic Differential Equations}}},
  author = {Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  date = {2021},
  abstract = {Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 ˆ 1024 images for the first time from a score-based generative model.},
  langid = {english}
}

@inproceedings{song_speechxlnet_2020,
  title = {Speech-{{XLNet}}: {{Unsupervised Acoustic Model Pretraining}} for {{Self-Attention Networks}}},
  shorttitle = {Speech-{{XLNet}}},
  booktitle = {Proceedings of the 21st {{Annual Conference}} of the {{International Speech Communication Association}} ({{Interspeech}})},
  author = {Song, Xingchen and Wang, Guangsen and Huang, Yiheng and Wu, Zhiyong and Su, Dan and Meng, Helen},
  date = {2020-10-25},
  pages = {3765--3769},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2020-1511},
  url = {https://www.isca-speech.org/archive/interspeech_2020/song20d_interspeech.html},
  urldate = {2021-10-12},
  langid = {english}
}

@article{soong_vector_1985,
  title = {A Vector Quantization Approach to Speaker Recognition},
  author = {Soong, F. and Rosenberg, A. and Juang, L. Rabiner; B.},
  date = {1985},
  journaltitle = {IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}
}

@inproceedings{sordoni_neural_2015,
  title = {A {{Neural Network Approach}} to {{Context-Sensitive Generation}} of {{Conversational Responses}}},
  booktitle = {Proceedings of the {{Annual Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}} ({{NAACL-HLT}})},
  author = {Sordoni, Alessandro and Galley, Michel and Auli, Michael and Brockett, Chris and Ji, Yangfeng and Mitchell, Margaret and Nie, Jian-Yun and Gao, Jianfeng and Dolan, Bill},
  date = {2015-06-22},
  eprint = {1506.06714},
  eprinttype = {arxiv},
  url = {https://arxiv.org/abs/1506.06714},
  urldate = {2018-06-06},
  abstract = {We present a novel response generation system that can be trained end to end on large quantities of unstructured Twitter conversations. A neural network architecture is used to address sparsity issues that arise when integrating contextual information into classic statistical models, allowing the system to take into account previous dialog utterances. Our dynamic-context generative models show consistent gains over both context-sensitive and non-context-sensitive Machine Translation and Information Retrieval baselines.},
  isbn = {978-1-941643-49-5}
}

@misc{sorokin_deep_2015,
  title = {Deep {{Attention Recurrent Q-Network}}},
  author = {Sorokin, Ivan and Seleznev, Alexey and Pavlov, Mikhail and Fedorov, Aleksandr and Ignateva, Anastasiia},
  date = {2015-12-05},
  eprint = {1512.01693},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1512.01693},
  urldate = {2017-11-12},
  abstract = {A deep learning approach to reinforcement learning led to a general learner able to train on visual input to play a variety of arcade games at the human and superhuman levels. Its creators at the Google DeepMind's team called the approach: Deep Q-Network (DQN). We present an extension of DQN by "soft" and "hard" attention mechanisms. Tests of the proposed Deep Attention Recurrent Q-Network (DARQN) algorithm on multiple Atari 2600 games show level of performance superior to that of DQN. Moreover, built-in attention mechanisms allow a direct online monitoring of the training process by highlighting the regions of the game screen the agent is focusing on when making decisions.}
}

@online{sorscher_neural_2022,
  title = {Beyond {{Neural Scaling Laws}}: {{Beating Power Law Scaling Via Data Pruning}}},
  shorttitle = {Beyond {{Neural Scaling Laws}}},
  author = {Sorscher, Ben and Geirhos, Robert and Shekhar, Shashank and Ganguli, Surya and Morcos, Ari S.},
  date = {2022-08-04},
  eprint = {2206.14486},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2206.14486},
  urldate = {2022-08-08},
  abstract = {Widely observed neural scaling laws, in which error falls off as a power of the training set size, model size, or both, have driven substantial performance improvements in deep learning. However, these improvements through scaling alone require considerable costs in compute and energy. Here we focus on the scaling of error with dataset size and show how both in theory and practice we can break beyond power law scaling and reduce it to exponential scaling instead if we have access to a high-quality data pruning metric that ranks the order in which training examples should be discarded to achieve any pruned dataset size. We then test this new exponential scaling prediction with pruned dataset size empirically, and indeed observe better than power law scaling performance on ResNets trained on CIFAR-10, SVHN, and ImageNet. Given the importance of finding high-quality pruning metrics, we perform the first large-scale benchmarking study of ten different data pruning metrics on ImageNet. We find most existing high performing metrics scale poorly to ImageNet, while the best are computationally intensive and require labels for every image. We therefore developed a new simple, cheap and scalable self-supervised pruning metric that demonstrates comparable performance to the best supervised metrics. Overall, our work suggests that the discovery of good data-pruning metrics may provide a viable path forward to substantially improved neural scaling laws, thereby reducing the resource costs of modern deep learning.},
  pubstate = {preprint}
}

@article{soudry_implicit_2018,
  title = {The {{Implicit Bias}} of {{Gradient Descent}} on {{Separable Data}}},
  author = {Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  date = {2018-03-21},
  url = {https://arxiv.org/abs/1710.10345},
  urldate = {2018-10-18},
  abstract = {We show that gradient descent on an unregularized logistic regression problem, for linearly separable datasets, converges to the direction of the max-margin (hard margin SVM) solution. The result generalizes also to other monotone decreasing loss functions with an infimum at infinity, to multi-class problems, and to training a weight layer in a deep network in a certain restricted setting. Furthermore, we show this convergence is very slow, and only logarithmic in the convergence of the loss itself. This can help explain the benefit of continuing to optimize the logistic or cross-entropy loss even after the training error is zero and the training loss is extremely small, and, as we show, even if the validation loss increases. Our methodology can also aid in understanding implicit regularization in more complex models and with other optimization methods.},
  langid = {english}
}

@article{spall_multivariate_1992,
  title = {Multivariate {{Stochastic Approximation Using}} a {{Simultaneous Perturbation Gradient Approximation}}},
  author = {Spall, James C.},
  date = {1992},
  journaltitle = {IEEE Transactions on Automatic Control},
  volume = {37},
  number = {3},
  pages = {332--341},
  issn = {15582523},
  doi = {10.1109/9.119632},
  abstract = {The problem of finding a root of the multivariate gradient equation that arises in function minimization is considered. When only noisy measurements of the function are available, a stochastic approximation (SA) algorithm for the general Kiefer-Wolfowitz type is appropriate for estimating the root. The paper presents an SA algorithm that is based on a simultaneous perturbation gradient approximation instead of the standard finite-difference approximation of Keifer-Wolfowitz type procedures. Theory and numerical experience indicate that the algorithm can be significantly more efficient than the standard algorithms in large-dimensional problems.}
}

@article{srivastava_dropout_2014,
  title = {Dropout: {{A Simple Way}} to {{Prevent Neural Networks}} from {{Overfitting}}},
  author = {Srivastava, Nitish and Hinton, Geoffrey E. and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
  date = {2014},
  journaltitle = {Journal of Machine Learning Research},
  volume = {15},
  pages = {1929--1958},
  abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different " thinned " networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.}
}

@inproceedings{srivastava_multimodal_2012,
  title = {Multimodal Learning with Deep {{Boltzmann}} Machines},
  booktitle = {Conference on {{Neural Information Processing Systems}} (Neurips)},
  author = {Srivastava, Nitish and Salakhutdinov, Ruslan R},
  date = {2012},
  pages = {2222--2230}
}

@misc{staines_variational_2012,
  title = {Variational {{Optimization}}},
  author = {Staines, Joe and Barber, David},
  date = {2012-12-18},
  eprint = {1212.4507},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1212.4507},
  urldate = {2017-11-27},
  abstract = {We discuss a general technique that can be used to form a differentiable bound on the optima of non-differentiable or discrete objective functions. We form a unified description of these methods and consider under which circumstances the bound is concave. In particular we consider two concrete applications of the method, namely sparse learning and support vector classification.},
  keywords = {★}
}

@article{stanfillSystematicLiteratureReview2010,
  title = {A Systematic Literature Review of Automated Clinical Coding and Classification Systems},
  author = {Stanfill, Mary H. and Williams, Margaret and Fenton, Susan H. and Jenders, Robert A. and Hersh, William R.},
  date = {2010},
  journaltitle = {Journal of the American Medical Informatics Association: JAMIA},
  volume = {17},
  number = {6},
  eprint = {20962126},
  eprinttype = {pmid},
  pages = {646--651},
  issn = {1527-974X},
  doi = {10.1136/jamia.2009.001024},
  abstract = {Clinical coding and classification processes transform natural language descriptions in clinical text into data that can subsequently be used for clinical care, research, and other purposes. This systematic literature review examined studies that evaluated all types of automated coding and classification systems to determine the performance of such systems. Studies indexed in Medline or other relevant databases prior to March 2009 were considered. The 113 studies included in this review show that automated tools exist for a variety of coding and classification purposes, focus on various healthcare specialties, and handle a wide variety of clinical document types. Automated coding and classification systems themselves are not generalizable, nor are the results of the studies evaluating them. Published research shows these systems hold promise, but these data must be considered in context, with performance relative to the complexity of the task and the desired outcome.},
  langid = {english},
  pmcid = {PMC3000748}
}

@article{stanley_hypercubebased_2009,
  title = {A {{Hypercube-Based Indirect Encoding}} for {{Evolving Large-Scale Neural Networks}}},
  author = {Stanley, Kenneth O. and D 'ambrosio, David and Gauci, Jason},
  date = {2009},
  journaltitle = {Artificial Life Journal},
  volume = {15},
  number = {2},
  eprint = {19199382},
  eprinttype = {pmid},
  pages = {1--28},
  issn = {1064-5462},
  doi = {10.1162/artl.2009.15.2.15202},
  abstract = {Research in neuroevolution, i.e. evolving artificial neural networks (ANNs) through evolutionary algo-rithms, is inspired by the evolution of biological brains. Because natural evolution discovered intelligent brains with billions of neurons and trillions of connections, perhaps neuroevolution can do the same. Yet while neuroevolution has produced successful results in a variety of domains, the scale of natural brains remains far beyond reach. This paper presents a method called Hypercube-based NeuroEvolution of Aug-menting Topologies (HyperNEAT) that aims to narrow this gap. HyperNEAT employs an indirect encoding called connective Compositional Pattern Producing Networks (connective CPPNs) that can produce con-nectivity patterns with symmetries and repeating motifs by interpreting spatial patterns generated within a hypercube as connectivity patterns in a lower-dimensional space. The advantage of this approach is that it can exploit the geometry of the task by mapping its regularities onto the topology of the network, thereby shifting problem difficulty away from dimensionality to underlying problem structure. Furthermore, con-nective CPPNs can represent the same connectivity pattern at any resolution, allowing ANNs to scale to new numbers of inputs and outputs without further evolution. HyperNEAT is demonstrated through visual dis-crimination and food gathering tasks, including successful visual discrimination networks containing over eight million connections. The main conclusion is that the ability to explore the space of regular connectivity patterns opens up a new class of complex high-dimensional tasks to neuroevolution.}
}

@article{stowell_automatic_2019,
  title = {Automatic Acoustic Detection of Birds through Deep Learning: {{The}} First {{Bird Audio Detection Challenge}}},
  author = {Stowell, Dan and Wood, Michael D and Pamuła, Hanna and Stylianou, Yannis and Glotin, Hervé},
  date = {2019},
  journaltitle = {Methods in Ecology and Evolution},
  volume = {10},
  number = {3},
  pages = {368--380},
  publisher = {{Wiley Online Library}}
}

@article{strassen_gaussian_1969,
  title = {Gaussian Elimination Is Not Optimal},
  author = {Strassen, Volker},
  date = {1969-08},
  journaltitle = {Numerische Mathematik},
  shortjournal = {Numer. Math.},
  volume = {13},
  number = {4},
  pages = {354--356},
  issn = {0029-599X, 0945-3245},
  doi = {10.1007/BF02165411},
  url = {http://link.springer.com/10.1007/BF02165411},
  urldate = {2023-07-10},
  langid = {english}
}

@misc{strauss_any_2022,
  title = {Any {{Variational Autoencoder Can Do Arbitrary Conditioning}}},
  author = {Strauss, Ryan R. and Oliva, Junier B.},
  date = {2022-01-28},
  eprint = {2201.12414},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2201.12414},
  urldate = {2022-02-09},
  abstract = {Arbitrary conditioning is an important problem in unsupervised learning, where we seek to model the conditional densities p(xu | xo) that underly some data, for all possible non-intersecting subsets o, u ⊂ \{1, . . . , d\}. However, the vast majority of density estimation only focuses on modeling the joint distribution p(x), in which important conditional dependencies between features are opaque. We propose a simple and general framework, coined Posterior Matching, that enables any Variational Autoencoder (VAE) to perform arbitrary conditioning, without modification to the VAE itself. Posterior Matching applies to the numerous existing VAE-based approaches to joint density estimation, thereby circumventing the specialized models required by previous approaches to arbitrary conditioning. We find that Posterior Matching achieves performance that is comparable or superior to current state-of-the-art methods for a variety of tasks.},
  langid = {english}
}

@report{su_differential_2015,
  title = {A {{Differential Equation}} for {{Modeling Nesterov}}'s {{Accelerated Gradient Method}}: {{Theory}} and {{Insights}}},
  author = {Su, Weijie and Boyd, Stephen and Candès, Emmanuel J},
  date = {2015},
  eprint = {1503.01243v2},
  eprinttype = {arxiv},
  abstract = {We derive a second-order ordinary differential equation (ODE) which is the limit of Nes-terov's accelerated gradient method. This ODE exhibits approximate equivalence to Nes-terov's scheme and thus can serve as a tool for analysis. We show that the continuous time ODE allows for a better understanding of Nesterov's scheme. As a byproduct, we obtain a family of schemes with similar convergence rates. The ODE interpretation also suggests restarting Nesterov's scheme leading to an algorithm, which can be rigorously proven to converge at a linear rate whenever the objective is strongly convex.}
}

@misc{subbaswamy_preventing_2019,
  title = {Preventing {{Failures Due}} to {{Dataset Shift}}: {{Learning Predictive Models That Transport}}},
  shorttitle = {Preventing {{Failures Due}} to {{Dataset Shift}}},
  author = {Subbaswamy, Adarsh and Schulam, Peter and Saria, Suchi},
  date = {2019-02-28},
  eprint = {1812.04597},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1812.04597},
  urldate = {2021-05-07},
  abstract = {Classical supervised learning produces unreliable models when training and target distributions differ, with most existing solutions requiring samples from the target domain. We propose a proactive approach which learns a relationship in the training domain that will generalize to the target domain by incorporating prior knowledge of aspects of the data generating process that are expected to differ as expressed in a causal selection diagram. Specifically, we remove variables generated by unstable mechanisms from the joint factorization to yield the Surgery Estimator—an interventional distribution that is invariant to the differences across environments. We prove that the surgery estimator finds stable relationships in strictly more scenarios than previous approaches which only consider conditional relationships, and demonstrate this in simulated experiments. We also evaluate on real world data for which the true causal diagram is unknown, performing competitively against entirely data-driven approaches.},
  langid = {english}
}

@misc{such_deep_2017,
  title = {Deep {{Neuroevolution}}: {{Genetic Algorithms Are}} a {{Competitive Alternative}} for {{Training Deep Neural Networks}} for {{Reinforcement Learning}}},
  author = {Such, Felipe Petroski and Madhavan, Vashisht and Conti, Edoardo and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
  date = {2017-12-18},
  eprint = {1712.06567},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1712.06567},
  urldate = {2018-01-03},
  abstract = {Deep artificial neural networks (DNNs) are typically trained via gradient-based learning algorithms, namely backpropagation. Evolution strategies (ES) can rival backprop-based algorithms such as Q-learning and policy gradients on challenging deep reinforcement learning (RL) problems. However, ES can be considered a gradient-based algorithm because it performs stochastic gradient descent via an operation similar to a finite-difference approximation of the gradient. That raises the question of whether non-gradient-based evolutionary algorithms can work at DNN scales. Here we demonstrate they can: we evolve the weights of a DNN with a simple, gradient-free, population-based genetic algorithm (GA) and it performs well on hard deep RL problems, including Atari and humanoid locomotion. The Deep GA successfully evolves networks with over four million free parameters, the largest neural networks ever evolved with a traditional evolutionary algorithm. These results (1) expand our sense of the scale at which GAs can operate, (2) suggest intriguingly that in some cases following the gradient is not the best choice for optimizing performance, and (3) make immediately available the multitude of neuroevolution techniques that improve performance. We demonstrate the latter by showing that combining DNNs with novelty search, which encourages exploration on tasks with deceptive or sparse reward functions, can solve a high-dimensional problem on which reward-maximizing algorithms (e.g.\textbackslash{} DQN, A3C, ES, and the GA) fail. Additionally, the Deep GA is faster than ES, A3C, and DQN (it can train Atari in \$\{\textbackslash raise.17ex\textbackslash hbox\{\$\textbackslash scriptstyle\textbackslash sim\$\}\}\$4 hours on one desktop or \$\{\textbackslash raise.17ex\textbackslash hbox\{\$\textbackslash scriptstyle\textbackslash sim\$\}\}\$1 hour distributed on 720 cores), and enables a state-of-the-art, up to 10,000-fold compact encoding technique.}
}

@article{sugiyama_dimensionality_2010,
  title = {Dimensionality {{Reduction}} for {{Density Ratio Estimation}} in {{High-dimensional Spaces}}},
  author = {Sugiyama, Masashi and Kawanabe, Motoaki and Chui, Pui Ling},
  date = {2010},
  journaltitle = {Proceedings of the International Joint Conference on Neural Networks (IJCNN)},
  volume = {23},
  number = {1},
  eprint = {19631506},
  eprinttype = {pmid},
  pages = {44--59},
  issn = {08936080},
  doi = {10.1016/j.neunet.2009.07.007},
  abstract = {The ratio of two probability density functions is becoming a quantity of interest these days in the machine learning and data mining communities since it can be used for various data processing tasks such as non-stationarity adaptation, outlier detection, and feature selection. Recently, several methods have been developed for directly estimating the density ratio without going through density estimation and were shown to work well in various practical problems. However, these methods still perform rather poorly when the dimensionality of the data domain is high. In this paper, we propose to incorporate a dimensionality reduction scheme into a density-ratio estimation procedure and experimentally show that the estimation accuracy in high-dimensional cases can be improved. © 2009 Elsevier Ltd. All rights reserved.}
}

@inproceedings{sukhbaatar_endtoend_2015,
  title = {End-{{To-End Memory Networks}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Neural Information Processing Systems}} ({{NIPS}})},
  author = {Sukhbaatar, Sainbayar and Szlam, Arthur and Weston, Jason and Fergus, Rob},
  date = {2015-03-30},
  eprint = {9377276},
  eprinttype = {pmid},
  issn = {10495258},
  doi = {v5},
  url = {http://arxiv.org/abs/1503.08895},
  urldate = {2018-06-07},
  abstract = {We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network (Weston et al., 2015) but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.},
  isbn = {1551-6709}
}

@inproceedings{sun_deep_2013,
  title = {Deep {{Convolutional Network Cascade}} for {{Facial Point Detection}}},
  booktitle = {2013 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Sun, Yi and Wang, Xiaogang and Tang, Xiaoou},
  date = {2013-06},
  pages = {3476--3483},
  publisher = {{IEEE}},
  location = {{Portland, OR, USA}},
  doi = {10.1109/CVPR.2013.446},
  url = {http://ieeexplore.ieee.org/document/6619290/},
  urldate = {2022-03-30},
  eventtitle = {2013 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-0-7695-4989-7},
  langid = {english}
}

@inproceedings{sun_efficient_2009,
  title = {Efficient {{Natural Evolution Strategies}}},
  booktitle = {Proceedings of the {{Conference}} on {{Genetic}} and {{Evolutionary Computation}} ({{GECCO}})},
  author = {Sun, Yi and Wierstra, Daan and Schaul, Tom and Schmidhuber, Jürgen},
  date = {2009},
  eprint = {1209.5853v1},
  eprinttype = {arxiv},
  pages = {539},
  doi = {10.1145/1569901.1569976},
  url = {https://arxiv.org/abs/1209.5853},
  urldate = {2018-04-17},
  abstract = {Efficient Natural Evolution Strategies (eNES) is a novel alternative to conventional evolutionary algorithms, using the natural gradient to adapt the mutation distribution. Unlike previous methods based on natural gradients, eNES uses a fast algorithm to calculate the inverse of the exact Fisher information matrix, thus increasing both robustness and performance of its evolution gradient estimation, even in higher dimensions. Additional novel aspects of eNES include optimal fitness baselines and importance mixing (a procedure for updating the population with very few fitness evaluations). The algorithm yields competitive results on both unimodal and multimodal benchmarks.},
  isbn = {978-1-60558-325-9}
}

@article{sun_react_2021,
  title = {React: {{Out-of-distribution}} Detection with Rectified Activations},
  author = {Sun, Yiyou and Guo, Chuan and Li, Yixuan},
  date = {2021},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {34},
  pages = {144--157}
}

@misc{sun_training_2018,
  title = {Training {{Augmentation}} with {{Adversarial Examples}} for {{Robust Speech Recognition}}},
  author = {Sun, Sining and Yeh, Ching-Feng and Ostendorf, Mari and Hwang, Mei-Yuh and Xie, Lei},
  date = {2018-06-07},
  eprint = {1806.02782},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1806.02782},
  urldate = {2019-01-11},
  abstract = {This paper explores the use of adversarial examples in training speech recognition systems to increase robustness of deep neural network acoustic models. During training, the fast gradient sign method is used to generate adversarial examples augmenting the original training data. Different from conventional data augmentation based on data transformations, the examples are dynamically generated based on current acoustic model parameters. We assess the impact of adversarial data augmentation in experiments on the Aurora-4 and CHiME-4 single-channel tasks, showing improved robustness against noise and channel variation. Further improvement is obtained when combining adversarial examples with teacher/student training, leading to a 23\% relative word error rate reduction on Aurora-4.}
}

@misc{surfingtechcoltd_stcmds_,
  title = {{{ST-CMDS}}: {{Free ST Chinese Mandarin Corpus}}},
  author = {{Surfingtech Co., Ltd.}}
}

@inproceedings{suri_counting_2011,
  title = {Counting Triangles and the Curse of the Last Reducer},
  booktitle = {Proceedings of the 20th International Conference on {{World}} Wide Web - {{WWW}} '11},
  author = {Suri, Siddharth and Vassilvitskii, Sergei},
  date = {2011},
  pages = {607},
  publisher = {{ACM Press}},
  location = {{New York, New York, USA}},
  doi = {10.1145/1963405.1963491},
  url = {http://portal.acm.org/citation.cfm?doid=1963405.1963491},
  urldate = {2016-10-14},
  isbn = {978-1-4503-0632-4}
}

@inproceedings{sutskever_importance_2013,
  title = {On the {{Importance}} of {{Initialization}} and {{Momentum}} in {{Deep Learning}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Sutskever, Ilya and Martens, James and Dahl, George E. and Hinton, Geoffrey E.},
  date = {2013},
  volume = {28},
  pages = {8609--8613},
  abstract = {Recently, pre-trained deep neural networks (DNNs) have outperformed traditional acoustic models based on Gaussian mixture models (GMMs) on a variety of large vocabulary speech recognition benchmarks. Deep neural nets have also achieved excellent results on various computer vision tasks using a random “dropout” procedure that drastically improves generalization error by randomly omitting a fraction of the hidden units in all layers. Since dropout helps avoid overfitting, it has also been successful on a small-scale phone recognition task using larger neural nets. However, training deep neural net acoustic models for large vocabulary speech recognition takes a very long time and dropout is likely to only increase training time. Neural networks with rectified linear unit (ReLU) non-linearities have been highly successful for computer vision tasks and proved faster to train than standard sigmoid units, sometimes also improving discriminative performance. In this work, we show on a 50-hour English Broadcast News task that modified deep neural networks using ReLUs trained with dropout during frame level training provide an 4.2\% relative improvement over a DNN trained with sigmoid units, and a 14.4\% relative improvement over a strong GMM/HMM system. We were able to obtain our results with minimal human hyper-parameter tuning using publicly available Bayesian optimization code}
}

@misc{sutskever_principled_2015,
  title = {Towards {{Principled Unsupervised Learning}}},
  author = {Sutskever, Ilya and Jozefowicz, Rafal and Gregor, Karol and Rezende, Danilo and Lillicrap, Tim and Vinyals, Oriol},
  date = {2015-12-03},
  eprint = {1511.06440},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1511.06440},
  urldate = {2021-03-12},
  abstract = {General unsupervised learning is a long-standing conceptual problem in machine learning. Supervised learning is successful because it can be solved by the minimization of the training error cost function. Unsupervised learning is not as successful, because the unsupervised objective may be unrelated to the supervised task of interest. For an example, density modelling and reconstruction have often been used for unsupervised learning, but they did not produced the sought-after performance gains, because they have no knowledge of the supervised tasks.},
  langid = {english}
}

@inproceedings{sutskever_sequence_2014,
  title = {Sequence to Sequence Learning with Neural Networks},
  booktitle = {Proceedings of the {{Conference}} on {{Neural Information Processing Systems}} ({{NIPS}})},
  author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
  date = {2014},
  eprint = {1409.3215v3},
  eprinttype = {arxiv},
  pages = {3104--3112},
  url = {https://arxiv.org/abs/1409.3215},
  urldate = {2018-04-01},
  abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excel-lent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the pas-sive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.}
}

@misc{sutter_generalized_2021,
  title = {Generalized {{Multimodal ELBO}}},
  author = {Sutter, Thomas M. and Daunhawer, Imant and Vogt, Julia E.},
  date = {2021-05-06},
  eprint = {2105.02470},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2105.02470},
  urldate = {2021-05-18},
  abstract = {Multiple data types naturally co-occur when describing real-world phenomena and learning from them is a long-standing goal in machine learning research. However, existing self-supervised generative models approximating an ELBO are not able to fulfill all desired requirements of multimodal models: their posterior approximation functions lead to a trade-off between the semantic coherence and the ability to learn the joint data distribution. We propose a new, generalized ELBO formulation for multimodal data that overcomes these limitations. The new objective encompasses two previous methods as special cases and combines their benefits without compromises. In extensive experiments, we demonstrate the advantage of the proposed method compared to state-of-the-art models in selfsupervised, generative learning tasks.},
  langid = {english}
}

@article{sutton_learning_1988,
  title = {Learning to {{Predict}} by the {{Method}} of {{Temporal Differences}}},
  author = {Sutton, Richard S.},
  date = {1988},
  journaltitle = {Journal of Machine Learning},
  volume = {3},
  number = {1},
  eprint = {22182453},
  eprinttype = {pmid},
  pages = {9--44},
  issn = {08856125},
  doi = {10.1023/A:1018056104778},
  url = {citeseer.ist.psu.edu/sutton88learning.html},
  urldate = {2017-09-27},
  abstract = {This article introduces a class of incremental learning procedures specialized for prediction--that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the differnce between \{\textbackslash em temporally succesive predictions\}. Although such \{\textbackslash em temporal-difference methods\} have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods; \{\textbackslash em and\} they produce more accurate predictions. We argue that most problems to which uspervised learning is currently applied are really prediction problems of the sort to which temporal-difference methods can be applied to advantage.},
  isbn = {0885-6125}
}

@book{sutton_reinforcement_1998,
  title = {Reinforcement {{Learning}}: {{An Introduction}}},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  date = {1998},
  issn = {1600-0579},
  abstract = {Reinforcement learning, one of the most active research areas in artificialintelligence, is a computational approach to learning whereby an agent tries to maximize the totalamount of reward it receives when interacting with a complex, uncertain environment. InReinforcement Learning, Richard Sutton and Andrew Barto provide a clear andsimple account of the key ideas and algorithms of reinforcement learning. Their discussion rangesfrom the history of the field's intellectual foundations to the most recent developments andapplications. The only necessary mathematical background is familiarity with elementary concepts ofprobability.},
  isbn = {978-0-262-19398-6}
}

@book{sutton_reinforcement_2018,
  title = {Reinforcement {{Learning}} - {{An Introduction}}},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  date = {2018},
  series = {Adaptive {{Computation}} and {{Machine Learning}}},
  edition = {2},
  publisher = {{MIT Press}}
}

@misc{suzuki_information_2014,
  title = {Information {{Geometry}} and {{Statistical Manifold}}},
  author = {Suzuki, Mashbat},
  date = {2014-10-09},
  eprint = {1410.3369},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1410.3369},
  urldate = {2018-05-12},
  abstract = {We review basic notions in the field of information geometry such as Fisher metric on statistical manifold, \$\textbackslash alpha\$-connection and corresponding curvature following Amari's work . We show application of information geometry to asymptotic statistical inference.}
}

@inproceedings{synnaeve_learning_2014,
  title = {Learning Words from Images and Speech},
  booktitle = {Conference on {{Neural Information Processing Systems}} ({{NeurIPS}}), {{Workshop}} on {{Learning Semantics}}},
  author = {Synnaeve, Gabriel and Versteegh, Maarten and Dupoux, Emmanuel},
  date = {2014}
}

@inproceedings{szegedy_going_2015,
  title = {Going Deeper with Convolutions},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  date = {2015},
  pages = {1--9}
}

@misc{szegedy_inceptionv4_2016,
  title = {Inception-v4, Inception-Resnet and the Impact of Residual Connections on Learning},
  author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alex},
  date = {2016},
  eprint = {1602.07261},
  eprinttype = {arxiv},
  url = {https://arxiv.org/abs/1602.07261}
}

@misc{szegedy_intriguing_2013,
  title = {Intriguing Properties of Neural Networks},
  author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian J. and Fergus, Rob},
  date = {2013-12-20},
  eprint = {22545027},
  eprinttype = {pmid},
  url = {http://arxiv.org/abs/1312.6199},
  urldate = {2017-09-09},
  abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
  isbn = {1549-9618}
}

@article{szegedy_rethinking_2015,
  title = {Rethinking the {{Inception Architecture}} for {{Computer Vision}}},
  author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
  date = {2015},
  journaltitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  eprint = {8190083},
  eprinttype = {pmid},
  pages = {2818--2826},
  issn = {08866236},
  doi = {10.1109/CVPR.2016.308},
  url = {http://arxiv.org/abs/1512.00567},
  abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2\% top-1 and 5.6\% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5\% top-5 error on the validation set (3.6\% error on the test set) and 17.3\% top-1 error on the validation set.},
  isbn = {978-1-4673-8851-1}
}

@article{tachbelie_using_2014,
  title = {Using Different Acoustic, Lexical and Language Modeling Units for {{ASR}} of an under-Resourced Language–{{Amharic}}},
  author = {Tachbelie, Martha Yifiru and Abate, Solomon Teferra and Besacier, Laurent},
  date = {2014},
  journaltitle = {Speech Communication},
  volume = {56},
  pages = {181--194},
  publisher = {{Elsevier}}
}

@article{tack_csi_2020,
  title = {{{CSI}}: {{Novelty}} Detection via Contrastive Learning on Distributionally Shifted Instances},
  author = {Tack, Jihoon and Mo, Sangwoo and Jeong, Jongheon and Shin, Jinwoo},
  date = {2020},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {33},
  pages = {11839--11852}
}

@article{tagliasacchi_pretraining_2020,
  title = {Pre-Training Audio Representations with Self-Supervision},
  author = {Tagliasacchi, Marco and Gfeller, Beat and Quitry, Félix de Chaumont and Roblek, Dominik},
  date = {2020},
  journaltitle = {IEEE Signal Processing Letters},
  volume = {27},
  pages = {600--604},
  issn = {1558-2361},
  doi = {10.1109/LSP.2020.2985586},
  abstract = {We explore self-supervision as a way to learn general purpose audio representations. Specifically, we propose two self-supervised tasks: Audio2Vec, which aims at reconstructing a spectrogram slice from past and future slices and TemporalGap, which estimates the distance between two short audio segments extracted at random from the same audio clip. We evaluate how the representations learned via self-supervision transfer to different downstream tasks, either training a task-specific linear classifier on top of the pretrained embeddings, or fine-tuning a model end-to-end for each downstream task. Our results show that the representations learned with Audio2Vec transfer better than those learned by fully-supervised training on Audioset. In addition, by fine-tuning Audio2Vec representations it is possible to outperform fully-supervised models trained from scratch on each task, when limited data is available, thus improving label efficiency.},
  eventtitle = {{{IEEE Signal Processing Letters}}}
}

@online{tagliasacchi_selfsupervised_2019,
  title = {Self-Supervised Audio Representation Learning for Mobile Devices},
  author = {Tagliasacchi, Marco and Gfeller, Beat and Quitry, Félix de Chaumont and Roblek, Dominik},
  date = {2019},
  eprint = {1905.11796},
  eprinttype = {arxiv},
  pubstate = {preprint}
}

@inproceedings{taigman_deepface_2014,
  title = {Deepface: {{Closing}} the Gap to Human-Level Performance in Face Verification},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Taigman, Yaniv and Yang, Ming and Ranzato, Marc'Aurelio and Wolf, Lior},
  date = {2014},
  pages = {1701--1708}
}

@inproceedings{talnikar_joint_2021,
  title = {Joint Masked {{CPC}} and {{CTC}} Training for {{ASR}}},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Talnikar, Chaitanya and Likhomanenko, Tatiana and Collobert, Ronan and Synnaeve, Gabriel},
  date = {2021},
  pages = {3045--3049},
  publisher = {{IEEE}}
}

@misc{tan_survey_2021,
  title = {A {{Survey}} on {{Neural Speech Synthesis}}},
  author = {Tan, Xu and Qin, Tao and Soong, Frank and Liu, Tie-Yan},
  date = {2021-06-30},
  eprint = {2106.15561},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2106.15561},
  urldate = {2021-07-08},
  abstract = {Text to speech (TTS), or speech synthesis, which aims to synthesize intelligible and natural speech given text, is a hot research topic in speech, language, and machine learning communities and has broad applications in the industry. As the development of deep learning and artificial intelligence, neural network-based TTS has significantly improved the quality of synthesized speech in recent years. In this paper, we conduct a comprehensive survey on neural TTS, aiming to provide a good understanding of current research and future trends. We focus on the key components in neural TTS, including text analysis, acoustic models, and vocoders, and several advanced topics, including fast TTS, low-resource TTS, robust TTS, expressive TTS, and adaptive TTS, etc. We further summarize resources related to TTS (e.g., datasets, opensource implementations) and discuss future research directions. This survey can serve both academic researchers and industry practitioners working on TTS.},
  langid = {english}
}

@article{tanaka_fisher_2013,
  title = {Fisher Vector Based on Full-Covariance Gaussian Mixture Model},
  author = {Tanaka, Masayuki and Torii, Akihiko and Okutomi, Masatoshi},
  date = {2013},
  journaltitle = {Information and Media Technologies},
  volume = {8},
  number = {4},
  pages = {1041--1045},
  publisher = {{Information and Media Technologies Editorial Board}}
}

@online{tang_quadtree_2022,
  title = {{{QuadTree Attention}} for {{Vision Transformers}}},
  author = {Tang, Shitao and Zhang, Jiahui and Zhu, Siyu and Tan, Ping},
  date = {2022-03-23},
  eprint = {2201.02767},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2201.02767},
  urldate = {2022-04-26},
  abstract = {Transformers have been successful in many vision tasks, thanks to their capability of capturing long-range dependency. However, their quadratic computational complexity poses a major obstacle for applying them to vision tasks requiring dense predictions, such as object detection, feature matching, stereo, etc. We introduce QuadTree Attention, which reduces the computational complexity from quadratic to linear. Our quadtree transformer builds token pyramids and computes attention in a coarse-to-fine manner. At each level, the top K patches with the highest attention scores are selected, such that at the next level, attention is only evaluated within the relevant regions corresponding to these top K patches. We demonstrate that quadtree attention achieves state-of-the-art performance in various vision tasks, e.g. with 4.0\% improvement in feature matching on ScanNet, about 50\% flops reduction in stereo matching, 0.4-1.5\% improvement in top-1 accuracy on ImageNet classification, 1.2-1.8\% improvement on COCO object detection, and 0.7-2.4\% improvement on semantic segmentation over previous state-of-the-art transformers. The codes are available at https://github.com/Tangshitao/QuadtreeAttention.},
  pubstate = {preprint},
  keywords = {Unread}
}

@article{tarvainen_mean_2017,
  title = {Mean Teachers Are Better Role Models: {{Weight-averaged}} Consistency Targets Improve Semi-Supervised Deep Learning Results},
  shorttitle = {Mean Teachers Are Better Role Models},
  author = {Tarvainen, Antti and Valpola, Harri},
  date = {2017-03-06},
  url = {https://arxiv.org/abs/1703.01780},
  urldate = {2018-09-19},
  langid = {english}
}

@article{tascikaraoglu_review_2014,
  title = {A Review of Combined Approaches for Prediction of Short-Term Wind Speed and Power},
  author = {Tascikaraoglu, A. and Uzunoglu, M.},
  date = {2014},
  journaltitle = {Renewable and Sustainable Energy Reviews},
  volume = {34},
  pages = {243--254},
  doi = {10.1016/j.rser.2014.03.033},
  abstract = {With the continuous increase of wind power penetration in power systems, the problems caused by the volatile nature of wind speed and its occurrence in the system operations such as scheduling and dispatching have drawn attention of system operators, utilities and researchers towards the state-of-the-art wind speed and power forecasting methods. These methods have the required capability of reducing the influence of the intermittent wind power on system operations as well as of harvesting the wind energy effectively. In this context, combining different methodologies in order to circumvent the challenging model selection and take advantage of the unique strength of plausible models have recently emerged as a promising research area. Therefore, a comprehensive research about the combined models is called on for how these models are constructed and affect the forecasting performance. Aiming to fill the mentioned research gap, this paper outlines the combined forecasting approaches and presents an up-to date annotated bibliography of the wind forecasting literature. Furthermore, the paper also points out the possible further research directions of combined techniques so as to help the researchers in the field develop more effective wind speed and power forecasting methods.}
}

@online{tay_efficient_2022,
  title = {Efficient {{Transformers}}: {{A Survey}}},
  shorttitle = {Efficient {{Transformers}}},
  author = {Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
  date = {2022-03-14},
  eprint = {2009.06732},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2009.06732},
  urldate = {2022-04-05},
  abstract = {Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of "X-former" models have been proposed - Reformer, Linformer, Performer, Longformer, to name a few - which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this paper characterizes a large and thoughtful selection of recent efficiency-flavored "X-former" models, providing an organized and comprehensive overview of existing work and models across multiple domains.},
  pubstate = {preprint}
}

@article{tengExplainablePredictionMedical2020,
  title = {Explainable {{Prediction}} of {{Medical Codes With Knowledge Graphs}}},
  author = {Teng, Fei and Yang, Wei and Chen, L. and Huang, Lufei and Xu, Qiang},
  date = {2020},
  journaltitle = {Frontiers in Bioengineering and Biotechnology},
  doi = {10.3389/fbioe.2020.00867},
  abstract = {A new method called G\_Coder integrates information across medical records using Multi-CNN and embeds knowledge into ICD codes, suggesting that the knowledge graph significantly improves the precision of code prediction and reduces the working pressure of the human coders. International Classification of Diseases (ICD) is an authoritative health care classification system of different diseases. It is widely used for disease and health records, assisted medical reimbursement decisions, and collecting morbidity and mortality statistics. The most existing ICD coding models only translate the simple diagnosis descriptions into ICD codes. And it obscures the reasons and details behind specific diagnoses. Besides, the label (code) distribution is uneven. And there is a dependency between labels. Based on the above considerations, the knowledge graph and attention mechanism were expanded into medical code prediction to improve interpretability. In this study, a new method called G\_Coder was presented, which mainly consists of Multi-CNN, graph presentation, attentional matching, and adversarial learning. The medical knowledge graph was constructed by extracting entities related to ICD-9 from freebase. Ontology contains 5 entity classes, which are disease, symptom, medicine, surgery, and examination. The result of G\_Coder on the MIMIC-III dataset showed that the micro-F1 score is 69.2\% surpassing the state of art. The following conclusions can be obtained through the experiment: G\_Coder integrates information across medical records using Multi-CNN and embeds knowledge into ICD codes. Adversarial learning is used to generate the adversarial samples to reconcile the writing styles of doctor. With the knowledge graph and attention mechanism, most relevant segments of medical codes can be explained. This suggests that the knowledge graph significantly improves the precision of code prediction and reduces the working pressure of the human coders.}
}

@article{tengReviewDeepNeural2022,
  title = {A Review on Deep Neural Networks for {{ICD}} Coding},
  author = {Teng, Fei and Liu, Yiming and Li, Tianrui and Zhang, Yi and Li, Shuangqing and Zhao, Yue},
  date = {2022},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  pages = {1--1},
  issn = {1041-4347, 1558-2191, 2326-3865},
  doi = {10.1109/TKDE.2022.3148267}
}

@article{terrell_gradient_2002,
  title = {The {{Gradient Statistic}}},
  author = {Terrell, George R},
  date = {2002},
  journaltitle = {Comput. Sci. Stat.},
  volume = {34},
  pages = {206--215}
}

@inproceedings{theis_note_2016,
  title = {A Note on the Evaluation of Generative Models},
  booktitle = {Proceedings of the 4th {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Theis, Lucas and family=Oord, given=Aäron, prefix=van den, useprefix=false and Bethge, Matthias},
  date = {2016-05},
  location = {{San Juan, Puerto Rico}},
  url = {http://arxiv.org/abs/1511.01844},
  urldate = {2020-01-16},
  abstract = {Probabilistic generative models can be used for compression, denoising, inpainting, texture synthesis, semi-supervised learning, unsupervised feature learning, and other tasks. Given this wide range of applications, it is not surprising that a lot of heterogeneity exists in the way these models are formulated, trained, and evaluated. As a consequence, direct comparison between models is often difficult. This article reviews mostly known but often underappreciated properties relating to the evaluation and interpretation of generative models with a focus on image models. In particular, we show that three of the currently most commonly used criteria—average log-likelihood, Parzen window estimates, and visual fidelity of samples—are largely independent of each other when the data is high-dimensional. Good performance with respect to one criterion therefore need not imply good performance with respect to the other criteria. Our results show that extrapolation from one criterion to another is not warranted and generative models need to be evaluated directly with respect to the application(s) they were intended for. In addition, we provide examples demonstrating that Parzen window estimates should generally be avoided.},
  langid = {english}
}

@unpublished{theodorakopoulos_parsimonious_2017,
  title = {Parsimonious {{Inference}} on {{Convolutional Neural Networks}}: {{Learning}} and Applying on-Line Kernel Activation Rules},
  shorttitle = {Parsimonious {{Inference}} on {{Convolutional Neural Networks}}},
  author = {Theodorakopoulos, I. and Pothos, V. and Kastaniotis, D. and Fragoulis, N.},
  date = {2017-01-31},
  eprint = {1701.05221},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1701.05221},
  urldate = {2022-03-30},
  abstract = {A new, radical CNN design approach is presented in this paper, considering the reduction of the total computational load during inference. This is achieved by a new holistic intervention on both the CNN architecture and the training procedure, which targets to the parsimonious inference by learning to exploit or remove the redundant capacity of a CNN architecture. This is accomplished, by the introduction of a new structural element that can be inserted as an add-on to any contemporary CNN architecture, whilst preserving or even improving its recognition accuracy. Our approach formulates a systematic and data-driven method for developing CNNs that are trained to eventually change size and form in real-time during inference, targeting to the smaller possible computational footprint. Results are provided for the optimal implementation on a few modern, high-end mobile computing platforms indicating a significant speed-up of up to x3 times.}
}

@inproceedings{thomas_efficient_2022,
  title = {Efficient {{Adapter Transfer}} of {{Self-Supervised Speech Models}} for {{Automatic Speech Recognition}}},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Thomas, Bethan and Kessler, Samuel and Karout, Salah},
  date = {2022}
}

@article{thorpe_introduction_nodate,
  title = {Introduction to {{Optimal Transport}}},
  author = {Thorpe, Matthew},
  pages = {56},
  langid = {english}
}

@article{tieleman_lecture_2012,
  title = {Lecture 6.5-{{RMSProp}}: {{Divide}} the Gradient by a Running Average of Its Recent Magnitude},
  author = {Tieleman, Tijmen and Hinton, Geoffrey and others},
  date = {2012},
  journaltitle = {Coursera: Neural Networks for Machine Learning}
}

@article{tipping_probabilistic_1999,
  title = {Probabilistic Principal Component Analysis},
  author = {Tipping, Michael E and Bishop, Christopher M},
  date = {1999},
  journaltitle = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume = {61},
  number = {3},
  pages = {611--622},
  publisher = {{Oxford University Press}}
}

@article{tishby_consistent_1988,
  title = {Consistent {{Inference}} of {{Probabilities}} in {{Layered Networks}}: {{Predictions}} and {{Generalization}}},
  author = {Tishby, Naftali and Levin, Esther and Solla, Sara A.},
  date = {1988},
  abstract = {The problem of learning a general input-output relation using a " layered neural network " is discussed in a statistical framework. By imposing the consistency condition that the error minimization be equivalent to a likelihood maximization for training the network, we anive at a Gibbs distribution on a canonical ensemble of networks with the same architecture. This statistical description enables us to evaluate the probability of a correct prediction of an independent example, after training the network on a given training set. The prediction probability is highly correlated with the generalization ability of the network, as measured outside the mining set. This suggests a general and practical criterion for training layered networks by minimizing prediction errors. We demonstrate the utility of this criterion for selecting the optimal architecture in the contiguity problem. As a theoretical application of the statistical formalism, we discuss the question of learning curves and estimate the sufficient training size needed for correct generalization, in a simple example.}
}

@article{tishby_deep_2015,
  title = {Deep {{Learning}} and the {{Information Bottleneck Principle}}},
  author = {Tishby, Naftali and Zaslavsky, Noga},
  date = {2015-03-09},
  url = {https://arxiv.org/abs/1503.02406},
  urldate = {2018-10-18},
  langid = {english}
}

@article{titsias_bayesian_2010,
  title = {Bayesian {{Gaussian Process Latent Variable Model}}},
  author = {Titsias, Michalis K and Lawrence, Neil D},
  date = {2010},
  journaltitle = {Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS)},
  pages = {8},
  abstract = {We introduce a variational inference framework for training the Gaussian process latent variable model and thus performing Bayesian nonlinear dimensionality reduction. This method allows us to variationally integrate out the input variables of the Gaussian process and compute a lower bound on the exact marginal likelihood of the nonlinear latent variable model. The maximization of the variational lower bound provides a Bayesian training procedure that is robust to overfitting and can automatically select the dimensionality of the nonlinear latent space. We demonstrate our method on real world datasets. The focus in this paper is on dimensionality reduction problems, but the methodology is more general. For example, our algorithm is immediately applicable for training Gaussian process models in the presence of missing or uncertain inputs.},
  langid = {english}
}

@inproceedings{tjandra_endtoend_2019,
  title = {End-to-End Feedback Loss in Speech Chain Framework via Straight-through Estimator},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Tjandra, Andros and Sakti, Sakriani and Nakamura, Satoshi},
  date = {2019},
  pages = {6281--6285}
}

@inproceedings{tjandra_listening_2017,
  title = {Listening While Speaking: {{Speech}} Chain by Deep Learning},
  booktitle = {{{IEEE Workshop}} on {{Automatic Speech Recognition}} and {{Understanding}} ({{ASRU}})},
  author = {Tjandra, Andros and Sakti, Sakriani and Nakamura, Satoshi},
  date = {2017},
  pages = {301--308}
}

@inproceedings{tjandra_machine_2018,
  title = {Machine Speech Chain with One-Shot Speaker Adaptation},
  booktitle = {Proceedings of the 19th {{Annual Conference}} of the {{International Speech Communication Association}} ({{Interspeech}})},
  author = {Tjandra, Andros and Sakti, Sakriani and Nakamura, Satoshi},
  editor = {Yegnanarayana, B.},
  date = {2018},
  pages = {887--891},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2018-1558},
  url = {https://doi.org/10.21437/Interspeech.2018-1558}
}

@inproceedings{tjandra_transformer_2020,
  title = {Transformer {{VQ-VAE}} for Unsupervised Unit Discovery and Speech Synthesis: {{ZeroSpeech}} 2020 Challenge},
  booktitle = {Annual {{Conference}} of the {{International Speech Communication Association}}},
  author = {Tjandra, Andros and Sakti, Sakriani and Nakamura, Satoshi},
  date = {2020}
}

@inproceedings{tjandra_vqvae_2019,
  title = {{{VQVAE}} Unsupervised Unit Discovery and Multi-Scale {{Code2Spec}} Inverter for {{ZeroSpeech}} Challenge 2019},
  booktitle = {{{IEEE Annual Conference}} of the {{International Speech Communication Association}} ({{Interspeech}})},
  author = {Tjandra, Andros and Sisman, Berrak and Zhang, Mingyang and Sakti, Sakriani and Li, Haizhou and Nakamura, Satoshi},
  date = {2019}
}

@misc{tolstikhin_mlpmixer_2021,
  title = {{{MLP-Mixer}}: {{An}} All-{{MLP Architecture}} for {{Vision}}},
  shorttitle = {{{MLP-Mixer}}},
  author = {Tolstikhin, Ilya and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Keysers, Daniel and Uszkoreit, Jakob and Lucic, Mario and Dosovitskiy, Alexey},
  date = {2021-05-04},
  eprint = {2105.01601},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2105.01601},
  urldate = {2021-05-12},
  abstract = {Convolutional Neural Networks (CNNs) are the go-to model for computer vision. Recently, attention-based networks, such as the Vision Transformer, have also become popular. In this paper we show that while convolutions and attention are both sufficient for good performance, neither of them are necessary. We present MLP-Mixer, an architecture based exclusively on multi-layer perceptrons (MLPs). MLP-Mixer contains two types of layers: one with MLPs applied independently to image patches (i.e. "mixing" the per-location features), and one with MLPs applied across patches (i.e. "mixing" spatial information). When trained on large datasets, or with modern regularization schemes, MLP-Mixer attains competitive scores on image classification benchmarks, with pre-training and inference cost comparable to state-of-the-art models. We hope that these results spark further research beyond the realms of well established CNNs and Transformers.},
  langid = {english}
}

@misc{tolstikhin_wasserstein_2019,
  title = {Wasserstein {{Auto-Encoders}}},
  author = {Tolstikhin, Ilya and Bousquet, Olivier and Gelly, Sylvain and Schoelkopf, Bernhard},
  date = {2019-12-05},
  eprint = {1711.01558},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1711.01558},
  urldate = {2021-03-04},
  abstract = {We propose the Wasserstein Auto-Encoder (WAE)—a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE) [1]. This regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE) [2]. Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality, as measured by the FID score.},
  langid = {english}
}

@online{tomczak_trouble_2022,
  title = {Trouble in Paradise: {{Does}} It Make Sense to Train Latent Variable Models with Variational Inference?},
  author = {Tomczak, Jakub},
  date = {2022-08-31},
  url = {https://jmtomczak.github.io/blog/13/13_trouble_in_paradise.html}
}

@inproceedings{tonekaboni_what_2019,
  title = {What {{Clinicians Want}}: {{Contextualizing Explainable Machine Learning}} for {{Clinical End Use}}},
  shorttitle = {What {{Clinicians Want}}},
  booktitle = {Proceedings of the 4th {{Machine Learning}} for {{Healthcare Conference}}},
  author = {Tonekaboni, Sana and Joshi, Shalmali and McCradden, Melissa D. and Goldenberg, Anna},
  date = {2019-10-28},
  pages = {359--380},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v106/tonekaboni19a.html},
  urldate = {2023-10-10},
  abstract = {Translating machine learning (ML) models effectively to clinical practice requires establishing clinicians’ trust. Explainability, or the ability of an ML model to justify its outcomes and assist clinicians in rationalizing the model prediction, has been generally understood to be critical to establishing trust. However, the eld suffers from the lack of concrete definitions for usable explanations in different settings. To identify specific aspects of explainability that may catalyze building trust in ML models, we surveyed clinicians from two distinct acute care specialties (Intenstive Care Unit and Emergency Department). We use their feedback to characterize when explainability helps to improve clinicians’ trust in ML models. We further identify the classes of explanations that clinicians identified as most relevant and crucial for effective translation to clinical practice. Finally, we discern concrete metrics for rigorous evaluation of clinical explainability methods. By integrating perceptions of explainability between clinicians and ML researchers we hope to facilitate the endorsement and broader adoption and sustained use of ML systems in healthcare.},
  eventtitle = {Machine {{Learning}} for {{Healthcare Conference}}},
  langid = {english}
}

@incollection{toolenaar_features_2004,
  title = {Features for {{Audio Classification}}},
  booktitle = {Algorithms in {{Ambient Intelligence}}},
  author = {Breebaart, Jeroen and McKinney, Martin F.},
  editor = {Verhaegh, Wim F. J. and Aarts, Emile and Korst, Jan},
  editorb = {Toolenaar, Frank},
  editorbtype = {redactor},
  date = {2004},
  volume = {2},
  pages = {113--129},
  publisher = {{Springer Netherlands}},
  location = {{Dordrecht}},
  doi = {10.1007/978-94-017-0703-9_6},
  url = {http://link.springer.com/10.1007/978-94-017-0703-9_6},
  urldate = {2019-01-09},
  abstract = {Four audio feature sets are evaluated in their ability to differentiate five audio classes: popular music, classical music, speech, noise and crowd noise. The feature sets include low-level signal properties, mel-frequency spectral coefficients, and two new sets based on perceptual models of hearing. The temporal behavior of the features is analyzed and parameterized and these parameters are included as additional features. Using a standard Gaussian framework for classification, results show that the temporal behavior of features is important for automatic audio classification. In addition, classification is better, on average, if based on features from models of auditory perception rather than on standard features.},
  isbn = {978-90-481-6490-5 978-94-017-0703-9},
  langid = {english}
}

@inproceedings{toshev_deeppose_2014,
  title = {Deeppose: {{Human}} Pose Estimation via Deep Neural Networks},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Toshev, Alexander and Szegedy, Christian},
  date = {2014},
  pages = {1653--1660}
}

@inproceedings{toshniwal_crosstask_2020,
  title = {A Cross-Task Analysis of Text Span Representations},
  booktitle = {Proceedings of the 5th {{Workshop}} on {{Representation Learning}} for {{NLP}}},
  author = {Toshniwal, Shubham and Shi, Haoyue and Shi, Bowen and Gao, Lingyu and Livescu, Karen and Gimpel, Kevin},
  date = {2020},
  pages = {166--176}
}

@inproceedings{touvron2021training,
  title = {Training Data-Efficient Image Transformers \& Distillation through Attention},
  booktitle = {International Conference on Machine Learning},
  author = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jégou, Hervé},
  date = {2021},
  pages = {10347--10357},
  organization = {{PMLR}}
}

@misc{townsend_lossless_2021,
  title = {Lossless {{Compression}} with {{Latent Variable Models}}},
  author = {Townsend, James},
  date = {2021-04-22},
  eprint = {2104.10544},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2104.10544},
  urldate = {2021-05-07},
  abstract = {We develop a simple and elegant method for lossless compression using latent variable models, which we call 'bits back with asymmetric numeral systems' (BB-ANS). The method involves interleaving encode and decode steps, and achieves an optimal rate when compressing batches of data. We demonstrate it firstly on the MNIST test set, showing that state-of-the-art lossless compression is possible using a small variational autoencoder (VAE) model. We then make use of a novel empirical insight, that fully convolutional generative models, trained on small images, are able to generalize to images of arbitrary size, and extend BB-ANS to hierarchical latent variable models, enabling state-of-the-art lossless compression of full-size colour images from the ImageNet dataset. We describe 'Craystack', a modular software framework which we have developed for rapid prototyping of compression using deep generative models.},
  langid = {english}
}

@inproceedings{townsend_practical_2019,
  title = {Practical {{Lossless Compression With Latent Variables Using Bits Back Coding}}},
  booktitle = {Proceedings of the 7th {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Townsend, James and Bird, Thomas and Barber, David},
  date = {2019},
  pages = {13},
  location = {{New Orleans, LA, USA}},
  abstract = {Deep latent variable models have seen recent success in many data domains. Lossless compression is an application of these models which, despite having the potential to be highly useful, has yet to be implemented in a practical manner. We present ‘Bits Back with ANS’ (BB-ANS), a scheme to perform lossless compression with latent variable models at a near optimal rate. We demonstrate this scheme by using it to compress the MNIST dataset with a variational autoencoder model (VAE), achieving compression rates superior to standard methods with only a simple VAE. Given that the scheme is highly amenable to parallelization, we conclude that with a sufficiently high quality generative model this scheme could be used to achieve substantial improvements in compression rate with acceptable running time. We make our implementation available open source at https://github.com/bits-back/bits-back.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english}
}

@misc{trischler_newsqa_2016,
  title = {{{NewsQA}}: {{A Machine Comprehension Dataset}}},
  shorttitle = {{{NewsQA}}},
  author = {Trischler, Adam and Wang, Tong and Yuan, Xingdi and Harris, Justin and Sordoni, Alessandro and Bachman, Philip and Suleman, Kaheer},
  date = {2016-11-29},
  eprint = {1611.09830},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1611.09830},
  urldate = {2019-06-19},
  abstract = {We present NewsQA, a challenging machine comprehension dataset of over 100,000 human-generated question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting of spans of text from the corresponding articles. We collect this dataset through a four-stage process designed to solicit exploratory questions that require reasoning. A thorough analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing textual entailment. We measure human performance on the dataset and compare it to several strong neural models. The performance gap between humans and machines (0.198 in F1) indicates that significant progress can be made on NewsQA through future research. The dataset is freely available at https://datasets.maluuba.com/NewsQA.},
  langid = {english}
}

@misc{trowitzsch_nigens_2020,
  title = {The {{NIGENS General Sound Events Database}}},
  author = {Trowitzsch, Ivo and Taghia, Jalil and Kashef, Youssef and Obermayer, Klaus},
  date = {2020-01-01},
  eprint = {1902.08314},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1902.08314},
  urldate = {2022-01-05},
  abstract = {Computational auditory scene analysis is gaining interest in the last years. Trailing behind the more mature field of speech recognition, it is particularly general sound event detection that is attracting increasing attention. Crucial for training and testing reasonable models is having available enough suitable data -- until recently, general sound event databases were hardly found. We release and present a database with 714 wav files containing isolated high quality sound events of 14 different types, plus 303 `general' wav files of anything else but these 14 types. All sound events are strongly labeled with perceptual on- and offset times, paying attention to omitting in-between silences. The amount of isolated sound events, the quality of annotations, and the particular general sound class distinguish NIGENS from other databases.}
}

@misc{tsai_note_2021,
  title = {A {{Note}} on {{Connecting Barlow Twins}} with {{Negative-Sample-Free Contrastive Learning}}},
  author = {Tsai, Yao-Hung Hubert and Bai, Shaojie and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  date = {2021-04-28},
  eprint = {2104.13712},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2104.13712},
  urldate = {2021-05-04},
  abstract = {In this report, we relate the algorithmic design of Barlow Twins' method to the Hilbert-Schmidt Independence Criterion (HSIC), thus establishing it as a contrastive learning approach that is free of negative samples. Through this perspective, we argue that Barlow Twins (and thus the class of negative-sample-free contrastive learning methods) suggests a possibility to bridge the two major families of self-supervised learning philosophies: non-contrastive and contrastive approaches. In particular, Barlow twins exemplified how we could combine the best practices of both worlds: avoiding the need of large training batch size and negative sample pairing (like non-contrastive methods) and avoiding symmetry-breaking network designs (like contrastive methods).},
  langid = {english}
}

@unpublished{tsai_selfsupervised_2021,
  title = {Self-Supervised {{Learning}} from a {{Multi-view Perspective}}},
  author = {Tsai, Yao-Hung Hubert and Wu, Yue and Salakhutdinov, Ruslan and Morency, Louis-Philippe},
  date = {2021-03-22},
  eprint = {2006.05576},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.05576},
  urldate = {2021-10-26},
  abstract = {As a subset of unsupervised representation learning, self-supervised representation learning adopts self-defined signals as supervision and uses the learned representation for downstream tasks, such as object detection and image captioning. Many proposed approaches for self-supervised learning follow naturally a multi-view perspective, where the input (e.g., original images) and the self-supervised signals (e.g., augmented images) can be seen as two redundant views of the data. Building from this multi-view perspective, this paper provides an information-theoretical framework to better understand the properties that encourage successful self-supervised learning. Specifically, we demonstrate that self-supervised learned representations can extract task-relevant information and discard task-irrelevant information. Our theoretical framework paves the way to a larger space of self-supervised learning objective design. In particular, we propose a composite objective that bridges the gap between prior contrastive and predictive learning objectives, and introduce an additional objective term to discard task-irrelevant information. To verify our analysis, we conduct controlled experiments to evaluate the impact of the composite objectives. We also explore our framework's empirical generalization beyond the multi-view perspective, where the cross-view redundancy may not be clearly observed.}
}

@misc{tschannen_recent_2018,
  title = {Recent {{Advances}} in {{Autoencoder-Based Representation Learning}}},
  author = {Tschannen, Michael and Bachem, Olivier and Lucic, Mario},
  date = {2018-12-12},
  eprint = {1812.05069},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1812.05069},
  urldate = {2019-12-23},
  abstract = {Learning useful representations with little or no supervision is a key challenge in artificial intelligence. We provide an in-depth review of recent advances in representation learning with a focus on autoencoder-based models. To organize these results we make use of meta-priors believed useful for downstream tasks, such as disentanglement and hierarchical organization of features. In particular, we uncover three main mechanisms to enforce such properties, namely (i) regularizing the (approximate or aggregate) posterior distribution, (ii) factorizing the encoding and decoding distribution, or (iii) introducing a structured prior distribution. While there are some promising results, implicit or explicit supervision remains a key enabler and all current methods use strong inductive biases and modeling assumptions. Finally, we provide an analysis of autoencoder-based representation learning through the lens of rate-distortion theory and identify a clear tradeoff between the amount of prior knowledge available about the downstream tasks, and how useful the representation is for this task.},
  langid = {english}
}

@article{tsengAdministrativeCostsAssociated2018,
  title = {Administrative {{Costs Associated With Physician Billing}} and {{Insurance-Related Activities}} at an {{Academic Health Care System}}},
  author = {Tseng, Phillip and Kaplan, Robert S. and Richman, Barak D. and Shah, Mahek A. and Schulman, Kevin A.},
  date = {2018-02},
  journaltitle = {JAMA : the journal of the American Medical Association},
  shortjournal = {JAMA},
  volume = {319},
  number = {7},
  pages = {691--697},
  issn = {0098-7484},
  doi = {10.1001/jama.2017.19148},
  abstract = {Administrative costs in the US health care system are an important component of total health care spending, and a substantial proportion of these costs are attributable to billing and insurance-related activities.To examine and estimate the administrative costs associated with physician billing activities in a large academic health care system with a certified electronic health record system.This study used time-driven activity-based costing. Interviews were conducted with 27 health system administrators and 34 physicians in 2016 and 2017 to construct a process map charting the path of an insurance claim through the revenue cycle management process. These data were used to calculate the cost for each major billing and insurance-related activity and were aggregated to estimate the health system's total cost of processing an insurance claim.Estimated time required to perform billing and insurance-related activities, based on interviews with management personnel and physicians.Estimated billing and insurance-related costs for 5 types of patient encounters: primary care visits, discharged emergency department visits, general medicine inpatient stays, ambulatory surgical procedures, and inpatient surgical procedures.Estimated processing time and total costs for billing and insurance-related activities were 13 minutes and \$20.49 for a primary care visit, 32 minutes and \$61.54 for a discharged emergency department visit, 73 minutes and \$124.26 for a general inpatient stay, 75 minutes and \$170.40 for an ambulatory surgical procedure, and 100 minutes and \$215.10 for an inpatient surgical procedure. Of these totals, time and costs for activities carried out by physicians were estimated at a median of 3 minutes or \$6.36 for a primary care visit, 3 minutes or \$10.97 for an emergency department visit, 5 minutes or \$13.29 for a general inpatient stay, 15 minutes or \$51.20 for an ambulatory surgical procedure, and 15 minutes or \$51.20 for an inpatient surgical procedure. Of professional revenue, professional billing costs were estimated to represent 14.5\% for primary care visits, 25.2\% for emergency department visits, 8.0\% for general medicine inpatient stays, 13.4\% for ambulatory surgical procedures, and 3.1\% for inpatient surgical procedures.In a time-driven activity-based costing study in a large academic health care system with a certified electronic health record system, the estimated costs of billing and insurance-related activities ranged from \$20 for a primary care visit to \$215 for an inpatient surgical procedure. Knowledge of how specific billing and insurance-related activities contribute to administrative costs may help inform policy solutions to reduce these expenses.}
}

@online{tu_unsupervised_2022,
  title = {Unsupervised {{Uncertainty Measures}} of {{Automatic Speech Recognition}} for {{Non-intrusive Speech Intelligibility Prediction}}},
  author = {Tu, Zehai and Ma, Ning and Barker, Jon},
  date = {2022-07-06},
  eprint = {2204.04288},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2204.04288},
  urldate = {2022-12-12},
  abstract = {Non-intrusive intelligibility prediction is important for its application in realistic scenarios, where a clean reference signal is difficult to access. The construction of many non-intrusive predictors require either ground truth intelligibility labels or clean reference signals for supervised learning. In this work, we leverage an unsupervised uncertainty estimation method for predicting speech intelligibility, which does not require intelligibility labels or reference signals to train the predictor. Our experiments demonstrate that the uncertainty from state-ofthe-art end-to-end automatic speech recognition (ASR) models is highly correlated with speech intelligibility. The proposed method is evaluated on two databases and the results show that the unsupervised uncertainty measures of ASR models are more correlated with speech intelligibility from listening results than the predictions made by widely used intrusive methods.},
  langid = {english},
  pubstate = {preprint}
}

@misc{tucci_introduction_2013,
  title = {Introduction to {{Judea Pearl}}'s {{Do-Calculus}}},
  author = {Tucci, Robert R.},
  date = {2013-04-25},
  eprint = {1305.5506},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1305.5506},
  urldate = {2019-08-05},
  abstract = {This is a purely pedagogical paper with no new results. The goal of the paper is to give a fairly self-contained introduction to Judea Pearl’s do-calculus, including proofs of his 3 rules.},
  langid = {english}
}

@inproceedings{tucker_doubly_2019,
  title = {Doubly {{Reparameterized Gradient Estimators}} for {{Monte Carlo Objectives}}},
  booktitle = {Proceedings of the 7th {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Tucker, George and Lawson, Dieterich and Gu, Shixiang and Maddison, Chris J.},
  date = {2019},
  publisher = {{OpenReview.net}},
  location = {{New Orleans, LA, USA}},
  url = {https://openreview.net/forum?id=HkG3e205K7},
  eventtitle = {International {{Conference}} on {{Learning Representations}}}
}

@misc{tucker_rebar_2017,
  title = {{{REBAR}}: {{Low-variance}}, Unbiased Gradient Estimates for Discrete Latent Variable Models},
  author = {Tucker, George and Mnih, Andriy and Maddison, Chris J. and Lawson, Dieterich and Sohl-Dickstein, Jascha},
  date = {2017-03-21},
  eprint = {1703.07370},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1703.07370},
  urldate = {2017-11-16},
  abstract = {Learning in models with discrete latent variables is challenging due to high variance gradient estimators. Generally, approaches have relied on control variates to reduce the variance of the REINFORCE estimator. Recent work (Jang et al. 2016, Maddison et al. 2016) has taken a different approach, introducing a continuous relaxation of discrete variables to produce low-variance, but biased, gradient estimates. In this work, we combine the two approaches through a novel control variate that produces low-variance, \textbackslash emph\{unbiased\} gradient estimates. Then, we introduce a modification to the continuous relaxation and show that the tightness of the relaxation can be adapted online, removing it as a hyperparameter. We show state-of-the-art variance reduction on several benchmark generative modeling tasks, generally leading to faster convergence to a better final log-likelihood.}
}

@misc{turek_efficient_2017,
  title = {Efficient, Sparse Representation of Manifold Distance Matrices for Classical Scaling},
  author = {Turek, Javier S. and Huth, Alexander},
  date = {2017},
  eprint = {1705.10887},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1705.10887},
  urldate = {2018-03-25},
  abstract = {Symmetric matrices are widely used in machine learning problems such as kernel machines and manifold learning. Using large datasets often requires computing low-rank approximations of these symmetric matrices so that they fit in memory. In this paper, we present a novel method based on biharmonic interpolation for low-rank matrix approximation. The method exploits knowledge of the data manifold to learn an interpolation operator that approximates values using a subset of randomly selected landmark points. This operator is readily sparsified, reducing memory requirements by at least two orders of magnitude without significant loss in accuracy. We show that our method can approximate very large datasets using twenty times more landmarks than other methods. Further, numerical results suggest that our method is stable even when numerical difficulties arise for other methods.}
}

@inproceedings{turian_hear_2022,
  title = {{{HEAR}}: {{Holistic}} Evaluation of Audio Representations},
  booktitle = {Proceedings of {{Machine Learning Research}} ({{PMLR}}): {{NeurIPS}} 2021 {{Competition Track}}},
  author = {Turian, Joseph and Shier, Jordie and Khan, Humair Raj and Raj, Bhiksha and Schuller, Björn W. and Steinmetz, Christian J. and Malloy, Colin and Tzanetakis, George and Velarde, Gissel and McNally, Kirk and Henry, Max and Pinto, Nicolas and Noufi, Camille and Clough, Christian and Herremans, Dorien and Fonseca, Eduardo and Engel, Jesse and Salamon, Justin and Esling, Philippe and Manocha, Pranay and Watanabe, Shinji and Jin, Zeyu and Bisk, Yonatan},
  date = {2022},
  volume = {176},
  pages = {125--145}
}

@article{turing_computing_1950,
  title = {Computing Machinery and Intelligence},
  author = {Turing, Alan M.},
  date = {1950},
  journaltitle = {Mind},
  volume = {59},
  number = {236},
  pages = {433--460},
  abstract = {Will computers and robots ever think and communicate\textbackslash nthe way humans do? When a computer crosses the\textbackslash nthreshold into self-consciousness, will it immediately\textbackslash njump into the Internet and create a World Mind? This is\textbackslash nan exploration of both the philosophical and\textbackslash nmethodological issues surrounding the search for true\textbackslash nartificial intelligence.},
  keywords = {★}
}

@inproceedings{ueno_multispeaker_2019,
  title = {Multi-Speaker Sequence-to-Sequence Speech Synthesis for Data Augmentation in Acoustic-to-Word Speech Recognition},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Ueno, Sei and Mimura, Masato and Sakai, Shinsuke and Kawahara, Tatsuya},
  date = {2019},
  pages = {6161--6165}
}

@inproceedings{ulyanov_texture_2016,
  title = {Texture Networks: {{Feed-forward}} Synthesis of Textures and Stylized Images},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Ulyanov, Dmitry and Lebedev, Vadim and Vedaldi, Andrea and Lempitsky, Victor},
  date = {2016}
}

@article{umscheid_development_2015,
  title = {Development, Implementation, and Impact of an Automated Early Warning and Response System for Sepsis},
  author = {Umscheid, Craig A and Betesh, Joel and VanZandbergen, Christine and Hanish, Asaf and Tait, Gordon and Mikkelsen, Mark E and French, Benjamin and Fuchs, Barry D},
  date = {2015},
  journaltitle = {Journal of hospital medicine},
  volume = {10},
  number = {1},
  pages = {26--31},
  publisher = {{Wiley Online Library}}
}

@online{uppal_implicit_2023,
  title = {Implicit {{Variational Inference}} for {{High-Dimensional Posteriors}}},
  author = {Uppal, Anshuk and Stensbo-Smidt, Kristoffer and Boomsma, Wouter K. and Frellsen, Jes},
  date = {2023-10-10},
  eprint = {2310.06643},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2310.06643},
  urldate = {2023-10-18},
  abstract = {In variational inference, the benefits of Bayesian models rely on accurately capturing the true posterior distribution. We propose using neural samplers that specify implicit distributions, which are well-suited for approximating complex multimodal and correlated posteriors in high-dimensional spaces. Our approach advances inference using implicit distributions by introducing novel bounds that come about by locally linearising the neural sampler. This is distinct from existing methods that rely on additional discriminator networks and unstable adversarial objectives. Furthermore, we present a new sampler architecture that, for the first time, enables implicit distributions over millions of latent variables, addressing computational concerns by using differentiable numerical approximations. Our empirical analysis indicates our method is capable of recovering correlations across layers in large Bayesian neural networks, a property that is crucial for a network's performance but notoriously challenging to achieve. To the best of our knowledge, no other method has been shown to accomplish this task for such large models. Through experiments in downstream tasks, we demonstrate that our expressive posteriors outperform state-of-the-art uncertainty quantification methods, validating the effectiveness of our training algorithm and the quality of the learned implicit approximation.},
  pubstate = {preprint}
}

@inproceedings{uria_rnade_2013,
  title = {{{RNADE}}: {{The}} Real-Valued Neural Autoregressive Density-Estimator},
  booktitle = {Proceedings of the 27th {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Uria, Benigno and Murray, Iain and Larochelle, Hugo},
  date = {2013},
  pages = {9},
  location = {{Lake Tahoe, Nevada, USA}},
  abstract = {We introduce RNADE, a new model for joint density estimation of real-valued vectors. Our model calculates the density of a datapoint as the product of onedimensional conditionals modeled using mixture density networks with shared parameters. RNADE learns a distributed representation of the data, while having a tractable expression for the calculation of densities. A tractable likelihood allows direct comparison with other methods and training by standard gradientbased optimizers. We compare the performance of RNADE on several datasets of heterogeneous and perceptual data, finding it outperforms mixture models in all but one case.},
  eventtitle = {Conference on {{Neural Information Processing Systems}}},
  langid = {english}
}

@artwork{usarmyresearchlaboratoryarltechnicallibrary_female_1940,
  title = {Female Programmers of the {{ENIAC}}},
  author = {{US Army Research Laboratory (ARL) Technical Library}},
  date = {1940/1945}
}

@inproceedings{vahdat_nvae_2020,
  title = {{{NVAE}}: {{A Deep Hierarchical Variational Autoencoder}}},
  shorttitle = {{{NVAE}}},
  booktitle = {34th {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Vahdat, Arash and Kautz, Jan},
  date = {2020-07-08},
  location = {{Virtual}},
  url = {http://arxiv.org/abs/2007.03898},
  urldate = {2020-07-13},
  abstract = {Normalizing flows, autoregressive models, variational autoencoders (VAEs), and deep energy-based models are among competing likelihood-based frameworks for deep generative learning. Among them, VAEs have the advantage of fast and tractable sampling and easy-to-access encoding networks. However, they are currently outperformed by other models such as normalizing flows and autoregressive models. While the majority of the research in VAEs is focused on the statistical challenges, we explore the orthogonal direction of carefully designing neural architectures for hierarchical VAEs. We propose Nouveau VAE (NVAE), a deep hierarchical VAE built for image generation using depth-wise separable convolutions and batch normalization. NVAE is equipped with a residual parameterization of Normal distributions and its training is stabilized by spectral regularization. We show that NVAE achieves state-of-the-art results among non-autoregressive likelihood-based models on the MNIST, CIFAR-10, and CelebA HQ datasets and it provides a strong baseline on FFHQ. For example, on CIFAR-10, NVAE pushes the state-of-the-art from 2.98 to 2.91 bits per dimension, and it produces high-quality images on CelebA HQ as shown in Fig. 1. To the best of our knowledge, NVAE is the first successful VAE applied to natural images as large as 256×256 pixels.},
  langid = {english}
}

@inproceedings{vahdat_scorebased_2021,
  title = {Score-Based Generative Modeling in Latent Space},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Vahdat, Arash and Kreis, Karsten and Kautz, Jan},
  editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P.S. and Vaughan, J. Wortman},
  date = {2021},
  volume = {34},
  pages = {11287--11302},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/5dca4c6b9e244d24a30b4c45601d9720-Paper.pdf}
}

@inproceedings{valk_voxlingua107_2021,
  title = {{{VoxLingua107}}: {{A}} Dataset for Spoken Language Recognition},
  booktitle = {Proceedings of the {{IEEE Spoken Language Technology Workshop}}},
  author = {Valk, Jörgen and Alumäe, Tanel},
  date = {2021},
  eventtitle = {{{IEEE Spoken Language Technology Workshop}}}
}

@article{van_de_schoot_bayesian_2021,
  title = {Bayesian Statistics and Modelling},
  author = {family=Schoot, given=Rens, prefix=van de, useprefix=true and Depaoli, Sarah and King, Ruth and Kramer, Bianca and Märtens, Kaspar and Tadesse, Mahlet G. and Vannucci, Marina and Gelman, Andrew and Veen, Duco and Willemsen, Joukje and Yau, Christopher},
  date = {2021-12},
  journaltitle = {Nature Reviews Methods Primers},
  shortjournal = {Nat Rev Methods Primers},
  volume = {1},
  number = {1},
  pages = {1},
  issn = {2662-8449},
  doi = {10.1038/s43586-020-00001-2},
  url = {http://www.nature.com/articles/s43586-020-00001-2},
  urldate = {2021-05-07},
  abstract = {Bayesian statistics is an approach to data analysis based on Bayes’ theorem, where available knowledge about parameters in a statistical model is updated with the information in observed data. The background knowledge is expressed as a prior distribution and combined with observational data in the form of a likelihood function to determine the posterior distribution. The posterior can also be used for making predictions about future events. This Primer describes the stages involved in Bayesian analysis, from specifying the prior and data models to deriving inference, model checking and refinement. We discuss the importance of prior and posterior predictive checking, selecting a proper technique for sampling from a posterior distribution, variational inference and variable selection. Examples of successful applications of Bayesian analysis across various research fields are provided, including in social sciences, ecology, genetics, medicine and more. We propose strategies for reproducibility and reporting standards, outlining an updated WAMBS (when to Worry and how to Avoid the Misuse of Bayesian Statistics) checklist. Finally, we outline the impact of Bayesian analysis on artificial intelligence, a major goal in the next decade.},
  langid = {english}
}

@article{van_hasselt_double_2010,
  title = {Double {{Q-learning}}},
  author = {family=Hasselt, given=Hado, prefix=van, useprefix=true},
  date = {2010},
  journaltitle = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
  pages = {1--9},
  abstract = {In some stochastic environments the well-known reinforcement learning algo-rithm Q-learning performs very poorly. This poor performance is caused by large overestimations of action values. These overestimations result from a positive bias that is introduced because Q-learning uses the maximum action value as an approximation for the maximum expected action value. We introduce an alter-native way to approximate the maximum expected value for any set of random variables. The obtained double estimator method is shown to sometimes under-estimate rather than overestimate the maximum expected value. We apply the double estimator to Q-learning to construct Double Q-learning, a new off-policy reinforcement learning algorithm. We show the new algorithm converges to the optimal policy and that it performs well in some settings in which Q-learning per-forms poorly due to its overestimation.},
  isbn = {9781617823800}
}

@inproceedings{van_kuyk_information_2017,
  title = {On the Information Rate of Speech Communication},
  booktitle = {2017 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Van Kuyk, Steven and Kleijn, W. Bastiaan and Hendriks, Richard C.},
  date = {2017-03},
  pages = {5625--5629},
  publisher = {{IEEE}},
  location = {{New Orleans, LA}},
  doi = {10.1109/ICASSP.2017.7953233},
  url = {http://ieeexplore.ieee.org/document/7953233/},
  urldate = {2021-09-03},
  abstract = {The key to the success of speech-based technology is an understanding of human speech communication. While significant advances have been made, a unified theory of speech communication that is both comprehensive and quantitative is yet to emerge. In this paper we approach speech communication from an information theoretical perspective. Without relying on prior knowledge of speech production, language, or auditory processing, we develop a new methodology for measuring the information rate of speech. Instead we rely on having recordings of multiple talkers saying the same utterance. In general, our results are consistent with a linguistic understanding of speech communication.},
  eventtitle = {2017 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  isbn = {978-1-5090-4117-6},
  langid = {english}
}

@online{vanamersfoort_feature_2022,
  title = {On {{Feature Collapse}} and {{Deep Kernel Learning}} for {{Single Forward Pass Uncertainty}}},
  author = {family=Amersfoort, given=Joost, prefix=van, useprefix=true and Smith, Lewis and Jesson, Andrew and Key, Oscar and Gal, Yarin},
  date = {2022-03-07},
  eprint = {2102.11409},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2102.11409},
  urldate = {2023-06-29},
  abstract = {Inducing point Gaussian process approximations are often considered a gold standard in uncertainty estimation since they retain many of the properties of the exact GP and scale to large datasets. A major drawback is that they have difficulty scaling to high dimensional inputs. Deep Kernel Learning (DKL) promises a solution: a deep feature extractor transforms the inputs over which an inducing point Gaussian process is defined. However, DKL has been shown to provide unreliable uncertainty estimates in practice. We study why, and show that with no constraints, the DKL objective pushes "far-away" data points to be mapped to the same features as those of training-set points. With this insight we propose to constrain DKL's feature extractor to approximately preserve distances through a bi-Lipschitz constraint, resulting in a feature space favorable to DKL. We obtain a model, DUE, which demonstrates uncertainty quality outperforming previous DKL and other single forward pass uncertainty methods, while maintaining the speed and accuracy of standard neural networks.},
  pubstate = {preprint}
}

@misc{vanbaalen_bayesian_2020,
  title = {Bayesian {{Bits}}: {{Unifying Quantization}} and {{Pruning}}},
  shorttitle = {Bayesian {{Bits}}},
  author = {family=Baalen, given=Mart, prefix=van, useprefix=true and Louizos, Christos and Nagel, Markus and Amjad, Rana Ali and Wang, Ying and Blankevoort, Tijmen and Welling, Max},
  date = {2020-05-15},
  eprint = {2005.07093},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2005.07093},
  urldate = {2020-06-13},
  abstract = {We introduce Bayesian Bits, a practical method for joint mixed precision quantization and pruning through gradient based optimization. Bayesian Bits employs a novel decomposition of the quantization operation, which sequentially considers doubling the bit width. At each new bit width, the residual error between the full precision value and the previously rounded value is quantized. We then decide whether or not to add this quantized residual error for a higher effective bit width and lower quantization noise. By starting with a power-of-two bit width, this decomposition will always produce hardware-friendly configurations, and through an additional 0-bit option, serves as a unified view of pruning and quantization. Bayesian Bits then introduces learnable stochastic gates, which collectively control the bit width of the given tensor. As a result, we can obtain low bit solutions by performing approximate inference over the gates, with prior distributions that encourage most of them to be switched off. We further show that, under some assumptions, L0 regularization of the network parameters corresponds to a specific instance of the aforementioned framework. We experimentally validate our proposed method on several benchmark datasets and show that we can learn pruned, mixed precision networks that provide a better trade-off between accuracy and efficiency than their static bit width equivalents.},
  langid = {english}
}

@article{vandermaaten_visualizing_2008,
  title = {Visualizing High-Dimensional Data Using t-Sne},
  author = {Van Der Maaten, L. J. P. and Hinton, Geoffrey E.},
  date = {2008},
  journaltitle = {Journal of Machine Learning Research},
  volume = {9},
  eprint = {20652508},
  eprinttype = {pmid},
  pages = {2579--2605},
  issn = {1532-4435},
  doi = {10.1007/s10479-011-0841-3},
  abstract = {We present a new technique called “t-SNE” that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence theway in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualiza- tions produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.},
  isbn = {1532-4435}
}

@misc{vanhasselt_deep_2015,
  title = {Deep {{Reinforcement Learning}} with {{Double Q-learning}}},
  author = {family=Hasselt, given=Hado, prefix=van, useprefix=true and Guez, Arthur and Silver, David},
  date = {2015-09-22},
  eprint = {26150344},
  eprinttype = {pmid},
  url = {http://arxiv.org/abs/1509.06461},
  urldate = {2017-11-06},
  abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
  isbn = {0004-3702}
}

@inproceedings{vanniekerk_analyzing_2021,
  title = {Analyzing Speaker Information in Self-Supervised Models to Improve Zero-Resource Speech Processing},
  booktitle = {Proceedings of the 22nd {{Annual Conference}} of the {{International Speech Communication Association}} ({{Interspeech}})},
  author = {family=Niekerk, given=Benjamin, prefix=van, useprefix=true and Nortje, Leanne and Baas, Matthew and Kamper, Herman},
  date = {2021},
  publisher = {{ISCA}}
}

@inproceedings{vanniekerk_vectorquantized_2020,
  title = {Vector-Quantized Neural Networks for Acoustic Unit Discovery in the {{ZeroSpeech}} 2020 Challenge},
  booktitle = {Annual {{Conference}} of the {{International Speech Communication Association}}},
  author = {family=Niekerk, given=Benjamin, prefix=van, useprefix=true and Nortje, Leanne and Kamper, Herman},
  date = {2020}
}

@inproceedings{vanstaden_comparison_2021,
  title = {A Comparison of Self-Supervised Speech Representations as Input Features for Unsupervised Acoustic Word Embeddings},
  booktitle = {2021 {{IEEE Spoken Language Technology Workshop}} ({{SLT}})},
  author = {family=Staden, given=Lisa, prefix=van, useprefix=true and Kamper, Herman},
  date = {2021},
  pages = {927--934}
}

@article{varga_assessment_1993,
  title = {Assessment for Automatic Speech Recognition: {{II}}. {{NOISEX-92}}: {{A}} Database and an Experiment to Study the Effect of Additive Noise on Speech Recognition Systems},
  shorttitle = {Assessment for Automatic Speech Recognition},
  author = {Varga, Andrew and Steeneken, Herman J.M.},
  date = {1993-07},
  journaltitle = {Speech Communication},
  volume = {12},
  number = {3},
  pages = {247--251},
  issn = {01676393},
  doi = {10/fwrpcf},
  url = {http://linkinghub.elsevier.com/retrieve/pii/0167639393900953},
  urldate = {2018-11-19},
  abstract = {The NOISEX-92 experiment and database is described and discussed. NOISEX-92 specifies a carefully controlled experiment on artificially noisy speech data, examining performance for a limited digit recognition task but with a relatively wide range of noises and signal-to-noise ratios. Example recognition results are given.},
  langid = {english}
}

@inproceedings{vaswani_attention_2017,
  title = {Attention {{Is All You Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2017-06-12},
  eprint = {1000303116},
  eprinttype = {pmid},
  pages = {5998--6008},
  doi = {10.1017/S0140525X16001837},
  url = {https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf},
  urldate = {2018-05-02},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  isbn = {978-1-57735-738-4},
  keywords = {★,Read}
}

@article{veaux_cstr_2016,
  title = {{{CSTR VCTK}} Corpus: {{English}} Multi-Speaker Corpus for {{CSTR}} Voice Cloning Toolkit},
  author = {Veaux, Christophe and Yamagishi, Junichi and MacDonald, Kirsten},
  date = {2016},
  publisher = {{University of Edinburgh, The Centre for Speech Technology Research (CSTR)}}
}

@inproceedings{venkataramani_support_2003,
  title = {Support Vector Machines for Segmental Minimum {{Bayes}} Risk Decoding of Continuous Speech},
  booktitle = {{{IEEE Workshop}} on {{Automatic Speech Recognition}} and {{Understanding}} ({{ASRU}})},
  author = {Venkataramani, Verra and Chakrabartty, Shantanu and Byrne, William},
  date = {2003}
}

@article{venkateshAutomatingOverburdenedClinical2023,
  title = {Automating the Overburdened Clinical Coding System: {{Challenges}} and next Steps},
  shorttitle = {Automating the Overburdened Clinical Coding System},
  author = {Venkatesh, Kaushik P. and Raza, Marium M. and Kvedar, Joseph C.},
  date = {2023-02},
  journaltitle = {npj Digital Medicine},
  volume = {6},
  number = {1},
  pages = {1--2},
  publisher = {{Nature Publishing Group}},
  issn = {2398-6352},
  doi = {10.1038/s41746-023-00768-0},
  abstract = {Artificial intelligence (AI) and natural language processing (NLP) have found a highly promising application in automated clinical coding (ACC), an innovation that will have profound impacts on the clinical coding industry, billing and revenue management, and potentially clinical care itself. Dong et al. recently analyzed the technical challenges of ACC and proposed future directions. Primary challenges for ACC exist at the technological and implementation levels; clinical documents are redundant and complex, code sets like the ICD-10 are rapidly evolving, training sets are not comprehensive of codes, and ACC models have yet to fully capture the logic and rules of coding decisions. Next steps include interdisciplinary collaboration with clinical coders, accessibility and transparency of datasets, and tailoring models to specific use cases.},
  copyright = {2023 The Author(s)},
  langid = {english}
}

@inproceedings{versteegh_zero_2015,
  title = {The {{Zero Resource Speech Challenge}} 2015},
  booktitle = {{{IEEE Annual Conference}} of the {{International Speech Communication Association}} ({{Interspeech}})},
  author = {Versteegh, Maarten and Thiolliere, Roland and Schatz, Thomas and Cao, Xuan Nga and Anguera, Xavier and Jansen, Aren and Dupoux, Emmanuel},
  date = {2015}
}

@book{villani2009optimal,
  title = {Optimal Transport: Old and New},
  author = {Villani, Cédric and others},
  date = {2009},
  volume = {338},
  publisher = {{Springer}}
}

@inproceedings{vincent_extracting_2008,
  title = {Extracting and Composing Robust Features with Denoising Autoencoders},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
  date = {2008},
  eprint = {15540460},
  eprinttype = {pmid},
  pages = {1096--1103},
  issn = {1605582050},
  doi = {10.1145/1390156.1390294},
  url = {http://portal.acm.org/citation.cfm?doid=1390156.1390294},
  abstract = {Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.},
  isbn = {978-1-60558-205-4}
}

@article{vincent_stacked_2010,
  title = {Stacked Denoising Autoencoders: {{Learning}} Useful Representations in a Deep Network with a Local Denoising Criterion},
  author = {Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine},
  date = {2010},
  journaltitle = {Journal of Machine Learning Research}
}

@article{vinyals_show_2015,
  title = {Show and {{Tell}}: {{A Neural Image Caption Generator}}},
  author = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
  date = {2015},
  journaltitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  eprint = {1411.4555},
  eprinttype = {arxiv},
  pages = {3156--3164},
  url = {http://arxiv.org/abs/1411.4555},
  abstract = {Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.},
  isbn = {9781467369640}
}

@inproceedings{virmaux_lipschitz_2018,
  title = {Lipschitz Regularity of Deep Neural Networks: Analysis and Efficient Estimation},
  booktitle = {32nd {{Conference}} on {{Neural Information Processing Systems}}},
  author = {Virmaux, Aladin and Scaman, Kevin},
  date = {2018},
  pages = {10},
  location = {{Montréal, Canada}},
  abstract = {Deep neural networks are notorious for being sensitive to small well-chosen perturbations, and estimating the regularity of such architectures is of utmost importance for safe and robust practical applications. In this paper, we investigate one of the key characteristics to assess the regularity of such methods: the Lipschitz constant of deep learning architectures. First, we show that, even for two layer neural networks, the exact computation of this quantity is NP-hard and state-ofart methods may significantly overestimate it. Then, we both extend and improve previous estimation methods by providing AutoLip, the first generic algorithm for upper bounding the Lipschitz constant of any automatically differentiable function. We provide a power method algorithm working with automatic differentiation, allowing efficient computations even on large convolutions. Second, for sequential neural networks, we propose an improved algorithm named SeqLip that takes advantage of the linear computation graph to split the computation per pair of consecutive layers. Third we propose heuristics on SeqLip in order to tackle very large networks. Our experiments show that SeqLip can significantly improve on the existing upper bounds. Finally, we provide an implementation of AutoLip in the PyTorch environment that may be used to better estimate the robustness of a given neural network to small perturbations or regularize it using more precise Lipschitz estimations.},
  eventtitle = {Conference on {{Neural Information Processing Systems}}},
  langid = {english}
}

@inproceedings{voita_bottomup_2019,
  title = {The Bottom-up Evolution of Representations in the {{Transformer}}: {{A}} Study with Machine Translation and Language Modeling Objectives},
  booktitle = {Proceedings of the {{Annual Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}} ({{NAACL}})},
  author = {Voita, Elena and Sennrich, Rico and Titov, Ivan},
  date = {2019}
}

@misc{voita_informationtheoretic_2020,
  title = {Information-{{Theoretic Probing}} with {{Minimum Description Length}}},
  author = {Voita, Elena and Titov, Ivan},
  date = {2020-03-27},
  eprint = {2003.12298},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2003.12298},
  urldate = {2020-08-20},
  abstract = {To measure how well pretrained representations encode some linguistic property, it is common to use accuracy of a probe, i.e. a classifier trained to predict the property from the representations. Despite widespread adoption of probes, differences in their accuracy fail to adequately reflect differences in representations. For example, they do not substantially favour pretrained representations over randomly initialized ones. Analogously, their accuracy can be similar when probing for genuine linguistic labels and probing for random synthetic tasks. To see reasonable differences in accuracy with respect to these random baselines, previous work had to constrain either the amount of probe training data or its model size. Instead, we propose an alternative to the standard probes, information-theoretic probing with minimum description length (MDL). With MDL probing, training a probe to predict labels is recast as teaching it to effectively transmit the data. Therefore, the measure of interest changes from probe accuracy to the description length of labels given representations. In addition to probe quality, the description length evaluates "the amount of effort" needed to achieve the quality. This amount of effort characterizes either (i) size of a probing model, or (ii) the amount of data needed to achieve the high quality. We consider two methods for estimating MDL which can be easily implemented on top of the standard probing pipelines: variational coding and online coding. We show that these methods agree in results and are more informative and stable than the standard probes.},
  langid = {english}
}

@inproceedings{vuLabelAttentionModel2020,
  title = {A {{Label Attention Model}} for {{ICD Coding}} from {{Clinical Text}}},
  booktitle = {Proceedings of the {{Twenty-Ninth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Vu, Thanh and Nguyen, Dat Quoc and Nguyen, Anthony},
  date = {2020-07},
  pages = {3335--3341},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  location = {{Yokohama, Japan}},
  doi = {10.24963/ijcai.2020/461},
  abstract = {ICD coding is a process of assigning the International Classification of Disease diagnosis codes to clinical/medical notes documented by health professionals (e.g. clinicians). This process requires significant human resources, and thus is costly and prone to error. To handle the problem, machine learning has been utilized for automatic ICD coding. Previous state-of-the-art models were based on convolutional neural networks, using a single/several fixed window sizes. However, the lengths and interdependence between text fragments related to ICD codes in clinical text vary significantly, leading to the difficulty of deciding what the best window sizes are. In this paper, we propose a new label attention model for automatic ICD coding, which can handle both the various lengths and the interdependence of the ICD code related text fragments. Furthermore, as the majority of ICD codes are not frequently used, leading to the extremely imbalanced data issue, we additionally propose a hierarchical joint learning mechanism extending our label attention model to handle the issue, using the hierarchical relationships among the codes. Our label attention model achieves new state-of-the-art results on three benchmark MIMIC datasets, and the joint learning mechanism helps improve the performances for infrequent codes.},
  isbn = {978-0-9992411-6-5},
  langid = {english}
}

@inproceedings{vyas_analyzing_2019,
  title = {Analyzing {{Uncertainties}} in {{Speech Recognition Using Dropout}}},
  booktitle = {{{ICASSP}} 2019 - 2019 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Vyas, Apoorv and Dighe, Pranay and Tong, Sibo and Bourlard, Herve},
  date = {2019-05},
  pages = {6730--6734},
  publisher = {{IEEE}},
  location = {{Brighton, United Kingdom}},
  doi = {10.1109/ICASSP.2019.8683086},
  url = {https://ieeexplore.ieee.org/document/8683086/},
  urldate = {2022-12-12},
  abstract = {The performance of Automatic Speech Recognition (ASR) systems is often measured using Word Error Rates (WER) which requires time-consuming and expensive manually transcribed data. In this paper, we use state-of-the-art ASR systems based on Deep Neural Networks (DNN) and propose a novel framework which uses “Dropout” at the test time to model uncertainty in prediction hypotheses. We systematically exploit this uncertainty to estimate WER without the need for explicit transcriptions. In addition, we show that the predictive uncertainty can also be used to accurately localize the errors made by the ASR system. We study the performance of our approach on Switchboard database where it predicts WER accurately within a range of 2.6\% and 5.0\% for HMM-DNN and Connectionist Temporal Classification (CTC) ASR systems, respectively.},
  eventtitle = {{{ICASSP}} 2019 - 2019 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  isbn = {978-1-4799-8131-1},
  langid = {english}
}

@inproceedings{vyas_fast_2020,
  title = {Fast Transformers with Clustered Attention},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Vyas, Apoorv and Katharopoulos, Angelos and Fleuret, François},
  editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M.F. and Lin, H.},
  date = {2020},
  volume = {33},
  pages = {21665--21674},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2020/file/f6a8dd1c954c8506aadc764cc32b895e-Paper.pdf}
}

@inproceedings{vyas_outofdistribution_2018,
  title = {Out-of-{{Distribution Detection Using}} an {{Ensemble}} of {{Self Supervised Leave-out Classifiers}}},
  booktitle = {Proceedings of the {{European Conference}} on {{Computer Vision}} ({{ECCV}})},
  author = {Vyas, Apoorv and Jammalamadaka, Nataraj and Zhu, Xia and Das, Dipankar and Kaul, Bharat and Willke, Theodore L.},
  date = {2018-09-04},
  eprint = {1809.03576},
  eprinttype = {arxiv},
  pages = {550--564},
  url = {http://arxiv.org/abs/1809.03576},
  urldate = {2022-09-04},
  abstract = {As deep learning methods form a critical part in commercially important applications such as autonomous driving and medical diagnostics, it is important to reliably detect out-of-distribution (OOD) inputs while employing these algorithms. In this work, we propose an OOD detection algorithm which comprises of an ensemble of classifiers. We train each classifier in a self-supervised manner by leaving out a random subset of training data as OOD data and the rest as in-distribution (ID) data. We propose a novel margin-based loss over the softmax output which seeks to maintain at least a margin \$m\$ between the average entropy of the OOD and in-distribution samples. In conjunction with the standard cross-entropy loss, we minimize the novel loss to train an ensemble of classifiers. We also propose a novel method to combine the outputs of the ensemble of classifiers to obtain OOD detection score and class prediction. Overall, our method convincingly outperforms Hendrycks et al.[7] and the current state-of-the-art ODIN[13] on several OOD detection benchmarks.}
}

@article{wald_tests_1943,
  title = {Tests of Statistical Hypotheses Concerning Several Parameters When the Number of Observations Is Large},
  author = {Wald, Abraham},
  date = {1943},
  journaltitle = {Transactions of the American Mathematical society},
  volume = {54},
  number = {3},
  pages = {426--482}
}

@misc{wan_generalized_2017,
  title = {Generalized {{End-to-End Loss}} for {{Speaker Verification}}},
  author = {Wan, Li and Wang, Quan and Papir, Alan and Moreno, Ignacio Lopez},
  date = {2017-10-28},
  eprint = {1710.10467},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1710.10467},
  urldate = {2019-02-26},
  abstract = {In this paper, we propose a new loss function called generalized end-to-end (GE2E) loss, which makes the training of speaker verification models more efficient than our previous tuple-based end-to-end (TE2E) loss function. Unlike TE2E, the GE2E loss function updates the network in a way that emphasizes examples that are difficult to verify at each step of the training process. Additionally, the GE2E loss does not require an initial stage of example selection. With these properties, our model with the new loss function decreases speaker verification EER by more than 10\%, while reducing the training time by 60\% at the same time. We also introduce the MultiReader technique, which allows us to do domain adaptation - training a more accurate model that supports multiple keywords (i.e. "OK Google" and "Hey Google") as well as multiple dialects.}
}

@inproceedings{wan_regularization_2013,
  title = {Regularization of {{Neural Networks}} Using {{Dropconnect}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Wan, Li and Zeiler, Matthew and Zhang, Sixin and LeCun, Yann and Fergus, Rob},
  date = {2013},
  number = {1},
  eprint = {797520},
  eprinttype = {pmid},
  pages = {109--111},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2017.2703082},
  url = {http://machinelearning.wustl.edu/mlpapers/papers/icml2013_wan13},
  urldate = {2018-06-02},
  abstract = {We introduce DropConnect, a generalization of Dropout (Hinton et al., 2012), for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recognition benchmarks by aggregating multiple DropConnect-trained models.}
}

@inproceedings{wan_svmsvm_2003,
  title = {{{SVMSVM}}: {{Support}} Vector Machine Speaker Verification Methodology},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Wan, V. and Renals, S.},
  date = {2003},
  pages = {221--224}
}

@article{wang_800_1998,
  title = {An 800 Bps {{VQ}}‐based {{LPC}} Voice Coder},
  author = {Wang, Xianglin and Kuo, C.‐C. Jay},
  date = {1998-05},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {103},
  number = {5},
  pages = {2778--2778},
  issn = {0001-4966},
  doi = {10.1121/1.422247},
  url = {http://asa.scitation.org/doi/10.1121/1.422247},
  urldate = {2021-10-30},
  langid = {english}
}

@misc{wang_computational_2018,
  title = {Computational {{Protein Design}} with {{Deep Learning Neural Networks}}},
  author = {Wang, Jingxue and Cao, Huali and Zhang, John Z. H. and Qi, Yifei},
  date = {2018-01-22},
  eprint = {1801.07130},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1801.07130},
  urldate = {2018-04-05},
  abstract = {Computational protein design has a wide variety of applications. Despite its remarkable success, designing a protein for a given structure and function is still a challenging task. On the other hand, the number of solved protein structures is rapidly increasing while the number of unique protein folds has reached a steady number, suggesting more structural information is being accumulated on each fold. Deep learning neural network is a powerful method to learn such big data set and has shown superior performance in many machine learning fields. In this study, we applied the deep learning neural network approach to computational protein design for predicting the probability of 20 natural amino acids on each residue in a protein. A large set of protein structures was collected and a multi-layer neural network was constructed. A number of structural properties were extracted as input features and the best network achieved an accuracy of 38.3\%. Using the network output as residue type restraints was able to improve the average sequence identity in designing three natural proteins using Rosetta. Moreover, the predictions from our network show \textasciitilde 3\% higher sequence identity than a previous method. Results from this study may benefit further development of computational protein design methods.}
}

@online{wang_covost_2020,
  title = {Covost 2 and {{Massively Multilingual Speech-to-Text Translation}}},
  author = {Wang, Changhan and Wu, Anne and Pino, Juan},
  date = {2020},
  eprint = {2007.10310},
  eprinttype = {arxiv},
  pubstate = {preprint}
}

@inproceedings{wang_deep_2015,
  title = {On Deep Multi-View Representation Learning},
  booktitle = {International {{Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Wang, W. and Arora, R and Livescu, K. and Bilmes, J. A.},
  date = {2015}
}

@online{wang_deep_2016,
  title = {Deep {{Variational Canonical Correlation Analysis}}},
  author = {Wang, Weiran and Yan, Xinchen and Lee, Honglak and Livescu, Karen},
  date = {2016-10-11},
  eprint = {1610.03454},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1610.03454},
  urldate = {2023-03-16},
  abstract = {We present deep variational canonical correlation analysis (VCCA), a deep multi-view learning model that extends the latent variable model interpretation of linear CCA to nonlinear observation models parameterized by deep neural networks. We derive variational lower bounds of the data likelihood by parameterizing the posterior probability of the latent variables from the view that is available at test time. We also propose a variant of VCCA called VCCA-private that can, in addition to the "common variables" underlying both views, extract the "private variables" within each view, and disentangles the shared and private information for multi-view data without hard supervision. Experimental results on real-world datasets show that our methods are competitive across domains.},
  pubstate = {preprint}
}

@inproceedings{wang_dnnhmmdnn_2020,
  title = {A {{DNN-HMM-DNN}} Hybrid Model for Discovering Word-like Units from Spoken Captions and Image Regions},
  booktitle = {Annual {{Conference}} of the {{International Speech Communication Association}}},
  author = {Wang, Liming and Hasegawa-Johnson, Mark},
  date = {2020}
}

@article{wang_fast_2013,
  title = {Fast Dropout Training},
  author = {Wang, Sida I. and Manning, Christopher D.},
  date = {2013},
  journaltitle = {Proceedings of the International Conference on Machine Learning (ICML)},
  volume = {28},
  pages = {118--126},
  url = {http://machinelearning.wustl.edu/mlpapers/papers/wang13a},
  urldate = {2018-04-16},
  abstract = {Preventing feature co-adaptation by encouraging independent contributions from different features often improves classification and regression performance. Dropout training (Hinton et al., 2012) does this by randomly dropping out (zeroing) hidden units and input features during training of neural networks. However, repeatedly sampling a random subset of input features makes training much slower. Based on an examination of the implied objective function of dropout training, we show how to do fast dropout training by sampling from or integrating a Gaussian approximation, instead of doing Monte Carlo optimization of this objective. This approximation, justified by the central limit theorem and empirical evidence, gives an order of magnitude speedup and more stability. We show how to do fast dropout training for classification, regression, and multilayer neural networks. Beyond dropout, our technique is extended to integrate out other types of noise and small image transformations.}
}

@online{wang_finetuned_2021,
  title = {A {{Fine-Tuned}} Wav2vec 2.0/{{HuBERT Benchmark}} for {{Speech Emotion Recognition}}, {{Speaker Verification}} and {{Spoken Language Understanding}}},
  author = {Wang, Yingzhi and Boumadane, Abdelmoumene and Heba, Abdelwahab},
  date = {2021},
  pubstate = {preprint}
}

@misc{wang_further_2020,
  title = {Further {{Analysis}} of {{Outlier Detection}} with {{Deep Generative Models}}},
  author = {Wang, Ziyu and Dai, Bin and Wipf, David and Zhu, Jun},
  date = {2020-10-25},
  eprint = {2010.13064},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2010.13064},
  urldate = {2020-11-08},
  abstract = {The recent, counter-intuitive discovery that deep generative models (DGMs) can frequently assign a higher likelihood to outliers has implications for both outlier detection applications as well as our overall understanding of generative modeling. In this work, we present a possible explanation for this phenomenon, starting from the observation that a model’s typical set and high-density region may not coincide. From this vantage point we propose a novel outlier test, the empirical success of which suggests that the failure of existing likelihood-based outlier tests does not necessarily imply that the corresponding generative model is uncalibrated. We also conduct additional experiments to help disentangle the impact of low-level texture versus high-level semantics in differentiating outliers. In aggregate, these results suggest that modifications to the standard evaluation practices and benchmarks commonly applied in the literature are needed.},
  langid = {english}
}

@inproceedings{wang_gate_2017,
  title = {Gate Activation Signal Analysis for Gated Recurrent Neural Networks and Its Correlation with Phoneme Boundaries},
  booktitle = {Proceedings of the 18th {{Annual Conference}} of the {{International Speech Communication Association}} ({{Interspeech}})},
  author = {Wang, Yu-Hsuan and Chung, Cheng-Tao and Lee, Hung-yi},
  date = {2017},
  publisher = {{ISCA}}
}

@misc{wang_glue_2018,
  title = {{{GLUE}}: {{A Multi-Task Benchmark}} and {{Analysis Platform}} for {{Natural Language Understanding}}},
  shorttitle = {{{GLUE}}},
  author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
  date = {2018-04-20},
  eprint = {1804.07461},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1804.07461},
  urldate = {2019-09-08},
  abstract = {For natural language understanding (NLU) technology to be maximally useful, it must be able to process language in a way that is not exclusive to a single task, genre, or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation (GLUE) benchmark, a collection of tools for evaluating the performance of models across a diverse set of existing NLU tasks. By including tasks with limited training data, GLUE is designed to favor and encourage models that share general linguistic knowledge across tasks. GLUE also includes a hand-crafted diagnostic test suite that enables detailed linguistic analysis of models. We evaluate baselines based on current methods for transfer and representation learning and find that multi-task training on all tasks performs better than training a separate model per task. However, the low absolute performance of our best model indicates the need for improved general NLU systems.},
  langid = {english}
}

@article{wang_identifying_2018,
  title = {Identifying {{Generalization Properties}} in {{Neural Networks}}},
  author = {Wang, Huan and Keskar, Nitish Shirish and Xiong, Caiming and Socher, Richard},
  date = {2018-09-19},
  url = {https://arxiv.org/abs/1809.07402v1},
  urldate = {2018-09-25},
  abstract = {While it has not yet been proven, empirical evidence suggests that model generalization is related to local properties of the optima which can be described via the Hessian. We connect model generalization with the local property of a solution under the PAC-Bayes paradigm. In particular, we prove that model generalization ability is related to the Hessian, the higher-order “smoothness” terms characterized by the Lipschitz constant of the Hessian, and the scales of the parameters. Guided by the proof, we propose a metric to score the generalization capability of the model, as well as an algorithm that optimizes the perturbed model accordingly.},
  langid = {english}
}

@inproceedings{wang_improving_2020,
  title = {Improving Speech Recognition Using Consistent Predictions on Synthesized Speech},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Wang, Gary and Rosenberg, Andrew and Chen, Zhehuai and Zhang, Yu and Ramabhadran, Bhuvana and Wu, Yonghui and Moreno, Pedro},
  date = {2020},
  pages = {7029--7033}
}

@inproceedings{wang_improving_2022,
  title = {Improving Noise Robustness of Contrastive Speech Representation Learning with Speech Reconstruction},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Wang, Heming and Qian, Yao and Wang, Xiaofei and Wang, Yiming and Wang, Chengyi and Liu, Shujie and Yoshioka, Takuya and Li, Jinyu and Wang, DeLiang},
  date = {2022},
  pages = {6062--6066}
}

@article{wang_industrialstrength_,
  title = {An {{Industrial-Strength Audio Search Algorithm}}},
  author = {Wang, Avery Li-Chun},
  pages = {7},
  langid = {english}
}

@online{wang_learning_2021,
  title = {Towards {{Learning Universal Audio Representations}}},
  author = {Wang, Luyu and Luc, Pauline and Wu, Yan and Recasens, Adria and Smaira, Lucas and Brock, Andrew and Jaegle, Andrew and Alayrac, Jean-Baptiste and Dieleman, Sander and Carreira, Joao and family=Oord, given=Aaron, prefix=van den, useprefix=true},
  date = {2021},
  number = {2111.12124},
  eprint = {2111.12124},
  eprinttype = {arxiv},
  pubstate = {preprint}
}

@unpublished{wang_linformer_2020,
  title = {Linformer: {{Self-Attention}} with {{Linear Complexity}}},
  shorttitle = {Linformer},
  author = {Wang, Sinong and Li, Belinda Z. and Khabsa, Madian and Fang, Han and Ma, Hao},
  date = {2020-06-14},
  eprint = {2006.04768},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.04768},
  urldate = {2022-04-19},
  abstract = {Large transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses \$O(n\^2)\$ time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from \$O(n\^2)\$ to \$O(n)\$ in both time and space. The resulting linear transformer, the \textbackslash textit\{Linformer\}, performs on par with standard Transformer models, while being much more memory- and time-efficient.},
  keywords = {Read}
}

@inproceedings{wang_not_2021,
  title = {Not {{All Images}} Are {{Worth}} 16x16 {{Words}}: {{Dynamic Transformers}} for {{Efficient Image Recognition}}},
  shorttitle = {Not {{All Images}} Are {{Worth}} 16x16 {{Words}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Wang, Yulin and Huang, Rui and Song, Shiji and Huang, Zeyi and Huang, Gao},
  date = {2021-10-26},
  eprint = {2105.15075},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2105.15075},
  urldate = {2022-05-27},
  abstract = {Vision Transformers (ViT) have achieved remarkable success in large-scale image recognition. They split every 2D image into a fixed number of patches, each of which is treated as a token. Generally, representing an image with more tokens would lead to higher prediction accuracy, while it also results in drastically increased computational cost. To achieve a decent trade-off between accuracy and speed, the number of tokens is empirically set to 16x16 or 14x14. In this paper, we argue that every image has its own characteristics, and ideally the token number should be conditioned on each individual input. In fact, we have observed that there exist a considerable number of "easy" images which can be accurately predicted with a mere number of 4x4 tokens, while only a small fraction of "hard" ones need a finer representation. Inspired by this phenomenon, we propose a Dynamic Transformer to automatically configure a proper number of tokens for each input image. This is achieved by cascading multiple Transformers with increasing numbers of tokens, which are sequentially activated in an adaptive fashion at test time, i.e., the inference is terminated once a sufficiently confident prediction is produced. We further design efficient feature reuse and relationship reuse mechanisms across different components of the Dynamic Transformer to reduce redundant computations. Extensive empirical results on ImageNet, CIFAR-10, and CIFAR-100 demonstrate that our method significantly outperforms the competitive baselines in terms of both theoretical computational efficiency and practical inference speed. Code and pre-trained models (based on PyTorch and MindSpore) are available at https://github.com/blackfeather-wang/Dynamic-Vision-Transformer and https://github.com/blackfeather-wang/Dynamic-Vision-Transformer-MindSpore.}
}

@inproceedings{wang_phrasebert_2021,
  title = {Phrase-{{BERT}}: {{Improved}} Phrase Embeddings from {{BERT}} with an Application to Corpus Exploration},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Wang, Shufan and Thompson, Laure and Iyyer, Mohit},
  date = {2021},
  pages = {10837--10851}
}

@article{wang_proposal_2007,
  title = {A {{Proposal}} of {{Adaptive PID Controller Based}} on {{Reinforcement Learning}}},
  author = {Wang, Xue-Song and Cheng, Yu-Hu and Sun, Wei},
  date = {2007},
  journaltitle = {Journal of China University of Mining and Technology},
  volume = {17},
  number = {1},
  pages = {40--44},
  issn = {10061266},
  doi = {10.1016/S1006-1266(07)60009-1},
  url = {http://production.datastore.cvt.dk/filestore?oid=539cd4b160ad71dd2500f789&targetid=539cd4b160ad71dd2500f78b},
  urldate = {2017-09-27},
  abstract = {Aimed at the lack of self-tuning PID parameters in conventional PID controllers, the structure and learning algorithm of an adaptive PID controller based on reinforcement learning were proposed. Actor-Critic learning was used to tune PID parameters in an adaptive way by taking advantage of the model-free and on-line learning properties of re-inforcement learning effectively. In order to reduce the demand of storage space and to improve the learning efficiency, a single RBF neural network was used to approximate the policy function of Actor and the value function of Critic si-multaneously. The inputs of RBF network are the system error, as well as the first and the second-order differences of error. The Actor can realize the mapping from the system state to PID parameters, while the Critic evaluates the outputs of the Actor and produces TD error. Based on TD error performance index and gradient descent method, the updating rules of RBF kernel function and network weights were given. Simulation results show that the proposed controller is efficient for complex nonlinear systems and it is perfectly adaptable and strongly robust, which is better than that of a conventional PID controller.},
  isbn = {8651683995}
}

@online{wang_pyramid_2021,
  title = {Pyramid {{Vision Transformer}}: {{A Versatile Backbone}} for {{Dense Prediction}} without {{Convolutions}}},
  shorttitle = {Pyramid {{Vision Transformer}}},
  author = {Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling},
  date = {2021-08-11},
  eprint = {2102.12122},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2102.12122},
  urldate = {2022-10-18},
  abstract = {Although using convolutional neural networks (CNNs) as backbones achieves great successes in computer vision, this work investigates a simple backbone network useful for many dense prediction tasks without convolutions. Unlike the recently-proposed Transformer model (e.g., ViT) that is specially designed for image classification, we propose Pyramid Vision Transformer\textasciitilde (PVT), which overcomes the difficulties of porting Transformer to various dense prediction tasks. PVT has several merits compared to prior arts. (1) Different from ViT that typically has low-resolution outputs and high computational and memory cost, PVT can be not only trained on dense partitions of the image to achieve high output resolution, which is important for dense predictions but also using a progressive shrinking pyramid to reduce computations of large feature maps. (2) PVT inherits the advantages from both CNN and Transformer, making it a unified backbone in various vision tasks without convolutions by simply replacing CNN backbones. (3) We validate PVT by conducting extensive experiments, showing that it boosts the performance of many downstream tasks, e.g., object detection, semantic, and instance segmentation. For example, with a comparable number of parameters, RetinaNet+PVT achieves 40.4 AP on the COCO dataset, surpassing RetinNet+ResNet50 (36.3 AP) by 4.1 absolute AP. We hope PVT could serve as an alternative and useful backbone for pixel-level predictions and facilitate future researches. Code is available at https://github.com/whai362/PVT.},
  pubstate = {preprint},
  keywords = {Low}
}

@inproceedings{wang_relaxed_2020,
  title = {Relaxed {{Multivariate Bernoulli Distribution}} and {{Its Applications}} to {{Deep Generative Models}}},
  booktitle = {Proceedings of the 36th {{Conference}} on {{Uncertainty}} in {{Artificial Intelligence}} ({{UAI}})},
  author = {Wang, Xi and Yin, Junming},
  date = {2020},
  pages = {10},
  location = {{2020}},
  abstract = {Recent advances in variational auto-encoder (VAE) have demonstrated the possibility of approximating the intractable posterior distribution with a variational distribution parameterized by a neural network. To optimize the variational objective of VAE, the reparameterization trick is commonly applied to obtain a lowvariance estimator of the gradient. The main idea of the trick is to express the variational distribution as a differentiable function of parameters and a random variable with a fixed distribution. To extend the reparameterization trick to inference involving discrete latent variables, a common approach is to use a continuous relaxation of the categorical distribution as the approximate posterior. However, when applying continuous relaxation to the multivariate cases, multiple variables are typically assumed to be independent, making it suboptimal in applications where modeling dependency is crucial to the overall performance. In this work, we propose a multivariate generalization of the Relaxed Bernoulli distribution, which can be reparameterized and can capture the correlation between variables via a Gaussian copula. We demonstrate its effectiveness in two tasks: density estimation with Bernoulli VAE and semisupervised multi-label classification.},
  eventtitle = {Conference on {{Uncertainty}} in {{Artificial Intelligence}} ({{UAI}})},
  langid = {english}
}

@article{wang_robust_2007,
  title = {Robust {{Speech Rate Estimation}} for {{Spontaneous Speech}}},
  author = {Wang, D. and Narayanan, S. S.},
  date = {2007-11},
  journaltitle = {IEEE Transactions on Audio, Speech, and Language Processing},
  volume = {15},
  number = {8},
  pages = {2190--2201},
  issn = {1558-7916},
  doi = {10/d6njgr},
  abstract = {In this paper, we propose a direct method for speech rate estimation from acoustic features without requiring any automatic speech transcription. We compare various spectral and temporal signal analysis and smoothing strategies to better characterize the underlying syllable structure to derive speech rate. The proposed algorithm extends the methods of spectral sub- band correlation by including temporal correlation and the use of prominent spectral subbands for improving the signal correlation essential for syllable detection. Furthermore, to address some of the practical robustness issues in previously proposed methods, we introduce some novel components into the algorithm such as the use of pitch confidence for filtering spurious syllable envelope peaks, magnifying window for tackling neighboring syllable smearing, and relative peak measure thresholds for pseudo peak rejection. We also describe an automated approach for learning algorithm parameters from data, and find the optimal settings through Monte Carlo simulations and parameter sensitivity analysis. Final experimental evaluations are conducted based on a portion of the Switchboard corpus for which manual phonetic segmentation information, and published results for direct comparison are available. The results show a correlation coefficient of 0.745 with respect to the ground truth based on manual segmentation. This result is about a 17\% improvement compared to the current best single estimator and a 11\% improvement over the multiestimator evaluated on the same Switchboard database.}
}

@inproceedings{wang_segmental_2018,
  title = {Segmental {{Audio}} Word2vec: {{Representing Utterances}} as {{Sequences}} of {{Vectors}} with {{Applications}} in {{Spoken Term Detection}}},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Wang, Yu-Hsuan and Lee, Hung-yi and Lee, Lin-shan},
  date = {2018}
}

@online{wang_selfsupervised_2021,
  title = {Self-{{Supervised Learning}} for Speech Recognition with {{Intermediate}} Layer Supervision},
  author = {Wang, Chengyi and Wu, Yu and Chen, Sanyuan and Liu, Shujie and Li, Jinyu and Qian, Yao and Yang, Zhenglu},
  date = {2021-12-16},
  eprint = {2112.08778},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2112.08778},
  urldate = {2023-04-12},
  abstract = {Recently, pioneer work finds that speech pre-trained models can solve full-stack speech processing tasks, because the model utilizes bottom layers to learn speaker-related information and top layers to encode content-related information. Since the network capacity is limited, we believe the speech recognition performance could be further improved if the model is dedicated to audio content information learning. To this end, we propose Intermediate Layer Supervision for Self-Supervised Learning (ILS-SSL), which forces the model to concentrate on content information as much as possible by adding an additional SSL loss on the intermediate layers. Experiments on LibriSpeech test-other set show that our method outperforms HuBERT significantly, which achieves a 23.5\%/11.6\% relative word error rate reduction in the w/o language model setting for base/large models. Detailed analysis shows the bottom layers of our model have a better correlation with phonetic units, which is consistent with our intuition and explains the success of our method for ASR.},
  pubstate = {preprint}
}

@misc{wang_speaker_2017,
  title = {Speaker {{Diarization}} with {{LSTM}}},
  author = {Wang, Quan and Downey, Carlton and Wan, Li and Mansfield, Philip Andrew and Moreno, Ignacio Lopez},
  date = {2017-10-28},
  eprint = {1710.10468},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1710.10468},
  urldate = {2019-02-26},
  abstract = {For many years, i-vector based audio embedding techniques were the dominant approach for speaker verification and speaker diarization applications. However, mirroring the rise of deep learning in various domains, neural network based audio embeddings, also known as d-vectors, have consistently demonstrated superior speaker verification performance. In this paper, we build on the success of d-vector based speaker verification systems to develop a new d-vector based approach to speaker diarization. Specifically, we combine LSTM-based d-vector audio embeddings with recent work in non-parametric clustering to obtain a state-of-the-art speaker diarization system. Our system is evaluated on three standard public datasets, suggesting that d-vector based diarization systems offer significant advantages over traditional i-vector based systems. We achieved a 12.0\% diarization error rate on NIST SRE 2000 CALLHOME, while our model is trained with out-of-domain data from voice search logs.}
}

@inproceedings{wang_t-cvae_2019,
  title = {T-{{CVAE}}: {{Transformer-Based Conditioned Variational Autoencoder}} for {{Story Completion}}},
  shorttitle = {T-{{CVAE}}},
  booktitle = {Proceedings of the 28th {{International Joint Conference}} on {{Artificial Intelligence}} ({{IJCAI}})},
  author = {Wang, Tianming and Wan, Xiaojun},
  date = {2019-08},
  pages = {5233--5239},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  location = {{Macao, China}},
  doi = {10.24963/ijcai.2019/727},
  url = {https://www.ijcai.org/proceedings/2019/727},
  urldate = {2022-02-01},
  abstract = {Story completion is a very challenging task of generating the missing plot for an incomplete story, which requires not only understanding but also inference of the given contextual clues. In this paper, we present a novel conditional variational autoencoder based on Transformer for missing plot generation. Our model uses shared attention layers for encoder and decoder, which make the most of the contextual clues, and a latent variable for learning the distribution of coherent story plots. Through drawing samples from the learned distribution, diverse reasonable plots can be generated. Both automatic and manual evaluations show that our model generates better story plots than stateof-the-art models in terms of readability, diversity and coherence.},
  eventtitle = {Twenty-{{Eighth International Joint Conference}} on {{Artificial Intelligence}}},
  isbn = {978-0-9992411-4-1},
  langid = {english}
}

@unpublished{wang_unispeech_2021,
  title = {{{UniSpeech}}: {{Unified Speech Representation Learning}} with {{Labeled}} and {{Unlabeled Data}}},
  shorttitle = {{{UniSpeech}}},
  author = {Wang, Chengyi and Wu, Yu and Qian, Yao and Kumatani, Kenichi and Liu, Shujie and Wei, Furu and Zeng, Michael and Huang, Xuedong},
  date = {2021-06-10},
  eprint = {2101.07597},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2101.07597},
  urldate = {2021-11-02},
  abstract = {In this paper, we propose a unified pre-training approach called UniSpeech to learn speech representations with both unlabeled and labeled data, in which supervised phonetic CTC learning and phonetically-aware contrastive self-supervised learning are conducted in a multi-task learning manner. The resultant representations can capture information more correlated with phonetic structures and improve the generalization across languages and domains. We evaluate the effectiveness of UniSpeech for cross-lingual representation learning on public CommonVoice corpus. The results show that UniSpeech outperforms self-supervised pretraining and supervised transfer learning for speech recognition by a maximum of 13.4\% and 17.8\% relative phone error rate reductions respectively (averaged over all testing languages). The transferability of UniSpeech is also demonstrated on a domain-shift speech recognition task, i.e., a relative word error rate reduction of 6\% against the previous approach.}
}

@inproceedings{wang_unsupervised_2015,
  title = {Unsupervised Learning of Acoustic Features via Deep Canonical Correlation Analysis},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Wang, Weiran and Arora, Raman and Livescu, Karen and Bilmes, Jeff},
  date = {2015},
  pages = {4590--4594}
}

@unpublished{wang_unsupervised_2020,
  title = {Unsupervised {{Pre-training}} of {{Bidirectional Speech Encoders}} via {{Masked Reconstruction}}},
  author = {Wang, Weiran and Tang, Qingming and Livescu, Karen},
  date = {2020-05-05},
  eprint = {2001.10603},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2001.10603},
  urldate = {2021-10-12},
  abstract = {We propose an approach for pre-training speech representations via a masked reconstruction loss. Our pre-trained encoder networks are bidirectional and can therefore be used directly in typical bidirectional speech recognition models. The pre-trained networks can then be fine-tuned on a smaller amount of supervised data for speech recognition. Experiments with this approach on the LibriSpeech and Wall Street Journal corpora show promising results. We find that the main factors that lead to speech recognition improvements are: masking segments of sufficient width in both time and frequency, pre-training on a much larger amount of unlabeled data than the labeled data, and domain adaptation when the unlabeled and labeled data come from different domains. The gain from pre-training is additive to that of supervised data augmentation.}
}

@inproceedings{wang_vim_2022,
  title = {Vim: {{Out-of-distribution}} with Virtual-Logit Matching},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition},
  author = {Wang, Haoqi and Li, Zhizhong and Feng, Litong and Zhang, Wayne},
  date = {2022},
  pages = {4921--4930}
}

@misc{wang_vlmo_2021,
  title = {{{VLMo}}: {{Unified Vision-Language Pre-Training}} with {{Mixture-of-Modality-Experts}}},
  shorttitle = {{{VLMo}}},
  author = {Wang, Wenhui and Bao, Hangbo and Dong, Li and Wei, Furu},
  date = {2021-11-03},
  eprint = {2111.02358},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2111.02358},
  urldate = {2022-02-03},
  abstract = {We present a unified Vision-Language pretrained Model (VLMo) that jointly learns a dual encoder and a fusion encoder with a modular Transformer network. Specifically, we introduce Mixture-of-Modality-Experts (MoME) Transformer, where each block contains a pool of modality-specific experts and a shared self-attention layer. Because of the modeling flexibility of MoME, pretrained VLMo can be fine-tuned as a fusion encoder for vision-language classification tasks, or used as a dual encoder for efficient image-text retrieval. Moreover, we propose a stagewise pre-training strategy, which effectively leverages large-scale image-only and text-only data besides image-text pairs. Experimental results show that VLMo achieves state-of-the-art results on various vision-language tasks, including VQA and NLVR2. The code and pretrained models are available at https://aka.ms/vlmo.}
}

@inproceedings{wang_voxpopuli_2021,
  title = {{{VoxPopuli}}: {{A}} Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation},
  booktitle = {Proceedings of the {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{ACL}})},
  author = {Wang, Changhan and Riviere, Morgane and Lee, Ann and Wu, Anne and Talnikar, Chaitanya and Haziza, Daniel and Williamson, Mary and Pino, Juan and Dupoux, Emmanuel},
  date = {2021},
  eventtitle = {Annual {{Meeting}} of the {{Association}} for {{Computational Linguistics}}}
}

@unpublished{wang_wav2vecswitch_2021,
  title = {Wav2vec-{{Switch}}: {{Contrastive Learning}} from {{Original-noisy Speech Pairs}} for {{Robust Speech Recognition}}},
  shorttitle = {Wav2vec-{{Switch}}},
  author = {Wang, Yiming and Li, Jinyu and Wang, Heming and Qian, Yao and Wang, Chengyi and Wu, Yu},
  date = {2021-10-10},
  eprint = {2110.04934},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2110.04934},
  urldate = {2021-10-18},
  abstract = {The goal of self-supervised learning (SSL) for automatic speech recognition (ASR) is to learn good speech representations from a large amount of unlabeled speech for the downstream ASR task. However, most SSL frameworks do not consider noise robustness which is crucial for real-world applications. In this paper we propose wav2vec-Switch, a method to encode noise robustness into contextualized representations of speech via contrastive learning. Specifically, we feed original-noisy speech pairs simultaneously into the wav2vec 2.0 network. In addition to the existing contrastive learning task, we switch the quantized representations of the original and noisy speech as additional prediction targets of each other. By doing this, it enforces the network to have consistent predictions for the original and noisy speech, thus allows to learn contextualized representation with noise robustness. Our experiments on synthesized and real noisy data show the effectiveness of our method: it achieves 2.9--4.9\% relative word error rate (WER) reduction on the synthesized noisy LibriSpeech data without deterioration on the original data, and 5.7\% on CHiME-4 real 1-channel noisy data compared to a data augmentation baseline even with a strong language model for decoding. Our results on CHiME-4 can match or even surpass those with well-designed speech enhancement components.}
}

@online{warden_speech_2018,
  title = {Speech {{Commands}}: {{A}} Dataset for Limited-Vocabulary Speech Recognition},
  author = {Warden, Pete},
  date = {2018},
  eprint = {1804.03209},
  eprinttype = {arxiv},
  pubstate = {preprint}
}

@book{watanabe_algebraic_2009,
  title = {Algebraic Geometry and Statistical Learning Theory},
  author = {Watanabe, Sumio},
  date = {2009},
  series = {Cambridge Monographs on Applied and Computational Mathematics},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge}}
}

@article{watanabe_asymptotic_2010,
  title = {Asymptotic Equivalence of {{Bayes}} Cross Validation and Widely Applicable Information Criterion in Singular Learning Theory.},
  author = {Watanabe, Sumio and Opper, Manfred},
  date = {2010},
  journaltitle = {Journal of Machine Learning Research (JMLR)},
  volume = {11},
  number = {12}
}

@article{weber_reinforced_2015,
  title = {Reinforced {{Variational Inference}}},
  author = {Weber, Theophane and Heess, Nicolas and Eslami, S. M. Ali and Schulman, John and Wingate, David and Silver, David},
  date = {2015},
  journaltitle = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
  number = {2},
  pages = {1--9}
}

@article{weiler_coordinate_nodate,
  title = {Coordinate {{Independent Convolutional Networks}}},
  author = {Weiler, Maurice and Verlinde, Erik and Forré, Patrick and Welling, Max},
  pages = {271},
  abstract = {Motivated by the vast success of deep convolutional networks, there is a great interest in generalizing convolutions to non-Euclidean manifolds. A major complication in comparison to flat spaces is that it is unclear in which alignment a convolution kernel should be applied on a manifold. The underlying reason for this ambiguity is that general manifolds do not come with a canonical choice of reference frames (gauge). Kernels and features therefore have to be expressed relative to arbitrary coordinates. We argue that the particular choice of coordinatization should not affect a network’s inference – it should be coordinate independent. A simultaneous demand for coordinate independence and weight sharing is shown to result in a requirement on the network to be equivariant under local gauge transformations (changes of local reference frames). The ambiguity of reference frames depends thereby on the G-structure of the manifold, such that the necessary level of gauge equivariance is prescribed by the corresponding structure group G. Coordinate independent convolutions are proven to be equivariant w.r.t. those isometries that are symmetries of the G-structure. The resulting theory is formulated in a coordinate free fashion in terms of fiber bundles. To exemplify the design of coordinate independent convolutions, we implement a convolutional network on the Möbius strip. The generality of our differential geometric formulation of convolutional networks is demonstrated by an extensive literature review which explains a large number of Euclidean CNNs, spherical CNNs and CNNs on general surfaces as specific instances of coordinate independent convolutions.},
  langid = {english}
}

@article{weinberger_speech_2011,
  title = {The {{Speech Accent Archive}}: {{Towards}} a {{Typology}} of {{English Accents}}},
  author = {Weinberger, Steven and Kunath, Stephen},
  date = {2011-12},
  journaltitle = {Language and Computers},
  volume = {73}
}

@report{weinstein_structured_2020,
  type = {preprint},
  title = {A Structured Observation Distribution for Generative Biological Sequence Prediction and Forecasting},
  author = {Weinstein, Eli N. and Marks, Debora S.},
  date = {2020-08-03},
  institution = {{Genomics}},
  doi = {10.1101/2020.07.31.231381},
  url = {http://biorxiv.org/lookup/doi/10.1101/2020.07.31.231381},
  urldate = {2021-06-02},
  abstract = {Generative probabilistic modeling of biological sequences has widespread existing and potential application across biology and biomedicine, from evolutionary biology to epidemiology to protein design. Many standard sequence analysis methods preprocess data using a multiple sequence alignment (MSA) algorithm, one of the most widely used computational methods in all of science(Van Noorden et al., 2014). However, as we show in this article, training generative probabilistic models with MSA preprocessing leads to statistical pathologies in the context of sequence prediction and forecasting. To address these problems, we propose a principled drop-in alternative to MSA preprocessing in the form of a structured observation distribution (the “MuE” distribution). The MuE is a latent alignment model in which not only the alignment variable but also the regressor sequence can be latent. We prove theoretically that the MuE distribution comprehensively generalizes popular methods for inferring biological sequence alignments, and provide a precise characterization of how such biological models have differed from natural language latent alignment models. We show empirically that models that use the MuE as an observation distribution outperform comparable methods across a variety of datasets, and apply MuE models to a novel problem for generative probabilistic sequence models: forecasting pathogen evolution.},
  langid = {english}
}

@misc{weiss_wavetacotron_2021,
  title = {Wave-{{Tacotron}}: {{Spectrogram-free}} End-to-End Text-to-Speech Synthesis},
  shorttitle = {Wave-{{Tacotron}}},
  author = {Weiss, Ron J. and Skerry-Ryan, R. J. and Battenberg, Eric and Mariooryad, Soroosh and Kingma, Diederik P.},
  date = {2021-02-05},
  eprint = {2011.03568},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2011.03568},
  urldate = {2021-03-20},
  abstract = {We describe a sequence-to-sequence neural network which directly generates speech waveforms from text inputs. The architecture extends the Tacotron model by incorporating a normalizing flow into the autoregressive decoder loop. Output waveforms are modeled as a sequence of non-overlapping fixed-length blocks, each one containing hundreds of samples. The interdependencies of waveform samples within each block are modeled using the normalizing flow, enabling parallel training and synthesis. Longer-term dependencies are handled autoregressively by conditioning each flow on preceding blocks. This model can be optimized directly with maximum likelihood, without using intermediate, hand-designed features nor additional loss terms. Contemporary state-of-the-art text-to-speech (TTS) systems use a cascade of separately learned models: one (such as Tacotron) which generates intermediate features (such as spectrograms) from text, followed by a vocoder (such as WaveRNN) which generates waveform samples from the intermediate features. The proposed system, in contrast, does not use a fixed intermediate representation, and learns all parameters end-to-end. Experiments show that the proposed model generates speech with quality approaching a state-of-the-art neural TTS system, with significantly improved generation speed.},
  langid = {english}
}

@article{welford_note_1962,
  title = {Note on a {{Method}} for {{Calculating Correct Sums}} of {{Squares}} and {{Products}}},
  author = {Welford, B. P.},
  date = {1962},
  journaltitle = {Technometrics},
  volume = {4},
  number = {3},
  pages = {419--420},
  issn = {00401706},
  url = {http://findit.dtu.dk/en/catalog/2255540095},
  urldate = {2018-04-04}
}

@article{welling_bayesian_2011,
  title = {Bayesian Posterior Contraction Rates for Linear Severely Ill-Posed Inverse Problems},
  author = {Welling, Max and Teh, Yee Whye},
  date = {2011},
  journaltitle = {Proceedings of the International Conference on Machine Learning (ICML)},
  volume = {22},
  number = {3},
  eprint = {1203.5753v5},
  eprinttype = {arxiv},
  pages = {681--688},
  issn = {10495258},
  doi = {10.1515/jip-2012-0071},
  abstract = {In this paper we propose a new framework for learning from large scale datasets based on iterative learning from small mini-batches. By adding the right amount of noise to a standard stochastic gradient optimization algorithm we show that the iterates will converge to samples from the true posterior distribution as we anneal the stepsize. This seamless transition between optimization and Bayesian posterior sampling provides an in-built protection against overfitting. We also propose a practical method for Monte Carlo estimates of posterior statistics which monitors a ``sampling threshold'' and collects samples after it has been surpassed. We apply the method to three models: a mixture of Gaussians, logistic regression and ICA with natural gradients.},
  isbn = {978-1-4503-0619-5}
}

@article{wenstrup_retrospective_2023,
  title = {A {{Retrospective Study}} on {{Machine Learning-Assisted Stroke Recognition}} for {{Medical Helpline Calls}}},
  author = {Wenstrup, Jonathan and Havtorn, Jakob D. and Borgholt, Lasse and Blomberg, Stig N. and Maaløe, Lars and Sayre, Michael and Christensen, Hanne and Kruuse, Christina},
  date = {2023},
  journaltitle = {npj Digital Medicine},
  annotation = {(under review)}
}

@inproceedings{wenzel_how_2020,
  title = {How {{Good}} Is the {{Bayes Posterior}} in {{Deep Neural Networks Really}}?},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Wenzel, Florian and Roth, Kevin and Veeling, Bastiaan S and Swiatkowski, Jakub and Tran, Linh and Mandt, Stephan and Snoek, Jasper and Salimans, Tim and Jenatton, Rodolphe and Nowozin, Sebastian},
  date = {2020},
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {119},
  pages = {10248--10259},
  publisher = {{PMLR}},
  location = {{Virtual}},
  url = {http://proceedings.mlr.press/v119/wenzel20a.html},
  abstract = {During the past five years the Bayesian deep learning community has developed increasingly accurate and efficient approximate inference procedures that allow for Bayesian inference in deep neural networks. However, despite this algorithmic progress and the promise of improved uncertainty quantification and sample efficiency there are—as of early 2020—no publicized deployments of Bayesian neural networks in industrial practice. In this work we cast doubt on the current understanding of Bayes posteriors in popular deep neural networks: we demonstrate through careful MCMC sampling that the posterior predictive induced by the Bayes posterior yields systematically worse predictions compared to simpler methods including point estimates obtained from SGD. Furthermore, we demonstrate that predictive performance is improved significantly through the use of a “cold posterior” that overcounts evidence. Such cold posteriors sharply deviate from the Bayesian paradigm but are commonly used as heuristic in Bayesian deep learning papers. We put forward several hypotheses that could explain cold posteriors and evaluate the hypotheses through experiments. Our work questions the goal of accurate posterior approximations in Bayesian deep learning: If the true Bayes posterior is poor, what is the use of more accurate approximations? Instead, we argue that it is timely to focus on understanding the origin of the improved performance of cold posteriors.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english}
}

@article{westbury_xray_1990,
  title = {X-Ray Microbeam Speech Production Database},
  author = {Westbury, J. and Milenkovic, P. and Weismer, G. and Kent, R.},
  date = {1990},
  journaltitle = {JASA},
  volume = {88},
  number = {S1},
  pages = {S56--S56}
}

@misc{weston_aicomplete_2015,
  title = {Towards {{AI-Complete Question Answering}}: {{A Set}} of {{Prerequisite Toy Tasks}}},
  shorttitle = {Towards {{AI-Complete Question Answering}}},
  author = {Weston, Jason and Bordes, Antoine and Chopra, Sumit and Rush, Alexander M. and family=Merriënboer, given=Bart, prefix=van, useprefix=true and Joulin, Armand and Mikolov, Tomas},
  date = {2015-02-19},
  eprint = {1502.05698},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1502.05698},
  urldate = {2019-06-19},
  abstract = {One long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. To measure progress towards that goal, we argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. We believe many existing learning systems can currently not solve them, and hence our aim is to classify these tasks into skill sets, so that researchers can identify (and then rectify) the failings of their systems. We also extend and improve the recently introduced Memory Networks model, and show it is able to solve some, but not all, of the tasks.},
  langid = {english}
}

@inproceedings{weston_memory_2014,
  title = {Memory {{Networks}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Weston, Jason and Chopra, Sumit and Bordes, Antoine},
  date = {2014},
  eprint = {9377276},
  eprinttype = {pmid},
  issn = {1098-7576},
  doi = {v0},
  url = {http://arxiv.org/abs/1410.3916},
  urldate = {2018-06-07},
  abstract = {We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.},
  isbn = {978-1-4244-6917-8}
}

@article{wickstrom_relax_2023,
  title = {{{RELAX}}: {{Representation Learning Explainability}}},
  shorttitle = {{{RELAX}}},
  author = {Wickstrøm, Kristoffer K. and Trosten, Daniel J. and Løkse, Sigurd and Boubekki, Ahcène and Mikalsen, Karl Øyvind and Kampffmeyer, Michael C. and Jenssen, Robert},
  date = {2023-06},
  journaltitle = {International Journal of Computer Vision},
  shortjournal = {Int J Comput Vis},
  volume = {131},
  number = {6},
  pages = {1584--1610},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-023-01773-2},
  url = {https://link.springer.com/10.1007/s11263-023-01773-2},
  urldate = {2023-10-10},
  abstract = {Despite the significant improvements that self-supervised representation learning has led to when learning from unlabeled data, no methods have been developed that explain what influences the learned representation. We address this need through our proposed approach, RELAX, which is the first approach for attribution-based explanations of representations. Our approach can also model the uncertainty in its explanations, which is essential to produce trustworthy explanations. RELAX explains representations by measuring similarities in the representation space between an input and masked out versions of itself, providing intuitive explanations that significantly outperform the gradient-based baselines. We provide theoretical interpretations of RELAX and conduct a novel analysis of feature extractors trained using supervised and unsupervised learning, providing insights into different learning strategies. Moreover, we conduct a user study to assess how well the proposed approach aligns with human intuition and show that the proposed method outperforms the baselines in both the quantitative and human evaluation studies. Finally, we illustrate the usability of RELAX in several use cases and highlight that incorporating uncertainty can be essential for providing faithful explanations, taking a crucial step towards explaining representations.},
  langid = {english},
  keywords = {Read},
  annotation = {1 citations (Crossref) [2023-10-10]}
}

@misc{wiegreffe_attention_2019,
  title = {Attention Is Not Not {{Explanation}}},
  author = {Wiegreffe, Sarah and Pinter, Yuval},
  date = {2019-09-05},
  eprint = {1908.04626},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1908.04626},
  urldate = {2021-03-30},
  abstract = {Attention mechanisms play a central role in NLP systems, especially within recurrent neural network (RNN) models. Recently, there has been increasing interest in whether or not the intermediate representations offered by these modules may be used to explain the reasoning for a model’s prediction, and consequently reach insights regarding the model’s decision-making process. A recent paper claims that ‘Attention is not Explanation’ (Jain and Wallace, 2019). We challenge many of the assumptions underlying this work, arguing that such a claim depends on one’s definition of explanation, and that testing it needs to take into account all elements of the model. We propose four alternative tests to determine when/whether attention can be used as explanation: a simple uniform-weights baseline; a variance calibration based on multiple random seed runs; a diagnostic framework using frozen weights from pretrained models; and an end-to-end adversarial attention training protocol. Each allows for meaningful interpretation of attention mechanisms in RNN models. We show that even when reliable adversarial distributions can be found, they don’t perform well on the simple diagnostic, indicating that prior work does not disprove the usefulness of attention mechanisms for explainability.},
  langid = {english}
}

@article{wierstra_natural_2008,
  title = {Natural {{Evolution Strategies}}},
  author = {Wierstra, Daan and Schaul, Tom and Peters, Jan and Schmidhuber, Jürgen},
  date = {2008},
  journaltitle = {Journal of Machine Learning Research},
  volume = {15},
  eprint = {1106.4487},
  eprinttype = {arxiv},
  pages = {3381--3387},
  issn = {15337928},
  doi = {10.1109/CEC.2008.4631255},
  abstract = {This paper presents Natural Evolution Strategies (NES), a novel algorithm for performing real-valued 'black box' function optimization: optimizing an unknown objective function where algorithm-selected function measurements constitute the only information accessible to the method. Natural Evolution Strategies search the fitness landscape using a multivariate normal distribution with a self-adapting mutation matrix to generate correlated mutations in promising regions. NES shares this property with Covariance Matrix Adaption (CMA), an Evolution Strategy (ES) which has been shown to perform well on a variety of high-precision optimization tasks. The Natural Evolution Strategies algorithm, however, is simpler, less ad-hoc and more principled. Self-adaptation of the mutation matrix is derived using a Monte Carlo estimate of the natural gradient towards better expected fitness. By following the natural gradient instead of the 'vanilla' gradient, we can ensure efficient update steps while preventing early convergence due to overly greedy updates, resulting in reduced sensitivity to local suboptima. We show NES has competitive performance with CMA on unimodal tasks, while outperforming it on several multimodal tasks that are rich in deceptive local optima. © 2008 IEEE.},
  isbn = {978-1-4244-1822-0},
  keywords = {★}
}

@book{wilcoxon_individual_1992,
  title = {Individual Comparisons by Ranking Methods},
  author = {Wilcoxon, Frank},
  date = {1992},
  publisher = {{Springer}}
}

@article{wildenschild_registration_2013,
  title = {Registration of Acute Stroke: {{Validity}} in the {{Danish Stroke Registry}} and the {{Danish National Registry}} of {{Patients}}},
  author = {Wildenschild, Cathrine and Mehnert, Frank and Thomsen, Reimar Wernich and Iversen, Helle Klingenberg and Vestergaard, Karsten and Ingeman, Annette and Johnsen, Søren Paaske},
  date = {2013},
  journaltitle = {Clinical epidemiology},
  pages = {27--36},
  publisher = {{Taylor \& Francis}}
}

@article{wilks_large-sample_1938,
  title = {The Large-Sample Distribution of the Likelihood Ratio for Testing Composite Hypotheses},
  author = {Wilks, S. S.},
  date = {1938-03},
  journaltitle = {Annals of Mathematical Statistics},
  volume = {9},
  number = {1},
  pages = {60--62},
  doi = {10.1214/aoms/1177732360},
  url = {https://doi.org/10.1214/aoms/1177732360}
}

@article{williams_simple_1992,
  title = {Simple {{Statistical Gradient-Following Algorithms}} for {{Connectionist Reinforcement Learning}}},
  author = {Williams, Ronald J.},
  date = {1992},
  journaltitle = {Journal of Machine Learning},
  volume = {8},
  number = {3},
  pages = {229--256},
  doi = {10.1007/978-1-4615-3618-5_2},
  abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
  isbn = {978-1-4615-3618-5}
}

@article{wilpon_modified_1985,
  title = {A Modified {{K-means}} Clustering Algorithm for Use in Isolated Work Recognition},
  author = {Wilpon, J. and Rabiner, L.},
  date = {1985},
  journaltitle = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume = {33},
  number = {3},
  pages = {587--594}
}

@misc{wilson_bayesian_2020,
  title = {Bayesian {{Deep Learning}} and a {{Probabilistic Perspective}} of {{Generalization}}},
  author = {Wilson, Andrew Gordon and Izmailov, Pavel},
  date = {2020-04-27},
  eprint = {2002.08791},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2002.08791},
  urldate = {2021-05-07},
  abstract = {The key distinguishing property of a Bayesian approach is marginalization, rather than using a single setting of weights. Bayesian marginalization can particularly improve the accuracy and calibration of modern deep neural networks, which are typically underspecified by the data, and can represent many compelling but different solutions. We show that deep ensembles provide an effective mechanism for approximate Bayesian marginalization, and propose a related approach that further improves the predictive distribution by marginalizing within basins of attraction, without significant overhead. We also investigate the prior over functions implied by a vague distribution over neural network weights, explaining the generalization properties of such models from a probabilistic perspective. From this perspective, we explain results that have been presented as mysterious and distinct to neural network generalization, such as the ability to fit images with random labels, and show that these results can be reproduced with Gaussian processes. We also show that Bayesian model averaging alleviates double descent, resulting in monotonic performance improvements with increased flexibility. Finally, we provide a Bayesian perspective on tempering for calibrating predictive distributions.},
  langid = {english}
}

@misc{wilson_evolving_2018,
  title = {Evolving Simple Programs for Playing {{Atari}} Games},
  author = {Wilson, Dennis G. and Cussat-Blanc, Sylvain and Luga, Hervé and Miller, Julian F.},
  date = {2018},
  eprint = {1806.05695v1},
  eprinttype = {arxiv},
  abstract = {Cartesian Genetic Programming (CGP) has previously shown capabilities in image processing tasks by evolving programs with a function set specialized for computer vision. A similar approach can be applied to Atari playing. Programs are evolved using mixed type CGP with a function set suited for matrix operations, including image processing, but allowing for controller behavior to emerge. While the programs are relatively small, many controllers are competitive with state of the art methods for the Atari benchmark set and require less training time. By evaluating the programs of the best evolved individuals, simple but eeective strategies can be found. CCS CONCEPTS •Computing methodologies → Artiicial intelligence; Model development and analysis;}
}

@article{wilson_harmonic_2019,
  title = {The Harmonic Mean P-Value for Combining Dependent Tests},
  author = {Wilson, Daniel J},
  date = {2019},
  journaltitle = {Proceedings of the National Academy of Sciences (PNAS)},
  pages = {1195--1200},
  publisher = {{National Academy of Sciences}}
}

@report{wilson_marginal_2017,
  title = {The {{Marginal Value}} of {{Adaptive Gradient Methods}} in {{Machine Learning}}},
  author = {Wilson, Ashia C and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nathan and Recht, Benjamin},
  date = {2017},
  abstract = {Adaptive optimization methods, which perform local optimization with a metric constructed from the history of iterates, are becoming increasingly popular for training deep neural networks. Examples include AdaGrad, RMSProp, and Adam. We show that for simple overparameterized problems, adaptive methods often find drastically different solutions than gradient descent (GD) or stochastic gradient descent (SGD). We construct an illustrative binary classification problem where the data is linearly separable, GD and SGD achieve zero test error, and AdaGrad, Adam, and RMSProp attain test errors arbitrarily close to half. We additionally study the empirical generalization capability of adaptive methods on several state-of-the-art deep learning models. We observe that the solutions found by adaptive methods generalize worse (often significantly worse) than SGD, even when these solutions have better training performance. These results suggest that practitioners should reconsider the use of adaptive methods to train neural networks.}
}

@misc{wingate_automated_2013,
  title = {Automated {{Variational Inference}} in {{Probabilistic Programming}}},
  author = {Wingate, David and Weber, Theophane},
  date = {2013},
  eprint = {1301.1299},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1301.1299},
  urldate = {2018-03-23},
  abstract = {We present a new algorithm for approximate inference in probabilistic programs, based on a stochastic gradient for variational programs. This method is efficient without restrictions on the probabilistic program; it is particularly practical for distributions which are not analytically tractable, including highly structured distributions that arise in probabilistic programs. We show how to automatically derive mean-field probabilistic programs and optimize them, and demonstrate that our perspective improves inference efficiency over other algorithms.}
}

@unpublished{winkens_contrastive_2020,
  title = {Contrastive Training for Improved Out-of-Distribution Detection},
  author = {Winkens, Jim and Bunel, Rudy and Roy, Abhijit Guha and Stanforth, Robert and Natarajan, Vivek and Ledsam, Joseph R and MacWilliams, Patricia and Kohli, Pushmeet and Karthikesalingam, Alan and Kohl, Simon and others},
  date = {2020},
  eprint = {2007.05566},
  eprinttype = {arxiv}
}

@article{wiseman_learning_nodate,
  title = {Learning {{Anaphoricity}} and {{Antecedent Ranking Features}} for {{Coreference Resolution}}},
  author = {Wiseman, Sam and Rush, Alexander M. and Shieber, Stuart M. and Weston, Jason},
  abstract = {We introduce a simple, non-linear mention-ranking model for coreference resolution that attempts to learn distinct feature representations for anaphoricity detection and antecedent ranking, which we encourage by pre-training on a pair of corresponding subtasks. Although we use only simple, unconjoined features, the model is able to learn useful representa-tions, and we report the best overall score on the CoNLL 2012 English test set to date.}
}

@article{wiskott_slow_2002,
  title = {Slow Feature Analysis: {{Unsupervised}} Learning of Invariances},
  author = {Wiskott, Laurenz and Sejnowski, Terrence J},
  date = {2002},
  journaltitle = {Neural Computation},
  volume = {14},
  number = {4},
  pages = {715--770}
}

@article{wolpert_no_1997,
  title = {No Free Lunch Theorems for Optimization},
  author = {Wolpert, D.H. and Macready, W.G.},
  date = {1997-04},
  journaltitle = {IEEE Transactions on Evolutionary Computation},
  volume = {1},
  number = {1},
  pages = {67--82},
  issn = {1089778X},
  doi = {10/d2x5nj},
  url = {http://ieeexplore.ieee.org/document/585893/},
  urldate = {2018-11-07},
  abstract = {A framework is developed to explore the connection between effective optimization algorithms and the problems they are solving. A number of “no free lunch” (NFL) theorems are presented which establish that for any algorithm, any elevated performance over one class of problems is offset by performance over another class. These theorems result in a geometric interpretation of what it means for an algorithm to be well suited to an optimization problem. Applications of the NFL theorems to information-theoretic aspects of optimization and benchmark measures of performance are also presented. Other issues addressed include time-varying optimization problems and a priori “head-to-head” minimax distinctions between optimization algorithms, distinctions that result despite the NFL theorems’ enforcing of a type of uniformity over all algorithms.},
  langid = {english}
}

@online{worldhealthorganisationwho_international_2023,
  title = {International {{Classification}} of {{Diseases}} ({{ICD}})},
  author = {{World Health Organisation (WHO)}},
  date = {2023},
  url = {https://icd.who.int/},
  urldate = {2023-08-26}
}

@article{wrench_new_2001,
  title = {A New Resource for Production Modelling in Speech Technology},
  author = {Wrench, A.},
  date = {2001},
  journaltitle = {Procedings of the Institute of Acoustics},
  volume = {23},
  number = {3},
  pages = {207--218}
}

@inproceedings{wu_characterizing_2022,
  title = {Characterizing the Adversarial Vulnerability of Speech Self-{{Supervised}} Learning},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Wu, Haibin and Zheng, Bo and Li, Xu and Wu, Xixin and Lee, Hung-Yi and Meng, Helen},
  date = {2022},
  pages = {3164--3168},
  publisher = {{IEEE}},
  location = {{Singapore}}
}

@inproceedings{wu_differentiable_2019,
  title = {Differentiable Antithetic Sampling for Variance Reduction in Stochastic Variational Inference},
  booktitle = {The 22nd {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}} ({{AISTATS}})},
  author = {Wu, Mike and Goodman, Noah D. and Ermon, Stefano},
  date = {2019},
  pages = {2877--2886},
  location = {{Naha, Okinawa, Japan}},
  url = {http://proceedings.mlr.press/v89/wu19c.html},
  eventtitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}}
}

@misc{wu_google_2016,
  title = {Google's {{Neural Machine Translation System}}: {{Bridging}} the {{Gap}} between {{Human}} and {{Machine Translation}}},
  author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus},
  date = {2016},
  eprint = {1609.08144},
  eprinttype = {arxiv},
  url = {https://arxiv.org/abs/1609.08144}
}

@inproceedings{wu_group_2018,
  title = {Group {{Normalization}}},
  booktitle = {Proceedings of the {{European Conference}} on {{Computer Vision}} ({{ECCV}})},
  author = {Wu, Yuxin and He, Kaiming},
  date = {2018-06-11},
  eprint = {1803.08494},
  eprinttype = {arxiv},
  pages = {pp. 3-19},
  url = {http://arxiv.org/abs/1803.08494},
  urldate = {2020-10-22},
  abstract = {Batch Normalization (BN) is a milestone technique in the development of deep learning, enabling various networks to train. However, normalizing along the batch dimension introduces problems — BN’s error increases rapidly when the batch size becomes smaller, caused by inaccurate batch statistics estimation. This limits BN’s usage for training larger models and transferring features to computer vision tasks including detection, segmentation, and video, which require small batches constrained by memory consumption. In this paper, we present Group Normalization (GN) as a simple alternative to BN. GN divides the channels into groups and computes within each group the mean and variance for normalization. GN’s computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GN has 10.6\% lower error than its BN counterpart when using a batch size of 2; when using typical batch sizes, GN is comparably good with BN and outperforms other normalization variants. Moreover, GN can be naturally transferred from pre-training to fine-tuning. GN can outperform its BNbased counterparts for object detection and segmentation in COCO,1 and for video classification in Kinetics, showing that GN can effectively replace the powerful BN in a variety of tasks. GN can be easily implemented by a few lines of code in modern libraries.},
  langid = {english}
}

@inproceedings{wu_ngc_2021,
  title = {{{NGC}}: {{A Unified Framework}} for {{Learning}} with {{Open-World Noisy Data}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Wu, Zhi-Fan and Wei, Tong and Jiang, Jianwen and Mao, Chaojie and Tang, Mingqian and Li, Yu-Feng},
  date = {2021},
  pages = {62--71}
}

@online{wu_streaming_2020,
  title = {Streaming {{Transformer-based Acoustic Models Using Self-attention}} with {{Augmented Memory}}},
  author = {Wu, Chunyang and Wang, Yongqiang and Shi, Yangyang and Yeh, Ching-Feng and Zhang, Frank},
  date = {2020-05-16},
  eprint = {2005.08042},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2005.08042},
  urldate = {2022-10-10},
  abstract = {Transformer-based acoustic modeling has achieved great suc-cess for both hybrid and sequence-to-sequence speech recogni-tion. However, it requires access to the full sequence, and thecomputational cost grows quadratically with respect to the in-put sequence length. These factors limit its adoption for stream-ing applications. In this work, we proposed a novel augmentedmemory self-attention, which attends on a short segment of theinput sequence and a bank of memories. The memory bankstores the embedding information for all the processed seg-ments. On the librispeech benchmark, our proposed methodoutperforms all the existing streamable transformer methods bya large margin and achieved over 15\% relative error reduction,compared with the widely used LC-BLSTM baseline. Our find-ings are also confirmed on some large internal datasets.},
  pubstate = {preprint},
  keywords = {High,Unread}
}

@article{wu_towards_2015,
  title = {Towards Dropout Training for Convolutional Neural Networks},
  author = {Wu, Haibing and Gu, Xiaodong},
  date = {2015},
  journaltitle = {Neural Networks},
  volume = {71},
  eprint = {26277608},
  eprinttype = {pmid},
  pages = {1--10},
  issn = {18792782},
  doi = {10.1016/j.neunet.2015.07.007},
  abstract = {Recently, dropout has seen increasing use in deep learning. For deep convolutional neural networks, dropout is known to work well in fully-connected layers. However, its effect in convolutional and pooling layers is still not clear. This paper demonstrates that max-pooling dropout is equivalent to randomly picking activation based on a multinomial distribution at training time. In light of this insight, we advocate employing our proposed probabilistic weighted pooling, instead of commonly used max-pooling, to act as model averaging at test time. Empirical evidence validates the superiority of probabilistic weighted pooling. We also empirically show that the effect of convolutional dropout is not trivial, despite the dramatically reduced possibility of over-fitting due to the convolutional architecture. Elaborately designing dropout training simultaneously in max-pooling and fully-connected layers, we achieve state-of-the-art performance on MNIST, and very competitive results on CIFAR-10 and CIFAR-100, relative to other approaches without data augmentation. Finally, we compare max-pooling dropout and stochastic pooling, both of which introduce stochasticity based on multinomial distributions at pooling stage.},
  isbn = {0893-6080}
}

@misc{wu_transferable_2019,
  title = {Transferable {{Multi-Domain State Generator}} for {{Task-Oriented Dialogue Systems}}},
  author = {Wu, Chien-Sheng and Madotto, Andrea and Hosseini-Asl, Ehsan and Xiong, Caiming and Socher, Richard and Fung, Pascale},
  date = {2019-05-21},
  eprint = {1905.08743},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1905.08743},
  urldate = {2019-08-01},
  abstract = {Over-dependence on domain ontology and lack of knowledge sharing across domains are two practical and yet less studied problems of dialogue state tracking. Existing approaches generally fall short in tracking unknown slot values during inference and often have difficulties in adapting to new domains. In this paper, we propose a TRAnsferable Dialogue statE generator (TRADE) that generates dialogue states from utterances using a copy mechanism, facilitating knowledge transfer when predicting (domain, slot, value) triplets not encountered during training. Our model is composed of an utterance encoder, a slot gate, and a state generator, which are shared across domains. Empirical results demonstrate that TRADE achieves state-of-the-art joint goal accuracy of 48.62\% for the five domains of MultiWOZ, a human-human dialogue dataset. In addition, we show its transferring ability by simulating zero-shot and few-shot dialogue state tracking for unseen domains. TRADE achieves 60.58\% joint goal accuracy in one of the zero-shot domains, and is able to adapt to few-shot cases without forgetting already trained domains.},
  langid = {english}
}

@inproceedings{xia_learning_2015,
  title = {Learning Discriminative Reconstructions for Unsupervised Outlier Removal},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Xia, Yan and Cao, Xudong and Wen, Fang and Hua, Gang and Sun, Jian},
  date = {2015},
  pages = {1511--1519},
  location = {{Santiago, Chile}}
}

@online{xia_vision_2022,
  title = {Vision {{Transformer}} with {{Deformable Attention}}},
  author = {Xia, Zhuofan and Pan, Xuran and Song, Shiji and Li, Li Erran and Huang, Gao},
  date = {2022-02-21},
  eprint = {2201.00520},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2201.00520},
  urldate = {2022-05-18},
  abstract = {Transformers have recently shown superior performances on various vision tasks. The large, sometimes even global, receptive field endows Transformer models with higher representation power over their CNN counterparts. Nevertheless, simply enlarging receptive field also gives rise to several concerns. On the one hand, using dense attention e.g., in ViT, leads to excessive memory and computational cost, and features can be influenced by irrelevant parts which are beyond the region of interests. On the other hand, the sparse attention adopted in PVT or Swin Transformer is data agnostic and may limit the ability to model long range relations. To mitigate these issues, we propose a novel deformable self-attention module, where the positions of key and value pairs in self-attention are selected in a data-dependent way. This flexible scheme enables the self-attention module to focus on relevant regions and capture more informative features. On this basis, we present Deformable Attention Transformer, a general backbone model with deformable attention for both image classification and dense prediction tasks. Extensive experiments show that our models achieve consistently improved results on comprehensive benchmarks. Code is available at https://github.com/LeapLabTHU/DAT.},
  pubstate = {preprint}
}

@inproceedings{xia_which_2020,
  title = {Which *{{BERT}}? {{A}} Survey Organizing Contextualized Encoders},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Xia, Patrick and Wu, Shijie and Van Durme, Benjamin},
  date = {2020-11},
  pages = {7516--7533},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.608},
  url = {https://aclanthology.org/2020.emnlp-main.608},
  eventtitle = {Conference on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})}
}

@misc{xiao_fashionmnist_2017,
  title = {Fashion-{{MNIST}}: A {{Novel Image Dataset}} for {{Benchmarking Machine Learning Algorithms}}},
  author = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  date = {2017},
  eprint = {2102.06171},
  eprinttype = {arxiv},
  url = {https://arxiv.org/abs/1708.07747},
  urldate = {2018-04-03},
  abstract = {We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at https://github.com/zalandoresearch/fashion-mnist}
}

@inproceedings{xiao_likelihood_2020,
  title = {Likelihood {{Regret}}: {{An Out-of-Distribution Detection Score}} for {{Variational Auto-Encoder}}},
  booktitle = {Proceedings of the 33rd {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Xiao, Zhisheng and Yan, Qing and Amit, Yali},
  date = {2020},
  location = {{Virtual}},
  url = {https://proceedings.neurips.cc/paper/2020/hash/eddea82ad2755b24c4e168c5fc2ebd40-Abstract.html},
  eventtitle = {Conference on {{Neural Information Processing Systems}}}
}

@online{xiao_scaling_2021,
  title = {Scaling {{ASR}} Improves Zero and Few Shot Learning},
  author = {Xiao, Alex and Zheng, Weiyi and Keren, Gil and Le, Duc and Zhang, Frank and Fuegen, Christian and Kalinli, Ozlem and Saraf, Yatharth and Mohamed, Abdelrahman},
  date = {2021},
  eprint = {2111.05948},
  eprinttype = {arxiv},
  pubstate = {preprint}
}

@online{xiao_we_2021,
  title = {Do We Really Need to Learn Representations from In-Domain Data for Outlier Detection?},
  author = {Xiao, Zhisheng and Yan, Qing and Amit, Yali},
  date = {2021},
  number = {2105.09270},
  eprint = {2105.09270},
  eprinttype = {arxiv},
  pubstate = {preprint}
}

@inproceedings{xieEHRCodingMultiscale2019,
  title = {{{EHR Coding}} with {{Multi-scale Feature Attention}} and {{Structured Knowledge Graph Propagation}}},
  booktitle = {Proceedings of the 28th {{ACM International Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Xie, Xiancheng and Xiong, Yun and Yu, Philip S. and Zhu, Yangyong},
  date = {2019-11},
  series = {{{CIKM}} '19},
  pages = {649--658},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3357384.3357897},
  abstract = {Assigning standard medical codes (e.g., ICD-9-CM) representing diagnoses or procedures to electronic health record (EHR) is an important task in the medical domain. However, automatic coding is difficult since the clinical note is composed of multiple long and heterogeneous textual narratives (e.g., discharge diagnosis, pathology reports, surgical procedure notes). Furthermore, the code label space is large and the label distribution is extremely unbalanced. The state-of-the-art methods mainly regard EHR coding as a multi-label text classification task and use shallow convolution neural network with fixed window size, which is incapable of learning variable n-gram features and the ontology structure between codes. In this paper, we leverage a densely connected convolutional neural network which is able to produce variable n-gram features for clinical note feature learning. We also incorporate a multi-scale feature attention to adaptively select multi-scale features since the most informative n-grams in clinical notes for each word can vary in length according to the neighborhood. Furthermore, we leverage graph convolutional neural network to capture both the hierarchical relationships among medical codes and the semantics of each code. Finally, We validate our method on the public dataset, and the evaluation results indicate that our method can significantly outperform other state-of-the-art models.},
  isbn = {978-1-4503-6976-3}
}

@misc{xiong_dcn_2017,
  title = {{{DCN}}+: {{Mixed Objective}} and {{Deep Residual Coattention}} for {{Question Answering}}},
  author = {Xiong, Caiming and Zhong, Victor and Socher, Richard},
  date = {2017-10-31},
  eprint = {1711.00106},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1711.00106},
  urldate = {2018-04-24},
  abstract = {Traditional models for question answering optimize using cross entropy loss, which encourages exact answers at the cost of penalizing nearby or overlapping answers that are sometimes equally accurate. We propose a mixed objective that combines cross entropy loss with self-critical policy learning. The objective uses rewards derived from word overlap to solve the misalignment between evaluation metric and optimization objective. In addition to the mixed objective, we improve dynamic coattention networks (DCN) with a deep residual coattention encoder that is inspired by recent work in deep self-attention and residual networks. Our proposals improve model performance across question types and input lengths, especially for long questions that requires the ability to capture long-term dependencies. On the Stanford Question Answering Dataset, our model achieves state-of-the-art results with 75.1\% exact match accuracy and 83.1\% F1, while the ensemble obtains 78.9\% exact match accuracy and 86.0\% F1.}
}

@misc{xiong_dynamic_2016,
  title = {Dynamic Memory Networks for Visual and Textual Question Answering},
  author = {Xiong, Caiming and Merity, Stephen and Socher, Richard},
  date = {2016},
  eprint = {1603.01417},
  eprinttype = {arxiv},
  url = {https://arxiv.org/abs/1603.01417}
}

@article{xu_deep_2014,
  title = {Deep Convolutional Neural Network for Image Deconvolution},
  author = {Xu, Li and Ren, Jimmy S and Liu, Ce and Jia, Jiaya},
  date = {2014},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {27}
}

@online{xu_evovit_2021,
  title = {Evo-{{ViT}}: {{Slow-Fast Token Evolution}} for {{Dynamic Vision Transformer}}},
  shorttitle = {Evo-{{ViT}}},
  author = {Xu, Yifan and Zhang, Zhijie and Zhang, Mengdan and Sheng, Kekai and Li, Ke and Dong, Weiming and Zhang, Liqing and Xu, Changsheng and Sun, Xing},
  date = {2021-12-06},
  eprint = {2108.01390},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2108.01390},
  urldate = {2022-10-08},
  abstract = {Vision transformers (ViTs) have recently received explosive popularity, but the huge computational cost is still a severe issue. Since the computation complexity of ViT is quadratic with respect to the input sequence length, a mainstream paradigm for computation reduction is to reduce the number of tokens. Existing designs include structured spatial compression that uses a progressive shrinking pyramid to reduce the computations of large feature maps, and unstructured token pruning that dynamically drops redundant tokens. However, the limitation of existing token pruning lies in two folds: 1) the incomplete spatial structure caused by pruning is not compatible with structured spatial compression that is commonly used in modern deep-narrow transformers; 2) it usually requires a time-consuming pre-training procedure. To tackle the limitations and expand the applicable scenario of token pruning, we present Evo-ViT, a self-motivated slow-fast token evolution approach for vision transformers. Specifically, we conduct unstructured instance-wise token selection by taking advantage of the simple and effective global class attention that is native to vision transformers. Then, we propose to update the selected informative tokens and uninformative tokens with different computation paths, namely, slow-fast updating. Since slow-fast updating mechanism maintains the spatial structure and information flow, Evo-ViT can accelerate vanilla transformers of both flat and deep-narrow structures from the very beginning of the training process. Experimental results demonstrate that our method significantly reduces the computational cost of vision transformers while maintaining comparable performance on image classification.},
  pubstate = {preprint}
}

@inproceedings{xu_iterative_2020,
  title = {Iterative Pseudo-Labeling for Speech Recognition},
  booktitle = {Annual {{Conference}} of the {{International Speech Communication Association}}},
  author = {Xu, Qiantong and Likhomanenko, Tatiana and Kahn, Jacob and Hannun, Awni and Synnaeve, Gabriel and Collobert, Ronan},
  date = {2020}
}

@article{xu_optimal_2011,
  title = {Towards {{Optimal One Pass Large Scale Learning}} with {{Averaged Stochastic Gradient Descent}}},
  author = {Xu, Wei},
  date = {2011-07-13},
  url = {https://arxiv.org/abs/1107.2490},
  urldate = {2018-09-18},
  langid = {english}
}

@article{xu_unsupervised_2017,
  title = {Unsupervised {{Feature Learning Based}} on {{Deep Models}} for {{Environmental Audio Tagging}}},
  author = {Xu, Yong and Huang, Qiang and Wang, Wenwu and Foster, Peter and Sigtia, Siddharth and Jackson, Philip J. B. and Plumbley, Mark D.},
  date = {2017-06},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {25},
  number = {6},
  pages = {1230--1241},
  issn = {2329-9304},
  doi = {10.1109/TASLP.2017.2690563},
  abstract = {Environmental audio tagging aims to predict only the presence or absence of certain acoustic events in the interested acoustic scene. In this paper, we make contributions to audio tagging in two parts, respectively, acoustic modeling and feature learning. We propose to use a shrinking deep neural network (DNN) framework incorporating unsupervised feature learning to handle the multilabel classification task. For the acoustic modeling, a large set of contextual frames of the chunk are fed into the DNN to perform a multilabel classification for the expected tags, considering that only chunk (or utterance) level rather than frame-level labels are available. Dropout and background noise aware training are also adopted to improve the generalization capability of the DNNs. For the unsupervised feature learning, we propose to use a symmetric or asymmetric deep denoising auto-encoder (syDAE or asyDAE) to generate new data-driven features from the logarithmic Mel-filter banks features. The new features, which are smoothed against background noise and more compact with contextual information, can further improve the performance of the DNN baseline. Compared with the standard Gaussian mixture model baseline of the DCASE 2016 audio tagging challenge, our proposed method obtains a significant equal error rate (EER) reduction from 0.21 to 0.13 on the development set. The proposed asyDAE system can get a relative 6.7\% EER reduction compared with the strong DNN baseline on the development set. Finally, the results also show that our approach obtains the state-of-the-art performance with 0.15 EER on the evaluation set of the DCASE 2016 audio tagging task while EER of the first prize of this challenge is 0.17.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}}
}

@misc{xue_byt5_2021,
  title = {{{ByT5}}: {{Towards}} a Token-Free Future with Pre-Trained Byte-to-Byte Models},
  shorttitle = {{{ByT5}}},
  author = {Xue, Linting and Barua, Aditya and Constant, Noah and Al-Rfou, Rami and Narang, Sharan and Kale, Mihir and Roberts, Adam and Raffel, Colin},
  date = {2021-05-28},
  eprint = {2105.13626},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2105.13626},
  urldate = {2021-06-02},
  abstract = {Most widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units. Encoding text as a sequence of tokens requires a tokenizer, which is typically created as an independent artifact from the model. Token-free models that instead operate directly on raw text (bytes or characters) have many benefits: they can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by removing complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences. We carefully characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.},
  langid = {english}
}

@online{yang_generalized_2022,
  title = {Generalized {{Out-of-Distribution Detection}}: {{A Survey}}},
  shorttitle = {Generalized {{Out-of-Distribution Detection}}},
  author = {Yang, Jingkang and Zhou, Kaiyang and Li, Yixuan and Liu, Ziwei},
  date = {2022-08-03},
  eprint = {2110.11334},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2110.11334},
  urldate = {2022-09-04},
  abstract = {Out-of-distribution (OOD) detection is critical to ensuring the reliability and safety of machine learning systems. For instance, in autonomous driving, we would like the driving system to issue an alert and hand over the control to humans when it detects unusual scenes or objects that it has never seen during training time and cannot make a safe decision. The term, OOD detection, first emerged in 2017 and since then has received increasing attention from the research community, leading to a plethora of methods developed, ranging from classification-based to density-based to distance-based ones. Meanwhile, several other problems, including anomaly detection (AD), novelty detection (ND), open set recognition (OSR), and outlier detection (OD), are closely related to OOD detection in terms of motivation and methodology. Despite common goals, these topics develop in isolation, and their subtle differences in definition and problem setting often confuse readers and practitioners. In this survey, we first present a unified framework called generalized OOD detection, which encompasses the five aforementioned problems, i.e., AD, ND, OSR, OOD detection, and OD. Under our framework, these five problems can be seen as special cases or sub-tasks, and are easier to distinguish. We then review each of these five areas by summarizing their recent technical developments, with a special focus on OOD detection methodologies. We conclude this survey with open challenges and potential research directions.},
  pubstate = {preprint}
}

@misc{yang_improved_2017,
  title = {Improved {{Variational Autoencoders}} for {{Text Modeling}} Using {{Dilated Convolutions}}},
  author = {Yang, Zichao and Hu, Zhiting and Salakhutdinov, Ruslan and Berg-Kirkpatrick, Taylor},
  date = {2017-06-17},
  eprint = {1702.08139},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1702.08139},
  urldate = {2021-03-04},
  abstract = {Recent work on generative text modeling has found that variational autoencoders (VAE) with LSTM decoders perform worse than simpler LSTM language models (Bowman et al., 2015). This negative result is so far poorly understood, but has been attributed to the propensity of LSTM decoders to ignore conditioning information from the encoder. In this paper, we experiment with a new type of decoder for VAE: a dilated CNN. By changing the decoder’s dilation architecture, we control the size of context from previously generated words. In experiments, we find that there is a trade-off between contextual capacity of the decoder and effective use of encoding information. We show that when carefully managed, VAEs can outperform LSTM language models. We demonstrate perplexity gains on two datasets, representing the first positive language modeling result with VAE. Further, we conduct an in-depth investigation of the use of VAE (with our new decoding architecture) for semi-supervised and unsupervised labeling tasks, demonstrating gains over several strong baselines.},
  langid = {english}
}

@inproceedings{yang_semantically_2021,
  title = {Semantically Coherent Out-of-Distribution Detection},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Yang, Jingkang and Wang, Haoqi and Feng, Litong and Yan, Xiaopeng and Zheng, Huabin and Zhang, Wayne and Liu, Ziwei},
  date = {2021},
  pages = {8301--8309}
}

@misc{yang_semisupervised_2017,
  title = {Semi-{{Supervised QA}} with {{Generative Domain-Adaptive Nets}}},
  author = {Yang, Zhilin and Hu, Junjie and Salakhutdinov, Ruslan and Cohen, William W.},
  date = {2017},
  eprint = {1702.02206},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1702.02206},
  abstract = {We study the problem of semi-supervised question answering----utilizing unlabeled text to boost the performance of question answering models. We propose a novel training framework, the Generative Domain-Adaptive Nets. In this framework, we train a generative model to generate questions based on the unlabeled text, and combine model-generated questions with human-generated questions for training question answering models. We develop novel domain adaptation algorithms, based on reinforcement learning, to alleviate the discrepancy between the model-generated data distribution and the human-generated data distribution. Experiments show that our proposed framework obtains substantial improvement from unlabeled text.},
  isbn = {9781945626753},
  keywords = {★}
}

@inproceedings{yang_stacked_2016,
  title = {Stacked Attention Networks for Image Question Answering},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Yang, Zichao and He, Xiaodong and Gao, Jianfeng and Deng, Li and Smola, Alex},
  date = {2016},
  pages = {21--29}
}

@inproceedings{yang_superb_2021,
  title = {{{SUPERB}}: {{Speech}} Processing Universal {{PERformance}} Benchmark},
  booktitle = {Annual {{Conference}} of the {{International Speech Communication Association}}},
  author = {Yang, Shu-wen and Chi, Po-Han and Chuang, Yung-Sung and Lai, Cheng-I Jeff and Lakhotia, Kushal and Lin, Yist Y. and Liu, Andy T. and Shi, Jiatong and Chang, Xuankai and Lin, Guan-Ting and Huang, Tzu-Hsien and Tseng, Wei-Cheng and Lee, Ko-tik and Liu, Da-Rong and Huang, Zili and Dong, Shuyan and Li, Shang-Wen and Watanabe, Shinji and Mohamed, Abdelrahman and Lee, Hung-yi},
  date = {2021}
}

@inproceedings{yang_understanding_2020,
  title = {Understanding Self-Attention of Self-Supervised Audio {{Transformers}}},
  booktitle = {Proceedings of the 21st {{Annual Conference}} of the {{International Speech Communication Association}} ({{Interspeech}})},
  author = {Yang, Shu-wen and Liu, Andy T. and Lee, Hung-yi},
  date = {2020},
  publisher = {{ISCA}}
}

@misc{yang_xlnet_2019,
  title = {{{XLNet}}: {{Generalized Autoregressive Pretraining}} for {{Language Understanding}}},
  shorttitle = {{{XLNet}}},
  author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
  date = {2019-06-19},
  eprint = {1906.08237},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1906.08237},
  urldate = {2019-08-01},
  abstract = {With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, XLNet outperforms BERT on 20 tasks, often by a large margin, and achieves state-of-the-art results on 18 tasks including question answering, natural language inference, sentiment analysis, and document ranking.1.},
  langid = {english}
}

@misc{yangKnowledgeInjectedPrompt2022,
  title = {Knowledge {{Injected Prompt Based Fine-tuning}} for {{Multi-label Few-shot ICD Coding}}},
  author = {Yang, Zhichao and Wang, Shufan and Rawat, Bhanu Pratap Singh and Mitra, Avijit and Yu, Hong},
  date = {2022-10},
  eprint = {2210.03304},
  eprinttype = {arxiv},
  abstract = {Automatic International Classification of Diseases (ICD) coding aims to assign multiple ICD codes to a medical note with average length of 3,000+ tokens. This task is challenging due to a high-dimensional space of multi-label assignment (tens of thousands of ICD codes) and the long-tail challenge: only a few codes (common diseases) are frequently assigned while most codes (rare diseases) are infrequently assigned. This study addresses the long-tail challenge by adapting a prompt-based fine-tuning technique with label semantics, which has been shown to be effective under few-shot setting. To further enhance the performance in medical domain, we propose a knowledge-enhanced longformer by injecting three domain-specific knowledge: hierarchy, synonym, and abbreviation with additional pretraining using contrastive learning. Experiments on MIMIC-III-full, a benchmark dataset of code assignment, show that our proposed method outperforms previous state-of-the-art method in 14.5\% in marco F1 (from 10.3 to 11.8, P{$<$}0.001). To further test our model on few-shot setting, we created a new rare diseases coding dataset, MIMIC-III-rare50, on which our model improves marco F1 from 17.1 to 30.4 and micro F1 from 17.2 to 32.6 compared to previous method.},
  issue = {arXiv:2210.03304},
  organization = {{arXiv}}
}

@inproceedings{yanMedicalCodingClassification2010,
  title = {Medical Coding Classification by Leveraging Inter-Code Relationships},
  booktitle = {Proceedings of the 16th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining},
  author = {Yan, Yan and Fung, Glenn and Dy, Jennifer G. and Rosales, Romer},
  date = {2010-07},
  series = {{{KDD}} '10},
  pages = {193--202},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/1835804.1835831},
  abstract = {Medical coding or classification is the process of transforming information contained in patient medical records into standard predefined medical codes. There are several worldwide accepted medical coding conventions associated with diagnoses and medical procedures; however, in the United States the Ninth Revision of ICD(ICD-9) provides the standard for coding clinical records. Accurate medical coding is important since it is used by hospitals for insurance billing purposes. Since after discharge a patient can be assigned or classified to several ICD-9 codes, the coding problem can be seen as a multi-label classification problem. In this paper, we introduce a multi-label large-margin classifier that automatically learns the underlying inter-code structure and allows the controlled incorporation of prior knowledge about medical code relationships. In addition to refining and learning the code relationships, our classifier can also utilize this shared information to improve its performance. Experiments on a publicly available dataset containing clinical free text and their associated medical codes showed that our proposed multi-label classifier outperforms related multi-label models in this problem.},
  isbn = {978-1-4503-0055-1}
}

@misc{yao_adahessian_2020,
  title = {{{ADAHESSIAN}}: {{An Adaptive Second Order Optimizer}} for {{Machine Learning}}},
  shorttitle = {{{ADAHESSIAN}}},
  author = {Yao, Zhewei and Gholami, Amir and Shen, Sheng and Keutzer, Kurt and Mahoney, Michael W.},
  date = {2020-06-01},
  eprint = {2006.00719},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2006.00719},
  urldate = {2020-11-08},
  abstract = {We introduce ADAHESSIAN, a second order stochastic optimization algorithm which dynamically incorporates the curvature of the loss function via ADAptive estimates of the HESSIAN. Second order algorithms are among the most powerful optimization algorithms with superior convergence properties as compared to first order methods such as SGD and Adam. The main disadvantage of traditional second order methods is their heavier periteration computation and poor accuracy as compared to first order methods. To address these, we incorporate several novel approaches in ADAHESSIAN, including: (i) a new variance reduction estimate of the Hessian diagonal with low computational overhead; (ii) a root-mean-square exponential moving average to smooth out variations of the Hessian diagonal across different iterations; and (iii) a block diagonal averaging to reduce the variance of Hessian diagonal elements. We show that ADAHESSIAN achieves new state-of-the-art results by a large margin as compared to other adaptive optimization methods, including variants of Adam. In particular, we perform extensive tests on CV, NLP, and recommendation system tasks and find that ADAHESSIAN: (i) achieves 1.80\%/1.45\% higher accuracy on ResNets20/32 on Cifar10, and 5.55\% higher accuracy on ImageNet as compared to Adam; (ii) outperforms AdamW for transformers by 0.27/0.33 BLEU score on IWSLT14/WMT14 and 1.8/1.0 PPL on PTB/Wikitext-103; and (iii) achieves 0.032\% better score than Adagrad for DLRM on the Criteo Ad Kaggle dataset. Importantly, we show that the cost per iteration of ADAHESSIAN is comparable to first-order methods, and that it exhibits robustness towards its hyperparameters. The code for ADAHESSIAN is open-sourced and publicly-available [1].},
  langid = {english}
}

@misc{yao_depthgated_2015,
  title = {Depth-{{Gated Recurrent Neural Networks}}},
  author = {Yao, Kaisheng and Cohn, Trevor and Vylomova, Katerina and Duh, Kevin and Dyer, Chris},
  date = {2015},
  eprint = {1508.03790},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1508.03790},
  urldate = {2018-05-04},
  abstract = {In this short note, we present an extension of long short-term memory (LSTM) neural networks to using a depth gate to connect memory cells of adjacent layers. Doing so introduces a linear dependence between lower and upper layer recurrent units. Importantly, the linear dependence is gated through a gating function, which we call depth gate. This gate is a function of the lower layer memory cell, the input to and the past memory cell of this layer. We conducted experiments and verified that this new architecture of LSTMs was able to improve machine translation and language modeling performances.}
}

@article{yarra_mode-shape_2016,
  title = {A Mode-Shape Classification Technique for Robust Speech Rate Estimation and Syllable Nuclei Detection},
  author = {Yarra, Chiranjeevi and Deshmukh, Om D. and Ghosh, Prasanta Kumar},
  date = {2016-04},
  journaltitle = {Speech Communication},
  volume = {78},
  pages = {62--71},
  issn = {01676393},
  doi = {10/f8hmsm},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S016763931600025X},
  urldate = {2018-11-23},
  abstract = {Acoustic feature based speech (syllable) rate estimation and syllable nuclei detection are important problems in automatic speech recognition (ASR), computer assisted language learning (CALL) and fluency analysis. A typical solution for both the problems consists of two stages. The first stage involves computing a short-time feature contour such that most of the peaks of the contour correspond to the syllabic nuclei. In the second stage, the peaks corresponding to the syllable nuclei are detected. In this work, instead of the peak detection, we perform a mode-shape classification, which is formulated as a supervised binary classification problem – mode-shapes representing the syllabic nuclei as one class and remaining as the other. We use the temporal correlation and selected sub-band correlation (TCSSBC) feature contour and the mode-shapes in the TCSSBC feature contour are converted into a set of feature vectors using an interpolation technique. A support vector machine classifier is used for the classification. Experiments are performed separately using Switchboard, TIMIT and CTIMIT corpora in a five-fold cross validation setup. The average correlation coefficients for the syllable rate estimation turn out to be 0.6761, 0.6928 and 0.3604 for three corpora respectively, which outperform those obtained by the best of the existing peak detection techniques. Similarly, the average F-scores (syllable level) for the syllable nuclei detection are 0.8917, 0.8200 and 0.7637 for three corpora respectively.},
  langid = {english}
}

@article{ye_deep_2021,
  title = {Deep {{Mixture Generative Autoencoders}}},
  author = {Ye, Fei and Bors, Adrian G.},
  date = {2021},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  shortjournal = {IEEE Trans. Neural Netw. Learning Syst.},
  pages = {1--15},
  issn = {2162-237X, 2162-2388},
  doi = {10.1109/TNNLS.2021.3071401},
  url = {https://ieeexplore.ieee.org/document/9408405/},
  urldate = {2021-05-07},
  abstract = {Variational autoencoders (VAEs) are one of the most popular unsupervised generative models which rely on learning latent representations of data. In this paper, we extend the classical concept of Gaussian mixtures into the deep variational framework by proposing a mixture of VAEs (MVAE). Each component in the MVAE model is implemented by a variational encoder and has an associated sub-decoder. The separation between the latent spaces modelled by different encoders is enforced using the d-variable Hilbert-Schmidt Independence Criterion (dHSIC) criterion. Each component would capture different data variational features. We also propose a mechanism for finding the appropriate number of VAE components for a given task, leading to an optimal architecture. The differentiable categorical Gumbel-Softmax distribution is used in order to generate dropout masking parameters within the end-toend backpropagation training framework. Extensive experiments show that the proposed MAVE model learns a rich latent data representation and is able to discover additional underlying data factors.},
  langid = {english}
}

@inproceedings{yeh_unsupervised_2019,
  title = {Unsupervised Speech Recognition via Segmental Empirical Output Distribution Matching},
  booktitle = {Proceedings of the 7th {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Yeh, Chih-Kuan and Chen, Jianshu and Yu, Chengzhu and Yu, Dong},
  date = {2019},
  location = {{New Orleans, LA, USA}},
  url = {https://openreview.net/forum?id=Bylmkh05KX},
  eventtitle = {International {{Conference}} on {{Learning Representations}}}
}

@inproceedings{yi_stochastic_2009,
  title = {Stochastic {{Search}} Using the {{Natural Gradient}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Yi, Sun and Wierstra, Daan and Schaul, Tom and Schmidhuber, Jürgen},
  date = {2009},
  pages = {1161--1168},
  location = {{Montreal, Quebec, Canada}},
  doi = {10.1145/1553374.1553522},
  abstract = {To optimize unknown 'fitness' functions, we present Natural Evolution Strategies, a novel algorithm that constitutes a principled alter-native to standard stochastic search meth-ods. It maintains a multinormal distribution on the set of solution candidates. The Nat-ural Gradient is used to update the distribu-tion's parameters in the direction of higher expected fitness, by efficiently calculating the inverse of the exact Fisher information ma-trix whereas previous methods had to use approximations. Other novel aspects of our method include optimal fitness baselines and importance mixing, a procedure adjusting batches with minimal numbers of fitness eval-uations. The algorithm yields competitive re-sults on a number of benchmarks.},
  isbn = {978-1-60558-516-1}
}

@inproceedings{yin_avit_2022,
  title = {A-{{ViT}}: {{Adaptive Tokens}} for {{Efficient Vision Transformer}}},
  shorttitle = {A-{{ViT}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Yin, Hongxu and Vahdat, Arash and Alvarez, Jose and Mallya, Arun and Kautz, Jan and Molchanov, Pavlo},
  date = {2022-03-29},
  eprint = {2112.07658},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2112.07658},
  urldate = {2022-04-19},
  abstract = {We introduce A-ViT, a method that adaptively adjusts the inference cost of vision transformer (ViT) for images of different complexity. A-ViT achieves this by automatically reducing the number of tokens in vision transformers that are processed in the network as inference proceeds. We reformulate Adaptive Computation Time (ACT) for this task, extending halting to discard redundant spatial tokens. The appealing architectural properties of vision transformers enables our adaptive token reduction mechanism to speed up inference without modifying the network architecture or inference hardware. We demonstrate that A-ViT requires no extra parameters or sub-network for halting, as we base the learning of adaptive halting on the original network parameters. We further introduce distributional prior regularization that stabilizes training compared to prior ACT approaches. On the image classification task (ImageNet1K), we show that our proposed A-ViT yields high efficacy in filtering informative spatial features and cutting down on the overall compute. The proposed method improves the throughput of DeiT-Tiny by 62\% and DeiT-Small by 38\% with only 0.3\% accuracy drop, outperforming prior art by a large margin.},
  keywords = {Unread}
}

@article{yin_expanded_2011,
  title = {The {{Expanded State Space Kalman}} Filter for {{GPS}} Navigation},
  author = {Yin, Jianjun and Gu, Ming and Zhang, Jianqiu},
  date = {2011-11-01},
  journaltitle = {Information Technology Journal},
  volume = {10},
  number = {11},
  pages = {2091--2097},
  issn = {18125638},
  doi = {10.3923/itj.2011.2091.2097},
  url = {http://www.scialert.net/abstract/?doi=itj.2011.2091.2097},
  urldate = {2016-11-29},
  abstract = {A novel dynamic state space model was established for Global Positioning System (GPS) navigation by adopting the polynomial predictive filtering idea and state dimension expansion. We called the new model expanded state space model which was established without the exact knowledge of the original state dynamics, i.e., we way use the proposed model to describe the state dynamics no matter we know the original state propagation well or not. A correspondent Expanded State Space Kalman filter (ESSKF) was then presented based on the proposed model. The results of the GPS navigation examples demonstrated that the proposed method did work better than the existed Extended Kalman Filter (EKF), especially in the situations that the state dynamics were not known well. © 2011 Asian Network for Scientific Information.}
}

@inproceedings{yin_neural_2016,
  title = {Neural {{Generative Question Answering}}},
  booktitle = {Proceedings of the {{International Joint Conference}} on {{Artificial Intelligence}} ({{IJCAI}})},
  author = {Yin, Jun and Jiang, Xin and Lu, Zhengdong and Shang, Lifeng and Li, Hang and Li, Xiaoming},
  date = {2016},
  abstract = {This paper presents an end-to-end neural net-work model, named Neural Generative Question Answering (GENQA), that can generate answers to simple factoid questions, based on the facts in a knowledge-base. More specifically, the model is built on the encoder-decoder framework for sequence-to-sequence learning, while equipped with the ability to enquire the knowledge-base, and is trained on a corpus of question-answer pairs, with their associated triples in the knowledge-base. Empirical study shows the proposed model can ef-fectively deal with the variations of questions and answers, and generate right and natural answers by referring to the facts in the knowledge-base. The experiment on question answering demon-strates that the proposed model can outperform an embedding-based QA model as well as a neural di-alogue model trained on the same data.}
}

@misc{yoshida_spectral_2017,
  title = {Spectral {{Norm Regularization}} for {{Improving}} the {{Generalizability}} of {{Deep Learning}}},
  author = {Yoshida, Yuichi and Miyato, Takeru},
  date = {2017-05-31},
  eprint = {1705.10941},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1705.10941},
  urldate = {2020-11-09},
  abstract = {We investigate the generalizability of deep learning based on the sensitivity to input perturbation. We hypothesize that the high sensitivity to the perturbation of data degrades the performance on it. To reduce the sensitivity to perturbation, we propose a simple and effective regularization method, referred to as spectral norm regularization, which penalizes the high spectral norm of weight matrices in neural networks. We provide supportive evidence for the abovementioned hypothesis by experimentally confirming that the models trained using spectral norm regularization exhibit better generalizability than other baseline methods.},
  langid = {english}
}

@article{yosinski_how_2014,
  title = {How Transferable Are Features in Deep Neural Networks?},
  author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
  date = {2014},
  journaltitle = {Proceedings of the Conference on Neural Information Processing Systems (NeurIPS)},
  eprint = {1411.1792},
  eprinttype = {arxiv},
  pages = {3320--3328},
  issn = {10495258},
  doi = {10.1109/IJCNN.2016.7727519},
  url = {http://arxiv.org/abs/1411.1792},
  abstract = {Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.},
  isbn = {978-1-5090-0620-5}
}

@misc{you_large_2017,
  title = {Large {{Batch Training}} of {{Convolutional Networks}}},
  author = {You, Yang and Gitman, Igor and Ginsburg, Boris},
  date = {2017-09-13},
  eprint = {1708.03888},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1708.03888},
  urldate = {2019-10-27},
  abstract = {A common way to speed up training of large convolutional networks is to add computational units. Training is then performed using data-parallel synchronous Stochastic Gradient Descent (SGD) with mini-batch divided between computational units. With an increase in the number of nodes, the batch size grows. But training with large batch size often results in the lower model accuracy. We argue that the current recipe for large batch training (linear learning rate scaling with warm-up) is not general enough and training may diverge. To overcome this optimization difficulties we propose a new training algorithm based on Layer-wise Adaptive Rate Scaling (LARS). Using LARS, we scaled Alexnet up to a batch size of 8K, and Resnet-50 to a batch size of 32K without loss in accuracy.},
  langid = {english}
}

@misc{you_large_2019,
  title = {Large {{Batch Optimization}} for {{Deep Learning}}: {{Training BERT}} in 76 Minutes},
  shorttitle = {Large {{Batch Optimization}} for {{Deep Learning}}},
  author = {You, Yang and Li, Jing and Reddi, Sashank and Hseu, Jonathan and Kumar, Sanjiv and Bhojanapalli, Srinadh and Song, Xiaodan and Demmel, James and Hsieh, Cho-Jui},
  date = {2019-04-01},
  eprint = {1904.00962},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1904.00962},
  urldate = {2019-08-01},
  abstract = {Training large deep neural networks on massive datasets is very challenging. One promising approach to tackle this issue is through the use of large batch stochastic optimization. However, our understanding of this approach in the context of deep learning is still very limited. Furthermore, the current approaches in this direction are heavily hand-tuned. To this end, we first study a general adaptation strategy to accelerate training of deep neural networks using large minibatches. Using this strategy, we develop a new layer-wise adaptive large batch optimization technique called LAMB. We also provide a formal convergence analysis of LAMB as well as the previous published layerwise optimizer LARS, showing convergence to a stationary point in general nonconvex settings. Our empirical results demonstrate the superior performance of LAMB for BERT and ResNet-50 training. In particular, for BERT training, our optimization technique enables use of very large batches sizes of 32868; thereby, requiring just 8599 iterations to train (as opposed to 1 million iterations in the original paper). By increasing the batch size to the memory limit of a TPUv3 pod, BERT training time can be reduced from 3 days to 76 minutes (Table 1). Finally, we also demonstrate that LAMB outperforms previous largebatch training algorithms for ResNet-50 on ImageNet; obtaining state-of-the-art performance in just a few minutes.},
  langid = {english}
}

@article{young_state_1994,
  title = {State Clustering in Hidden {{Markov}} Model-Based Continuous Speech Recognition},
  author = {Young, S.J. and Woodland, P.C.},
  date = {1994},
  journaltitle = {Computer Speech \& Language},
  volume = {8},
  number = {4},
  pages = {369--383},
  issn = {0885-2308}
}

@online{yu_fastemit_2021,
  title = {{{FastEmit}}: {{Low-latency Streaming ASR}} with {{Sequence-level Emission Regularization}}},
  shorttitle = {{{FastEmit}}},
  author = {Yu, Jiahui and Chiu, Chung-Cheng and Li, Bo and Chang, Shuo-yiin and Sainath, Tara N. and He, Yanzhang and Narayanan, Arun and Han, Wei and Gulati, Anmol and Wu, Yonghui and Pang, Ruoming},
  date = {2021-02-03},
  eprint = {2010.11148},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2010.11148},
  urldate = {2023-03-08},
  abstract = {Streaming automatic speech recognition (ASR) aims to emit each hypothesized word as quickly and accurately as possible. However, emitting fast without degrading quality, as measured by word error rate (WER), is highly challenging. Existing approaches including Early and Late Penalties and Constrained Alignments penalize emission delay by manipulating per-token or per-frame probability prediction in sequence transducer models. While being successful in reducing delay, these approaches suffer from significant accuracy regression and also require additional word alignment information from an existing model. In this work, we propose a sequence-level emission regularization method, named FastEmit, that applies latency regularization directly on per-sequence probability in training transducer models, and does not require any alignment. We demonstrate that FastEmit is more suitable to the sequence-level optimization of transducer models for streaming ASR by applying it on various end-to-end streaming ASR networks including RNN-Transducer, Transformer-Transducer, ConvNet-Transducer and Conformer-Transducer. We achieve 150-300 ms latency reduction with significantly better accuracy over previous techniques on a Voice Search test set. FastEmit also improves streaming ASR accuracy from 4.4\%/8.9\% to 3.1\%/7.5\% WER, meanwhile reduces 90th percentile latency from 210 ms to only 30 ms on LibriSpeech.},
  pubstate = {preprint}
}

@article{yu_improved_nodate,
  title = {Improved {{Bottleneck Features Using Pretrained Deep Neural Networks}}},
  author = {Yu, Dong and Seltzer, Michael L},
  pages = {4},
  abstract = {Bottleneck features have been shown to be effective in improving the accuracy of automatic speech recognition (ASR) systems. Conventionally, bottleneck features are extracted from a multi-layer perceptron (MLP) trained to predict context-independent monophone states. The MLP typically has three hidden layers and is trained using the backpropagation algorithm. In this paper, we propose two improvements to the training of bottleneck features motivated by recent advances in the use of deep neural networks (DNNs) for speech recognition. First, we show how the use of unsupervised pretraining of a DNN enhances the network’s discriminative power and improves the bottleneck features it generates. Second, we show that a neural network trained to predict context-dependent senone targets produces better bottleneck features than one trained to predict monophone states. Bottleneck features trained using the proposed methods produced a 16\% relative reduction in sentence error rate over conventional bottleneck features on a large vocabulary business search task.},
  langid = {english}
}

@article{yu_incorporating_2007,
  title = {Incorporating {{Prior Domain Knowledge Into Inductive Machine Learning}}},
  author = {Yu, Ting and Jan, Tony and Simoff, Simeon and Debenham, John},
  date = {2007-01-01},
  abstract = {The paper reviews the recent developments of incorporating prior domain knowledge into inductive machine learning, and proposes a guideline that incorporates prior domain knowledge in three key issues of inductive machine learning algorithms: consistency, gen-eralization and convergence. With respect to each issue, this paper gives some approaches to improve the performance of the inductive machine learning algorithms and discusses the risks of incorporating domain knowledge. As a case study, a hierarchical modelling method, VQSVM, is proposed and tested over some imbalanced data sets with various imbalance ratios and various numbers of subclasses.}
}

@inproceedings{yu_unsupervised_2019,
  title = {Unsupervised Out-of-Distribution Detection by Maximum Classifier Discrepancy},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Yu, Qing and Aizawa, Kiyoharu},
  date = {2019},
  pages = {9518--9526}
}

@misc{yuan_machine_2017,
  title = {Machine {{Comprehension}} by {{Text-to-Text Neural Question Generation}}},
  author = {Yuan, Xingdi and Wang, Tong and Gulcehre, Caglar and Sordoni, Alessandro and Bachman, Philip and Subramanian, Sandeep and Zhang, Saizheng and Trischler, Adam},
  date = {2017},
  eprint = {1705.02012},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1705.02012},
  urldate = {2018-06-06},
  abstract = {We propose a recurrent neural model that generates natural-language questions from documents, conditioned on answers. We show how to train the model using a combination of supervised and reinforcement learning. After teacher forcing for standard maximum likelihood training, we fine-tune the model using policy gradient techniques to maximize several rewards that measure question quality. Most notably, one of these rewards is the performance of a question-answering system. We motivate question generation as a means to improve the performance of question answering systems. Our model is trained and evaluated on the recent question-answering dataset SQuAD.}
}

@inproceedings{yuanCodeSynonymsMatter2022,
  title = {Code {{Synonyms Do Matter}}: {{Multiple Synonyms Matching Network}} for {{Automatic ICD Coding}}},
  shorttitle = {Code {{Synonyms Do Matter}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  author = {Yuan, Zheng and Tan, Chuanqi and Huang, Songfang},
  date = {2022},
  pages = {808--814},
  publisher = {{Association for Computational Linguistics}},
  location = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-short.91},
  abstract = {Automatic ICD coding is defined as assigning disease codes to electronic medical records (EMRs). Existing methods usually apply label attention with code representations to match related text snippets. Unlike these works that model the label with the code hierarchy or description, we argue that the code synonyms can provide more comprehensive knowledge based on the observation that the code expressions in EMRs vary from their descriptions in ICD. By aligning codes to concepts in UMLS, we collect synonyms of every code. Then, we propose a multiple synonyms matching network to leverage synonyms for better code representation learning, and finally help the code classification. Experiments on the MIMIC-III dataset show that our proposed method outperforms previous state-of-the-art methods.},
  langid = {english}
}

@article{yue_phonetically_2021,
  title = {Phonetically Motivated Self-Supervised Speech Representation Learning},
  author = {Yue, Xianghu and Li, Haizhou},
  date = {2021},
  journaltitle = {Annual Conference of the International Speech Communication Association}
}

@inproceedings{yusuf_hierarchical_2021,
  title = {A {{Hierarchical Subspace Model}} for {{Language-Attuned Acoustic Unit Discovery}}},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Yusuf, Bolaji and Ondel, Lucas and Burget, Lukáš and Černocký, Jan and Saraçlar, Murat},
  date = {2021-06},
  pages = {3710--3714},
  issn = {2379-190X},
  doi = {10.1109/ICASSP39728.2021.9414899},
  abstract = {In this work, we propose a hierarchical subspace model for acoustic unit discovery. In this approach, we frame the task as one of learning embeddings on a low-dimensional phonetic subspace, and simultaneously specify the subspace itself as an embedding on a hyper-subspace. We train the hyper-subspace on a set of transcribed languages and transfer it to the target language. In the target language, we infer both the language and unit embeddings in an unsupervised manner, and in so doing, we simultaneously learn a subspace of units specific to that language and the units that dwell on it. We conduct experiments on TIMIT and two low-resource languages: Mboshi and Yoruba. Results show that our model outperforms major acoustic unit discovery techniques, both in terms of clustering quality and segmentation accuracy.},
  eventtitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})}
}

@inproceedings{zadeh_multimodal_2018,
  title = {Multimodal Language Analysis in the Wild: {{CMU-MOSEI}} Dataset and Interpretable Dynamic Fusion Graph},
  booktitle = {Proceedings of the {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  date = {2018}
}

@inproceedings{zadeh_tensor_2017,
  title = {Tensor {{Fusion Network}} for {{Multimodal Sentiment Analysis}}},
  booktitle = {Proceedings of the 2017 {{Conference}} on {{Empirical Methods}} in {{Natural}}           {{Language Processing}}},
  author = {Zadeh, Amir and Chen, Minghai and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  date = {2017},
  pages = {1103--1114},
  publisher = {{Association for Computational Linguistics}},
  location = {{Copenhagen, Denmark}},
  doi = {10/ggcmm9},
  url = {http://aclweb.org/anthology/D17-1115},
  urldate = {2019-11-08},
  abstract = {Multimodal sentiment analysis is an increasingly popular research area, which extends the conventional language-based definition of sentiment analysis to a multimodal setup where other relevant modalities accompany language. In this paper, we pose the problem of multimodal sentiment analysis as modeling intra-modality and inter-modality dynamics. We introduce a novel model, termed Tensor Fusion Network, which learns both such dynamics end-to-end. The proposed approach is tailored for the volatile nature of spoken language in online videos as well as accompanying gestures and voice. In the experiments, our model outperforms state-ofthe-art approaches for both multimodal and unimodal sentiment analysis.},
  eventtitle = {Proceedings of the 2017 {{Conference}} on {{Empirical Methods}} in {{Natural}}           {{Language Processing}}},
  langid = {english}
}

@inproceedings{zadrozny_transforming_2002,
  title = {Transforming Classifier Scores into Accurate Multiclass Probability Estimates},
  booktitle = {Proceedings of the Eighth {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining},
  author = {Zadrozny, Bianca and Elkan, Charles},
  date = {2002},
  pages = {694--699}
}

@inproceedings{zaeemzadeh_outofdistribution_2021,
  title = {Out-of-Distribution Detection Using Union of 1-Dimensional Subspaces},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zaeemzadeh, Alireza and Bisagno, Niccolo and Sambugaro, Zeno and Conci, Nicola and Rahnavard, Nazanin and Shah, Mubarak},
  date = {2021},
  pages = {9452--9461},
  eventtitle = {Conference on {{Computer Vision}} and {{Pattern Recognition}}}
}

@inproceedings{zaken_bitfit_2022,
  title = {{{BitFit}}: {{Simple}} Parameter-Efficient Fine-Tuning for Transformer-Based Masked Language-Models},
  booktitle = {Proceedings of the {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{ACL}})},
  author = {Zaken, Elad Ben and Goldberg, Yoav and Ravfogel, Shauli},
  editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
  date = {2022},
  pages = {1--9},
  publisher = {{Association for Computational Linguistics}},
  location = {{Dublin, Ireland}},
  url = {https://doi.org/10.18653/v1/2022.acl-short.1}
}

@misc{zaremba_reinforcement_2015,
  title = {Reinforcement {{Learning Neural Turing Machines}}},
  author = {Zaremba, Wojciech and Sutskever, Ilya},
  date = {2015},
  eprint = {1505.00521v1},
  eprinttype = {arxiv},
  abstract = {The expressive power of a machine learning model is closely related to the num-ber of sequential computational steps it can learn. For example, Deep Neural Networks have been more successful than shallow networks because they can per-form a greater number of sequential computational steps (each highly parallel). The Neural Turing Machine (NTM) [8] is a model that can compactly express an even greater number of sequential computational steps, so it is even more powerful than a DNN. Its memory addressing operations are designed to be differentiable; thus the NTM can be trained with backpropagation. While differentiable memory is relatively easy to implement and train, it necessi-tates accessing the entire memory content at each computational step. This makes it difficult to implement a fast NTM. In this work, we use the Reinforce algorithm to learn where to access the memory, while using backpropagation to learn what to write to the memory. We call this model the RL-NTM. Reinforce allows our model to access a constant number of memory cells at each computational step, so its implementation can be faster. The RL-NTM is the first model that can, in principle, learn programs of unbounded running time. We successfully trained the RL-NTM to solve a number of algorithmic tasks that are simpler than the ones solvable by the fully differentiable NTM. As the RL-NTM is a fairly intricate model, we needed a method for verifying the correctness of our implementation. To do so, we developed a simple technique for numerically checking arbitrary implementations of models that use Reinforce, which may be of independent interest.},
  isbn = {9781424438617}
}

@misc{zbontar_barlow_2021,
  title = {Barlow {{Twins}}: {{Self-Supervised Learning}} via {{Redundancy Reduction}}},
  shorttitle = {Barlow {{Twins}}},
  author = {Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, Stéphane},
  date = {2021-05-03},
  eprint = {2103.03230},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2103.03230},
  urldate = {2021-05-10},
  abstract = {Self-supervised learning (SSL) is rapidly closing the gap with supervised methods on large computer vision benchmarks. A successful approach to SSL is to learn representations which are invariant to distortions of the input sample. However, a recurring issue with this approach is the existence of trivial constant solutions. Most current methods avoid such solutions by careful implementation details. We propose an objective function that naturally avoids such collapse by measuring the cross-correlation matrix between the outputs of two identical networks fed with distorted versions of a sample, and making it as close to the identity matrix as possible. This causes the representation vectors of distorted versions of a sample to be similar, while minimizing the redundancy between the components of these vectors. The method is called Barlow Twins, owing to neuroscientist H. Barlow's redundancy-reduction principle applied to a pair of identical networks. Barlow Twins does not require large batches nor asymmetry between the network twins such as a predictor network, gradient stopping, or a moving average on the weight updates. It allows the use of very high-dimensional output vectors. Barlow Twins outperforms previous methods on ImageNet for semi-supervised classification in the low-data regime, and is on par with current state of the art for ImageNet classification with a linear classifier head, and for transfer tasks of classification and object detection.},
  langid = {english}
}

@misc{zeiler_adadelta_2012,
  title = {{{ADADELTA}}: {{An Adaptive Learning Rate Method}}},
  author = {Zeiler, Matthew D.},
  date = {2012-12-22},
  eprint = {1212.5701},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1212.5701},
  urldate = {2018-05-24},
  abstract = {We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.}
}

@inproceedings{zeiler_rectified_2013,
  title = {On Rectified Linear Units for Speech Processing},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Zeiler, Matthew D and Ranzato, M and Monga, Rajat and Mao, Min and Yang, Kun and Le, Quoc Viet and Nguyen, Patrick and Senior, Alan and Vanhoucke, Vincent and Dean, Jeffrey and others},
  date = {2013},
  pages = {3517--3521}
}

@inproceedings{zeiler_visualizing_2014,
  title = {Visualizing and Understanding Convolutional Networks},
  booktitle = {Lecture {{Notes}} in {{Computer Science}} (Including Subseries {{Lecture Notes}} in {{Artificial Intelligence}} and {{Lecture Notes}} in {{Bioinformatics}})},
  author = {Zeiler, Matthew D. and Fergus, Rob},
  date = {2014},
  volume = {8689 LNCS},
  eprint = {26353135},
  eprinttype = {pmid},
  pages = {818--833},
  issn = {16113349},
  doi = {10.1007/978-3-319-10590-1_53},
  url = {http://link.springer.com/10.1007/978-3-319-10590-1_53%5Cnhttp://arxiv.org/abs/1311.2901%5Cnpapers3://publication/uuid/44feb4b1-873a-4443-8baa-1730ecd16291},
  abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky \textbackslash etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
  isbn = {978-3-319-10589-5},
  issue = {PART 1}
}

@inproceedings{zeng_multistage_2013,
  title = {Multi-Stage {{Contextual Deep Learning}} for {{Pedestrian Detection}}},
  booktitle = {2013 {{IEEE International Conference}} on {{Computer Vision}}},
  author = {Zeng, Xingyu and Ouyang, Wanli and Wang, Xiaogang},
  date = {2013-12},
  pages = {121--128},
  publisher = {{IEEE}},
  location = {{Sydney, Australia}},
  doi = {10.1109/ICCV.2013.22},
  url = {http://ieeexplore.ieee.org/document/6751124/},
  urldate = {2022-03-30},
  abstract = {Cascaded classifiers1 have been widely used in pedestrian detection and achieved great success. These classifiers are trained sequentially without joint optimization. In this paper, we propose a new deep model that can jointly train multi-stage classifiers through several stages of backpropagation. It keeps the score map output by a classifier within a local region and uses it as contextual information to support the decision at the next stage. Through a specific design of the training strategy, this deep architecture is able to simulate the cascaded classifiers by mining hard samples to train the network stage-by-stage. Each classifier handles samples at a different difficulty level. Unsupervised pre-training and specifically designed stage-wise supervised training are used to regularize the optimization problem. Both theoretical analysis and experimental results show that the training strategy helps to avoid overfitting. Experimental results on three datasets (Caltech, ETH and TUD-Brussels) show that our approach outperforms the state-of-the-art approaches.},
  eventtitle = {2013 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-4799-2840-8},
  langid = {english}
}

@inproceedings{zeng_not_2022,
  title = {Not {{All Tokens Are Equal}}: {{Human-centric Visual Analysis}} via {{Token Clustering Transformer}}},
  shorttitle = {Not {{All Tokens Are Equal}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zeng, Wang and Jin, Sheng and Liu, Wentao and Qian, Chen and Luo, Ping and Ouyang, Wanli and Wang, Xiaogang},
  date = {2022-06},
  pages = {11091--11101},
  publisher = {{IEEE}},
  location = {{New Orleans, LA, USA}},
  doi = {10.1109/CVPR52688.2022.01082},
  url = {https://ieeexplore.ieee.org/document/9878406/},
  urldate = {2022-10-08},
  abstract = {Vision transformers have achieved great successes in many computer vision tasks. Most methods generate vision tokens by splitting an image into a regular and fixed grid and treating each cell as a token. However, not all regions are equally important in human-centric vision tasks, e.g., the human body needs a fine representation with many tokens, while the image background can be modeled by a few tokens. To address this problem, we propose a novel Vision Transformer, called Token Clustering Transformer (TCFormer), which merges tokens by progressive clustering, where the tokens can be merged from different locations with flexible shapes and sizes. The tokens in TCFormer can not only focus on important areas but also adjust the token shapes to fit the semantic concept and adopt a fine resolution for regions containing critical details, which is beneficial to capturing detailed information. Extensive experiments show that TCFormer consistently outperforms its counterparts on different challenging human-centric tasks and datasets, including whole-body pose estimation on COCO-WholeBody and 3D human mesh reconstruction on 3DPW. Code is available at https://github.com/ zengwang430521/TCFormer.git.},
  eventtitle = {{{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-66546-946-3},
  langid = {english},
  keywords = {Medium}
}

@inproceedings{zeng_robust_2006,
  title = {Robust {{GMM Based Gender Classification}} Using {{Pitch}} and {{RASTA-PLP Parameters}} of {{Speech}}},
  booktitle = {2006 {{International Conference}} on {{Machine Learning}} and {{Cybernetics}}},
  author = {Zeng, Yu-min and Wu, Zhen-yang and Falk, Tiago and Chan, Wai-yip},
  date = {2006},
  pages = {3376--3379},
  publisher = {{IEEE}},
  location = {{Dalian, China}},
  doi = {10/c76ns3},
  url = {http://ieeexplore.ieee.org/document/4028651/},
  urldate = {2019-01-04},
  abstract = {A novel gender classification system has been proposed based on Gaussian Mixture Models, which apply the combined parameters of pitch and 10th order relative spectral perceptual linear predictive coefficients to model the characteristics of male and female speech. The performances of gender classification system have been evaluated on the conditions of clean speech, noisy speech and multi-language. The simulations show that the performance of the proposed gender classifier is excellent; it is very robust for noise and completely independent of languages; the classification accuracy is as high as above 98\% for all clean speech and remains 95\% for most noisy speech, even the SNR of speech is degraded to 0dB.},
  eventtitle = {2006 {{International Conference}} on {{Machine Learning}} and {{Cybernetics}}},
  isbn = {978-1-4244-0061-4},
  langid = {english}
}

@inproceedings{zhai_s4l_2019,
  title = {{{S4L}}: {{Self-Supervised Semi-Supervised Learning}}},
  shorttitle = {{{S4L}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Zhai, Xiaohua and Oliver, Avital and Kolesnikov, Alexander and Beyer, Lucas},
  date = {2019-10},
  pages = {1476--1485},
  issn = {2380-7504},
  doi = {10.1109/ICCV.2019.00156},
  abstract = {This work tackles the problem of semi-supervised learning of image classifiers. Our main insight is that the field of semi-supervised learning can benefit from the quickly advancing field of self-supervised visual representation learning. Unifying these two approaches, we propose the framework of self-supervised semi-supervised learning (S4L) and use it to derive two novel semi-supervised image classification methods. We demonstrate the effectiveness of these methods in comparison to both carefully tuned baselines, and existing semi-supervised learning methods. We then show that S4L and existing semi-supervised methods can be jointly trained, yielding a new state-of-the-art result on semi-supervised ILSVRC-2012 with 10\% of labels.},
  eventtitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})}
}

@inproceedings{zhai_scaling_2022,
  title = {Scaling {{Vision Transformers}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zhai, Xiaohua and Kolesnikov, Alexander and Houlsby, Neil and Beyer, Lucas},
  date = {2022-06},
  pages = {1204--1213},
  publisher = {{IEEE}},
  location = {{New Orleans, LA, USA}},
  doi = {10.1109/CVPR52688.2022.01179},
  url = {https://ieeexplore.ieee.org/document/9880094/},
  urldate = {2022-10-16},
  abstract = {Attention-based neural networks such as the Vision Transformer (ViT) have recently attained state-of-the-art results on many computer vision benchmarks. Scale is a primary ingredient in attaining excellent results, therefore, understanding a model’s scaling properties is a key to designing future generations effectively. While the laws for scaling Transformer language models have been studied, it is unknown how Vision Transformers scale. To address this, we scale ViT models and data, both up and down, and characterize the relationships between error rate, data, and compute. Along the way, we refine the architecture and training of ViT, reducing memory consumption and increasing accuracy of the resulting models. As a result, we successfully train a ViT model with two billion parameters, which attains a new state-of-the-art on ImageNet of 90.45\% top-1 accuracy. The model also performs well for few-shot transfer, for example, reaching 84.86\% top-1 accuracy on ImageNet with only 10 examples per class.},
  eventtitle = {{{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-66546-946-3},
  langid = {english}
}

@misc{zhang_advances_2018,
  title = {Advances in {{Variational Inference}}},
  author = {Zhang, Cheng and Butepage, Judith and Kjellstrom, Hedvig and Mandt, Stephan},
  date = {2018-10-23},
  eprint = {1711.05597},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1711.05597},
  urldate = {2020-01-18},
  abstract = {Many modern unsupervised or semi-supervised machine learning algorithms rely on Bayesian probabilistic models. These models are usually intractable and thus require approximate inference. Variational inference (VI) lets us approximate a high-dimensional Bayesian posterior with a simpler variational distribution by solving an optimization problem. This approach has been successfully applied to various models and large-scale applications. In this review, we give an overview of recent trends in variational inference. We first introduce standard mean field variational inference, then review recent advances focusing on the following aspects: (a) scalable VI, which includes stochastic approximations, (b) generic VI, which extends the applicability of VI to a large class of otherwise intractable models, such as non-conjugate models, (c) accurate VI, which includes variational models beyond the mean field approximation or with atypical divergences, and (d) amortized VI, which implements the inference over local latent variables with inference networks. Finally, we provide a summary of promising future research directions.},
  langid = {english}
}

@inproceedings{zhang_colorful_2016,
  title = {Colorful Image Colorization},
  booktitle = {European {{Conference}} on {{Computer Vision}}},
  author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A.},
  date = {2016},
  pages = {649--666}
}

@misc{zhang_fully_2018,
  title = {Fully {{Supervised Speaker Diarization}}},
  author = {Zhang, Aonan and Wang, Quan and Zhu, Zhenyao and Paisley, John and Wang, Chong},
  date = {2018-10-10},
  eprint = {1810.04719},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1810.04719},
  urldate = {2019-02-26},
  abstract = {In this paper, we propose a fully supervised speaker diarization approach, named unbounded interleaved-state recurrent neural networks (UIS-RNN). Given extracted speaker-discriminative embeddings (a.k.a. d-vectors) from input utterances, each individual speaker is modeled by a parameter-sharing RNN, while the RNN states for different speakers interleave in the time domain. This RNN is naturally integrated with a distance-dependent Chinese restaurant process (ddCRP) to accommodate an unknown number of speakers. Our system is fully supervised and is able to learn from examples where time-stamped speaker labels are annotated. We achieved a 7.6\% diarization error rate on NIST SRE 2000 CALLHOME, which is better than the state-of-the-art method using spectral clustering. Moreover, our method decodes in an online fashion while most state-of-the-art systems rely on offline clustering.},
  keywords = {★}
}

@inproceedings{zhang_learning_2019,
  title = {Learning {{Latent Representations}} for {{Style Control}} and {{Transfer}} in {{End-to-end Speech Synthesis}}},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Zhang, Ya-Jie and Pan, Shifeng and He, Lei and Ling, Zhen-Hua},
  date = {2019-05},
  pages = {6945--6949},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2019.8683623},
  abstract = {In this paper, we introduce the Variational Autoencoder (VAE) to an end-to-end speech synthesis model, to learn the latent representation of speaking styles in an unsupervised manner. The style representation learned through VAE shows good properties such as disentangling, scaling, and combination, which makes it easy for style control. Style transfer can be achieved in this framework by first inferring style representation through the recognition network of VAE, then feeding it into TTS network to guide the style in synthesizing speech. To avoid Kullback-Leibler (KL) divergence collapse in training, several techniques are adopted. Finally, the proposed model shows good performance of style control and outperforms Global Style Token (GST) model in ABX preference tests on style transfer.},
  eventtitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})}
}

@misc{zhang_neglected_2021,
  title = {The Neglected Sibling: {{Isotropic}} Gaussian Posterior for {{VAE}}},
  author = {Zhang, Lan and Buntine, Wray and Shareghi, Ehsan},
  date = {2021},
  eprint = {2110.07383},
  eprinttype = {arxiv}
}

@misc{zhang_noisy_2017,
  title = {Noisy {{Natural Gradient}} as {{Variational Inference}}},
  author = {Zhang, Guodong and Sun, Shengyang and Duvenaud, David and Grosse, Roger},
  date = {2017},
  eprint = {1712.02390},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1712.02390},
  urldate = {2018-04-11},
  abstract = {Variational Bayesian neural nets combine the flexibility of deep learning with Bayesian uncertainty estimation. Unfortunately, there is a tradeoff between cheap but simple variational families (e.g.\textasciitilde fully factorized) or expensive and complicated inference procedures. We show that natural gradient ascent with adaptive weight noise implicitly fits a variational posterior to maximize the evidence lower bound (ELBO). This insight allows us to train full-covariance, fully factorized, or matrix-variate Gaussian variational posteriors using noisy versions of natural gradient, Adam, and K-FAC, respectively, making it possible to scale up to modern-size ConvNets. On standard regression benchmarks, our noisy K-FAC algorithm makes better predictions and matches Hamiltonian Monte Carlo's predictive variances better than existing methods. Its improved uncertainty estimates lead to more efficient exploration in active learning, and intrinsic motivation for reinforcement learning.}
}

@inproceedings{zhang_pushing_2020,
  title = {Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition},
  booktitle = {Proceedings of the {{Workshop}} on {{Self-Supervised Learning}} for {{Speech}} and {{Audio Processing}} at {{NeurIPS}}},
  author = {Zhang, Yu and Qin, James and Park, Daniel S and Han, Wei and Chiu, Chung-Cheng and Pang, Ruoming and Le, Quoc V and Wu, Yonghui},
  date = {2020},
  eprint = {2010.10504},
  eprinttype = {arxiv},
  abstract = {We employ a combination of recent developments in semi-supervised learning for automatic speech recognition to obtain state-of-the-art results on LibriSpeech utilizing the unlabeled audio of the Libri-Light dataset. More precisely, we carry out noisy student training with SpecAugment using giant Conformer models pre-trained using wav2vec 2.0 pre-training. By doing so, we are able to achieve word-error-rates (WERs) 1.4\%/2.6\% on the LibriSpeech test/test-other sets against the current state-of-the-art WERs 1.7\%/3.3\%.}
}

@misc{zhang_relationship_2017,
  title = {On the {{Relationship Between}} the {{OpenAI Evolution Strategy}} and {{Stochastic Gradient Descent}}},
  author = {Zhang, Xingwen and Clune, Jeff and Stanley, Kenneth O.},
  date = {2017},
  eprint = {1712.06564},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1712.06564},
  urldate = {2018-01-03},
  abstract = {Because stochastic gradient descent (SGD) has shown promise optimizing neural networks with millions of parameters and few if any alternatives are known to exist, it has moved to the heart of leading approaches to reinforcement learning (RL). For that reason, the recent result from OpenAI showing that a particular kind of evolution strategy (ES) can rival the performance of SGD-based deep RL methods with large neural networks provoked surprise. This result is difficult to interpret in part because of the lingering ambiguity on how ES actually relates to SGD. The aim of this paper is to significantly reduce this ambiguity through a series of MNIST-based experiments designed to uncover their relationship. As a simple supervised problem without domain noise (unlike in most RL), MNIST makes it possible (1) to measure the correlation between gradients computed by ES and SGD and (2) then to develop an SGD-based proxy that accurately predicts the performance of different ES population sizes. These innovations give a new level of insight into the real capabilities of ES, and lead also to some unconventional means for applying ES to supervised problems that shed further light on its differences from SGD. Incorporating these lessons, the paper concludes by demonstrating that ES can achieve 99\% accuracy on MNIST, a number higher than any previously published result for any evolutionary method. While not by any means suggesting that ES should substitute for SGD in supervised learning, the suite of experiments herein enables more informed decisions on the application of ES within RL and other paradigms.}
}

@inproceedings{zhang_speech_2009,
  title = {Speech Rhythm Guided Syllable Nuclei Detection},
  booktitle = {2009 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  author = {Zhang, Yaodong and Glass, James R.},
  date = {2009-04},
  pages = {3797--3800},
  publisher = {{IEEE}},
  location = {{Taipei, Taiwan}},
  doi = {10.1109/ICASSP.2009.4960454},
  url = {http://ieeexplore.ieee.org/document/4960454/},
  urldate = {2018-11-23},
  abstract = {In this paper, we present a novel speech-rhythm-guided syllablenuclei location detection algorithm. As a departure from conventional methods, we introduce an instantaneous speech rhythm estimator to predict possible regions where syllable nuclei can appear. Within a possible region, a simple slope based peak counting algorithm is used to get the exact location of each syllable nucleus. We verify the correctness of our method by investigating the syllable nuclei interval distribution in TIMIT dataset, and evaluate the performance by comparing with a state-of-the-art syllable nuclei based speech rate detection approach.},
  eventtitle = {{{ICASSP}} 2009 - 2009 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  isbn = {978-1-4244-2353-8},
  langid = {english}
}

@inproceedings{zhang_understanding_2017,
  title = {Understanding {{Deep Learning Requires Rethinking Generalization}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  date = {2017},
  url = {https://arxiv.org/abs/1611.03530},
  urldate = {2018-10-12},
  abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training.  Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice.  We interpret our experimental findings by comparison with traditional models.},
  langid = {english}
}

@inproceedings{zhang_understanding_2021,
  title = {Understanding {{Failures}} in {{Out-of-Distribution Detection}} with {{Deep Generative Models}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Zhang, Lily H and Goldstein, Mark and Ranganath, Rajesh},
  date = {2021},
  volume = {139},
  pages = {10},
  publisher = {{PMLR}},
  abstract = {Deep generative models (DGMs) seem a natural fit for detecting out-of-distribution (OOD) inputs, but such models have been shown to assign higher probabilities or densities to OOD images than images from the training distribution. In this work, we explain why this behavior should be attributed to model misestimation. We first prove that no method can guarantee performance beyond random chance without assumptions on which out-distributions are relevant. We then interrogate the typical set hypothesis, the claim that relevant out-distributions can lie in high likelihood regions of the data distribution, and that OOD detection should be defined based on the data distribution’s typical set. We highlight the consequences implied by assuming support overlap between in- and out-distributions, as well as the arbitrariness of the typical set for OOD detection. Our results suggest that estimation error is a more plausible explanation than the misalignment between likelihood-based OOD detection and outdistributions of interest, and we illustrate how even minimal estimation error can lead to OOD detection failures, yielding implications for future work in deep generative modeling and OOD detection.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english}
}

@article{zhang_very_2016,
  title = {Very {{Deep Convolutional Networks}} for {{End-to-End Speech Recognition}}},
  author = {Zhang, Yu and Chan, William and Jaitly, Navdeep},
  date = {2016-10-10},
  url = {https://arxiv.org/abs/1610.03022},
  urldate = {2018-11-14},
  abstract = {Sequence-to-sequence models have shown success in end-to-end speech recognition. However these models have only used shallow acoustic encoder networks. In our work, we successively train very deep convolutional networks to add more expressive power and better generalization for end-to-end ASR models. We apply network-in-network principles, batch normalization, residual connections and convolutional LSTMs to build very deep recurrent and convolutional structures. Our models exploit the spectral structure in the feature space and add computational depth without overfitting issues. We experiment with the WSJ ASR task and achieve 10.5\textbackslash\% word error rate without any dictionary or language using a 15 layer deep network.},
  langid = {english},
  keywords = {★}
}

@inproceedings{zhangBERTXMLLargeScale2020,
  title = {{{BERT-XML}}: {{Large Scale Automated ICD Coding Using BERT Pretraining}}},
  shorttitle = {{{BERT-XML}}},
  booktitle = {Proceedings of the 3rd {{Clinical Natural Language Processing Workshop}}},
  author = {Zhang, Zachariah and Liu, Jingshu and Razavian, Narges},
  date = {2020-11},
  pages = {24--34},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.clinicalnlp-1.3},
  abstract = {ICD coding is the task of classifying and cod-ing all diagnoses, symptoms and proceduresassociated with a patient's visit. The process isoften manual, extremely time-consuming andexpensive for hospitals as clinical interactionsare usually recorded in free text medical notes.In this paper, we propose a machine learningmodel, BERT-XML, for large scale automatedICD coding of EHR notes, utilizing recentlydeveloped unsupervised pretraining that haveachieved state of the art performance on a va-riety of NLP tasks. We train a BERT modelfrom scratch on EHR notes, learning with vo-cabulary better suited for EHR tasks and thusoutperform off-the-shelf models. We furtheradapt the BERT architecture for ICD codingwith multi-label attention. We demonstratethe effectiveness of BERT-based models on thelarge scale ICD code classification task usingmillions of EHR notes to predict thousands ofunique codes.}
}

@misc{zhao_endtoend_2016,
  title = {Towards {{End-to-End Learning}} for {{Dialog State Tracking}} and {{Management}} Using {{Deep Reinforcement Learning}}},
  author = {Zhao, Tiancheng and Eskenazi, Maxine},
  date = {2016-06-08},
  eprint = {1606.02560},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1606.02560},
  urldate = {2019-06-11},
  abstract = {This paper presents an end-to-end framework for task-oriented dialog systems using a variant of Deep Recurrent QNetworks (DRQN). The model is able to interface with a relational database and jointly learn policies for both language understanding and dialog strategy. Moreover, we propose a hybrid algorithm that combines the strength of reinforcement learning and supervised learning to achieve faster learning speed. We evaluated the proposed model on a 20 Question Game conversational game simulator. Results show that the proposed method outperforms the modular-based baseline and learns a distributed representation of the latent dialog state.},
  langid = {english}
}

@misc{zhao_infovae_2018,
  title = {{{InfoVAE}}: {{Information Maximizing Variational Autoencoders}}},
  shorttitle = {{{InfoVAE}}},
  author = {Zhao, Shengjia and Song, Jiaming and Ermon, Stefano},
  date = {2018-05-30},
  eprint = {1706.02262},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1706.02262},
  urldate = {2021-03-04},
  abstract = {A key advance in learning generative models is the use of amortized inference distributions that are jointly trained with the models. We find that existing training objectives for variational autoencoders can lead to inaccurate amortized inference distributions and, in some cases, improving the objective provably degrades the inference quality. In addition, it has been observed that variational autoencoders tend to ignore the latent variables when combined with a decoding distribution that is too flexible. We again identify the cause in existing training criteria and propose a new class of objectives (InfoVAE) that mitigate these problems. We show that our model can significantly improve the quality of the variational posterior and can make effective use of the latent features regardless of the flexibility of the decoding distribution. Through extensive qualitative and quantitative analyses, we demonstrate that our models outperform competing approaches on multiple performance metrics.},
  langid = {english}
}

@inproceedings{zhao_learning_2017,
  title = {Learning {{Hierarchical Features}} from {{Deep Generative Models}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Zhao, Shengjia and Song, Jiaming and Ermon, Stefano},
  date = {2017},
  pages = {4091--4099},
  location = {{Sydney, Australia}},
  url = {http://proceedings.mlr.press/v70/zhao17c.html},
  abstract = {Deep neural networks have been shown to be very successful at learning feature hierarchies in supervised learning tasks. Generative models, on the other hand, have benefited less from hierarchical models with multiple layers of latent variables. In this paper, we prove that hierarchical latent variable models do not take advantage of the hierarchical structure when trained with existing variational methods, and provide some limitations on the kind of features existing models can learn. Finally we propose an alternative architecture that do not suffer from these limitations. Our model is able to learn highly interpretable and disentangled hierarchical features on several natural image datasets with no task specific regularization or prior knowledge.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}}
}

@inproceedings{zhao_stochastic_2014,
  title = {Stochastic {{Optimization}} with {{Importance Sampling}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Zhao, Peilin and Zhang, Tong},
  date = {2014},
  eprint = {1401.2753},
  eprinttype = {arxiv},
  location = {{Lille, France}},
  url = {http://arxiv.org/abs/1401.2753},
  urldate = {2018-04-08},
  abstract = {Uniform sampling of training data has been commonly used in traditional stochastic optimization algorithms such as Proximal Stochastic Gradient Descent (prox-SGD) and Proximal Stochastic Dual Coordinate Ascent (prox-SDCA). Although uniform sampling can guarantee that the sampled stochastic quantity is an unbiased estimate of the corresponding true quantity, the resulting estimator may have a rather high variance, which negatively affects the convergence of the underlying optimization procedure. In this paper we study stochastic optimization with importance sampling, which improves the convergence rate by reducing the stochastic variance. Specifically, we study prox-SGD (actually, stochastic mirror descent) with importance sampling and prox-SDCA with importance sampling. For prox-SGD, instead of adopting uniform sampling throughout the training process, the proposed algorithm employs importance sampling to minimize the variance of the stochastic gradient. For prox-SDCA, the proposed importance sampling scheme aims to achieve higher expected dual value at each dual coordinate ascent step. We provide extensive theoretical analysis to show that the convergence rates with the proposed importance sampling methods can be significantly improved under suitable conditions both for prox-SGD and for prox-SDCA. Experiments are provided to verify the theoretical analysis.}
}

@article{zheng_conditional_2015,
  title = {Conditional {{Random Fields}} as {{Recurrent Neural Networks}}},
  author = {Zheng, Shuai and Jayasumana, Sadeep and Torr, Philip H. S.},
  date = {2015},
  journaltitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  pages = {1529--1537},
  doi = {10.1109/ICCV.2015.179}
}

@inproceedings{zheng_rethinking_2021,
  title = {Rethinking {{Semantic Segmentation}} from a {{Sequence-to-Sequence Perspective}} with {{Transformers}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zheng, Sixiao and Lu, Jiachen and Zhao, Hengshuang and Zhu, Xiatian and Luo, Zekun and Wang, Yabiao and Fu, Yanwei and Feng, Jianfeng and Xiang, Tao and Torr, Philip H.S. and Zhang, Li},
  date = {2021-06},
  pages = {6877--6886},
  publisher = {{IEEE}},
  location = {{Nashville, TN, USA}},
  doi = {10.1109/CVPR46437.2021.00681},
  url = {https://ieeexplore.ieee.org/document/9578646/},
  urldate = {2022-10-16},
  abstract = {Most recent semantic segmentation methods adopt a fully-convolutional network (FCN) with an encoderdecoder architecture. The encoder progressively reduces the spatial resolution and learns more abstract/semantic visual concepts with larger receptive fields. Since context modeling is critical for segmentation, the latest efforts have been focused on increasing the receptive field, through either dilated/atrous convolutions or inserting attention modules. However, the encoder-decoder based FCN architecture remains unchanged. In this paper, we aim to provide an alternative perspective by treating semantic segmentation as a sequence-to-sequence prediction task. Specifically, we deploy a pure transformer (i.e., without convolution and resolution reduction) to encode an image as a sequence of patches. With the global context modeled in every layer of the transformer, this encoder can be combined with a simple decoder to provide a powerful segmentation model, termed SEgmentation TRansformer (SETR). Extensive experiments show that SETR achieves new state of the art on ADE20K (50.28\% mIoU), Pascal Context (55.83\% mIoU) and competitive results on Cityscapes. Particularly, we achieve the first position in the highly competitive ADE20K test server leaderboard on the day of submission.},
  eventtitle = {{{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-66544-509-2},
  langid = {english}
}

@inproceedings{zhou_anomaly_2017,
  title = {Anomaly Detection with Robust Deep Autoencoders},
  booktitle = {Proceedings of the 23rd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Zhou, Chong and Paffenroth, Randy C},
  date = {2017},
  pages = {665--674}
}

@inproceedings{zhou_comparison_2021,
  title = {A Comparison of Discrete Latent Variable Models for Speech Representation Learning},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Zhou, Henry and Baevski, Alexei and Auli, Michael},
  date = {2021},
  eprint = {2010.14230},
  eprinttype = {arxiv},
  pages = {3050--3054},
  url = {https://arxiv.org/abs/2010.14230}
}

@inproceedings{zhou_computation_1988,
  title = {Computation of {{Optical Flow}} Using a {{Neural Network}}},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Neural Networks}}},
  author = {Zhou, Y. T. and Chellappa, R.},
  date = {1988},
  number = {86},
  pages = {71-78 vol.2},
  publisher = {{IEEE}},
  doi = {10.1109/ICNN.1988.23914},
  url = {http://ieeexplore.ieee.org/document/23914/},
  urldate = {2018-05-24},
  isbn = {0-7803-0999-5}
}

@inproceedings{zhou_deep_2017,
  title = {Deep Forest: {{Towards}} an {{Alternative}} to {{Deep Neural Networks}}},
  booktitle = {{{IJCAI International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Zhou, Zhi Hua and Feng, Ji},
  date = {2017},
  eprint = {1702.08835},
  eprinttype = {arxiv},
  pages = {3553--3559},
  issn = {10450823},
  doi = {10.24963/ijcai.2017/497},
  abstract = {In this paper, we propose gcForest, a decision tree ensemble approach with performance highly competitive to deep neural networks. In contrast to deep neural networks which require great effort in hyper-parameter tuning, gcForest is much easier to train. Actually, even when gcForest is applied to different data from different domains, excellent performance can be achieved by almost same settings of hyper-parameters. The training process of gcForest is efficient and scalable. In our experiments its training time running on a PC is comparable to that of deep neural networks running with GPU facilities, and the efficiency advantage may be more apparent because gcForest is naturally apt to parallel implementation. Furthermore, in contrast to deep neural networks which require large-scale training data, gcForest can work well even when there are only small-scale training data. Moreover, as a tree-based approach, gcForest should be easier for theoretical analysis than deep neural networks.},
  isbn = {978-0-9992411-0-3}
}

@article{zhou_improved_,
  title = {Improved {{Regularization Techniques}} for {{End-To-End Speech Recognition}}},
  author = {Zhou, Yingbo and Xiong, Caiming and Socher, Richard},
  pages = {5},
  abstract = {Regularization is important for end-to-end speech models, since the models are highly flexible and easy to overfit. Data augmentation and dropout has been important for improving end-to-end models in other domains. However, they are relatively under explored for end-to-end speech models. Therefore, we investigate the effectiveness of both methods for end-to-end trainable, deep speech recognition models. We augment audio data through random perturbations of tempo, pitch, volume, temporal alignment, and adding random noise. We further investigate the effect of dropout when applied to the inputs of all layers of the network. We show that the combination of data augmentation and dropout give a relative performance improvement on both Wall Street Journal (WSJ) and LibriSpeech dataset of over 20\%. Our model performance is also competitive with other end-to-end speech models on both datasets.},
  langid = {english}
}

@article{zhou_minimal_2016,
  title = {Minimal Gated Unit for Recurrent Neural Networks},
  author = {Zhou, Guo Bing and Wu, Jianxin and Zhang, Chen Lin and Zhou, Zhi-Hua},
  date = {2016-03-30},
  journaltitle = {International Journal of Automation and Computing},
  volume = {13},
  number = {3},
  eprint = {1603.09420},
  eprinttype = {arxiv},
  pages = {226--234},
  issn = {17518520},
  doi = {10.1007/s11633-016-1006-2},
  url = {http://arxiv.org/abs/1603.09420},
  urldate = {2018-05-06},
  abstract = {Recently recurrent neural networks (RNN) has been very successful in handling sequence data. However, understanding RNN and finding the best practices for RNN is a difficult task, partly because there are many competing and complex hidden units (such as LSTM and GRU). We propose a gated unit for RNN, named as Minimal Gated Unit (MGU), since it only contains one gate, which is a minimal design among all gated hidden units. The design of MGU benefits from evaluation results on LSTM and GRU in the literature. Experiments on various sequence data show that MGU has comparable accuracy with GRU, but has a simpler structure, fewer parameters, and faster training. Hence, MGU is suitable in RNN's applications. Its simple architecture also means that it is easier to evaluate and tune, and in principle it is easier to study MGU's properties theoretically and empirically.},
  keywords = {★}
}

@article{zhou_survey_2022,
  title = {A Survey on Epistemic (Model) Uncertainty in Supervised Learning: {{Recent}} Advances and Applications},
  author = {Zhou, Xinlei and Liu, Han and Pourpanah, Farhad and Zeng, Tieyong and Wang, Xizhao},
  date = {2022},
  journaltitle = {Neurocomputing},
  volume = {489},
  pages = {449--465},
  publisher = {{Elsevier}}
}

@inproceedings{zhouAutomaticICDCoding2021,
  title = {Automatic {{ICD Coding}} via {{Interactive Shared Representation Networks}} with {{Self-distillation Mechanism}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Zhou, Tong and Cao, Pengfei and Chen, Yubo and Liu, Kang and Zhao, Jun and Niu, Kun and Chong, Weifeng and Liu, Shengping},
  date = {2021},
  pages = {5948--5957},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2021.acl-long.463},
  abstract = {The ICD coding task aims at assigning codes of the International Classification of Diseases in clinical notes. Since manual coding is very laborious and prone to errors, many methods have been proposed for the automatic ICD coding task. However, existing works either ignore the long-tail of code frequency or the noisy clinical notes. To address the above issues, we propose an Interactive Shared Representation Network with Self-Distillation mechanism. Specifically, an interactive shared representation network targets building connections among codes while modeling the cooccurrence, consequently alleviating the longtail problem. Moreover, to cope with the noisy text issue, we encourage the model to focus on the clinical note's noteworthy part and extract valuable information through a self-distillation learning mechanism. Experimental results on two MIMIC datasets demonstrate the effectiveness of our method.},
  langid = {english}
}

@inproceedings{zhu_generative_2016,
  title = {Generative Visual Manipulation on the Natural Image Manifold},
  booktitle = {European {{Conference}} on {{Computer Vision}}},
  author = {Zhu, Jun-Yan and Krähenbühl, Philipp and Shechtman, Eli and Efros, Alexei A.},
  date = {2016},
  pages = {597--613}
}

@online{zhu_make_2021,
  title = {Make {{A Long Image Short}}: {{Adaptive Token Length}} for {{Vision Transformers}}},
  shorttitle = {Make {{A Long Image Short}}},
  author = {Zhu, Yichen and Zhu, Yuqin and Du, Jie and Wang, Yi and Ou, Zhicai and Feng, Feifei and Tang, Jian},
  date = {2021-12-05},
  eprint = {2112.01686},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2112.01686},
  urldate = {2022-10-08},
  abstract = {The vision transformer splits each image into a sequence of tokens with fixed length and processes the tokens in the same way as words in natural language processing. More tokens normally lead to better performance but considerably increased computational cost. Motivated by the proverb "A picture is worth a thousand words" we aim to accelerate the ViT model by making a long image short. To this end, we propose a novel approach to assign token length adaptively during inference. Specifically, we first train a ViT model, called Resizable-ViT (ReViT), that can process any given input with diverse token lengths. Then, we retrieve the "token-length label" from ReViT and use it to train a lightweight Token-Length Assigner (TLA). The token-length labels are the smallest number of tokens to split an image that the ReViT can make the correct prediction, and TLA is learned to allocate the optimal token length based on these labels. The TLA enables the ReViT to process the image with the minimum sufficient number of tokens during inference. Thus, the inference speed is boosted by reducing the token numbers in the ViT model. Our approach is general and compatible with modern vision transformer architectures and can significantly reduce computational expanse. We verified the effectiveness of our methods on multiple representative ViT models (DeiT, LV-ViT, and TimesFormer) across two tasks (image classification and action recognition).},
  pubstate = {preprint}
}

@inproceedings{zhu_noiserobust_2022,
  title = {A Noise-Robust Self-Supervised Pre-Training Model Based Speech Representation Learning for Automatic Speech Recognition},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Zhu, Qiu-Shi and Zhang, Jie and Zhang, Zi-Qiang and Wu, Ming-Hui and Fang, Xin and Dai, Li-Rong},
  date = {2022},
  pages = {3174--3178}
}

@article{zhu_recent_2016,
  title = {Recent Progress in Machine Learning-Based Methods for Protein Fold Recognition},
  author = {Zhu, Jianwei and Zhang, Haicang and Li, Shuai Cheng and Wang, Chao and Kong, Lupeng and Sun, Shiwei and Zheng, Wei-mou and Bu, Dongbo and Wei, Leyi and Zou, Quan},
  date = {2016},
  journaltitle = {International Journal of Molecular Sciences},
  volume = {17},
  number = {12},
  eprint = {27999256},
  eprinttype = {pmid},
  pages = {1--13},
  issn = {14220067},
  doi = {10.3390/ijms17122118},
  abstract = {Knowledge on protein folding has a profound impact on understanding the heterogeneity and molecular function of proteins, further facilitating drug design. Predicting the 3D structure (fold) of a protein is a key problem in molecular biology. Determination of the fold of a protein mainly relies on molecular experimental methods. With the development of next-generation sequencing techniques, the discovery of new protein sequences has been rapidly increasing. With such a great number of proteins, the use of experimental techniques to determine protein folding is extremely difficult because these techniques are time consuming and expensive. Thus, developing computational prediction methods that can automatically, rapidly, and accurately classify unknown protein sequences into specific fold categories is urgently needed. Computational recognition of protein folds has been a recent research hotspot in bioinformatics and computational biology. Many computational efforts have been made, generating a variety of computational prediction methods. In this review, we conduct a comprehensive survey of recent computational methods, especially machine learning-based methods, for protein fold recognition. This review is anticipated to assist researchers in their pursuit to systematically understand the computational recognition of protein folds.},
  isbn = {8617092261008}
}

@inproceedings{zhu_s3vae_2020,
  title = {{{S3VAE}}: {{Self-supervised}} Sequential {{VAE}} for Representation Disentanglement and Data Generation},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zhu, Yizhe and Min, Martin Renqiang and Kadav, Asim and Graf, Hans Peter},
  date = {2020},
  pages = {6537--6546},
  location = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.00657},
  url = {https://doi.org/10.1109/CVPR42600.2020.00657}
}

@unpublished{zhu_wav2vecs_2021,
  title = {Wav2vec-{{S}}: {{Semi-Supervised Pre-Training}} for {{Speech Recognition}}},
  shorttitle = {Wav2vec-{{S}}},
  author = {Zhu, Han and Wang, Li and Hou, Ying and Wang, Jindong and Cheng, Gaofeng and Zhang, Pengyuan and Yan, Yonghong},
  date = {2021-10-09},
  eprint = {2110.04484},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2110.04484},
  urldate = {2021-10-22},
  abstract = {Self-supervised pre-training has dramatically improved the performance of automatic speech recognition (ASR). However, most existing self-supervised pre-training approaches are task-agnostic, i.e., could be applied to various downstream tasks. And there is a gap between the task-agnostic pre-training and the task-specific downstream fine-tuning, which may degrade the downstream performance. In this work, we propose a novel pre-training paradigm called wav2vec-S, where we use task-specific semi-supervised pre-training to bridge this gap. Specifically, the semi-supervised pre-training is conducted on the basis of self-supervised pre-training such as wav2vec 2.0. Experiments on ASR show that compared to wav2vec 2.0, wav2vec-S only requires marginal increment of pre-training time but could significantly improve ASR performance on in-domain, cross-domain and cross-lingual datasets. The average relative WER reductions are 26.3\% and 6.3\% for 1h and 10h fine-tuning, respectively.}
}

@misc{zilly_recurrent_2016,
  title = {Recurrent {{Highway Networks}}},
  author = {Zilly, Julian Georg and Srivastava, Rupesh Kumar and Koutník, Jan and Schmidhuber, Jürgen},
  date = {2016-07-12},
  eprint = {1607.03474},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1607.03474},
  urldate = {2019-09-20},
  abstract = {Many sequential processing tasks require complex nonlinear transition functions from one step to the next. However, recurrent neural networks with "deep" transition functions remain difficult to train, even when using Long Short-Term Memory (LSTM) networks. We introduce a novel theoretical analysis of recurrent networks based on Geršgorin’s circle theorem that illuminates several modeling and optimization issues and improves our understanding of the LSTM cell. Based on this analysis we propose Recurrent Highway Networks, which extend the LSTM architecture to allow step-to-step transition depths larger than one. Several language modeling experiments demonstrate that the proposed architecture results in powerful and efficient models. On the Penn Treebank corpus, solely increasing the transition depth from 1 to 10 improves word-level perplexity from 90.6 to 65.4 using the same number of parameters. On the larger Wikipedia datasets for character prediction (text8 and enwik8), RHNs outperform all previous results and achieve an entropy of 1.27 bits per character.},
  langid = {english}
}

@inproceedings{zong_deep_2018,
  title = {Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection},
  booktitle = {Proceedings of the {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Zong, Bo and Song, Qi and Min, Martin Renqiang and Cheng, Wei and Lumezanu, Cristian and Cho, Daeki and Chen, Haifeng},
  date = {2018},
  location = {{Vancouver, BC, Canada}}
}
