
@inproceedings{abdolmaleki_deriving_2017,
  title = {Deriving and Improving {{CMA-ES}} with Information Geometric Trust Regions},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference}} on   - {{GECCO}} '17},
  author = {Abdolmaleki, Abbas and Price, Bob and Lau, Nuno and Reis, Luis Paulo and Neumann, Gerhard},
  year = {2017},
  number = {17},
  eprint = {1602.05561v1},
  eprinttype = {arxiv},
  pages = {657--664},
  address = {{Berlin, Germany}},
  issn = {16130073},
  abstract = {CMA-ES is one of the most popular stochastic search algorithms. It performs favourably in many tasks without the need of extensive parameter tuning. The algorithm has many beneficial properties, including automatic step-size adaptation, efficient covariance up-dates that incorporates the current samples as well as the evolution path and its invariance properties. Its update rules are composed of well established heuristics where the theoretical foundations of some of these rules are also well understood. In this paper we will fully derive all CMA-ES update rules within the framework of expectation-maximisation-based stochastic search algorithms using information-geometric trust regions. We show that the use of the trust region results in similar updates to CMA-ES for the mean and the covariance matrix while it allows for the derivation of an improved update rule for the step-size. Our new algorithm, Trust-Region Co-variance Matrix Adaptation Evolution Strategy (TR-CMA-ES) is fully derived from first order optimization principles and performs favourably in compare to standard CMA-ES algorithm.},
  archiveprefix = {arXiv},
  isbn = {978-1-4503-4920-8}
}

@incollection{aggarwal_surprising_2001,
  title = {On the {{Surprising Behavior}} of {{Distance Metrics}} in {{High Dimensional Space}}},
  author = {Aggarwal, Charu C. and Hinneburg, Alexander and Keim, Daniel A.},
  year = {2001},
  eprint = {0812.0624},
  eprinttype = {arxiv},
  pages = {420--434},
  issn = {0956-7925},
  abstract = {In recent years, the effect of the curse of high dimensionality has been studied in great detail on several problems such as clustering, nearest neighbor search, and indexing. In high dimensional space the data becomes sparse, and traditional indexing and algorithmic techniques fail from a effciency and/or effectiveness perspective. Recent research results show that in high dimensional space, the concept of proximity, distance or nearest neighbor may not even be qualitatively meaningful. In this paper, we view the dimensionality curse from the point of view of the distance metrics which are used to measure the similarity between objects. We specifically examine the behavior of the commonly used L k norm and show that the problem of meaningfulness in high dimensionality is sensitive to the value of k. For example, this means that the Manhattan distance metric L(1 norm) is consistently more preferable than the Euclidean distance metric L(2 norm) for high dimensional data mining applications. Using the intuition derived from our analysis, we introduce and examine a natural extension of the L k norm to fractional distance metrics. We show that the fractional distance metric provides more meaningful results both from the theoretical and empirical perspective. The results show that fractional distance metrics can significantly improve the effectiveness of standard clustering algorithms such as the k-means algorithm.},
  archiveprefix = {arXiv},
  isbn = {978-3-540-41456-8},
  pmid = {25246403},
  keywords = {★}
}

@article{agic_multilingual_2016,
  title = {Multilingual {{Projection}} for {{Parsing Truly Low-Resource Languages}}},
  author = {Agi{\'c}, {\v Z}eljko and Johannsen, Anders and Plank, Barbara and Alonso, H{\'e}ctor Mart{\'i}nez and Schluter, Natalie and S{\o}gaard, Anders},
  year = {2016},
  month = dec,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {4},
  pages = {301--312},
  issn = {2307-387X},
  abstract = {We propose a novel approach to cross-lingual part-of-speech tagging and dependency parsing for truly low-resource languages. Our annotation projection-based approach yields tagging and parsing models for over 100 languages. All that is needed are freely available parallel texts, and taggers and parsers for resource-rich languages. The empirical evaluation across 30 test languages shows that our method consistently provides top-level accuracies, close to established upper bounds, and outperforms several competitive baselines.},
  langid = {english}
}

@inproceedings{aksan_stcn_2019,
  title = {{{STCN}}: {{Stochastic Temporal Convolutional Networks}}},
  shorttitle = {{{STCN}}},
  booktitle = {Proceedings of the 7th {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Aksan, Emre and Hilliges, Otmar},
  year = {2019},
  month = feb,
  address = {{New Orleans, LA, USA}},
  abstract = {Convolutional architectures have recently been shown to be competitive on many sequence modelling tasks when compared to the de-facto standard of recurrent neural networks (RNNs), while providing computational and modeling advantages due to inherent parallelism. However, currently there remains a performance gap to more expressive stochastic RNN variants, especially those with several layers of dependent random variables. In this work, we propose stochastic temporal convolutional networks (STCNs), a novel architecture that combines the computational advantages of temporal convolutional networks (TCN) with the representational power and robustness of stochastic latent spaces. In particular, we propose a hierarchy of stochastic latent variables that captures temporal dependencies at different time-scales. The architecture is modular and flexible due to decoupling of deterministic and stochastic layers. We show that the proposed architecture achieves state of the art log-likelihoods across several tasks. Finally, the model is capable of predicting high-quality synthetic samples over a long-range temporal horizon in modeling of handwritten text.},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{alain_variance_2015,
  title = {Variance {{Reduction}} in {{SGD}} by {{Distributed Importance Sampling}}},
  author = {Alain, Guillaume and Lamb, Alex and Sankar, Chinnadhurai and Courville, Aaron and Bengio, Yoshua},
  year = {2015},
  month = nov,
  eprint = {1511.06481},
  eprinttype = {arxiv},
  abstract = {Humans are able to accelerate their learning by selecting training materials that are the most informative and at the appropriate level of difficulty. We propose a framework for distributing deep learning in which one set of workers search for the most informative examples in parallel while a single worker updates the model on examples selected by importance sampling. This leads the model to update using an unbiased estimate of the gradient which also has minimum variance when the sampling proposal is proportional to the L2-norm of the gradient. We show experimentally that this method reduces gradient variance even in a context where the cost of synchronization across machines cannot be ignored, and where the factors for importance sampling are not updated instantly across the training set.},
  archiveprefix = {arXiv}
}

@inproceedings{alemi_deep_2017,
  title = {Deep {{Variational Information Bottleneck}}},
  booktitle = {Proceedings of the 5th {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Alemi, Alexander A. and Fischer, Ian and Dillon, Joshua V. and Murphy, Kevin},
  year = {2017},
  address = {{Toulon, France}}
}

@misc{alemi_fixing_2018,
  title = {Fixing a {{Broken ELBO}}},
  author = {Alemi, Alexander A. and Poole, Ben and Fischer, Ian and Dillon, Joshua V. and Saurous, Rif A. and Murphy, Kevin},
  year = {2018},
  month = feb,
  eprint = {1711.00464},
  eprinttype = {arxiv},
  abstract = {Recent work in unsupervised representation learning has focused on learning deep directed latentvariable models. Fitting these models by maximizing the marginal likelihood or evidence is typically intractable, thus a common approximation is to maximize the evidence lower bound (ELBO) instead. However, maximum likelihood training (whether exact or approximate) does not necessarily result in a good latent representation, as we demonstrate both theoretically and empirically. In particular, we derive variational lower and upper bounds on the mutual information between the input and the latent variable, and use these bounds to derive a rate-distortion curve that characterizes the tradeoff between compression and reconstruction accuracy. Using this framework, we demonstrate that there is a family of models with identical ELBO, but different quantitative and qualitative characteristics. Our framework also suggests a simple new method to ensure that latent variable models with powerful stochastic decoders do not ignore their latent code.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{alemi_uncertainty_2018,
  title = {Uncertainty in the {{Variational Information Bottleneck}}},
  author = {Alemi, Alexander A. and Fischer, Ian and Dillon, Joshua V.},
  year = {2018},
  month = jul,
  eprint = {1807.00906},
  eprinttype = {arxiv},
  abstract = {We present a simple case study, demonstrating that Variational Information Bottleneck (VIB) can improve a network's classification calibration as well as its ability to detect out-ofdistribution data. Without explicitly being designed to do so, VIB gives two natural metrics for handling and quantifying uncertainty.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@phdthesis{alex_supervised_2008,
  title = {Supervised {{Sequence Labelling}} with {{Recurrent Neural Networks}}},
  author = {Alex, Graves},
  year = {2008},
  address = {{Munich}},
  abstract = {Recurrent neural networks are powerful sequence learners. They are able to incorporate context information in a flexible way, and are robust to localised distortions of the input data. These properties make them well suited to sequence labelling, where input sequences are transcribed with streams of labels. Long short-term memory is an especially promising recurrent architecture, able to bridge long time delays between relevant input and output events, and thereby access long range context. The aim of this thesis is to advance the state-of-the-art in supervised sequence labelling with recurrent networks in general, and long short-term memory in particular. Its two main contributions are (1) a new type of output layer that allows recurrent networks to be trained directly for sequence labelling tasks where the alignment between the inputs and the labels is unknown, and (2) an extension of long short-term memory to multidimensional data, such as images and video sequences. Experimental results are presented on speech recognition, online and offline handwriting recognition, keyword spotting, image segmentation and image classification, demonstrating the advantages of advanced recurrent networks over other sequential algorithms, such as hidden Markov Models.},
  langid = {english},
  school = {Technical University of Munich}
}

@inproceedings{ali_automation_2010,
  title = {Automation of {{Question Generation}} from {{Sentences}}},
  booktitle = {Proceedings of {{QG2010}}: {{The Third Workshop}} on {{Question Generation}}},
  author = {Ali, Husam and Chali, Yllias and Hasan, S.},
  year = {2010},
  pages = {58--67},
  abstract = {Question Generation (QG) and Question Answering (QA) are key challenges facing systems that interact with natural languages. The potential benefits of using automated systems to generate questions helps reduce the dependency on humans to generate questions and other needs associated with systems interacting with natural languages. In this paper we consider a system that automates generation of questions from a sentence, given a sentence, the system, will generate all possible questions which this sentence contain these questions answers. Since the given sentence may be a complex sentence, the system will generate elementary sentences, from the input complex sentences, using a syntactic parser. A part of speech tagger and a named entity recogniser are used to encode needed information. Based on the subject, verb, object and preposition the sentence will be classified, in order determine the type of questions that can possibly be generated from this sentence. We use development data provided by the Question Generation Shared Task Evaluation Challenge 2010. Keywords:},
  keywords = {Elementary Sentence,Named Entity Tagging,Natural Language Processing,POS Tagging,Question generation,Recall,Syntactic Parsing}
}

@article{alquraishi_end--end_2018,
  title = {End-to-End Differentiable Learning of Protein Structure},
  author = {AlQuraishi, Mohammed},
  year = {2018},
  journal = {bioRxiv},
  abstract = {Accurate prediction of protein structure is one of the central challenges of biochemistry. Despite significant progress made by co-evolution methods to predict protein structure from signatures of residue-residue coupling found in the evolutionary record, a direct and explicit mapping between protein sequence and structure remains elusive, with no substantial recent progress. Meanwhile, rapid developments in deep learning, which have found remarkable success in computer vision, natural language processing, and quantum chemistry raise the question of whether a deep learning based approach to protein structure could yield similar advancements. A key ingredient of the success of deep learning is the reformulation of complex, human-designed, multi-stage pipelines with differentiable models that can be jointly optimized end-to-end. We report the development of such a model, which reformulates the entire structure prediction pipeline using differentiable primitives. Achieving this required combining four technical ideas: (1) the adoption of a recurrent neural architecture to encode the internal representation of protein sequence, (2) the parameterization of (local) protein structure by torsional angles, which provides a way to reason over protein conformations without violating the covalent chemistry of protein chains, (3) the coupling of local protein structure to its global representation via recurrent geometric units, and (4) the use of a differentiable loss function to capture deviations between predicted and experimental structures. To our knowledge this is the first end-to-end differentiable model for learning of protein structure. We test the effectiveness of this approach using two challenging tasks: the prediction of novel protein folds without the use of co-evolutionary information, and the prediction of known protein folds without the use of structural templates. On the first task the model achieves state-of-the-art performance, even when compared to methods that rely on co-evolutionary data. On the second task the model is competitive with methods that use experimental protein structures as templates, achieving 3-7\AA{} accuracy despite being template-free. Beyond protein structure prediction, end-to-end differentiable models of proteins represent a new paradigm for learning and modeling protein structure, with potential applications in docking, molecular dynamics, and protein design.}
}

@article{amari_natural_1998,
  title = {Natural {{Gradient Works Efficiently}} in {{Learning}}},
  author = {Amari, Shun-Ichi},
  year = {1998},
  journal = {Neural Computation},
  volume = {10},
  number = {2},
  pages = {251--276},
  issn = {0899-7667},
  abstract = {When a parameter space has a certain underlying structure, the ordinary gradient of a function does not represent its steepest direction, but the natural gradient does. Information geometry is used for calculating the natural gradients in the parameter space of perceptrons, the space of matrices (for blind source separation), and the space of linear dynamical systems (for blind source deconvolution). The dynamical behavior of natural gradient online learning is analyzed and is proved to be Fisher efficient, implying that it has asymptotically the same performance as the optimal batch estimation of parameters. This suggests that the plateau phenomenon, which appears in the backpropagation learning algorithm of multilayer perceptrons, might disappear or might not be so serious when the natural gradient is used. An adaptive method of updating the learning rate is proposed and analyzed.}
}

@inproceedings{amdahl_validity_1967,
  title = {Validity of the Single Processor Approach to Achieving Large Scale Computing Capabilities},
  booktitle = {Proceedings of the {{Spring Joint Computer Conference}} ({{AFIPS}})},
  author = {Amdahl, Gene M.},
  year = {1967},
  pages = {483},
  abstract = {For over a decade prophets have voiced the contention that the organization of a single computer has reached its limits and that truly significant advances can be made only by interconnection of a multiplicity of computers in such a manner as to permit cooperative solution. Variously the proper direction has been pointed out as general purpose computers with a generalized interconnection of memories, or as specialized computers with geometrically related memory interconnections and controlled by one or more instruction streams.}
}

@article{amodei_deep_2016,
  title = {Deep {{Speech}} 2: {{End-to-End Speech Recognition}} in {{English}} and {{Mandarin}}},
  author = {Amodei, Dario and Ananthanarayanan, Sundaram and Anubhai, Rishita and Bai, Jingliang and Battenberg, Eric and Case, Carl and Casper, Jared and Catanzaro, Bryan and Cheng, Qiang and Chen, Guoliang and Chen, Jie and Chen, Jingdong and Chen, Zhijie and Chrzanowski, Mike and Coates, Adam and Diamos, Greg and Ding, Ke and Du, Niandong and Elsen, Erich and Engel, Jesse and Fang, Weiwei and Fan, Linxi and Fougner, Christopher and Gao, Liang and Gong, Caixia and Hannun, Awni and Han, Tony and Johannes, Lappi Vaino and Jiang, Bing and Ju, Cai and Jun, Billy and LeGresley, Patrick and Lin, Libby and Liu, Junjie and Liu, Yang and Li, Weigao and Li, Xiangang and Ma, Dongpeng and Narang, Sharan and Ng, Andrew and Ozair, Sherjil and Peng, Yiping and Prenger, Ryan and Qian, Sheng and Quan, Zongfeng and Raiman, Jonathan and Rao, Vinay and Satheesh, Sanjeev and Seetapun, David and Sengupta, Shubho and Srinet, Kavya and Sriram, Anuroop and Tang, Haiyuan and Tang, Liliang and Wang, Chong and Wang, Jidong and Wang, Kaifu and Wang, Yi and Wang, Zhijian and Wang, Zhiqian and Wu, Shuang and Wei, Likai and Xiao, Bo and Xie, Wen and Xie, Yan and Yogatama, Dani and Yuan, Bin and Zhan, Jun and Zhu, Zhenyao},
  year = {2016},
  pages = {10},
  abstract = {We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech\textendash two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, enabling experiments that previously took weeks to now run in days. This allows us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.},
  langid = {english}
}

@misc{ancona_explaining_2019,
  title = {Explaining {{Deep Neural Networks}} with a {{Polynomial Time Algorithm}} for {{Shapley Values Approximation}}},
  author = {Ancona, Marco and {\"O}ztireli, Cengiz and Gross, Markus},
  year = {2019},
  month = jun,
  eprint = {1903.10992},
  eprinttype = {arxiv},
  abstract = {The problem of explaining the behavior of deep neural networks has recently gained a lot of attention. While several attribution methods have been proposed, most come without strong theoretical foundations, which raises questions about their reliability. On the other hand, the literature on cooperative game theory suggests Shapley values as a unique way of assigning relevance scores such that certain desirable properties are satisfied. Unfortunately, the exact evaluation of Shapley values is prohibitively expensive, exponential in the number of input features. In this work, by leveraging recent results on uncertainty propagation, we propose a novel, polynomial-time approximation of Shapley values in deep neural networks. We show that our method produces significantly better approximations of Shapley values than existing state-of-the-art attribution methods.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{andersson_learning_2021,
  title = {Learning Deep Autoregressive Models for Hierarchical Data},
  author = {Andersson, Carl R. and Wahlstr{\"o}m, Niklas and Sch{\"o}n, Thomas B.},
  year = {2021},
  month = apr,
  eprint = {2104.13853},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {We propose a model for hierarchical structured data as an extension to the stochastic temporal convolutional network (STCN). The proposed model combines an autoregressive model with a hierarchical variational autoencoder and downsampling to achieve superior computational complexity. We evaluate the proposed model on two different types of sequential data: speech and handwritten text. The results are promising with the proposed model achieving state-of-the-art performance.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control}
}

@misc{andreoli_convolution_2019,
  title = {Convolution Is Outer Product},
  author = {Andreoli, Jean-Marc},
  year = {2019},
  month = may,
  eprint = {1905.01289},
  eprinttype = {arxiv},
  abstract = {The inner product operation between tensors is the corner stone of deep neural network architectures, directly inherited from linear algebra. There is a striking contrast between the unicity of this basic construct and the extreme diversity of high level constructs which have been invented to address various application domains. This paper is interested in an intermediate construct, convolution, and its corollary, attention, which have become ubiquitous in many applications, but are still presented in an ad-hoc fashion depending on the application context. We first identify the common problem addressed by most existing forms of convolution, and show how the solution to that problem naturally involves another very generic operation of linear algebra, the outer product between tensors. We then proceed to show that attention is a form of convolution, called "content based" convolution, hence amenable to the generic formulation based on the outer product. The reader looking for yet another architecture yielding better performance results on a specific task is in for some disappointment. The reader aiming at a better, more grounded understanding of familiar concepts may find food for thought.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{andrychowicz_learning_2016,
  title = {Learning to Learn by Gradient Descent by Gradient Descent},
  author = {Andrychowicz, Marcin and Denil, Misha and G{\'o}mez, Sergio and Hoffman, Matthew W and Pfau, David and Schaul, Tom and de Freitas, Nando},
  year = {2016},
  journal = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
  pages = {3981--3989}
}

@article{anonymous_cross-entropy_2018,
  title = {Cross-{{Entropy Loss Leads To Poor Margins}}},
  author = {Anonymous, Anonymous},
  year = {2018},
  abstract = {Neural networks could misclassify inputs that are slightly different from their training data, which indicates a small margin between their decision boundaries and the training dataset. In this work, we study the binary classification of linearly separable datasets and show that linear classifiers could also have decision boundaries that lie close to their training dataset if cross-entropy loss is used for training. In particular, we show that if the features of the training dataset lie in a low-dimensional affine subspace and the cross-entropy loss is minimized by using a gradient method, the margin between the training points and the decision boundary could be much smaller than the optimal value. This result is contrary to the conclusions of recent related works such as (Soudry et al., 2018), and we identify the reason for this contradiction. In order to improve the margin, we introduce differential training, which is a novel training paradigm that uses a loss function defined on pairs of points from each class. We show that the decision boundary of a linear classifier trained with differential training indeed achieves the maximum margin. The results reveal the use of cross-entropy loss as one of the hidden culprits of adversarial examples and introduces a new direction to make neural networks robust against them.},
  keywords = {Adversarial examples,Binary classification,Cross-entropy loss,Differential training,Low-rank features}
}

@inproceedings{antol_vqa_2015,
  title = {\{\vphantom\}{{VQA}}\vphantom\{\}: {{Visual}} Question Answering},
  booktitle = {{{IEEE International Conference}} on {{Computer Vision}}},
  author = {Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Lawrence Zitnick, C. and Parikh, Devi},
  year = {2015},
  pages = {2425--2433}
}

@misc{arik_deep_2017,
  title = {Deep {{Voice}}: {{Real-time Neural Text-to-Speech}}},
  author = {Arik, Sercan O. and Chrzanowski, Mike and Coates, Adam and Diamos, Gregory and Gibiansky, Andrew and Kang, Yongguo and Li, Xian and Miller, John and Ng, Andrew Y. and Raiman, Jonathan and Sengupta, Shubho and Shoeybi, Mohammad},
  year = {2017},
  month = feb,
  eprint = {1702.07825},
  eprinttype = {arxiv},
  abstract = {We present Deep Voice, a production-quality text-to-speech system constructed entirely from deep neural networks. Deep Voice lays the groundwork for truly end-to-end neural speech synthesis. The system comprises five major building blocks: a segmentation model for locating phoneme boundaries, a grapheme-to-phoneme conversion model, a phoneme duration prediction model, a fundamental frequency prediction model, and an audio synthesis model. For the segmentation model, we propose a novel way of performing phoneme boundary detection with deep neural networks using connectionist temporal classification (CTC) loss. For the audio synthesis model, we implement a variant of WaveNet that requires fewer parameters and trains faster than the original. By using a neural network for each component, our system is simpler and more flexible than traditional text-to-speech systems, where each component requires laborious feature engineering and extensive domain expertise. Finally, we show that inference with our system can be performed faster than real time and describe optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x speedups over existing implementations.},
  archiveprefix = {arXiv}
}

@article{arora_simple_2017,
  title = {A {{Simple}} but {{Tough-to-Beat Baseline}} for {{Sentence Embeddings}}},
  author = {Arora, Sanjeev and Liang, Yingyu and Ma, Tengyu},
  year = {2017},
  pages = {16},
  abstract = {The success of neural network methods for computing word embeddings has motivated methods for generating semantic embeddings of longer pieces of text, such as sentences and paragraphs. Surprisingly, Wieting et al (ICLR'16) showed that such complicated methods are outperformed, especially in out-of-domain (transfer learning) settings, by simpler methods involving mild retraining of word embeddings and basic linear regression. The method of Wieting et al. requires retraining with a substantial labeled dataset such as Paraphrase Database (Ganitkevitch et al., 2013).},
  langid = {english}
}

@article{arora_theoretical_2019,
  title = {A {{Theoretical Analysis}} of {{Contrastive Unsupervised Representation Learning}}},
  author = {Arora, Sanjeev and Khandeparkar, Hrishikesh and Khodak, Mikhail and Plevrakis, Orestis and Saunshi, Nikunj},
  year = {2019},
  month = feb,
  journal = {arXiv:1902.09229 [cs, stat]},
  eprint = {1902.09229},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Recent empirical works have successfully used unlabeled data to learn feature representations that are broadly useful in downstream classification tasks. Several of these methods are reminiscent of the well-known word2vec embedding algorithm: leveraging availability of pairs of semantically "similar" data points and "negative samples," the learner forces the inner product of representations of similar pairs with each other to be higher on average than with negative samples. The current paper uses the term contrastive learning for such algorithms and presents a theoretical framework for analyzing them by introducing latent classes and hypothesizing that semantically similar points are sampled from the same latent class. This framework allows us to show provable guarantees on the performance of the learned representations on the average classification task that is comprised of a subset of the same set of latent classes. Our generalization bound also shows that learned representations can reduce (labeled) sample complexity on downstream tasks. We conduct controlled experiments in both the text and image domains to support the theory.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{arpit_closer_2017,
  title = {A {{Closer Look}} at {{Memorization}} in {{Deep Networks}}},
  author = {Arpit, Devansh and Jastrz{\k{e}}bski, Stanis{\l}aw and Ballas, Nicolas and Krueger, David and Bengio, Emmanuel and Kanwal, Maxinder S. and Maharaj, Tegan and Fischer, Asja and Courville, Aaron and Bengio, Yoshua and {Lacoste-Julien}, Simon},
  year = {2017},
  month = jun,
  langid = {english}
}

@misc{arras_explaining_2017,
  title = {Explaining {{Recurrent Neural Network Predictions}} in {{Sentiment Analysis}}},
  author = {Arras, Leila and Montavon, Gr{\'e}goire and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  year = {2017},
  month = jun,
  eprint = {1706.07206},
  eprinttype = {arxiv},
  abstract = {Recently, a technique called Layer-wise Relevance Propagation (LRP) was shown to deliver insightful explanations in the form of input space relevances for understanding feed-forward neural network classification decisions. In the present work, we extend the usage of LRP to recurrent neural networks. We propose a specific propagation rule applicable to multiplicative connections as they arise in recurrent network architectures such as LSTMs and GRUs. We apply our technique to a word-based bi-directional LSTM model on a five-class sentiment prediction task, and evaluate the resulting LRP relevances both qualitatively and quantitatively, obtaining better results than a gradient-based related method which was used in previous work.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
}

@inproceedings{arthur_kmeans_2007,
  title = {K-Means++: {{The Advantages}} of {{Careful Seeding}}},
  booktitle = {Proceedings of the Eighteenth Annual {{ACM-SIAM}} Symposium on {{Discrete}} Algorithms},
  author = {Arthur, David and Vassilvitskii, Sergei},
  year = {2007},
  pages = {1027--1025},
  issn = {0898716241},
  abstract = {" ... Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, held January 7-9, 2007, in New Orleans, Louisiana ... jointly sponsored by the SIAM Activity Group on Discrete Mathematics and by SIGACT, the ACM Special Interest Group on Algorithms and Computation Theory."--Page xiii. Title from PDF title page (viewed Mar. 26, 2010).},
  isbn = {978-0-89871-624-5},
  pmid = {1000164511}
}

@article{arulkumaran_deep_2017,
  title = {Deep Reinforcement Learning: {{A}} Brief Survey},
  author = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
  year = {2017},
  month = aug,
  journal = {IEEE Signal Processing Magazine},
  volume = {34},
  number = {6},
  eprint = {1708.05866},
  eprinttype = {arxiv},
  pages = {26--38},
  issn = {10535888},
  abstract = {Deep reinforcement learning is poised to revolutionise the field of AI and represents a step towards building autonomous systems with a higher level understanding of the visual world. Currently, deep learning is enabling reinforcement learning to scale to problems that were previously intractable, such as learning to play video games directly from pixels. Deep reinforcement learning algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of reinforcement learning, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep reinforcement learning, including the deep \$Q\$-network, trust region policy optimisation, and asynchronous advantage actor-critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via reinforcement learning. To conclude, we describe several current areas of research within the field.},
  archiveprefix = {arXiv},
  isbn = {9781424469178},
  pmid = {25719670}
}

@inproceedings{arvanitidis_latent_2018,
  title = {Latent {{Space Oddity}}: {{On}} the {{Curvature}} of {{Deep Generative Models}}},
  booktitle = {Proceedings of the 6th {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Arvanitidis, Georgios and Hansen, Lars Kai and Hauberg, S{\o}ren},
  year = {2018},
  pages = {15},
  address = {{Vancouver, BC, Canada}},
  abstract = {Deep generative models provide a systematic way to learn nonlinear data distributions through a set of latent variables and a nonlinear ``generator'' function that maps latent points into the input space. The nonlinearity of the generator implies that the latent space gives a distorted view of the input space. Under mild conditions, we show that this distortion can be characterized by a stochastic Riemannian metric, and we demonstrate that distances and interpolants are significantly improved under this metric. This in turn improves probability distributions, sampling algorithms and clustering in the latent space. Our geometric analysis further reveals that current generators provide poor variance estimates and we propose a new generator architecture with vastly improved variance estimates. Results are demonstrated on convolutional and fully connected variational autoencoders, but the formalism easily generalizes to other deep generative models.},
  langid = {english}
}

@article{atan_deep-treat_nodate,
  title = {Deep-{{Treat}}: {{Learning Optimal Personalized Treatments}} from {{Observational Data}} Using {{Neural Networks}}},
  author = {Atan, Onur and Jordon, James},
  pages = {8},
  abstract = {We propose a novel approach for constructing effective treatment policies when the observed data is biased and lacks counterfactual information. Learning in settings where the observed data does not contain all possible outcomes for all treatments is difficult since the observed data is typically biased due to existing clinical guidelines. This is an important problem in the medical domain as collecting unbiased data is expensive and so learning from the wealth of existing biased data is a worthwhile task. Our approach separates the problem into two stages: first we reduce the bias by learning a representation map using a novel auto-encoder network \textendash{} this allows us to control the trade-off between the bias-reduction and the information loss \textendash{} and then we construct effective treatment policies on the transformed data using a novel feedforward network. Separation of the problem into these two stages creates an algorithm that can be adapted to the problem at hand \textendash{} the bias-reduction step can be performed as a preprocessing step for other algorithms. We compare our algorithm against state-of-art algorithms on two semi-synthetic datasets and demonstrate that our algorithm achieves a significant improvement in performance.},
  langid = {english}
}

@inproceedings{athiwaratkun_probabilistic_2018,
  title = {Probabilistic {{FastText}} for {{Multi-Sense Word Embeddings}}},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Athiwaratkun, Ben and Wilson, Andrew and Anandkumar, Anima},
  year = {2018},
  month = jul,
  pages = {1--11},
  publisher = {{Association for Computational Linguistics}},
  address = {{Melbourne, Australia}},
  abstract = {We introduce Probabilistic FastText, a new model for word embeddings that can capture multiple word senses, sub-word structure, and uncertainty information. In particular, we represent each word with a Gaussian mixture density, where the mean of a mixture component is given by the sum of n-grams. This representation allows the model to share the ``strength'' across sub-word structures (e.g. Latin roots), producing accurate representations of rare, misspelt, or even unseen words. Moreover, each component of the mixture can capture a different word sense. Probabilistic FastText outperforms both FastText, which has no probabilistic model, and dictionary-level probabilistic embeddings, which do not incorporate subword structures, on several word-similarity benchmarks, including English RareWord and foreign language datasets. We also achieve state-of-art performance on benchmarks that measure ability to discern different meanings. Thus, our model is the first to achieve best of both the worlds: multi-sense representations while having enriched semantics on rare words.}
}

@misc{audhkhasi_direct_2017,
  title = {Direct {{Acoustics-to-Word Models}} for {{English Conversational Speech Recognition}}},
  author = {Audhkhasi, Kartik and Ramabhadran, Bhuvana and Saon, George and Picheny, Michael and Nahamoo, David},
  year = {2017},
  month = mar,
  eprint = {1703.07754},
  eprinttype = {arxiv},
  abstract = {Recent work on end-to-end automatic speech recognition (ASR) has shown that the connectionist temporal classification (CTC) loss can be used to convert acoustics to phone or character sequences. Such systems are used with a dictionary and separately-trained Language Model (LM) to produce word sequences. However, they are not truly end-to-end in the sense of mapping acoustics directly to words without an intermediate phone representation. In this paper, we present the first results employing direct acoustics-to-word CTC models on two well-known public benchmark tasks: Switchboard and CallHome. These models do not require an LM or even a decoder at run-time and hence recognize speech with minimal complexity. However, due to the large number of word output units, CTC word models require orders of magnitude more data to train reliably compared to traditional systems. We present some techniques to mitigate this issue. Our CTC word model achieves a word error rate of 13.0\%/18.8\% on the Hub5-2000 Switchboard/CallHome test sets without any LM or decoder compared with 9.6\%/16.0\% for phone-based CTC with a 4-gram LM. We also present rescoring results on CTC word model lattices to quantify the performance benefits of a LM, and contrast the performance of word and phone CTC models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
}

@misc{aytar_playing_2018,
  title = {Playing Hard Exploration Games by Watching {{YouTube}}},
  author = {Aytar, Yusuf and Pfaff, Tobias and Budden, David and Paine, Tom Le and Wang, Ziyu and {de Freitas}, Nando},
  year = {2018},
  month = may,
  eprint = {1805.11592},
  eprinttype = {arxiv},
  abstract = {Deep reinforcement learning methods traditionally struggle with tasks where environment rewards are particularly sparse. One successful method of guiding exploration in these domains is to imitate trajectories provided by a human demonstrator. However, these demonstrations are typically collected under artificial conditions, i.e. with access to the agent's exact environment setup and the demonstrator's action and reward trajectories. Here we propose a two-stage method that overcomes these limitations by relying on noisy, unaligned footage without access to such data. First, we learn to map unaligned videos from multiple sources to a common representation using self-supervised objectives constructed over both time and modality (i.e. vision and sound). Second, we embed a single YouTube video in this representation to construct a reward function that encourages an agent to imitate human gameplay. This method of one-shot imitation allows our agent to convincingly exceed human-level performance on the infamously hard exploration games Montezuma's Revenge, Pitfall! and Private Eye for the first time, even if the agent is not presented with any environment rewards.},
  archiveprefix = {arXiv}
}

@misc{ba_layer_2016,
  title = {Layer {{Normalization}}},
  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  year = {2016},
  eprint = {1607.06450},
  eprinttype = {arxiv},
  abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
  archiveprefix = {arXiv},
  isbn = {978-3-642-04273-7}
}

@misc{baevski_data2vec_2022,
  title = {Data2vec: {{A General Framework}} for {{Self-supervised Learning}} in {{Speech}}, {{Vision}} and {{Language}}},
  author = {Baevski, Alexei and Hsu, Wei-Ning and Xu, Qiantong and Babu, Arun and Gu, Jiatao and Auli, Michael},
  year = {2022},
  month = jan,
  abstract = {While the general idea of self-supervised learning is identical across modalities, the actual algorithms and objectives differ widely because they were developed with a single modality in mind. To get us closer to general self-supervised learning, we present data2vec, a framework that uses the same learning method for either speech, NLP or computer vision. The core idea is to predict latent representations of the full input data based on a masked view of the input in a selfdistillation setup using a standard Transformer architecture. Instead of predicting modality-specific targets such as words, visual tokens or units of human speech which are local in nature, data2vec predicts contextualized latent representations that contain information from the entire input. Experiments on the major benchmarks of speech recognition, image classification, and natural language understanding demonstrate a new state of the art or competitive performance to predominant approaches. Models and code are available at www.github.com/pytorch/fairseq/ tree/master/examples/data2vec.},
  langid = {english}
}

@article{baevski_effectiveness_2020,
  title = {Effectiveness of Self-Supervised Pre-Training for Speech Recognition},
  author = {Baevski, Alexei and Auli, Michael and Mohamed, Abdelrahman},
  year = {2020},
  month = may,
  journal = {arXiv:1911.03912 [cs]},
  eprint = {1911.03912},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We compare self-supervised representation learning algorithms which either explicitly quantize the audio data or learn representations without quantization. We find the former to be more accurate since it builds a good vocabulary of the data through vq-wav2vec [1] to enable learning of effective representations in subsequent BERT training. Different to previous work, we directly fine-tune the pre-trained BERT models on transcribed speech using a Connectionist Temporal Classification (CTC) loss instead of feeding the representations into a task-specific model. We also propose a BERT-style model learning directly from the continuous audio data and compare pre-training on raw audio to spectral features. Fine-tuning a BERT model on 10 hour of labeled Librispeech data with a vq-wav2vec vocabulary is almost as good as the best known reported system trained on 100 hours of labeled data on testclean, while achieving a 25\% WER reduction on test-other. When using only 10 minutes of labeled data, WER is 25.2 on test-other and 16.3 on test-clean. This demonstrates that self-supervision can enable speech recognition systems trained on a near-zero amount of transcribed data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@article{baevski_vq-wav2vec_2020,
  title = {Vq-Wav2vec: {{Self-Supervised Learning}} of {{Discrete Speech Representations}}},
  shorttitle = {Vq-Wav2vec},
  author = {Baevski, Alexei and Schneider, Steffen and Auli, Michael},
  year = {2020},
  month = feb,
  journal = {arXiv:1910.05453 [cs]},
  eprint = {1910.05453},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We propose vq-wav2vec to learn discrete representations of audio segments through a wav2vec-style self-supervised context prediction task. The algorithm uses either a gumbel softmax or online k-means clustering to quantize the dense representations. Discretization enables the direct application of algorithms from the NLP community which require discrete inputs. Experiments show that BERT pre-training achieves a new state of the art on TIMIT phoneme classification and WSJ speech recognition.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{baevski_wav2vec_2020,
  title = {Wav2vec 2.0: {{A Framework}} for {{Self-Supervised Learning}} of {{Speech Representations}}},
  shorttitle = {Wav2vec 2.0},
  author = {Baevski, Alexei and Zhou, Henry and Mohamed, Abdelrahman and Auli, Michael},
  year = {2020},
  month = jun,
  eprint = {2006.11477},
  eprinttype = {arxiv},
  abstract = {We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. We set a new state of the art on both the 100 hour subset of Librispeech as well as on TIMIT phoneme recognition. When lowering the amount of labeled data to one hour, our model outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 5.7/10.1 WER on the noisy/clean test sets of Librispeech. This demonstrates the feasibility of speech recognition with limited amounts of labeled data. Fine-tuning on all of Librispeech achieves 1.9/3.5 WER using a simple baseline model architecture. We will release code and models.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@article{bahari_age_nodate,
  title = {Age {{Estimation}} from {{Telephone Speech}} Using I-Vectors},
  author = {Bahari, Mohamad Hasan and McLaren, Mitchell},
  pages = {4},
  abstract = {Motivated by the success of i-vectors in the field of speaker recognition, this paper proposes a new approach for age estimation from telephone speech patterns based on i-vectors. In this method, each utterance is modeled by its corresponding ivector. Then, Support Vector Regression (SVR) is applied to estimate the age of speakers. The proposed method is trained and tested on telephone conversations of the National Institute for Standard in Technology (NIST) 2010 and 2008 Speaker Recognition Evaluations databases. Evaluation results show that the proposed method outperforms different conventional methods in speaker age estimation.},
  langid = {english},
  keywords = {⛔ No DOI found}
}

@misc{bahdanau_actor-critic_2016,
  title = {An {{Actor-Critic Algorithm}} for {{Sequence Prediciton}}},
  author = {Bahdanau, Dzmitry and Brakel, Philemon and Xu, Kelvin and Goyal, Anirudh and Lowe, Ryan and Pineau, Joelle and Courville, Aaron and Bengio, Yoshua},
  year = {2016},
  month = jul,
  eprint = {1607.07086},
  eprinttype = {arxiv},
  abstract = {We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a critic network that is trained to predict the value of an output token, given the policy of an actor network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling.},
  archiveprefix = {arXiv}
}

@article{bahdanau_end--end_2015,
  title = {End-to-{{End Attention-based Large Vocabulary Speech Recognition}}},
  author = {Bahdanau, Dzmitry and Chorowski, Jan and Serdyuk, Dmitriy and Brakel, Philemon and Bengio, Yoshua},
  year = {2015},
  month = aug,
  journal = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  eprint = {1508.04395},
  eprinttype = {arxiv},
  abstract = {Many of the current state-of-the-art Large Vocabulary Continuous Speech Recognition Systems (LVCSR) are hybrids of neural networks and Hidden Markov Models (HMMs). Most of these systems contain separate components that deal with the acoustic modelling, language modelling and sequence decoding. We investigate a more direct approach in which the HMM is replaced with a Recurrent Neural Network (RNN) that performs sequence prediction directly at the character level. Alignment between the input features and the desired character sequence is learned automatically by an attention mechanism built into the RNN. For each predicted character, the attention mechanism scans the input sequence and chooses relevant frames. We propose two methods to speed up this operation: limiting the scan to a subset of most promising frames and pooling over time the information contained in neighboring frames, thereby reducing source sequence length. Integrating an n-gram language model into the decoding process yields recognition accuracies similar to other HMM-free RNN-based approaches.},
  archiveprefix = {arXiv}
}

@inproceedings{bahdanau_neural_2014,
  title = {Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  year = {2014},
  month = sep,
  eprint = {1409.0473},
  eprinttype = {arxiv},
  issn = {0147-006X},
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  archiveprefix = {arXiv},
  isbn = {0147-006X (Print)},
  pmid = {14527267},
  keywords = {★}
}

@inproceedings{bai_contrastively_2021,
  title = {Contrastively {{Disentangled Sequential Variational Autoencoder}}},
  booktitle = {Proceedings of the 35th {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Bai, Junwen and Wang, Weiran and Gomes, Carla},
  year = {2021},
  month = dec,
  pages = {14},
  abstract = {Self-supervised disentangled representation learning is a critical task in sequence modeling. The learnt representations contribute to better model interpretability as well as the data generation, and improve the sample efficiency for downstream tasks. We propose a novel sequence representation learning method, named Contrastively Disentangled Sequential Variational Autoencoder (C-DSVAE), to extract and separate the static (time-invariant) and dynamic (time-variant) factors in the latent space. Different from previous sequential variational autoencoder methods, we use a novel evidence lower bound which maximizes the mutual information between the input and the latent factors, while penalizes the mutual information between the static and dynamic factors. We leverage contrastive estimations of the mutual information terms in training, together with simple yet effective augmentation techniques, to introduce additional inductive biases. Our experiments show that C-DSVAE significantly outperforms the previous state-of-the-art methods on multiple metrics.},
  langid = {english}
}

@article{bai_empirical_nodate,
  title = {An {{Empirical Evaluation}} of {{Generic Convolutional}} and {{Recurrent Networks}} for {{Sequence Modeling}}},
  author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
  abstract = {For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional ar-chitectures can outperform recurrent networks on tasks such as audio synthesis and machine trans-lation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convo-lutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our re-sults indicate that a simple convolutional archi-tecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common associ-ation between sequence modeling and recurrent networks should be reconsidered, and convolu-tional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at http://github.com/locuslab/TCN.}
}

@inproceedings{bai_supervised_2009,
  title = {Supervised Semantic Indexing},
  booktitle = {Proceedings of the {{Conference}} on {{Information}} and {{Knowledge Management}} ({{CIKM}})},
  author = {Bai, Bing and Weston, Jason and Collobert, Ronan and Grangier, David},
  year = {2009},
  volume = {5478 LNCS},
  pages = {761--765},
  issn = {03029743},
  abstract = {In this article we propose Supervised Semantic Indexing (SSI), an algorithm that is trained on (query, document) pairs of text documents to predict the quality of their match. Like Latent Semantic Indexing (LSI), our models take ac- count of correlations between words (synonymy, polysemy). However, unlike LSI our models are trained with a super- vised signal directly on the ranking task of interest, which we argue is the reason for our superior results. As the query and target texts are modeled separately, our approach is easily generalized to different retrieval tasks, such as online advertising placement. Dealing with models on all pairs of words features is computationally challenging. We propose several improvements to our basic model for addressing this issue, including low rank (but diagonal preserving) represen- tations, and correlated feature hashing (CFH). We provide an empirical study of all these methods on retrieval tasks based onWikipedia documents as well as an Internet adver- tisement task. We obtain state-of-the-art performance while providing realistically scalable methods.},
  isbn = {3-642-00957-3},
  keywords = {Document ranking,Semantic indexing,Supervised learning}
}

@misc{bapna_slam_2021,
  title = {{{SLAM}}: {{A Unified Encoder}} for {{Speech}} and {{Language Modeling}} via {{Speech-Text Joint Pre-Training}}},
  shorttitle = {{{SLAM}}},
  author = {Bapna, Ankur and Chung, Yu-an and Wu, Nan and Gulati, Anmol and Jia, Ye and Clark, Jonathan H. and Johnson, Melvin and Riesa, Jason and Conneau, Alexis and Zhang, Yu},
  year = {2021},
  month = oct,
  eprint = {2110.10329},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Unsupervised pre-training is now the predominant approach for both text and speech understanding. Self-attention models pre-trained on large amounts of unannotated data have been hugely successful when fine-tuned on downstream tasks from a variety of domains and languages. This paper takes the universality of unsupervised language pre-training one step further, by unifying speech and text pre-training within a single model. We build a single encoder with the BERT objective on unlabeled text together with the w2v-BERT objective on unlabeled speech. To further align our model representations across modalities, we leverage alignment losses, specifically Translation Language Modeling (TLM) and Speech Text Matching (STM) that make use of supervised speech-text recognition data. We demonstrate that incorporating both speech and text data during pre-training can significantly improve downstream quality on CoVoST 2 speech translation, by around 1 BLEU compared to single-modality pre-trained models, while retaining close to SotA performance on LibriSpeech and SpeechStew ASR tasks. On four GLUE tasks and text-normalization, we observe evidence of capacity limitations and interference between the two modalities, leading to degraded performance compared to an equivalent text-only model, while still being competitive with BERT. Through extensive empirical analysis we also demonstrate the importance of the choice of objective function for speech pre-training, and the beneficial effect of adding additional supervised signals on the quality of the learned representations.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@inproceedings{barber_ensemble_1998,
  title = {Ensemble Learning in {{Bayesian}} Neural Networks},
  booktitle = {Generalization in {{Neural Networks}} and {{Machine Learning}}},
  author = {Barber, David and Bishop, Christopher M.},
  year = {1998},
  pages = {215--237},
  publisher = {{Springer}},
  abstract = {Bayesian treatments of learning in neural networks are typically based either on a local Gaussian approximation to a mode of the posterior weight distribution, or on Markov chain Monte Carlo simulations. A third approach, called `ensemble learning', was introduced by Hinton (1993). It aims to approximate the posterior distribution by minimizing the Kullback-Leibler divergence between the true posterior and a parametric approximating distribution. The original derivation of a deterministic algorithm relied on the use of a Gaussian approximating distribution with a diagonal covariance matrix and hence was unable to capture the posterior correlations between parameters. In this chapter we show how the ensemble learning approach can be extended to full-covariance Gaussian distributions while remaining computationally tractable. We also extend the framework to deal with hyperparameters, leading to a simple re-estimation procedure. One of the benefits of our approach is that it yields a strict lower bound on the marginal likelihood, in contrast to other approximate procedures.}
}

@misc{barber_evolutionary_2017,
  title = {Evolutionary {{Optimization}} as a {{Variational Method}}},
  author = {Barber, David},
  year = {2017},
  abstract = {A simple connection between evolutionary optimisation and variational methods},
  howpublished = {https://davidbarber.github.io/blog/2017/04/03/variational-optimisation/},
  keywords = {Deep learning,Evolutionary computation,Optimization,Reinforcement learning,Variational optimization}
}

@book{bartholomew_latent_2011,
  title = {Latent Variable Models and Factor Analysis: A Unified Approach},
  shorttitle = {Latent Variable Models and Factor Analysis},
  author = {Bartholomew, David J. and Knott, M. and Moustaki, Irini},
  year = {2011},
  series = {Wiley Series in Probability and Statistics},
  edition = {3rd ed},
  publisher = {{Wiley}},
  address = {{Chichester, West Sussex}},
  isbn = {978-0-470-97192-5 978-1-119-97059-0 978-1-119-97058-3 978-1-119-97370-6 978-1-119-97371-3},
  lccn = {QA278.6 .B37 2011},
  keywords = {Factor analysis,Latent structure analysis,Latent variables},
  annotation = {OCLC: ocn710044915}
}

@article{battaglia_relational_2018,
  title = {Relational Inductive Biases, Deep Learning, and Graph Networks},
  author = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and {Sanchez-Gonzalez}, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
  year = {2018},
  month = jun,
  langid = {english}
}

@article{batygin_planet_2019,
  title = {The {{Planet Nine Hypothesis}}},
  author = {Batygin, Konstantin and Adams, Fred C. and Brown, Michael E. and Becker, Juliette C.},
  year = {2019},
  month = may,
  journal = {Physics Reports},
  volume = {805},
  eprint = {1902.10103},
  eprinttype = {arxiv},
  pages = {1--53},
  issn = {03701573},
  abstract = {Over the course of the past two decades, observational surveys have unveiled the intricate orbital structure of the Kuiper Belt, a field of icy bodies orbiting the Sun beyond Neptune. In addition to a host of readily-predictable orbital behavior, the emerging census of trans-Neptunian objects displays dynamical phenomena that cannot be accounted for by interactions with the known eight-planet solar system alone. Specifically, explanations for the observed physical clustering of orbits with semi-major axes in excess of {$\sim$} 250 AU, the detachment of perihelia of select Kuiper belt objects from Neptune, as well as the dynamical origin of highly inclined/retrograde long-period orbits remain elusive within the context of the classical view of the solar system. This newly outlined dynamical architecture of the distant solar system points to the existence of a new planet with mass of m9 {$\sim$} 5 - 10 M{$\oplus$}, residing on a moderately inclined orbit (i9 {$\sim$} 15 - 25 deg) with semi-major axis a9 {$\sim$} 400 - 800 AU and eccentricity between e9 {$\sim$} 0.2 - 0.5. This paper reviews the observational motivation, dynamical constraints, and prospects for detection of this proposed object known as Planet Nine.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Astronomy,Astrophysics - Earth and Planetary Astrophysics}
}

@inproceedings{bauer_generalized_2021,
  title = {Generalized {{Doubly-Reparameterized Gradient Estimators}}},
  booktitle = {3rd {{Symposium}} on {{Advances}} in {{Approximate Bayesian Inference}}},
  author = {Bauer, Matthias and Mnih, Andriy},
  year = {2021},
  pages = {21}
}

@article{bauer_regularization_2007,
  title = {On Regularization Algorithms in Learning Theory},
  author = {Bauer, Frank and Pereverzev, Sergei and Rosasco, Lorenzo},
  year = {2007},
  journal = {Journal of Complexity},
  volume = {23},
  number = {1},
  pages = {52--72},
  issn = {0885064X},
  abstract = {In this paper we discuss a relation between Learning Theory and Regularization of linear ill-posed inverse problems. It is well known that Tikhonov regularization can be profitably used in the context of supervised learning, where it usually goes under the name of regularized least-squares algorithm. Moreover, the gradient descent algorithm was studied recently, which is an analog of Landweber regularization scheme. In this paper we show that a notion of regularization defined according to what is usually done for ill-posed inverse problems allows to derive learning algorithms which are consistent and provide a fast convergence rate. It turns out that for priors expressed in term of variable Hilbert scales in reproducing kernel Hilbert spaces our results for Tikhonov regularization match those in Smale and Zhou [Learning theory estimates via integral operators and their approximations, submitted for publication, retrievable at {$\langle$}http://www.tti-c.org/smale.html{$\rangle$}, 2005] and improve the results for Landweber iterations obtained in Yao et al. [On early stopping in gradient descent learning, Constructive Approximation (2005), submitted for publication]. The remarkable fact is that our analysis shows that the same properties are shared by a large class of learning algorithms which are essentially all the linear regularization schemes. The concept of operator monotone functions turns out to be an important tool for the analysis. \textcopyright{} 2006 Elsevier Inc. All rights reserved.},
  isbn = {0885-064X},
  keywords = {Learning theory,Non-parametric statistics,Regularization theory}
}

@misc{bayer_learning_2015,
  title = {Learning {{Stochastic Recurrent Networks}}},
  author = {Bayer, Justin and Osendorfer, Christian},
  year = {2015},
  month = mar,
  eprint = {1411.7610},
  eprinttype = {arxiv},
  primaryclass = {stat.ML},
  abstract = {Leveraging advances in variational inference, we propose to enhance recurrent neural networks with latent variables, resulting in Stochastic Recurrent Networks (STORNs). The model i) can be trained with stochastic gradient methods, ii) allows structured and multi-modal conditionals at each time step, iii) features a reliable estimator of the marginal likelihood and iv) is a generalisation of deterministic recurrent neural networks. We evaluate the method on four polyphonic musical data sets and motion capture data.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{bayer_mind_2021,
  title = {Mind the {{Gap}} When {{Conditioning Amortised Inference}} in {{Sequential Latent-Variable Models}}},
  booktitle = {Proceedings of the 9th {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Bayer, Justin and Soelch, Maximilian and Mirchev, Atanas and Kayalibay, Baris and {van der Smagt}, Patrick},
  year = {2021},
  month = may,
  address = {{Virtual}},
  abstract = {Amortised inference enables scalable learning of sequential latent-variable models (LVMs) with the evidence lower bound (ELBO). In this setting, variational posteriors are often only partially conditioned. While the true posteriors depend, e. g., on the entire sequence of observations, approximate posteriors are only informed by past observations. This mimics the Bayesian filter\textemdash a mixture of smoothing posteriors. Yet, we show that the ELBO objective forces partially-conditioned amortised posteriors to approximate products of smoothing posteriors instead. Consequently, the learned generative model is compromised. We demonstrate these theoretical findings in three scenarios: traffic flow, handwritten digits, and aerial vehicle dynamics. Using fully-conditioned approximate posteriors, performance improves in terms of generative modelling and multi-step prediction.},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{beckmann_speech-vgg_2020,
  title = {Speech-{{VGG}}: {{A}} Deep Feature Extractor for Speech Processing},
  shorttitle = {Speech-{{VGG}}},
  author = {Beckmann, Pierre and Kegler, Mikolaj and Saltini, Hugues and Cernak, Milos},
  year = {2020},
  month = may,
  journal = {arXiv:1910.09909 [cs, eess]},
  eprint = {1910.09909},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {Recent breakthroughs in deep learning often rely on representation learning and knowledge transfer. In particular, readily available models pre-trained on large datasets are key for the efficient transfer of knowledge. They can be applied as feature extractors for data preprocessing, fine-tuned to perform a variety of tasks, or used for computing feature losses in the training of deep learning systems. While applications of transfer learning are common in the fields of computer vision and natural language processing, audio- and speech processing are surprisingly lacking readily available and transferable models. Here, we introduce speechVGG, a flexible, transferable feature extractor tailored for integration with deep learning frameworks for speech processing. Our transferable model adopts the classic VGG-16 architecture and is trained on a spoken word classification task. We demonstrate the application of the pre-trained model in four speech processing tasks, including speech enhancement, language identification, speech, noise and music classification, and speaker identification. Each time, we compare the performance of our approach to existing baselines. Our results confirm that the representation of natural speech captured using speechVGG is transferable and generalizable across various speech processing problems and datasets. Notably, relatively simple applications of our pre-trained model are capable of achieving competitive results.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@inproceedings{belabbas_fast_2007,
  title = {Fast Low-Rank Approximation for Covariance Matrices},
  booktitle = {{{IEEE International Workshop}} on {{Computational Advances}} in {{Multi-Sensor Adaptive Processing}} ({{CAMSAP}})},
  author = {Belabbas, Mohamed Ali and Wolfe, Patrick J.},
  year = {2007},
  month = dec,
  pages = {293--296},
  publisher = {{IEEE}},
  abstract = {Computing an efficient low-rank approximation of a given positive definite matrix is a ubiquitous task in statistical signal processing and numerical linear algebra. The optimal solution is well known and is given by the singular value decomposition; however, its complexity scales as the cube of the matrix dimension. Here we introduce a low-complexity alternative which approximates this optimal low-rank solution, together with a bound on its worst-case error. Our methodology also reveals a connection between the approximation of matrix products and Schur complements. We present simulation results that verify performance improvements relative to contemporary randomized algorithms for low-rank approximation.},
  isbn = {978-1-4244-1714-8}
}

@inproceedings{belghazi_mine_2018,
  title = {{{MINE}}: {{Mutual Information Neural Estimation}}},
  shorttitle = {{{MINE}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Belghazi, Mohamed Ishmael and Baratin, Aristide and Rajeswar, Sai and Ozair, Sherjil and Bengio, Yoshua and Courville, Aaron and Hjelm, R. Devon},
  year = {2018},
  address = {{Stockholm, Sweden}},
  abstract = {We argue that the estimation of mutual information between high dimensional continuous random variables can be achieved by gradient descent over neural networks. We present a Mutual Information Neural Estimator (MINE) that is linearly scalable in dimensionality as well as in sample size, trainable through back-prop, and strongly consistent. We present a handful of applications on which MINE can be used to minimize or maximize mutual information. We apply MINE to improve adversarially trained generative models. We also use MINE to implement Information Bottleneck, applying it to supervised classification; our results demonstrate substantial improvement in flexibility and performance in these settings.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{bender_climbing_nodate,
  title = {Climbing towards {{NLU}}: {{On Meaning}}, {{Form}}, and {{Understanding}} in the {{Age}} of {{Data}}},
  author = {Bender, Emily M and Koller, Alexander},
  pages = {14},
  abstract = {The success of the large neural language models on many NLP tasks is exciting. However, we find that these successes sometimes lead to hype in which these models are being described as ``understanding'' language or capturing ``meaning''. In this position paper, we argue that a system trained only on form has a priori no way to learn meaning. In keeping with the ACL 2020 theme of ``Taking Stock of Where We've Been and Where We're Going'', we argue that a clear understanding of the distinction between form and meaning will help guide the field towards better science around natural language understanding.},
  langid = {english},
  keywords = {⛔ No DOI found}
}

@inproceedings{bengio_advances_2013,
  title = {Advances in {{Optimizing Recurrent Networks}}},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Bengio, Yoshua and {Boulanger-Lewandowski}, Nicolas and Pascanu, Razvan},
  year = {2013},
  eprint = {1212.0901},
  eprinttype = {arxiv},
  pages = {8624--8628},
  issn = {15206149},
  abstract = {After a more than decade-long period of relatively little research activity in the area of recurrent neural networks, several new developments will be reviewed here that have allowed substantial progress both in understanding and in technical solutions towards more efficient training of recurrent networks. These advances have been motivated by and related to the optimization issues surrounding deep learning. Although recurrent networks are extremely powerful in what they can in principle represent in terms of modelling sequences,their training is plagued by two aspects of the same issue regarding the learning of long-term dependencies. Experiments reported here evaluate the use of clipping gradients, spanning longer time ranges with leaky integration, advanced momentum techniques, using more powerful output probability models, and encouraging sparser gradients to help symmetry breaking and credit assignment. The experiments are performed on text and music data and show off the combined effects of these techniques in generally improving both training and test error.},
  archiveprefix = {arXiv},
  isbn = {978-1-4799-0356-6},
  pmid = {25246403},
  keywords = {Deep learning,Long-term dependencies,Recurrent neural network,Representation learning}
}

@misc{bengio_deep_2013,
  title = {Deep {{Generative Stochastic Networks Trainable}} by {{Backprop}}},
  author = {Bengio, Yoshua and {Thibodeau-Laufer}, {\'E}ric and Alain, Guillaume and Yosinski, Jason},
  year = {2013},
  eprint = {1306.1091},
  eprinttype = {arxiv},
  abstract = {We introduce a novel training principle for probabilistic models that is an alternative to maximum likelihood. The proposed Generative Stochastic Networks (GSN) framework is based on learning the transition operator of a Markov chain whose stationary distribution estimates the data distribution. The transition distribution of the Markov chain is conditional on the previous state, generally involving a small move, so this conditional distribution has fewer dominant modes, being unimodal in the limit of small moves. Thus, it is easier to learn because it is easier to approximate its partition function, more like learning to perform supervised function approximation, with gradients that can be obtained by backprop. We provide theorems that generalize recent work on the probabilistic interpretation of denoising autoencoders and obtain along the way an interesting justification for dependency networks and generalized pseudolikelihood, along with a definition of an appropriate joint distribution and sampling mechanism even when the conditionals are not consistent. GSNs can be used with missing inputs and can be used to sample subsets of variables given the rest. We validate these theoretical results with experiments on two image datasets using an architecture that mimics the Deep Boltzmann Machine Gibbs sampler but allows training to proceed with simple backprop, without the need for layerwise pretraining.},
  archiveprefix = {arXiv},
  isbn = {9781634393973}
}

@misc{bengio_estimating_2013,
  title = {Estimating or {{Propagating Gradients Through Stochastic Neurons}} for {{Conditional Computation}}},
  author = {Bengio, Yoshua and L{\'e}onard, Nicholas and Courville, Aaron},
  year = {2013},
  month = aug,
  eprint = {1308.3432},
  eprinttype = {arxiv},
  abstract = {Stochastic neurons and hard non-linearities can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic or non-smooth neurons? I.e., can we ``back-propagate'' through these stochastic neurons? We examine this question, existing approaches, and compare four families of solutions, applicable in different settings. One of them is the minimum variance unbiased gradient estimator for stochatic binary neurons (a special case of the REINFORCE algorithm). A second approach, introduced here, decomposes the operation of a binary stochastic neuron into a stochastic binary part and a smooth differentiable part, which approximates the expected effect of the pure stochatic binary neuron to first order. A third approach involves the injection of additive or multiplicative noise in a computational graph that is otherwise differentiable. A fourth approach heuristically copies the gradient with respect to the stochastic output directly as an estimator of the gradient with respect to the sigmoid argument (we call this the straight-through estimator). To explore a context where these estimators are useful, we consider a small-scale version of conditional computation, where sparse stochastic units form a distributed representation of gaters that can turn off in combinatorially many ways large chunks of the computation performed in the rest of the neural network. In this case, it is important that the gating units produce an actual 0 most of the time. The resulting sparsity can be potentially be exploited to greatly reduce the computational cost of large deep networks for which conditional computation would be useful.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning}
}

@article{bengio_learning_1994,
  title = {Learning {{Long-Term Dependencies}} with {{Gradient Descent}} Is {{Difficult}}},
  author = {Bengio, Yoshua and Simard, Patrice Y. and Frasconi, Paolo},
  year = {1994},
  journal = {IEEE Transactions on Neural Networks},
  volume = {5},
  number = {2},
  eprint = {1211.5063v2},
  eprinttype = {arxiv},
  pages = {157--166},
  issn = {19410093},
  abstract = {Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered.},
  archiveprefix = {arXiv},
  isbn = {1045-9227 VO - 5},
  pmid = {18267787}
}

@article{bengio_neural_2003,
  title = {A {{Neural Probabilistic Language Model}}},
  author = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Janvin, Christian},
  year = {2003},
  journal = {Journal of Machine Learning Research},
  volume = {3},
  eprint = {1301.3781v3},
  eprinttype = {arxiv},
  pages = {1137--1155},
  issn = {15324435},
  abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
  archiveprefix = {arXiv},
  isbn = {1532-4435},
  pmid = {18244602},
  keywords = {Artificial neural networks,Curse of dimensionality,Ddistributed representation,Statistical language modeling}
}

@article{bengio_practical_2012,
  title = {Practical Recommendations for Gradient-Based Training of Deep Architectures},
  author = {Bengio, Yoshua},
  year = {2012},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  volume = {7700 LECTU},
  eprint = {1206.5533},
  eprinttype = {arxiv},
  pages = {437--478},
  issn = {03029743},
  abstract = {Learning algorithms related to artificial neural networks and in particular for Deep Learning may seem to involve many bells and whistles, called hyper-parameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyper-parameters, in particular in the context of learning algorithms based on back-propagated gradient and gradient-based optimization. It also discusses how to deal with the fact that more interesting results can be obtained when allowing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observed with deeper architectures.},
  archiveprefix = {arXiv},
  isbn = {9783642352881},
  pmid = {25497547}
}

@article{bengio_representation_2013,
  title = {Representation Learning: {{A}} Review and New Perspectives},
  author = {Bengio, Yoshua and Courville, Aaron C. and Vincent, Pascal},
  year = {2013},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {35},
  number = {8},
  pages = {1798--1828},
  abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.}
}

@article{bengio_scheduled_2015,
  title = {Scheduled {{Sampling}} for {{Sequence Prediction}} with {{Recurrent Neural Networks}}},
  author = {Bengio, Samy and Vinyals, Oriol and Jaitly, Navdeep and Shazeer, Noam},
  year = {2015},
  month = jun,
  langid = {english},
  keywords = {Scheduled sampling}
}

@inproceedings{bengio_word_2014,
  title = {Word {{Embeddings}} for {{Speech Recognition}}},
  booktitle = {Proceedings of the 15th {{Conference}} of the {{International Speech Communication Association}}, {{Interspeech}}},
  author = {Bengio, Samy and Heigold, Georg},
  year = {2014}
}

@incollection{bengtsson_curse--dimensionality_2008,
  title = {Curse-of-Dimensionality Revisited: {{Collapse}} of the Particle Filter in Very Large Scale Systems},
  booktitle = {Probability and {{Statistics}}: {{Essays}} in {{Honor}} of {{David A}}. {{Freedman}}},
  author = {Bengtsson, Thomas and Bickel, Peter and Li, Bo},
  year = {2008},
  eprint = {0805.3034},
  eprinttype = {arxiv},
  pages = {316--334},
  abstract = {It has been widely realized that Monte Carlo methods (approximation via a sample ensemble) may fail in large scale systems. This work offers some theoretical insight into this phenomenon in the context of the particle filter. We demonstrate that the maximum of the weights associated with the sample ensemble converges to one as both the sample size and the system dimension tends to infinity. Specifically, under fairly weak assumptions, if the ensemble size grows sub-exponentially in the cube root of the system dimension, the convergence holds for a single update step in state-space models with independent and identically distributed kernels. Further, in an important special case, more refined arguments show (and our simulations suggest) that the convergence to unity occurs unless the ensemble grows super-exponentially in the system dimension. The weight singularity is also established in models with more general multivariate likelihoods, e.g. Gaussian and Cauchy. Although presented in the context of atmospheric data assimilation for numerical weather prediction, our results are generally valid for high-dimensional particle filters.},
  archiveprefix = {arXiv},
  isbn = {0-940600-74-9}
}

@misc{berg_idf_2020,
  title = {{{IDF}}++: {{Analyzing}} and {{Improving Integer Discrete Flows}} for {{Lossless Compression}}},
  shorttitle = {{{IDF}}++},
  author = {van den Berg, Rianne and Gritsenko, Alexey A. and Dehghani, Mostafa and S{\o}nderby, Casper Kaae and Salimans, Tim},
  year = {2020},
  month = jun,
  eprint = {2006.12459},
  eprinttype = {arxiv},
  abstract = {In this paper we analyse and improve integer discrete flows for lossless compression. Integer discrete flows are a recently proposed class of models that learn invertible transformations for integer-valued random variables. Due to its discrete nature, they can be combined in a straightforward manner with entropy coding schemes for lossless compression without the need for bits-back coding. We discuss the potential difference in flexibility between invertible flows for discrete random variables and flows for continuous random variables and show that (integer) discrete flows are more flexible than previously claimed. We furthermore investigate the influence of quantization operators on optimization and gradient bias in integer discrete flows. Finally, we introduce modifications to the architecture to improve the performance of this model class for lossless compression.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@incollection{bergstra_algorithms_2011,
  title = {Algorithms for {{Hyper-Parameter Optimization}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 24},
  author = {Bergstra, James S. and Bardenet, R{\'e}mi and Bengio, Yoshua and K{\'e}gl, Bal{\'a}zs},
  editor = {{Shawe-Taylor}, J. and Zemel, R. S. and Bartlett, P. L. and Pereira, F. and Weinberger, K. Q.},
  year = {2011},
  pages = {2546--2554},
  publisher = {{Curran Associates, Inc.}}
}

@article{bergstra_making_nodate,
  title = {Making a {{Science}} of {{Model Search}}: {{Hyperparameter Optimization}} in {{Hundreds}} of {{Dimensions}} for {{Vision Architectures}}},
  author = {Bergstra, J and Yamins, D and Cox, D D},
  pages = {9},
  abstract = {Many computer vision algorithms depend on configuration settings that are typically hand-tuned in the course of evaluating the algorithm for a particular data set. While such parameter tuning is often presented as being incidental to the algorithm, correctly setting these parameter choices is frequently critical to realizing a method's full potential. Compounding matters, these parameters often must be re-tuned when the algorithm is applied to a new problem domain, and the tuning process itself often depends on personal experience and intuition in ways that are hard to quantify or describe. Since the performance of a given technique depends on both the fundamental quality of the algorithm and the details of its tuning, it is sometimes difficult to know whether a given technique is genuinely better, or simply better tuned.},
  langid = {english},
  keywords = {⛔ No DOI found}
}

@article{bergstra_random_2012,
  title = {Random {{Search}} for {{Hyper-Parameter Optimization}}},
  author = {Bergstra, James and Bengio, Yoshua},
  year = {2012},
  journal = {Journal of Machine Learning Research},
  volume = {13},
  pages = {281--305},
  abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimiza-tion. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a compar-ison with a large previous study that used grid search and manual search to configure neural net-works and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising con-figuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent " High Throughput " methods achieve surprising success\textemdash they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural base-line against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
  keywords = {Deep learning,Global optimization,Model selection,Neural network,Response surface modeling}
}

@misc{berner_modern_2021,
  title = {The {{Modern Mathematics}} of {{Deep Learning}}},
  author = {Berner, Julius and Grohs, Philipp and Kutyniok, Gitta and Petersen, Philipp},
  year = {2021},
  month = may,
  eprint = {2105.04026},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We describe the new field of mathematical analysis of deep learning. This field emerged around a list of research questions that were not answered within the classical framework of learning theory. These questions concern: the outstanding generalization power of overparametrized neural networks, the role of depth in deep architectures, the apparent absence of the curse of dimensionality, the surprisingly successful optimization performance despite the non-convexity of the problem, understanding what features are learned, why deep architectures perform exceptionally well in physical problems, and which fine aspects of an architecture affect the behavior of a learning task in which way. We present an overview of modern approaches that yield partial answers to these questions. For selected approaches, we describe the main ideas in more detail.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{bernstein_learning_2020,
  title = {Learning Compositional Functions via Multiplicative Weight Updates},
  author = {Bernstein, Jeremy and Zhao, Jiawei and Meister, Markus and Liu, Ming-Yu and Anandkumar, Anima and Yue, Yisong},
  year = {2020},
  month = jun,
  eprint = {2006.14560},
  eprinttype = {arxiv},
  abstract = {Compositionality is a basic structural feature of both biological and artificial neural networks. Learning compositional functions via gradient descent incurs well known problems like vanishing and exploding gradients, making careful learning rate tuning essential for real-world applications. This paper proves that multiplicative weight updates satisfy a descent lemma tailored to compositional functions. Based on this lemma, we derive Madam\textemdash a multiplicative version of the Adam optimiser\textemdash and show that it can train state of the art neural network architectures without learning rate tuning. We further show that Madam is easily adapted to train natively compressed neural networks by representing their weights in a logarithmic number system. We conclude by drawing connections between multiplicative weight updates and recent findings about synapses in biology.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Numerical Analysis,Statistics - Machine Learning}
}

@misc{betancourt_conceptual_2018,
  title = {A {{Conceptual Introduction}} to {{Hamiltonian Monte Carlo}}},
  author = {Betancourt, Michael},
  year = {2018},
  month = jul,
  eprint = {1701.02434},
  eprinttype = {arxiv},
  abstract = {Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous understanding of why it performs so well on difficult problems and how it is best applied in practice. Unfortunately, that understanding is confined within the mathematics of differential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Methodology}
}

@misc{bhalodia_dpvaes:_2019,
  title = {{{dpVAEs}}: {{Fixing Sample Generation}} for {{Regularized VAEs}}},
  shorttitle = {{{dpVAEs}}},
  author = {Bhalodia, Riddhish and Lee, Iain and Elhabian, Shireen},
  year = {2019},
  month = nov,
  eprint = {1911.10506},
  eprinttype = {arxiv},
  abstract = {Unsupervised representation learning via generative modeling is a staple to many computer vision applications in the absence of labeled data. Variational Autoencoders (VAEs) are powerful generative models that learn representations useful for data generation. However, due to inherent challenges in the training objective, VAEs fail to learn useful representations amenable for downstream tasks. Regularization-based methods that attempt to improve the representation learning aspect of VAEs come at a price: poor sample generation. In this paper, we explore this representation-generation trade-off for regularized VAEs and introduce a new family of priors, namely decoupled priors, or dpVAEs, that decouple the representation space from the generation space. This decoupling enables the use of VAE regularizers on the representation space without impacting the distribution used for sample generation, and thereby reaping the representation learning benefits of the regularizations without sacrificing the sample generation. dpVAE leverages invertible networks to learn a bijective mapping from an arbitrarily complex representation distribution to a simple, tractable, generative distribution. Decoupled priors can be adapted to the state-of-the-art VAE regularizers without additional hyperparameter tuning. We showcase the use of dpVAEs with different regularizers. Experiments on MNIST, SVHN, and CelebA demonstrate, quantitatively and qualitatively, that dpVAE fixes sample generation for regularized VAEs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@techreport{bishop_mixture_1994,
  title = {Mixture {{Density Networks}}},
  author = {Bishop, Christopher M.},
  year = {1994},
  month = feb,
  number = {NCRG/94/4288},
  address = {{Aston University}},
  institution = {{Neural Computing Research Group - Dept. of Computer Science and Applied Mathematics}},
  abstract = {Minimization of a sum-of-squares or cross-entropy error function leads to network outputs which approximate the conditional averages of the target data, conditioned on the input vector. For classi cations problems, with a suitably chosen target coding scheme, these averages represent the posterior probabilities of class membership, and so can be regarded as optimal. For problems involving the prediction of continuous variables, however, the conditional averages provide only a very limited description of the properties of the target variables. This is particularly true for problems in which the mapping to be learned is multi-valued, as often arises in the solution of inverse problems, since the average of several correct target values is not necessarily itself a correct value. In order to obtain a complete description of the data, for the purposes of predicting the outputs corresponding to new input vectors, we must model the conditional probability distribution of the target data, again conditioned on the input vector. In this paper we introduce a new class of network models obtained by combining a conventional neural network with a mixture density model. The complete system is called a Mixture Density Network, and can in principle represent arbitrary conditional probability distributions in the same way that a conventional neural network can represent arbitrary functions. We demonstrate the e ectiveness of Mixture Density Networks using both a toy problem and a problem involving robot inverse kinematics.}
}

@article{bishop_novelty_1994,
  title = {Novelty {{Detection}} and {{Neural-Network Validation}}},
  author = {Bishop, Christopher M.},
  year = {1994},
  journal = {IEE Proceedings - Vision, Image and Signal Processing},
  volume = {141},
  number = {4},
  pages = {217--222},
  issn = {1350245x, 13597108},
  langid = {english}
}

@book{bishop_pattern_2006,
  title = {Pattern {{Recognition}} and {{Machine Learning}}},
  author = {Bishop, Christopher M.},
  year = {2006},
  journal = {New York: springer.},
  publisher = {{Springer}},
  isbn = {978-0-387-31073-2},
  keywords = {★}
}

@article{blaabjerg_early_2016,
  title = {Early Stage Investigation of Sensor Technologies and Related Bioassay for in Vivo Diagnosis of Colorectal Cancer},
  author = {Blaabjerg, Lasse and Havtorn, Jakob D. and Svendsen, Winnie E. and Abi, Alireza and Kwasny, Dorota},
  year = {2016}
}

@misc{blei_distance_2009,
  title = {Distance {{Dependent Chinese Restaurant Processes}}},
  author = {Blei, David M. and Frazier, Peter I.},
  year = {2009},
  month = oct,
  eprint = {0910.1022},
  eprinttype = {arxiv},
  abstract = {We develop the distance dependent Chinese restaurant process (CRP), a flexible class of distributions over partitions that allows for non-exchangeability. This class can be used to model many kinds of dependencies between data in infinite clustering models, including dependencies across time or space. We examine the properties of the distance dependent CRP, discuss its connections to Bayesian nonparametric mixture models, and derive a Gibbs sampler for both observed and mixture settings. We study its performance with three text corpora. We show that relaxing the assumption of exchangeability with distance dependent CRPs can provide a better fit to sequential data. We also show its alternative formulation of the traditional CRP leads to a faster-mixing Gibbs sampling algorithm than the one based on the original formulation.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning,Statistics - Methodology}
}

@article{blei_variational_2017,
  title = {Variational {{Inference}}: {{A Review}} for {{Statisticians}}},
  author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
  year = {2017},
  month = jan,
  journal = {Journal of the American Statistical Association},
  volume = {112},
  number = {518},
  eprint = {1601.00670},
  eprinttype = {arxiv},
  pages = {859--877},
  issn = {1537274X},
  abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
  archiveprefix = {arXiv},
  isbn = {1601.00670},
  pmid = {303902},
  keywords = {Algorithms,Computationally intensive methods,Statistical computing}
}

@misc{blier_learning_2018,
  title = {Learning with {{Random Learning Rates}}},
  author = {Blier, L{\'e}onard and Wolinski, Pierre and Ollivier, Yann},
  year = {2018},
  month = oct,
  eprint = {1810.01322},
  eprinttype = {arxiv},
  abstract = {Hyperparameter tuning is a bothersome step in the training of deep learning models. One of the most sensitive hyperparameters is the learning rate of the gradient descent. We present the All Learning Rates At Once (Alrao) optimization method for neural networks: each unit or feature in the network gets its own learning rate sampled from a random distribution spanning several orders of magnitude. This comes at practically no computational cost. Perhaps surprisingly, stochastic gradient descent (SGD) with Alrao performs close to SGD with an optimally tuned learning rate, for various architectures and problems. Alrao could save time when testing deep learning models: a range of models could be quickly assessed with Alrao, and the most promising models could then be trained more extensively. This text comes with a PyTorch implementation of the method, which can be plugged on an existing PyTorch model: https://github.com/leonardblier/alrao.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
}

@article{block_analysis_1962,
  title = {Analysis of a {{Four-Layer Series-Coupled Perceptron II}}},
  author = {Block, H. D. and Knight, Bruce and Rosenblatt, Frank},
  year = {1962},
  journal = {Reviews of Modern Physics},
  volume = {34},
  number = {1},
  abstract = {Block, H. D., Knight, B. W., Jr., Rosenblatt, F. (1962) Analysis of a four-layer series-coupled perceptron. II. Rev. Mod. Phys. 34: 135 THE PERC EPTION. I 100 C\textemdash{} ISPLAY 75-DISPLAY 50 \textasciitilde{} \textasciitilde\textasciitilde\textasciitilde\textasciitilde{} \textasciitilde{} \textasciitilde{} \textasciitilde{} \textasciitilde{} \textasciitilde{} \textasciitilde{} \textasciitilde{} \textasciitilde\textasciitilde\textasciitilde CHANCE EXPECTAHCY I/2 ASSOCIATIOH 3/4 ASSOCIATIOH \&\& ASSOCIATION UNITS REMOVED UNITS RKMOYED UNITS REIAO+ED IHT ACT PERCEPTROH (240 ASSOCIATIOH IjHITS) I'ro. 13. EGect of association unit removal on trained "E"\textemdash{} "X" discrimination. simple perceptron of Fig. 4 is no longer adequate. We shall find certain temporal e8ects in the paper which follows, but for others it is necessary to introduce time delays into the system. ' A speech recognizing perceptron which utilizes such delays is currently being built at Cornell University. Other activities now in progress' include quantitative studies of cross-coupled and multi-layer systems (by means of analysis and digital simulation), studies of selective attention mechanisms, the effects of geometric constraints on network organization, new types of reinforcement rules, and attempts at relating this research to biological data. Work is also in progress on development of electrolytic and other low-cost inte-grating devices and additional electronic components necessary for the construction of large-scale physical models. It is clear that we are still far from the point of understanding how the brain functions. It is equally clear, we believe, that a promising road is open for further investigation.}
}

@inproceedings{blundell_weight_2015,
  title = {Weight {{Uncertainty}} in {{Neural Networks}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
  year = {2015},
  eprint = {1505.05424},
  eprinttype = {arxiv},
  address = {{Lille, France}},
  issn = {1938-7228},
  abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
  archiveprefix = {arXiv},
  isbn = {978-1-5108-1058-7}
}

@article{bock_improvement_2018,
  title = {An Improvement of the Convergence Proof of the {{ADAM-Optimizer}}},
  author = {Bock, Sebastian and Goppold, Josef and Wei{\ss}, Martin},
  year = {2018},
  abstract = {\textemdash A common way to train neural networks is the Backpropagation. This algorithm includes a gradient descent method, which needs an adaptive step size. In the area of neural networks, the ADAM-Optimizer is one of the most popular adaptive step size methods. It was invented in [1] by Kingma and Ba. The 5865 citations in only three years shows additionally the importance of the given paper. We discovered that the given convergence proof of the optimizer contains some mistakes, so that the proof will be wrong. In this paper we give an improvement to the convergence proof of the ADAM-Optimizer.},
  keywords = {ADAM-Optimizer,Method of moments}
}

@inproceedings{bocklet_age_2008,
  title = {Age and Gender Recognition for Telephone Applications Based on {{GMM}} Supervectors and Support Vector Machines},
  booktitle = {2008 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  author = {Bocklet, Tobias and Maier, Andreas and Bauer, Josef G. and Burkhardt, Felix and Noth, Elmar},
  year = {2008},
  month = mar,
  pages = {1605--1608},
  publisher = {{IEEE}},
  address = {{Las Vegas, NV, USA}},
  abstract = {This paper compares two approaches of automatic age and gender classification with 7 classes. The first approach are Gaussian Mixture Models (GMMs) with Universal Background Models (UBMs), which is well known for the task of speaker identification/verification. The training is performed by the EM algorithm or MAP adaptation respectively. For the second approach for each speaker of the test and training set a GMM model is trained. The means of each model are extracted and concatenated, which results in a GMM supervector for each speaker. These supervectors are then used in a support vector machine (SVM). Three different kernels were employed for the SVM approach: a polynomial kernel (with different polynomials), an RBF kernel and a linear GMM distance kernel, based on the KL divergence. With the SVM approach we improved the recognition rate to 74\% (p {$<$} 0.001) and are in the same range as humans.},
  isbn = {978-1-4244-1483-3 978-1-4244-1484-0},
  langid = {english}
}

@article{bolukbasi_man_2016,
  title = {Man Is to {{Computer Programmer}} as {{Woman}} Is to {{Homemaker}}? {{Debiasing Word Embeddings}}},
  author = {Bolukbasi, Tolga and Chang, Kai-wei and Zou, James and Saligrama, Venkatesh and Kalai, Adam},
  year = {2016},
  month = jul,
  journal = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
  eprint = {1607.06520},
  eprinttype = {arxiv},
  abstract = {The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between between the words receptionist and female, while maintaining desired associations such as between the words queen and female. We define metrics to quantify both direct and indirect gender biases in embeddings, and develop algorithms to "debias" the embedding. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.},
  archiveprefix = {arXiv},
  keywords = {★}
}

@misc{bond-taylor_deep_2021,
  title = {Deep {{Generative Modelling}}: {{A Comparative Review}} of {{VAEs}}, {{GANs}}, {{Normalizing Flows}}, {{Energy-Based}} and {{Autoregressive Models}}},
  shorttitle = {Deep {{Generative Modelling}}},
  author = {{Bond-Taylor}, Sam and Leach, Adam and Long, Yang and Willcocks, Chris G.},
  year = {2021},
  month = mar,
  eprint = {2103.04922},
  eprinttype = {arxiv},
  abstract = {Deep generative modelling is a class of techniques that train deep neural networks to model the distribution of training samples. Research has fragmented into various interconnected approaches, each of which making trade-offs including run-time, diversity, and architectural restrictions. In particular, this compendium covers energy-based models, variational autoencoders, generative adversarial networks, autoregressive models, normalizing flows, in addition to numerous hybrid approaches. These techniques are drawn under a single cohesive framework, comparing and contrasting to explain the premises behind each, while reviewing current state-of-the-art advances and implementations.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {68T01 (Primary); 68T07 (Secondary),Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,G.3,I.4.0,I.5.0,Statistics - Machine Learning}
}

@misc{bonheme_be_2021,
  title = {Be {{More Active}}! {{Understanding}} the {{Differences}} between {{Mean}} and {{Sampled Representations}} of {{Variational Autoencoders}}},
  author = {Bonheme, Lisa and Grzes, Marek},
  year = {2021},
  month = sep,
  eprint = {2109.12679},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The ability of Variational Autoencoders to learn disentangled representations has made them appealing for practical applications. However, their mean representations, which are generally used for downstream tasks, have recently been shown to be more correlated than their sampled counterpart, on which disentanglement is usually measured. In this paper, we refine this observation through the lens of selective posterior collapse, which states that only a subset of the learned representations, the active variables, is encoding useful information while the rest (the passive variables) is discarded. We first extend the existing definition, originally proposed for sampled representations, to mean representations and show that active variables are equally disentangled in both representations. Based on this new definition and the pre-trained models from disentanglement lib, we then isolate the passive variables and show that they are responsible for the discrepancies between mean and sampled representations. Specifically, passive variables exhibit high correlation scores with other variables in mean representations while being fully uncorrelated in sampled ones. We thus conclude that despite what their higher correlation might suggest, mean representations are still good candidates for downstream tasks applications. However, it may be beneficial to remove their passive variables, especially when used with models sensitive to correlated features.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,G.3,I.2.6,Statistics - Machine Learning}
}

@inproceedings{borgholt_brief_2022,
  title = {A {{Brief Overview}} of {{Neural Speech Representation Learning}}},
  booktitle = {The 2nd {{Workshop}} on {{Self-supervised Learning}} for {{Audio}} and {{Speech Processing}} ({{SAS}})},
  author = {Borgholt, Lasse and Havtorn, Jakob D. and Edin, Joakim and Maal{\o}e, Lars and Igel, Christian},
  year = {2022},
  month = feb,
  address = {{Virtual}},
  abstract = {Unsupervised representation learning for speech processing has matured greatly in the last few years. Work in computer vision and natural language processing has paved the way, but speech data offers unique challenges. As a result, methods from other domains rarely translate directly. We review the development of unsupervised representation learning for speech over the last decade. We identify two primary model categories: self-supervised methods and probabilistic latent variable models. We describe the models and develop a comprehensive taxonomy. Finally, we discuss and compare models from the two categories.},
  copyright = {All rights reserved}
}

@inproceedings{borgholt_end--end_2020,
  title = {Do End-to-End Speech Recognition Models Care about Context?},
  booktitle = {Proceedings of the 21st {{Annual Conference}} of the {{International Speech Communication Association}} ({{INTERSPEECH}})},
  author = {Borgholt, Lasse and Havtorn, Jakob D. and Agic, Zeljko and S{\o}gaard, Anders and Maal{\o}e, Lars and Igel, Christian},
  editor = {Meng, Helen and Xu, Bo and Zheng, Thomas Fang},
  year = {2020},
  pages = {4352--4356},
  publisher = {{ISCA}},
  address = {{Virtual}},
  copyright = {All rights reserved}
}

@inproceedings{borgholt_scaling_2021,
  title = {On {{Scaling Contrastive Representations}} for {{Low-Resource Speech Recognition}}},
  booktitle = {Ieee {{International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Borgholt, Lasse and Tax, Tycho M. S. and Havtorn, Jakob D. and Maal{\o}e, Lars and Igel, Christian},
  year = {2021},
  pages = {3885--3889},
  publisher = {{IEEE}},
  address = {{Virtual}},
  copyright = {All rights reserved}
}

@inproceedings{bornschein_variational_2017,
  title = {Variational Memory Addressing in Generative Models},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30: {{Annual Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Bornschein, J{\"o}rg and Mnih, Andriy and Zoran, Daniel and Rezende, Danilo Jimenez},
  year = {2017},
  pages = {3920--3929},
  address = {{Long Beach, CA, USA}},
  keywords = {Memory}
}

@misc{borsos_online_2018,
  title = {Online {{Variance Reduction}} for {{Stochastic Optimization}}},
  author = {Borsos, Zal{\'a}n and Krause, Andreas and Levy, Kfir Y.},
  year = {2018},
  eprint = {1802.04715},
  eprinttype = {arxiv},
  abstract = {Modern stochastic optimization methods often rely on uniform sampling which is agnostic to the underlying characteristics of the data. This might degrade the convergence by yielding estimates that suffer from a high variance. A possible remedy is to employ non-uniform importance sampling techniques, which take the structure of the dataset into account. In this work, we investigate a recently proposed setting which poses variance reduction as an online optimization problem with bandit feedback. We devise a novel and efficient algorithm for this setting that finds a sequence of importance sampling distributions competitive with the best fixed distribution in hindsight, the first result of this kind. While we present our method for sampling datapoints, it naturally extends to selecting coordinates or even blocks of thereof. Empirical validations underline the benefits of our method in several settings.},
  archiveprefix = {arXiv}
}

@misc{botev_practical_2017,
  title = {Practical {{Gauss-Newton Optimisation}} for {{Deep Learning}}},
  author = {Botev, Aleksandar and Ritter, Hippolyt and Barber, David},
  year = {2017},
  eprint = {1706.03662},
  eprinttype = {arxiv},
  abstract = {We present an efficient block-diagonal ap- proximation to the Gauss-Newton matrix for feedforward neural networks. Our result- ing algorithm is competitive against state- of-the-art first order optimisation methods, with sometimes significant improvement in optimisation performance. Unlike first-order methods, for which hyperparameter tuning of the optimisation parameters is often a labo- rious process, our approach can provide good performance even when used with default set- tings. A side result of our work is that for piecewise linear transfer functions, the net- work objective function can have no differ- entiable local maxima, which may partially explain why such transfer functions facilitate effective optimisation.},
  archiveprefix = {arXiv}
}

@article{bottou_counterfactual_nodate,
  title = {Counterfactual {{Reasoning}} and {{Learning Systems}}: {{The Example}} of {{Computational Advertising}}},
  author = {Bottou, L{\'e}on and Peters, Jonas and {Qui{\~n}onero-Candela}, Joaquin and Charles, Denis X and Chickering, D Max and Portugaly, Elon and Ray, Dipankar and Simard, Patrice and Snelson, Ed},
  pages = {54},
  abstract = {This work shows how to leverage causal inference to understand the behavior of complex learning systems interacting with their environment and predict the consequences of changes to the system. Such predictions allow both humans and algorithms to select the changes that would have improved the system performance. This work is illustrated by experiments on the ad placement system associated with the Bing search engine.},
  langid = {english},
  keywords = {⛔ No DOI found}
}

@inproceedings{boulanger-lewandowski_modeling_2012,
  title = {Modeling {{Temporal Dependencies}} in {{High-Dimensional Sequences}}: {{Application}} to {{Polyphonic Music Generation}} and {{Transcription}}},
  booktitle = {Proceedings of the 29th {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {{Boulanger-Lewandowski}, Nicolas and Bengio, Yoshua and Vincent, Pascal},
  year = {2012},
  pages = {8},
  address = {{Edinburgh, Scotland, UK}},
  abstract = {We investigate the problem of modeling symbolic sequences of polyphonic music in a completely general piano-roll representation. We introduce a probabilistic model based on distribution estimators conditioned on a recurrent neural network that is able to discover temporal dependencies in high-dimensional sequences. Our approach outperforms many traditional models of polyphonic music on a variety of realistic datasets. We show how our musical language model can serve as a symbolic prior to improve the accuracy of polyphonic transcription.},
  langid = {english}
}

@inproceedings{bowman_generating_2016,
  title = {Generating {{Sentences}} from a {{Continuous Space}}},
  booktitle = {Proceedings of the 20th {{SIGNLL Conference}} on {{Computational Natural Language Learning}} ({{CoNLL}})},
  author = {Bowman, Samuel R. and Vilnis, Luke and Vinyals, Oriol and Dai, Andrew M. and J{\'o}zefowicz, Rafal and Bengio, Samy},
  year = {2016},
  pages = {10--21},
  address = {{Berlin, Germany}}
}

@book{boyd_convex_2004,
  title = {Convex Optimization},
  author = {Boyd, Stephen P. and Vandenberghe, Lieven},
  year = {2004},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge, UK ; New York}},
  isbn = {978-0-521-83378-3},
  langid = {english},
  lccn = {QA402.5 .B69 2004},
  keywords = {★,Convex functions,Mathematical optimization}
}

@misc{bradbury_quasi-recurrent_2016,
  title = {Quasi-{{Recurrent Neural Networks}}},
  author = {Bradbury, James and Merity, Stephen and Xiong, Caiming and Socher, Richard},
  year = {2016},
  month = nov,
  eprint = {1611.01576},
  eprinttype = {arxiv},
  abstract = {Recurrent neural networks are a powerful tool for modeling sequential data, but the dependence of each timestep's computation on the previous timestep's output limits parallelism and makes RNNs unwieldy for very long sequences. We introduce quasi-recurrent neural networks (QRNNs), an approach to neural sequence modeling that alternates convolutional layers, which apply in parallel across timesteps, and a minimalist recurrent pooling function that applies in parallel across channels. Despite lacking trainable recurrent layers, stacked QRNNs have better predictive accuracy than stacked LSTMs of the same hidden size. Due to their increased parallelism, they are up to 16 times faster at train and test time. Experiments on language modeling, sentiment classification, and character-level neural machine translation demonstrate these advantages and underline the viability of QRNNs as a basic building block for a variety of sequence tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Convolutional neural networks,Recurrent neural networks}
}

@misc{brandenburg_mpeg-2_1998,
  title = {{{MPEG-2 Audio Layer III}} ({{MP3}})},
  author = {Brandenburg, Karlheinz and Eberlein, Ernst and Gerh{\"a}user, Heinz and Grill, Bernhard and Herre, J{\"u}rgen and Popp, Harald},
  year = {1998},
  address = {{Germany}},
  howpublished = {Fraunhofer Society}
}

@misc{braun_curriculum_2016,
  title = {A {{Curriculum Learning Method}} for {{Improved Noise Robustness}} in {{Automatic Speech Recognition}}},
  author = {Braun, Stefan and Neil, Daniel and Liu, Shih-Chii},
  year = {2016},
  month = jun,
  eprint = {1606.06864},
  eprinttype = {arxiv},
  abstract = {The performance of automatic speech recognition systems under noisy environments still leaves room for improvement. Speech enhancement or feature enhancement techniques for increasing noise robustness of these systems usually add components to the recognition system that need careful optimization. In this work, we propose the use of a relatively simple curriculum training strategy called accordion annealing (ACCAN). It uses a multi-stage training schedule where samples at signal-to-noise ratio (SNR) values as low as 0dB are first added and samples at increasing higher SNR values are gradually added up to an SNR value of 50dB. We also use a method called per-epoch noise mixing (PEM) that generates noisy training samples online during training and thus enables dynamically changing the SNR of our training data. Both the ACCAN and the PEM methods are evaluated on a end-to-end speech recognition pipeline on the Wall Street Journal corpus. ACCAN decreases the average word error rate (WER) on the 20dB to -10dB SNR range by up to 31.4\% when compared to a conventional multi-condition training method.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Curriculum learning,Data augmentation}
}

@misc{breen_newton_2019,
  title = {Newton vs the Machine: Solving the Chaotic Three-Body Problem Using Deep Neural Networks},
  shorttitle = {Newton vs the Machine},
  author = {Breen, Philip G. and Foley, Christopher N. and Boekholt, Tjarda and Zwart, Simon Portegies},
  year = {2019},
  month = oct,
  eprint = {1910.07291},
  eprinttype = {arxiv},
  abstract = {Since its formulation by Sir Isaac Newton, the problem of solving the equations of motion for three bodies under their own gravitational force has remained practically unsolved. Currently, the solution for a given initialization can only be found by performing laborious iterative calculations that have unpredictable and potentially infinite computational cost, due to the system's chaotic nature. We show that an ensemble of solutions obtained using an arbitrarily precise numerical integrator can be used to train a deep artificial neural network (ANN) that, over a bounded time interval, provides accurate solutions at fixed computational cost and up to 100 million times faster than a state-of-the-art solver. Our results provide evidence that, for computationally challenging regions of phase-space, a trained ANN can replace existing numerical solvers, enabling fast and scalable simulations of many-body systems to shed light on outstanding phenomena such as the formation of black-hole binary systems or the origin of the core collapse in dense star clusters.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {★,⛔ No DOI found,Astrophysics - Astrophysics of Galaxies,Astrophysics - Solar and Stellar Astrophysics,Computer Science - Machine Learning,Physics - Computational Physics}
}

@inproceedings{brock_characterizing_2021,
  title = {Characterizing Signal Propagation to Close the Performance Gap in Unnormalized {{ResNets}}},
  booktitle = {Proceedings of the 9th {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Brock, Andrew and De, Soham and Smith, Samuel L.},
  year = {2021},
  month = jan,
  eprint = {2101.08692},
  eprinttype = {arxiv},
  abstract = {Batch Normalization is a key component in almost all state-of-the-art image classifiers, but it also introduces practical challenges: it breaks the independence between training examples within a batch, can incur compute and memory overhead, and often results in unexpected bugs. Building on recent theoretical analyses of deep ResNets at initialization, we propose a simple set of analysis tools to characterize signal propagation on the forward pass, and leverage these tools to design highly performant ResNets without activation normalization layers. Crucial to our success is an adapted version of the recently proposed Weight Standardization. Our analysis tools show how this technique preserves the signal in networks with ReLU or Swish activation functions by ensuring that the per-channel activation means do not grow with depth. Across a range of FLOP budgets, our networks attain performance competitive with the state-of-the-art EfficientNets on ImageNet.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{brock_high-performance_2021,
  title = {High-{{Performance Large-Scale Image Recognition Without Normalization}}},
  author = {Brock, Andrew and De, Soham and Smith, Samuel L. and Simonyan, Karen},
  year = {2021},
  month = feb,
  eprint = {2102.06171},
  eprinttype = {arxiv},
  primaryclass = {cs.CV},
  abstract = {Batch normalization is a key component of most image classification models, but it has many undesirable properties stemming from its dependence on the batch size and interactions between examples. Although recent work has succeeded in training deep ResNets without normalization layers, these models do not match the test accuracies of the best batch-normalized networks, and are often unstable for large learning rates or strong data augmentations. In this work, we develop an adaptive gradient clipping technique which overcomes these instabilities, and design a significantly improved class of Normalizer-Free ResNets. Our smaller models match the test accuracy of an EfficientNet-B7 on ImageNet while being up to 8.7x faster to train, and our largest models attain a new state-of-the-art top-1 accuracy of 86.5\%. In addition, Normalizer-Free models attain significantly better performance than their batch-normalized counterparts when finetuning on ImageNet after large-scale pre-training on a dataset of 300 million labeled images, with our best models obtaining an accuracy of 89.2\%. Our code is available at https://github.com/deepmind/ deepmind-research/tree/master/nfnets},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{brockman_openai_2016,
  title = {{{OpenAI Gym}}},
  author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  year = {2016},
  eprint = {1606.01540},
  eprinttype = {arxiv},
  abstract = {OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.},
  archiveprefix = {arXiv}
}

@misc{bronstein_geometric_2021,
  title = {Geometric {{Deep Learning}}: {{Grids}}, {{Groups}}, {{Graphs}}, {{Geodesics}}, and {{Gauges}}},
  shorttitle = {Geometric {{Deep Learning}}},
  author = {Bronstein, Michael M. and Bruna, Joan and Cohen, Taco and Veli{\v c}kovi{\'c}, Petar},
  year = {2021},
  month = apr,
  eprint = {2104.13478},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach -- such as computer vision, playing Go, or protein folding -- are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent type methods, typically implemented as backpropagation. While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world. This text is concerned with exposing these regularities through unified geometric principles that can be applied throughout a wide spectrum of applications. Such a 'geometric unification' endeavour, in the spirit of Felix Klein's Erlangen Program, serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computational Geometry,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{bubeck_geometric_2015,
  title = {A Geometric Alternative to {{Nesterov}}'s Accelerated Gradient Descent},
  author = {Bubeck, S{\'e}bastien and Lee, Yin Tat and Singh, Mohit},
  year = {2015},
  eprint = {1506.08187},
  eprinttype = {arxiv},
  abstract = {We propose a new method for unconstrained optimization of a smooth and strongly convex function, which attains the optimal rate of convergence of Nesterov's accelerated gradient descent. The new algorithm has a simple geometric interpretation, loosely inspired by the ellipsoid method. We provide some numerical evidence that the new method can be superior to Nesterov's accelerated gradient descent.},
  archiveprefix = {arXiv}
}

@article{buehrer_mathematical_nodate,
  title = {A {{Mathematical Framework}} for {{Superintelligent Machines}}},
  author = {Buehrer, Daniel J.}
}

@misc{bulatov_notmnist_2011,
  title = {{{notMNIST}} Dataset},
  author = {Bulatov, Yaroslav},
  year = {2011},
  month = sep,
  journal = {notMNIST dataset},
  keywords = {Dataset}
}

@article{buntine_bayesian_1991,
  title = {Bayesian {{Back-Propagation}}},
  author = {Buntine, Wray L.},
  year = {1991},
  journal = {Complex Systems},
  volume = {5},
  eprint = {1011.1669v3},
  eprinttype = {arxiv},
  pages = {603--643},
  issn = {00308870},
  abstract = {Connectionist feed-forward networks, t rained with back-prop agat ion, can be used both for nonlinear regression and for (dis-crete one-of-C) classification. T his pap er present s approximate Bayes-ian meth ods to statistical compo nents of back-pro pagat ion: choosing a cost funct ion and penalty term (interpreted as a form of prior prob-ability), pruning insignifican t weights, est imat ing th e uncertainty of weights, predict ing for new pat tern s ("out -of-sample") , est imating t he uncertainty in the choice of t his prediction ("erro r bars"), estima t-ing the generalizat ion erro r, comparing different network st ructures, and handling missing values in t he t raining pattern s. These meth -ods extend some heuristic techniques suggested in the literature, and in most cases require a small additional facto r in comp ut ation during back-propagation, or computation once back-pro pagat ion has finished. 1. I nt r o d u ction Back-propagati on [32] is a p opular sche me for t raining feed-for ward connec -t ionist networks. It can b e applied t o bo t h t he tasks of cla ssification (predic-t ion of discret e variables t aking one of C mutua lly exclusive and exha ust ive va lues) and regr ession (p redi ct ion of rea l va ria bles). Design issu es in t his sche me are prim arily com puta tional-for inst ance, what varia t ions of gradi-ent descen t should b e used-or probabilistic -for inst a nce, what cost func-ti on should b e used a nd how generalizat ion err or ca n b e pred ict ed . Her e we frame t he probabilistic component of back-prop agation in a Bayesian conte xt [6, 3, 7, 27]. In adopt ing a Bayesian justi fica t ion for t he method s present ed , we are not claiming any neurological valid ity for our metho ds. We view},
  archiveprefix = {arXiv},
  isbn = {0030-8870},
  pmid = {24439530}
}

@inproceedings{burda_importance_2016,
  title = {Importance {{Weighted Autoencoders}}},
  booktitle = {Proceedings of the 4th {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan R.},
  year = {2016},
  pages = {8},
  address = {{San Juan, Puerto Rico}},
  abstract = {The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.}
}

@misc{burg_memorization_2021,
  title = {On {{Memorization}} in {{Probabilistic Deep Generative Models}}},
  author = {van den Burg, Gerrit J. J. and Williams, Christopher K. I.},
  year = {2021},
  month = jun,
  eprint = {2106.03216},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Recent advances in deep generative models have led to impressive results in a variety of application domains. Motivated by the possibility that deep learning models might memorize part of the input data, there have been increased efforts to understand how memorization can occur. In this work, we extend a recently proposed measure of memorization for supervised learning (Feldman, 2019) to the unsupervised density estimation problem and simplify the accompanying estimator. Next, we present an exploratory study that demonstrates how memorization can arise in probabilistic deep generative models, such as variational autoencoders. This reveals that the form of memorization to which these models are susceptible differs fundamentally from mode collapse and overfitting. Finally, we discuss several strategies that can be used to limit memorization in practice.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {68T07,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{buse_likelihood_1982,
  title = {The Likelihood Ratio, {{Wald}}, and {{Lagrange}} Multiplier Tests: {{An}} Expository Note},
  author = {Buse, Adolf},
  year = {1982},
  journal = {The American Statistician},
  volume = {36},
  number = {3a},
  pages = {153--157}
}

@article{campbell_deep_2002,
  title = {Deep {{Blue}}},
  author = {Campbell, Murray and Hoane Jr., A. Joseph and Hsu, Feng-hsiung},
  year = {2002},
  journal = {Artificial Intelligence},
  volume = {134},
  number = {1-2},
  pages = {57--83},
  issn = {00043702},
  abstract = {Deep Blue is the chess machine that defeated then-reigning World Chess Champion Garry Kasparov in a six-game match in 1997. There were a number of factors that contributed to this success, including: \textbullet a single-chip chess search engine,\textbullet a massively parallel system with multiple levels of parallelism,\textbullet a strong emphasis on search extensions,\textbullet a complex evaluation function, and\textbullet effective use of a Grandmaster game database. This paper describes the Deep Blue system, and gives some of the rationale that went into the design decisions behind Deep Blue.},
  isbn = {0004-3702},
  keywords = {Evaluation,Game tree search,Parallel search,Search extensions,Selective search}
}

@misc{candes_robust_2009,
  title = {Robust {{Principal Component Analysis}}?},
  author = {Candes, Emmanuel J. and Li, Xiaodong and Ma, Yi and Wright, John},
  year = {2009},
  month = dec,
  eprint = {0912.3599},
  eprinttype = {arxiv},
  abstract = {This paper is about a curious phenomenon. Suppose we have a data matrix, which is the superposition of a low-rank component and a sparse component. Can we recover each component individually? We prove that under some suitable assumptions, it is possible to recover both the low-rank and the sparse components exactly by solving a very convenient convex program called Principal Component Pursuit; among all feasible decompositions, simply minimize a weighted combination of the nuclear norm and of the 1 norm. This suggests the possibility of a principled approach to robust principal component analysis since our methodology and results assert that one can recover the principal components of a data matrix even though a positive fraction of its entries are arbitrarily corrupted. This extends to the situation where a fraction of the entries are missing as well. We discuss an algorithm for solving this optimization problem, and present applications in the area of video surveillance, where our methodology allows for the detection of objects in a cluttered background, and in the area of face recognition, where it offers a principled way of removing shadows and specularities in images of faces.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Information Theory}
}

@inproceedings{carpentier_adaptive_2012,
  title = {Adaptive {{Stratified Sampling}} for {{Monte-Carlo}} Integration of {{Differentiable}} Functions.},
  booktitle = {Proceedings of the {{Conference}} on {{Neural Information Processing Systems}} ({{NIPS}})},
  author = {Carpentier, Alexandra and Munos, R{\'e}mi},
  year = {2012},
  eprint = {1210.5345v1},
  eprinttype = {arxiv},
  pages = {1--23},
  issn = {10495258},
  abstract = {We consider the problem of adaptive stratified sampling for Monte Carlo integra-tion of a differentiable function given a finite number of evaluations to the func-tion. We construct a sampling scheme that samples more often in regions where the function oscillates more, while allocating the samples such that they are well spread on the domain (this notion shares similitude with low discrepancy). We prove that the estimate returned by the algorithm is almost similarly accurate as the estimate that an optimal oracle strategy (that would know the variations of the function everywhere) would return, and provide a finite-sample analysis.},
  archiveprefix = {arXiv},
  isbn = {978-1-62748-003-1}
}

@article{carr_self-supervised_2021,
  title = {Self-{{Supervised Learning}} of {{Audio Representations}} from {{Permutations}} with {{Differentiable Ranking}}},
  author = {Carr, Andrew N. and Berthet, Quentin and Blondel, Mathieu and Teboul, Olivier and Zeghidour, Neil},
  year = {2021},
  journal = {IEEE Signal Processing Letters},
  volume = {28},
  eprint = {2103.09879},
  eprinttype = {arxiv},
  pages = {708--712},
  issn = {1070-9908, 1558-2361},
  abstract = {Self-supervised pre-training using so-called "pretext" tasks has recently shown impressive performance across a wide range of modalities. In this work, we advance self-supervised learning from permutations, by pre-training a model to reorder shuffled parts of the spectrogram of an audio signal, to improve downstream classification performance. We make two main contributions. First, we overcome the main challenges of integrating permutation inversions into an end-to-end training scheme, using recent advances in differentiable ranking. This was heretofore sidestepped by casting the reordering task as classification, fundamentally reducing the space of permutations that can be exploited. Our experiments validate that learning from all possible permutations improves the quality of the pre-trained representations over using a limited, fixed set. Second, we show that inverting permutations is a meaningful pretext task for learning audio representations in an unsupervised fashion. In particular, we improve instrument classification and pitch estimation of musical notes by reordering spectrogram patches in the time-frequency space.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@article{caruana_multitask_1997,
  title = {Multitask {{Learning}}},
  author = {Caruana, Rich},
  year = {1997},
  month = jul,
  journal = {Journal of Machine Learning},
  volume = {28},
  number = {1},
  pages = {41--75},
  abstract = {Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers task relatedness without the need of supervisory signals, and presents new results for MTL with k-nearest neighbor and kernel regression. In this paper we demonstrate multitask learning in three domains. We explain how multitask learning works, and show that there are many opportunities for multitask learning in real domains. We present an algorithm and results for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Because multitask learning works, can be applied to many different kinds of domains, and can be used with different learning algorithms, we conjecture there will be many opportunities for its use on real-world problems.},
  keywords = {Generalization,Multitask learning}
}

@misc{cemgil_autoencoding_2020,
  title = {Autoencoding {{Variational Autoencoder}}},
  author = {Cemgil, A. Taylan and Ghaisas, Sumedh and Dvijotham, Krishnamurthy and Gowal, Sven and Kohli, Pushmeet},
  year = {2020},
  month = dec,
  eprint = {2012.03715},
  eprinttype = {arxiv},
  abstract = {Does a Variational AutoEncoder (VAE) consistently encode typical samples generated from its decoder? This paper shows that the perhaps surprising answer to this question is `No'; a (nominally trained) VAE does not necessarily amortize inference for typical samples that it is capable of generating. We study the implications of this behaviour on the learned representations and also the consequences of fixing it by introducing a notion of self consistency. Our approach hinges on an alternative construction of the variational approximation distribution to the true posterior of an extended VAE model with a Markov chain alternating between the encoder and the decoder. The method can be used to train a VAE model from scratch or given an already trained VAE, it can be run as a post processing step in an entirely self supervised way without access to the original training data. Our experimental analysis reveals that encoders trained with our self-consistency approach lead to representations that are robust (insensitive) to perturbations in the input introduced by adversarial attacks. We provide experimental results on the ColorMnist and CelebA benchmark datasets that quantify the properties of the learned representations and compare the approach with a baseline that is specifically trained for the desired property.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@book{chacon_pro_nodate,
  title = {Pro {{Git}}},
  author = {Chacon, Scott and Straub, Ben},
  edition = {Second}
}

@article{chadebec_data_nodate,
  title = {Data {{Augmentation}} in {{High Dimensional Low Sample Size Setting Using}} a {{Geometry-Based Variational Autoencoder}}},
  author = {Chadebec, Cl{\'e}ment and {Thibeau-Sutre}, Elina and Burgos, Ninon and Allassonni{\`e}re, St{\'e}phanie},
  pages = {26},
  abstract = {In this paper, we propose a new method to perform data augmentation in a reliable way in the High Dimensional Low Sample Size (HDLSS) setting using a geometry-based variational autoencoder. Our approach combines a proper latent space modeling of the VAE seen as a Riemannian manifold with a new generation scheme which produces more meaningful samples especially in the context of small data sets. The proposed method is tested through a wide experimental study where its robustness to data sets, classifiers and training samples size is stressed. It is also validated on a medical imaging classification task on the challenging ADNI database where a small number of 3D brain MRIs are considered and augmented using the proposed VAE framework. In each case, the proposed method allows for a significant and reliable gain in the classification metrics. For instance, balanced accuracy jumps from 66.3\% to 74.3\% for a state-of-the-art CNN classifier trained with 50 MRIs of cognitively normal (CN) and 50 Alzheimer disease (AD) patients and from 77.7\% to 86.3\% when trained with 243 CN and 210 AD while improving greatly sensitivity and specificity metrics.},
  langid = {english}
}

@misc{chan_listen_2015,
  title = {Listen, {{Attend}} and {{Spell}}},
  author = {Chan, William and Jaitly, Navdeep and Le, Quoc V. and Vinyals, Oriol},
  year = {2015},
  month = aug,
  eprint = {1508.01211},
  eprinttype = {arxiv},
  abstract = {We present Listen, Attend and Spell (LAS), a neural network that learns to transcribe speech utterances to characters. Unlike traditional DNN-HMM models, this model learns all the components of a speech recognizer jointly. Our system has two components: a listener and a speller. The listener is a pyramidal recurrent network encoder that accepts filter bank spectra as inputs. The speller is an attention-based recurrent network decoder that emits characters as outputs. The network produces character sequences without making any independence assumptions between the characters. This is the key improvement of LAS over previous end-to-end CTC models. On a subset of the Google voice search task, LAS achieves a word error rate (WER) of 14.1\% without a dictionary or a language model, and 10.3\% with language model rescoring over the top 32 beams. By comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0\%.},
  archiveprefix = {arXiv},
  keywords = {★,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
}

@article{chan_updating_1979,
  title = {Updating Formulae and a Pairwise Algorithm for Computing Sample Variances},
  author = {Chan, T. F. and Golub, G. H. and LeVeque, R. J.},
  year = {1979},
  journal = {Annals of Physics},
  volume = {54},
  pages = {258},
  abstract = {A general formula is presented for computing the sample v;iiiancc for a sample of size m+ n given the means and variances for two subsnn+lcs of sizes m and n. This formula is used in the construction of a pa.irwisc nl\textasciitilde :orithm for computing the variance. Other applications are discussed as v\textasciitilde ll, including the use of updating formulae in a parallel computin g cnviornmcn t. Wc present numerical results and rounding error analyses for several numerical schcr\textasciitilde\textasciitilde s.},
  isbn = {3705100025}
}

@misc{chang_batch-normalized_2015,
  title = {Batch-Normalized {{Maxout Network}} in {{Network}}},
  author = {Chang, Jia-Ren and Chen, Yong-Sheng},
  year = {2015},
  eprint = {1511.02583},
  eprinttype = {arxiv},
  abstract = {This paper reports a novel deep architecture referred to as Maxout network In Network (MIN), which can enhance model discriminability and facilitate the process of information abstraction within the receptive field. The proposed network adopts the framework of the recently developed Network In Network structure, which slides a universal approximator, multilayer perceptron (MLP) with rectifier units, to exact features. Instead of MLP, we employ maxout MLP to learn a variety of piecewise linear activation functions and to mediate the problem of vanishing gradients that can occur when using rectifier units. Moreover, batch normalization is applied to reduce the saturation of maxout units by pre-conditioning the model and dropout is applied to prevent overfitting. Finally, average pooling is used in all pooling layers to regularize maxout MLP in order to facilitate information abstraction in every receptive field while tolerating the change of object position. Because average pooling preserves all features in the local patch, the proposed MIN model can enforce the suppression of irrelevant information during training. Our experiments demonstrated the state-of-the-art classification performance when the MIN model was applied to MNIST, CIFAR-10, and CIFAR-100 datasets and comparable performance for SVHN dataset.},
  archiveprefix = {arXiv}
}

@article{chang_distilhubert_2021,
  title = {{{DistilHuBERT}}: {{Speech Representation Learning}} by {{Layer-wise Distillation}} of {{Hidden-unit BERT}}},
  shorttitle = {{{DistilHuBERT}}},
  author = {Chang, Heng-Jui and Yang, Shu-wen and Lee, Hung-yi},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.01900 [cs, eess]},
  eprint = {2110.01900},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {Self-supervised speech representation learning methods like wav2vec 2.0 and Hidden-unit BERT (HuBERT) leverage unlabeled speech data for pre-training and offer good representations for numerous speech processing tasks. Despite the success of these methods, they require large memory and high pre-training costs, making them inaccessible for researchers in academia and small companies. Therefore, this paper introduces DistilHuBERT, a novel multi-task learning framework to distill hidden representations from a HuBERT model directly. This method reduces HuBERT's size by 75\% and 73\% faster while retaining most performance in ten different tasks. Moreover, DistilHuBERT required little training time and data, opening the possibilities of pre-training personal and on-device SSL models for speech.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@article{chang_exploration_2021,
  title = {An {{Exploration}} of {{Self-Supervised Pretrained Representations}} for {{End-to-End Speech Recognition}}},
  author = {Chang, Xuankai and Maekaku, Takashi and Guo, Pengcheng and Shi, Jing and Lu, Yen-Ju and Subramanian, Aswin Shanmugam and Wang, Tianzi and Yang, Shu-wen and Tsao, Yu and Lee, Hung-yi and Watanabe, Shinji},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.04590 [cs, eess]},
  eprint = {2110.04590},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {Self-supervised pretraining on speech data has achieved a lot of progress. High-fidelity representation of the speech signal is learned from a lot of untranscribed data and shows promising performance. Recently, there are several works focusing on evaluating the quality of self-supervised pretrained representations on various tasks without domain restriction, e.g. SUPERB. However, such evaluations do not provide a comprehensive comparison among many ASR benchmark corpora. In this paper, we focus on the general applications of pretrained speech representations, on advanced end-to-end automatic speech recognition (E2E-ASR) models. We select several pretrained speech representations and present the experimental results on various open-source and publicly available corpora for E2E-ASR. Without any modification of the back-end model architectures or training strategy, some of the experiments with pretrained representations, e.g., WSJ, WSJ0-2mix with HuBERT, reach or outperform current state-of-the-art (SOTA) recognition performance. Moreover, we further explore more scenarios for whether the pretraining representations are effective, such as the cross-language or overlapped speech. The scripts, configuratons and the trained models have been released in ESPnet to let the community reproduce our experiments and improve them.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@misc{chang_neural_2018,
  title = {Neural {{Network Quine}}},
  author = {Chang, Oscar and Lipson, Hod},
  year = {2018},
  eprint = {1803.05859},
  eprinttype = {arxiv},
  abstract = {Self-replication is a key aspect of biological life that has been largely overlooked in Artificial Intelligence systems. Here we describe how to build and train self-replicating neural networks. The network replicates itself by learning to output its own weights. The network is designed using a loss function that can be optimized with either gradient-based or non-gradient-based methods. We also describe a method we call regeneration to train the network without explicit optimization, by injecting the network with predictions of its own parameters. The best solution for a self-replicating network was found by alternating between regeneration and optimization steps. Finally, we describe a design for a self-replicating neural network that can solve an auxiliary task such as MNIST image classification. We observe that there is a trade-off between the network's ability to classify images and its ability to replicate, but training is biased towards increasing its specialization at image classification at the expense of replication. This is analogous to the trade-off between reproduction and other tasks observed in nature. We suggest that a self-replication mechanism for artificial intelligence is useful because it introduces the possibility of continual improvement through natural selection.},
  archiveprefix = {arXiv}
}

@misc{chatfield_return_2014,
  title = {Return of the Devil in the Details: {{Delving}} Deep into Convolutional Nets},
  author = {Chatfield, Ken and Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  year = {2014},
  eprint = {1405.3531},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@misc{chaudhari_stochastic_2018,
  title = {Stochastic Gradient Descent Performs Variational Inference, Converges to Limit Cycles for Deep Networks},
  author = {Chaudhari, Pratik and Soatto, Stefano},
  year = {2018},
  month = jan,
  eprint = {1710.11029},
  eprinttype = {arxiv},
  abstract = {Stochastic gradient descent (SGD) is widely believed to perform implicit regularization when used to train deep neural networks, but the precise manner in which this occurs has thus far been elusive. We prove that SGD minimizes an average potential over the posterior distribution of weights along with an entropic regularization term. This potential is however not the original loss function in general. So SGD does perform variational inference, but for a different loss than the one used to compute the gradients. Even more surprisingly, SGD does not even converge in the classical sense: we show that the most likely trajectories of SGD for deep networks do not behave like Brownian motion around critical points. Instead, they resemble closed loops with deterministic components. We prove that such ``out-of-equilibrium'' behavior is a consequence of highly non-isotropic gradient noise in SGD; the covariance matrix of mini-batch gradients for deep networks has a rank as small as 1\% of its dimension. We provide extensive empirical validation of these claims, proven in the appendix.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Condensed Matter - Statistical Mechanics,Mathematics - Optimization and Control,Statistics - Machine Learning}
}

@misc{chen_infogan_2016,
  title = {{{InfoGAN}}: {{Interpretable Representation Learning}} by {{Information Maximizing Generative Adversarial Nets}}},
  author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
  year = {2016},
  eprint = {1606.03657},
  eprinttype = {arxiv},
  abstract = {This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods.},
  archiveprefix = {arXiv},
  isbn = {978-3-319-16816-6},
  pmid = {23459267}
}

@inproceedings{chen_isolating_2018,
  title = {Isolating {{Sources}} of {{Disentanglement}} in {{VAEs}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31: {{Annual Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Chen, Tian Qi and Li, Xuechen and Grosse, Roger B. and Duvenaud, David},
  year = {2018},
  pages = {2615--2625},
  address = {{Montreal, Quebec, Canada}}
}

@misc{chen_semantic_2014,
  title = {Semantic {{Image Segmentation}} with {{Deep Convolutional Nets}} and {{Fully Connected CRFs}}},
  author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
  year = {2014},
  eprint = {1412.7062},
  eprinttype = {arxiv},
  abstract = {Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called "semantic image segmentation"). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our "DeepLab" system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6\% IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the 'hole' algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU.},
  archiveprefix = {arXiv},
  isbn = {9783901608353},
  pmid = {28463186}
}

@article{chen_speech_2021,
  title = {Speech {{Representation Learning Through Self-supervised Pretraining And Multi-task Finetuning}}},
  author = {Chen, Yi-Chen and Yang, Shu-wen and Lee, Cheng-Kuang and See, Simon and Lee, Hung-yi},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.09930 [cs, eess]},
  eprint = {2110.09930},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {Speech representation learning plays a vital role in speech processing. Among them, self-supervised learning (SSL) has become an important research direction. It has been shown that an SSL pretraining model can achieve excellent performance in various downstream tasks of speech processing. On the other hand, supervised multi-task learning (MTL) is another representation learning paradigm, which has been proven effective in computer vision (CV) and natural language processing (NLP). However, there is no systematic research on the general representation learning model trained by supervised MTL in speech processing. In this paper, we show that MTL finetuning can further improve SSL pretraining. We analyze the generalizability of supervised MTL finetuning to examine if the speech representation learned by MTL finetuning can generalize to unseen new tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@misc{chen_survey_2017,
  title = {A {{Survey}} on {{Dialogue Systems}}: {{Recent Advances}} and {{New Frontiers}}},
  shorttitle = {A {{Survey}} on {{Dialogue Systems}}},
  author = {Chen, Hongshen and Liu, Xiaorui and Yin, Dawei and Tang, Jiliang},
  year = {2017},
  month = nov,
  eprint = {1711.01731},
  eprinttype = {arxiv},
  abstract = {Dialogue systems have attracted more and more attention. Recent advances on dialogue systems are overwhelmingly contributed by deep learning techniques, which have been employed to enhance a wide range of big data applications such as computer vision, natural language processing, and recommender systems. For dialogue systems, deep learning can leverage a massive amount of data to learn meaningful feature representations and response generation strategies, while requiring a minimum amount of hand-crafting. In this article, we give an overview to these recent advances on dialogue systems from various perspectives and discuss some possible research directions. In particular, we generally divide existing dialogue systems into task-oriented and nontask-oriented models, then detail how deep learning techniques help them with representative algorithms and finally discuss some appealing research directions that can bring the dialogue system research into a new frontier.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language}
}

@misc{chen_variational_2017,
  title = {Variational {{Lossy Autoencoder}}},
  author = {Chen, Xi and Kingma, Diederik P. and Salimans, Tim and Duan, Yan and Dhariwal, Prafulla and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
  year = {2017},
  month = mar,
  eprint = {1611.02731},
  eprinttype = {arxiv},
  abstract = {Representation learning seeks to expose certain aspects of observed data in a learned representation that's amenable to downstream tasks like classification. For instance, a good representation for 2D images might be one that describes only global structure and discards information about detailed texture. In this paper, we present a simple but principled method to learn such global representations by combining Variational Autoencoder (VAE) with neural autoregressive models such as RNN, MADE and PixelRNN/CNN. Our proposed VAE model allows us to have control over what the global latent code can learn and by designing the architecture accordingly, we can force the global latent code to discard irrelevant information such as texture in 2D images, and hence the VAE only ``autoencodes'' data in a lossy fashion. In addition, by leveraging autoregressive models as both prior distribution p(z) and decoding distribution p(x|z), we can greatly improve generative modeling performance of VAEs, achieving new state-of-the-art results on MNIST, OMNIGLOT and Caltech-101 Silhouettes density estimation tasks as well as competitive results on CIFAR10.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{chen_wavegrad_2020,
  title = {{{WaveGrad}}: {{Estimating Gradients}} for {{Waveform Generation}}},
  shorttitle = {{{WaveGrad}}},
  author = {Chen, Nanxin and Zhang, Yu and Zen, Heiga and Weiss, Ron J. and Norouzi, Mohammad and Chan, William},
  year = {2020},
  month = sep,
  eprint = {2009.00713},
  eprinttype = {arxiv},
  abstract = {This paper introduces WaveGrad, a conditional model for waveform generation through estimating gradients of the data density. This model is built on the prior work on score matching and diffusion probabilistic models. It starts from Gaussian white noise and iteratively refines the signal via a gradient-based sampler conditioned on the mel-spectrogram. WaveGrad is non-autoregressive, and requires only a constant number of generation steps during inference. It can use as few as 6 iterations to generate high fidelity audio samples. WaveGrad is simple to train, and implicitly optimizes for the weighted variational lower-bound of the log-likelihood. Empirical experiments reveal WaveGrad to generate high fidelity audio samples matching a strong likelihood-based autoregressive baseline with less sequential operations.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning}
}

@article{chen_wavlm_2021,
  title = {{{WavLM}}: {{Large-Scale Self-Supervised Pre-Training}} for {{Full Stack Speech Processing}}},
  shorttitle = {{{WavLM}}},
  author = {Chen, Sanyuan and Wang, Chengyi and Chen, Zhengyang and Wu, Yu and Liu, Shujie and Chen, Zhuo and Li, Jinyu and Kanda, Naoyuki and Yoshioka, Takuya and Xiao, Xiong and Wu, Jian and Zhou, Long and Ren, Shuo and Qian, Yanmin and Qian, Yao and Wu, Jian and Zeng, Micheal and Wei, Furu},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.13900 [cs, eess]},
  eprint = {2110.13900},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {Self-supervised learning (SSL) achieves great success in speech recognition, while limited exploration has been attempted for other speech processing tasks. As speech signal contains multi-faceted information including speaker identity, paralinguistics, spoken content, etc., learning universal representations for all speech tasks is challenging. In this paper, we propose a new pre-trained model, WavLM, to solve full-stack downstream speech tasks. WavLM is built based on the HuBERT framework, with an emphasis on both spoken content modeling and speaker identity preservation. We first equip the Transformer structure with gated relative position bias to improve its capability on recognition tasks. For better speaker discrimination, we propose an utterance mixing training strategy, where additional overlapped utterances are created unsupervisely and incorporated during model training. Lastly, we scale up the training dataset from 60k hours to 94k hours of public audio data, and optimize its training procedure for better representation extraction. WavLM Large achieves state-of-the-art performance on the SUPERB benchmark, and brings significant improvements for various speech processing tasks on their representative benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@article{chi_audio_2020,
  title = {Audio {{ALBERT}}: {{A Lite BERT}} for {{Self-supervised Learning}} of {{Audio Representation}}},
  shorttitle = {Audio {{ALBERT}}},
  author = {Chi, Po-Han and Chung, Pei-Hung and Wu, Tsung-Han and Hsieh, Chun-Cheng and Chen, Yen-Hao and Li, Shang-Wen and Lee, Hung-yi},
  year = {2020},
  month = may,
  abstract = {For self-supervised speech processing, it is crucial to use pretrained models as speech representation extractors. In recent works, increasing the size of the model has been utilized in acoustic model training in order to achieve better performance. In this paper, we propose Audio ALBERT, a lite version of the self-supervised speech representation model. We use the representations with two downstream tasks, speaker identification, and phoneme classification. We show that Audio ALBERT is capable of achieving competitive performance with those huge models in the downstream tasks while utilizing 91\textbackslash\% fewer parameters. Moreover, we use some simple probing models to measure how much the information of the speaker and phoneme is encoded in latent representations. In probing experiments, we find that the latent representations encode richer information of both phoneme and speaker than that of the last layer.},
  langid = {english}
}

@inproceedings{child_very_2021,
  title = {Very {{Deep VAEs Generalize Autoregressive Models}} and {{Can Outperform Them}} on {{Images}}},
  booktitle = {Proceedings of the 9th {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Child, Rewon},
  year = {2021},
  abstract = {We present a hierarchical VAE that, for the first time, outperforms the PixelCNN in log-likelihood on all natural image benchmarks. We begin by observing that VAEs can actually implement autoregressive models, and other, more efficient generative models, if made sufficiently deep. Despite this, autoregressive models have traditionally outperformed VAEs. We test if insufficient depth explains why by scaling a VAE to greater stochastic depth than previously explored and evaluating it on CIFAR-10, ImageNet, and FFHQ. We find that, in comparison to the PixelCNN, these very deep VAEs achieve higher likelihoods, use fewer parameters, generate samples thousands of times faster, and are more easily applied to high-resolution images. We visualize the generative process and show the VAEs learn efficient hierarchical visual representations. We release our source code and models at https://github.com/openai/vdvae.}
}

@misc{chiu_self-supervised_2022,
  title = {Self-Supervised {{Learning}} with {{Random-projection Quantizer}} for {{Speech Recognition}}},
  author = {Chiu, Chung-Cheng and Qin, James and Zhang, Yu and Yu, Jiahui and Wu, Yonghui},
  year = {2022},
  month = feb,
  eprint = {2202.01855},
  eprinttype = {arxiv},
  abstract = {We present a simple and effective self-supervised learning approach for speech recognition. The approach learns a model to predict the masked speech signals, in the form of discrete labels generated with a random-projection quantizer. In particular the quantizer projects speech inputs with a randomly initialized matrix, and does a nearest-neighbor lookup in a randomly-initialized codebook. Neither the matrix nor the codebook is updated during self-supervised learning. Since the random-projection quantizer is not trained and is separated from the speech recognition model, the design makes the approach flexible and is compatible with universal speech recognition architecture. On LibriSpeech our approach achieves similar word-error-rates as previous work using self-supervised learning with non-streaming models, and provides lower word-error-rates and latency than wav2vec 2.0 and w2v-BERT with streaming models. On multilingual tasks the approach also provides significant improvement over wav2vec 2.0 and w2v-BERT.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@misc{cho_learning_2014,
  title = {Learning {{Phrase Representations}} Using {{RNN Encoder-Decoder}} for {{Statistical Machine Translation}}},
  author = {Cho, Kyunghyun and {van Merrienboer}, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  year = {2014},
  eprint = {1406.1078},
  eprinttype = {arxiv},
  abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
  archiveprefix = {arXiv},
  isbn = {9781937284961},
  pmid = {2079951}
}

@misc{cho_properties_2014,
  title = {On the Properties of Neural Machine Translation: {{Encoder-decoder}} Approaches},
  author = {Cho, Kyunghyun and {van Merrienboer}, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
  year = {2014},
  eprint = {1409.1259},
  eprinttype = {arxiv},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv}
}

@article{choi_explaining_2016,
  title = {Explaining {{Deep Convolutional Neural Networks}} on {{Music Classification}}},
  author = {Choi, Keunwoo and Fazekas, George and Sandler, Mark},
  year = {2016},
  month = jul,
  abstract = {Deep convolutional neural networks (CNNs) have been actively adopted in the field of music information retrieval, e.g. genre classification, mood detection, and chord recognition. However, the process of learning and prediction is little understood, particularly when it is applied to spectrograms. We introduce auralisation of a CNN to understand its underlying mechanism, which is based on a deconvolution procedure introduced in [2]. Auralisation of a CNN is converting the learned convolutional features that are obtained from deconvolution into audio signals. In the experiments and discussions, we explain trained features of a 5-layer CNN based on the deconvolved spectrograms and auralised signals. The pairwise correlations per layers with varying different musical attributes are also investigated to understand the evolution of the learnt features. It is shown that in the deep layers, the features are learnt to capture textures, the patterns of continuous distributions, rather than shapes of lines.},
  langid = {english}
}

@misc{choi_waic_2019,
  title = {{{WAIC}}, but {{Why}}? {{Generative Ensembles}} for {{Robust Anomaly Detection}}},
  shorttitle = {{{WAIC}}, but {{Why}}?},
  author = {Choi, Hyunsun and Jang, Eric and Alemi, Alexander A.},
  year = {2019},
  month = may,
  eprint = {1810.01392},
  eprinttype = {arxiv},
  abstract = {Machine learning models encounter Out-ofDistribution (OoD) errors when the data seen at test time are generated from a different stochastic generator than the one used to generate the training data. One proposal to scale OoD detection to high-dimensional data is to learn a tractable likelihood approximation of the training distribution, and use it to reject unlikely inputs. However, likelihood models on natural data are themselves susceptible to OoD errors, and even assign large likelihoods to samples from other datasets. To mitigate this problem, we propose Generative Ensembles, which robustify density-based OoD detection by way of estimating epistemic uncertainty of the likelihood model. We present a puzzling observation in need of an explanation \textendash{} although likelihood measures cannot account for the typical set of a distribution, and therefore should not be suitable on their own for OoD detection, WAIC performs surprisingly well in practice.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{choromanska_loss_2014,
  title = {The {{Loss Surfaces}} of {{Multilayer Networks}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}} ({{AISTATS}})},
  author = {Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, G{\'e}rard Ben and LeCun, Yann},
  year = {2014},
  volume = {38},
  eprint = {1412.0233},
  eprinttype = {arxiv},
  issn = {15337928},
  abstract = {We study the connection between the highly non-convex loss function of a simple model of the fully-connected feed-forward neural network and the Hamiltonian of the spherical spin-glass model under the assumptions of: i) variable independence, ii) redundancy in network parametrization, and iii) uniformity. These assumptions enable us to explain the complexity of the fully decoupled neural network through the prism of the results from random matrix theory. We show that for large-size decoupled networks the lowest critical values of the random loss function form a layered structure and they are located in a well-defined band lower-bounded by the global minimum. The number of local minima outside that band diminishes exponentially with the size of the network. We empirically verify that the mathematical model exhibits similar behavior as the computer simulations, despite the presence of high dependencies in real networks. We conjecture that both simulated annealing and SGD converge to the band of low critical points, and that all critical points found there are local minima of high quality measured by the test error. This emphasizes a major difference between large- and small-size networks where for the latter poor quality local minima have non-zero probability of being recovered. Finally, we prove that recovering the global minimum becomes harder as the network size increases and that it is in practice irrelevant as global minimum often leads to overfitting.},
  archiveprefix = {arXiv}
}

@misc{choromanski_rethinking_2021,
  title = {Rethinking {{Attention}} with {{Performers}}},
  author = {Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and Belanger, David and Colwell, Lucy and Weller, Adrian},
  year = {2021},
  month = mar,
  eprint = {2009.14794},
  eprinttype = {arxiv},
  abstract = {We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can be also used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{chorowski_unsupervised_2019,
  title = {Unsupervised {{Speech Representation Learning Using WaveNet Autoencoders}}},
  author = {Chorowski, Jan and Weiss, Ron J. and Bengio, Samy and van den Oord, A{\"a}ron},
  year = {2019},
  month = dec,
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {27},
  number = {12},
  eprint = {1901.08810},
  eprinttype = {arxiv},
  pages = {2041--2053},
  issn = {2329-9290, 2329-9304},
  abstract = {We consider the task of unsupervised extraction of meaningful latent representations of speech by applying autoencoding neural networks to speech waveforms. The goal is to learn a representation able to capture high level semantic content from the signal, e.g. phoneme identities, while being invariant to confounding low level details in the signal such as the underlying pitch contour or background noise. Since the learned representation is tuned to contain only phonetic content, we resort to using a high capacity WaveNet decoder to infer information discarded by the encoder from previous samples. Moreover, the behavior of autoencoder models depends on the kind of constraint that is applied to the latent representation. We compare three variants: a simple dimensionality reduction bottleneck, a Gaussian Variational Autoencoder (VAE), and a discrete Vector Quantized VAE (VQ-VAE). We analyze the quality of learned representations in terms of speaker independence, the ability to predict phonetic content, and the ability to accurately reconstruct individual spectrogram frames. Moreover, for discrete encodings extracted using the VQ-VAE, we measure the ease of mapping them to phonemes. We introduce a regularization scheme that forces the representations to focus on the phonetic content of the utterance and report performance comparable with the top entries in the ZeroSpeech 2017 unsupervised acoustic unit discovery task.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning}
}

@inproceedings{chou_capacity_1988,
  title = {The {{Capacity}} of the {{Kanerva Associative Memory}} Is {{Exponential}}},
  booktitle = {Neural {{Information Processing Systems}}},
  author = {Chou, Philip},
  editor = {Anderson, D.},
  year = {1988},
  pages = {184--191},
  publisher = {{American Institute of Physics}}
}

@book{christensen_functions_2010,
  title = {Functions, {{Spaces}}, and {{Expansions}}},
  author = {Christensen, Ole},
  year = {2010},
  eprint = {1011.1669v3},
  eprinttype = {arxiv},
  publisher = {{Birkh\"auser}},
  address = {{Boston}},
  issn = {1098-6596},
  abstract = {The purpose of this book is to present some mathematical tools that play key roles in mathematics as well as in appliedmathematics, physics, and en- gineering. The treatment is mathematical in nature, and we do not go into concrete applications; but it is important to stress that all the considered topics are selected because they actually play a role outside pure mathe- matics. The hope is that the book will be useful for students in many fields of science and engineering, and professionals who want a deeper insight in some of the topics appearing in the scientific literature.},
  archiveprefix = {arXiv},
  isbn = {978-0-8176-4979-1},
  pmid = {25246403}
}

@article{chung_audio_2016,
  title = {Audio {{Word2Vec}}: {{Unsupervised Learning}} of {{Audio Segment Representations}} Using {{Sequence-to-sequence Autoencoder}}},
  shorttitle = {Audio {{Word2Vec}}},
  author = {Chung, Yu-An and Wu, Chao-Chung and Shen, Chia-Hao and Lee, Hung-Yi and Lee, Lin-Shan},
  year = {2016},
  month = mar,
  abstract = {The vector representations of fixed dimensionality for words (in text) offered by Word2Vec have been shown to be very useful in many application scenarios, in particular due to the semantic information they carry. This paper proposes a parallel version, the Audio Word2Vec. It offers the vector representations of fixed dimensionality for variable-length audio segments. These vector representations are shown to describe the sequential phonetic structures of the audio segments to a good degree, with very attractive real world applications such as query-by-example Spoken Term Detection (STD). In this STD application, the proposed approach significantly outperformed the conventional Dynamic Time Warping (DTW) based approaches at significantly lower computation requirements. We propose unsupervised learning of Audio Word2Vec from audio data without human annotation using Sequence-to-sequence Audoencoder (SA). SA consists of two RNNs equipped with Long Short-Term Memory (LSTM) units: the first RNN (encoder) maps the input audio sequence into a vector representation of fixed dimensionality, and the second RNN (decoder) maps the representation back to the input audio sequence. The two RNNs are jointly trained by minimizing the reconstruction error. Denoising Sequence-to-sequence Autoencoder (DSA) is furthered proposed offering more robust learning.},
  langid = {english}
}

@inproceedings{chung_generative_2020,
  title = {Generative {{Pre-Training}} for {{Speech}} with {{Autoregressive Predictive Coding}}},
  booktitle = {{{ICASSP}} 2020 - 2020 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Chung, Yu-An and Glass, James},
  year = {2020},
  month = may,
  pages = {3497--3501},
  issn = {2379-190X},
  abstract = {Learning meaningful and general representations from unannotated speech that are applicable to a wide range of tasks remains challenging. In this paper we propose to use autoregressive predictive coding (APC), a recently proposed self-supervised objective, as a generative pre-training approach for learning meaningful, non-specific, and transferable speech representations. We pre-train APC on large-scale unlabeled data and conduct transfer learning experiments on three speech applications that require different information about speech characteristics to perform well: speech recognition, speech translation, and speaker identification. Extensive experiments show that APC not only outperforms surface features (e.g., log Mel spectrograms) and other popular representation learning methods on all three tasks, but is also effective at reducing downstream labeled data size and model parameters. We also investigate the use of Transformers for modeling APC and find it superior to RNNs.},
  keywords = {autoregressive modeling,Data models,pre-training,Predictive coding,representation learning,self-supervised learning,Speech coding,Speech recognition,Task analysis,Training,transfer learning,Transfer learning}
}

@misc{chung_hierarchical_2017,
  title = {Hierarchical {{Multiscale Recurrent Neural Networks}}},
  author = {Chung, Junyoung and Ahn, Sungjin and Bengio, Yoshua},
  year = {2017},
  month = mar,
  eprint = {1609.01704},
  eprinttype = {arxiv},
  abstract = {Learning both hierarchical and temporal representation has been among the longstanding challenges of recurrent neural networks. Multiscale recurrent neural networks have been considered as a promising approach to resolve this issue, yet there has been a lack of empirical evidence showing that this type of models can actually capture the temporal dependencies by discovering the latent hierarchical structure of the sequence. In this paper, we propose a novel multiscale approach, called the hierarchical multiscale recurrent neural network, that can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales using a novel update mechanism. We show some evidence that the proposed model can discover underlying hierarchical structure in the sequences without using explicit boundary information. We evaluate our proposed model on character-level language modelling and handwriting sequence generation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning}
}

@misc{chung_improved_2020,
  title = {Improved {{Speech Representations}} with {{Multi-Target Autoregressive Predictive Coding}}},
  author = {Chung, Yu-An and Glass, James},
  year = {2020},
  month = apr,
  eprint = {2004.05274},
  eprinttype = {arxiv},
  abstract = {Training objectives based on predictive coding have recently been shown to be very effective at learning meaningful representations from unlabeled speech. One example is Autoregressive Predictive Coding (Chung et al., 2019), which trains an autoregressive RNN to generate an unseen future frame given a context such as recent past frames. The basic hypothesis of these approaches is that hidden states that can accurately predict future frames are a useful representation for many downstream tasks. In this paper we extend this hypothesis and aim to enrich the information encoded in the hidden states by training the model to make more accurate future predictions. We propose an auxiliary objective that serves as a regularization to improve generalization of the future frame prediction task. Experimental results on phonetic classification, speech recognition, and speech translation not only support the hypothesis, but also demonstrate the effectiveness of our approach in learning representations that contain richer phonetic content.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@article{chung_learning_2017,
  title = {Learning {{Word Embeddings}} from {{Speech}}},
  author = {Chung, Yu-An and Glass, James},
  year = {2017},
  month = nov,
  journal = {arXiv:1711.01515 [cs]},
  eprint = {1711.01515},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In this paper, we propose a novel deep neural network architecture, Sequence-to-Sequence Audio2Vec, for unsupervised learning of fixed-length vector representations of audio segments excised from a speech corpus, where the vectors contain semantic information pertaining to the segments, and are close to other vectors in the embedding space if their corresponding segments are semantically similar. The design of the proposed model is based on the RNN Encoder-Decoder framework, and borrows the methodology of continuous skip-grams for training. The learned vector representations are evaluated on 13 widely used word similarity benchmarks, and achieved competitive results to that of GloVe. The biggest advantage of the proposed model is its capability of extracting semantic information of audio segments taken directly from raw speech, without relying on any other modalities such as text or images, which are challenging and expensive to collect and annotate.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@inproceedings{chung_recurrent_2015,
  title = {A {{Recurrent Latent Variable Model}} for {{Sequential Data}}},
  booktitle = {Proceedings of the 29th {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Chung, Junyoung and Kastner, Kyle and Dinh, Laurent and Goel, Kratarth and Courville, Aaron C and Bengio, Yoshua},
  year = {2015},
  pages = {9},
  address = {{Montr\'eal, Quebec, Canada}},
  abstract = {In this paper, we explore the inclusion of latent random variables into the hidden state of a recurrent neural network (RNN) by combining the elements of the variational autoencoder. We argue that through the use of high-level latent random variables, the variational RNN (VRNN)1 can model the kind of variability observed in highly structured sequential data such as natural speech. We empirically evaluate the proposed model against other related sequential models on four speech datasets and one handwriting dataset. Our results show the important roles that latent random variables can play in the RNN dynamics.},
  langid = {english},
  keywords = {⛔ No DOI found}
}

@inproceedings{chung_similarity_2021,
  title = {Similarity {{Analysis}} of {{Self-Supervised Speech Representations}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Chung, Yu-An and Belinkov, Yonatan and Glass, James},
  year = {2021},
  month = jun,
  pages = {3040--3044},
  issn = {2379-190X},
  abstract = {Self-supervised speech representation learning has recently been a prosperous research topic. Many algorithms have been proposed for learning useful representations from large-scale unlabeled data, and their applications to a wide range of speech tasks have also been investigated. However, there has been little research focusing on understanding the properties of existing approaches. In this work, we aim to provide a comparative study of some of the most representative self-supervised algorithms. Specifically, we quantify the similarities between different self-supervised representations using existing similarity measures. We also design probing tasks to study the correlation between the models' pre-training loss and the amount of specific speech information contained in their learned representations. In addition to showing how various self-supervised models behave differently given the same input, our study also finds that the training objective has a higher impact on representation similarity than architectural choices such as building blocks (RNN/Transformer/CNN) and directionality (uni/bidirectional). Our results also suggest that there exists a strong correlation between pre-training loss and downstream performance for some self-supervised algorithms.},
  keywords = {comparative analysis,Correlation,Phonetics,Predictive coding,Self-supervised learning,Signal processing algorithms,Speech coding,speech representation learning,Training,Training data,unsupervised pre-training}
}

@article{chung_speech2vec_2018,
  title = {{{Speech2Vec}}: {{A Sequence-to-Sequence Framework}} for {{Learning Word Embeddings}} from {{Speech}}},
  shorttitle = {{{Speech2Vec}}},
  author = {Chung, Yu-An and Glass, James},
  year = {2018},
  month = jun,
  journal = {arXiv:1803.08976 [cs]},
  eprint = {1803.08976},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In this paper, we propose a novel deep neural network architecture, Speech2Vec, for learning fixed-length vector representations of audio segments excised from a speech corpus, where the vectors contain semantic information pertaining to the underlying spoken words, and are close to other vectors in the embedding space if their corresponding underlying spoken words are semantically similar. The proposed model can be viewed as a speech version of Word2Vec. Its design is based on a RNN Encoder-Decoder framework, and borrows the methodology of skipgrams or continuous bag-of-words for training. Learning word embeddings directly from speech enables Speech2Vec to make use of the semantic information carried by speech that does not exist in plain text. The learned word embeddings are evaluated and analyzed on 13 widely used word similarity benchmarks, and outperform word embeddings learned by Word2Vec from the transcriptions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{chung_splat_2021,
  title = {{{SPLAT}}: {{Speech-Language Joint Pre-Training}} for {{Spoken Language Understanding}}},
  shorttitle = {{{SPLAT}}},
  author = {Chung, Yu-An and Zhu, Chenguang and Zeng, Michael},
  year = {2021},
  month = mar,
  eprint = {2010.02295},
  eprinttype = {arxiv},
  abstract = {Spoken language understanding (SLU) requires a model to analyze input acoustic signal to understand its linguistic content and make predictions. To boost the models' performance, various pre-training methods have been proposed to learn rich representations from large-scale unannotated speech and text. However, the inherent disparities between the two modalities necessitate a mutual analysis. In this paper, we propose a novel semi-supervised learning framework, SPLAT, to jointly pre-train the speech and language modules. Besides conducting a self-supervised masked language modeling task on the two individual modules using unpaired speech and text, SPLAT aligns representations from the two modules in a shared latent space using a small amount of paired speech and text. Thus, during fine-tuning, the speech module alone can produce representations carrying both acoustic information and contextual semantic knowledge of an input acoustic signal. Experimental results verify the effectiveness of our approach on various SLU tasks. For example, SPLAT improves the previous state-of-the-art performance on the Spoken SQuAD dataset by more than 10\%.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{chung_unsupervised_2019,
  title = {An {{Unsupervised Autoregressive Model}} for {{Speech Representation Learning}}},
  author = {Chung, Yu-An and Hsu, Wei-Ning and Tang, Hao and Glass, James},
  year = {2019},
  month = jun,
  eprint = {1904.03240},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {This paper proposes a novel unsupervised autoregressive neural model for learning generic speech representations. In contrast to other speech representation learning methods that aim to remove noise or speaker variabilities, ours is designed to preserve information for a wide range of downstream tasks. In addition, the proposed model does not require any phonetic or word boundary labels, allowing the model to benefit from large quantities of unlabeled data. Speech representations learned by our model significantly improve performance on both phone classification and speaker verification over the surface features and other supervised and unsupervised approaches. Further analysis shows that different levels of speech information are captured by our model at different layers. In particular, the lower layers tend to be more discriminative for speakers, while the upper layers provide more phonetic content.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@article{chung_vector-quantized_2020,
  title = {Vector-{{Quantized Autoregressive Predictive Coding}}},
  author = {Chung, Yu-An and Tang, Hao and Glass, James},
  year = {2020},
  month = may,
  journal = {arXiv:2005.08392 [cs, eess]},
  eprint = {2005.08392},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {Autoregressive Predictive Coding (APC), as a self-supervised objective, has enjoyed success in learning representations from large amounts of unlabeled data, and the learned representations are rich for many downstream tasks. However, the connection between low self-supervised loss and strong performance in downstream tasks remains unclear. In this work, we propose Vector-Quantized Autoregressive Predictive Coding (VQ-APC), a novel model that produces quantized representations, allowing us to explicitly control the amount of information encoded in the representations. By studying a sequence of increasingly limited models, we reveal the constituents of the learned representations. In particular, we confirm the presence of information with probing tasks, while showing the absence of information with mutual information, uncovering the model's preference in preserving speech information as its capacity becomes constrained. We find that there exists a point where phonetic and speaker information are amplified to maximize a self-supervised objective. As a byproduct, the learned codes for a particular model capacity correspond well to English phones.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@article{ciresan_multi-column_2012,
  title = {Multi-Column Deep Neural Network for Traffic Sign Classification},
  author = {Cire{\c s}an, Dan and Meier, Ueli and Masci, Jonathan and Schmidhuber, J{\"u}rgen},
  year = {2012},
  month = aug,
  journal = {Proceedings of the International Joint Conference on Neural Networks (IJCNN)},
  volume = {32},
  number = {Sp. Iss. SI},
  eprint = {1202.2745v1},
  eprinttype = {arxiv},
  pages = {333--338},
  issn = {08936080},
  abstract = {We describe the approach that won the final phase of the German traffic sign recognition benchmark. Our method is the only one that achieved a better-than-human recognition rate of 99.46\%. We use a fast, fully parameterizable GPU implementation of a Deep Neural Network (DNN) that does not require careful design of pre-wired feature extractors, which are rather learned in a supervised way. Combining various DNNs trained on differently preprocessed data into a Multi-Column DNN (MCDNN) further boosts recognition performance, making the system insensitive also to variations in contrast and illumination. \textcopyright{} 2012 Elsevier Ltd.},
  archiveprefix = {arXiv},
  isbn = {0893-6080},
  pmid = {22386783},
  keywords = {Deep neural networks,Image classification,Image preprocessing,Traffic signs}
}

@article{ciresan_multi-column_2012-1,
  title = {Multi-Column {{Deep Neural Networks}} for {{Image Classification}}},
  author = {Cire{\c s}an, Dan and Meier, Ueli and Schmidhuber, J{\"u}rgen},
  year = {2012},
  month = feb,
  journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CCVPR)},
  eprint = {1202.2745},
  eprinttype = {arxiv},
  pages = {3642--3649},
  issn = {10636919},
  abstract = {Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks.},
  archiveprefix = {arXiv},
  isbn = {9781467312264},
  pmid = {22386783}
}

@misc{clanuwat_deep_2018,
  title = {Deep {{Learning}} for {{Classical Japanese Literature}}},
  author = {Clanuwat, Tarin and {Bober-Irizar}, Mikel and Kitamoto, Asanobu and Lamb, Alex and Yamamoto, Kazuaki and Ha, David},
  year = {2018},
  month = dec,
  eprint = {1812.01718},
  eprinttype = {arxiv},
  abstract = {Much of machine learning research focuses on producing models which perform well on benchmark tasks, in turn improving our understanding of the challenges associated with those tasks. From the perspective of ML researchers, the content of the task itself is largely irrelevant, and thus there have increasingly been calls for benchmark tasks to more heavily focus on problems which are of social or cultural relevance. In this work, we introduce Kuzushiji-MNIST, a dataset which focuses on Kuzushiji (cursive Japanese), as well as two larger, more challenging datasets, Kuzushiji-49 and Kuzushiji-Kanji. Through these datasets, we wish to engage the machine learning community into the world of classical Japanese literature.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{clark_improving_2016,
  title = {Improving {{Coreference Resolution}} by {{Learning Entity-Level Distributed Representations}}},
  author = {Clark, Kevin and Manning, Christopher D.},
  year = {2016},
  eprint = {1606.01323},
  eprinttype = {arxiv},
  abstract = {A long-standing challenge in coreference resolution has been the incorporation of entity-level information - features defined over clusters of mentions instead of mention pairs. We present a neural network based coreference system that produces high-dimensional vector representations for pairs of coreference clusters. Using these representations, our system learns when combining clusters is desirable. We train the system with a learning-to-search algorithm that teaches it which local decisions (cluster merges) will lead to a high-scoring final coreference partition. The system substantially outperforms the current state-of-the-art on the English and Chinese portions of the CoNLL 2012 Shared Task dataset despite using few hand-engineered features.},
  archiveprefix = {arXiv},
  isbn = {9781510827585}
}

@article{clark_learning_2015,
  title = {Learning {{Knowledge Graphs}} for {{Question Answering}} through {{Conversational Dialog}}},
  author = {Clark, Peter and Hixon, Ben},
  year = {2015},
  journal = {The Annual Conference of the North American Chapter of the ACL},
  pages = {851--861},
  abstract = {We describe how a question-answering system can learn about its domain from conver-sational dialogs. Our system learns to relate concepts in science questions to propositions in a fact corpus, stores new concepts and re-lations in a knowledge graph (KG), and uses the graph to solve questions. We are the first to acquire knowledge for question-answering from open, natural language dialogs without a fixed ontology or domain model that predeter-mines what users can say. Our relation-based strategies complete more successful dialogs than a query expansion baseline, our task-driven relations are more effective for solving science questions than relations from general knowledge sources, and our method is practical enough to generalize to other domains.},
  isbn = {9781941643495}
}

@misc{clevert_fast_2015,
  title = {Fast and {{Accurate Deep Network Learning}} by {{Exponential Linear Units}} ({{ELUs}})},
  author = {Clevert, Djork-Arn{\'e} and Unterthiner, Thomas and Hochreiter, Sepp},
  year = {2015},
  eprint = {1511.07289},
  eprinttype = {arxiv},
  abstract = {We introduce the "exponential linear unit" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PReLUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However, ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore, ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks significantly outperform ReLU networks with batch normalization while batch normalization does not improve ELU networks. ELU networks are among the top 10 reported CIFAR-10 results and yield the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10\% classification error for a single crop, single model network.},
  archiveprefix = {arXiv},
  isbn = {9781614996712}
}

@misc{coalson_free_2019,
  title = {Free {{Lossless Audio Encoding}} ({{FLAC}})},
  author = {Coalson, Josh and de Castro Lopo, Erik},
  year = {2019},
  month = aug,
  abstract = {FLAC stands for Free Lossless Audio Codec, an audio format similar to MP3, but lossless, meaning that audio is compressed in FLAC without any loss in quality. This is similar to how Zip works, except with FLAC you will get much better compression because it is designed specifically for audio, and you can play back compressed FLAC files in your favorite player (or your car or home stereo, see supported devices) just like you would an MP3 file. FLAC stands out as the fastest and most widely supported lossless audio codec, and the only one that at once is non-proprietary, is unencumbered by patents, has an open-source reference implementation, has a well documented format and API, and has several other independent implementations.},
  copyright = {GNU GPL (Command-line tools) and BSD (Libraries)},
  howpublished = {xiph.org}
}

@phdthesis{cohen_equivariant_2021,
  title = {Equivariant {{Convolutional Networks}}},
  author = {Cohen, Taco},
  year = {2021},
  address = {{Amsterdam, Netherlands}},
  school = {University of Amsterdam}
}

@misc{coles_quantum_2018,
  title = {Quantum {{Algorithm Implementations}} for {{Beginners}}},
  author = {Coles, Patrick J. and Eidenbenz, Stephan and Pakin, Scott and Adedoyin, Adetokunbo and Ambrosiano, John and Anisimov, Petr and Casper, William and Chennupati, Gopinath and Coffrin, Carleton and Djidjev, Hristo and Gunter, David and Karra, Satish and Lemons, Nathan and Lin, Shizeng and Lokhov, Andrey and Malyzhenkov, Alexander and Mascarenas, David and Mniszewski, Susan and Nadiga, Balu and O'Malley, Dan and Oyen, Diane and Prasad, Lakshman and Roberts, Randy and Romero, Phil and Santhi, Nandakishore and Sinitsyn, Nikolai and Swart, Pieter and Vuffray, Marc and Wendelberger, Jim and Yoon, Boram and Zamora, Richard and Zhu, Wei},
  year = {2018},
  eprint = {1804.03719},
  eprinttype = {arxiv},
  abstract = {As quantum computers have become available to the general public, the need has arisen to train a cohort of quantum programmers, many of whom have been developing classic computer programs for most of their career. While currently available quantum computers have less than 100 qubits, quantum computer hardware is widely expected to grow in terms of qubit counts, quality, and connectivity. Our article aims to explain the principles of quantum programming, which are quite different from classical programming, with straight-forward algebra that makes understanding the underlying quantum mechanics optional (but still fascinating). We give an introduction to quantum computing algorithms and their implementation on real quantum hardware. We survey 20 different quantum algorithms, attempting to describe each in a succintc and self-contained fashion; we show how they are implemented on IBM's quantum computer; and in each case we discuss the results of the implementation with respect to differences of the simulator and the actual hardware runs. This article introduces computer scientists and engineers to quantum algorithms and provides a blueprint for their implementations.},
  archiveprefix = {arXiv}
}

@inproceedings{collier_vaes_2021,
  title = {{{VAEs}} in the {{Presence}} of {{Missing Data}}},
  booktitle = {Proceedings of the {{ICML Workshop}} on the {{Art}} of {{Learning}} with {{Missing Values}} ({{Artemiss}})},
  author = {Collier, Mark and Nazabal, Alfredo and Williams, Christopher K. I.},
  year = {2021},
  month = mar,
  eprint = {2006.05301},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Real world datasets often contain entries with missing elements e.g. in a medical dataset, a patient is unlikely to have taken all possible diagnostic tests. Variational Autoencoders (VAEs) are popular generative models often used for unsupervised learning. Despite their widespread use it is unclear how best to apply VAEs to datasets with missing data. We develop a novel latent variable model of a corruption process which generates missing data, and derive a corresponding tractable evidence lower bound (ELBO). Our model is straightforward to implement, can handle both missing completely at random (MCAR) and missing not at random (MNAR) data, scales to high dimensional inputs and gives both the VAE encoder and decoder principled access to indicator variables for whether a data element is missing or not. On the MNIST and SVHN datasets we demonstrate improved marginal log-likelihood of observed data and better missing data imputation, compared to existing approaches.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{collins_log-linear_nodate,
  title = {Log-{{Linear Models}}, {{MEMMs}}, and {{CRFs}}},
  author = {Collins, Michael},
  pages = {11},
  langid = {english},
  keywords = {⛔ No DOI found}
}

@article{collobert_fast_nodate,
  title = {Fast {{Semantic Extraction Using}} a {{Novel Neural Network Architecture}}},
  author = {Collobert, Ronan and Weston, Jason},
  abstract = {We describe a novel neural network architecture for the problem of semantic role labeling. Many current solutions are complicated , consist of several stages and hand-built features, and are too slow to be applied as part of real applications that require such semantic labels, partly because of their use of a syntactic parser (Pradhan et al., 2004; Gildea and Jurafsky, 2002). Our method instead learns a direct mapping from source sentence to semantic tags for a given predicate without the aid of a parser or a chun-ker. Our resulting system obtains accuracies comparable to the current state-of-the-art at a fraction of the computational cost.}
}

@article{collobert_natural_2011,
  title = {Natural {{Language Processing}} (Almost) from {{Scratch}}},
  author = {Collobert, Ronan and Weston, Jason and Bottou, L{\'e}on and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel P.},
  year = {2011},
  journal = {Computing Research Repository},
  volume = {abs/1103.0},
  pages = {1--34},
  abstract = {We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.},
  keywords = {Deep learning,Natural Language Processing,Neural network}
}

@inproceedings{collobert_unified_2008,
  title = {A Unified Architecture for Natural Language Processing},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Collobert, Ronan and Weston, Jason},
  year = {2008},
  eprint = {1603.06111},
  eprinttype = {arxiv},
  pages = {160--167},
  issn = {07224028},
  abstract = {We describe a single convolutional neural net- work architecture that, given a sentence, out- puts a host of language processing predic- tions: part-of-speech tags, chunks, named en- tity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semanti- cally) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data ex- cept the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the general- ization of the shared tasks, resulting in state- of-the-art performance.},
  archiveprefix = {arXiv},
  isbn = {978-1-60558-205-4},
  pmid = {2975184}
}

@misc{collobert_wav2letter:_2016,
  title = {{{Wav2Letter}}: An {{End-to-End ConvNet-based Speech Recognition System}}},
  author = {Collobert, Ronan and Puhrsch, Christian and Synnaeve, Gabriel},
  year = {2016},
  month = sep,
  eprint = {1609.03193},
  eprinttype = {arxiv},
  abstract = {This paper presents a simple end-to-end model for speech recognition, combining a convolutional network based acoustic model and a graph decoding. It is trained to output letters, with transcribed speech, without the need for force alignment of phonemes. We introduce an automatic segmentation criterion for training from sequence annotation without alignment that is on par with CTC while being simpler. We show competitive results in word error rate on the Librispeech corpus with MFCC features, and promising results from raw waveform.},
  archiveprefix = {arXiv}
}

@article{combes_learning_2018,
  title = {On the {{Learning Dynamics}} of {{Deep Neural Networks}}},
  author = {des Combes, Remi Tachet and Pezeshki, Mohammad and Shabanian, Samira and Courville, Aaron and Bengio, Yoshua},
  year = {2018},
  month = sep,
  abstract = {While a lot of progress has been made in recent years, the dynamics of learning in deep nonlinear neural networks remain to this day largely misunderstood. In this work, we study the case of binary classification and prove various properties of learning in such networks under strong assumptions such as linear separability of the data. Extending existing results from the linear case, we confirm empirical observations by proving that the classification error also follows a sigmoidal shape in nonlinear architectures. We show that given proper initialization, learning expounds parallel independent modes and that certain regions of parameter space might lead to failed training. We also demonstrate that input norm and features' frequency in the dataset lead to distinct convergence speeds which might shed some light on the generalization capabilities of deep neural networks. We provide a comparison between the dynamics of learning with cross-entropy and hinge losses, which could prove useful to understand recent progress in the training of generative adversarial networks. Finally, we identify a phenomenon that we baptize gradient starvation where the most frequent features in a dataset prevent the learning of other less frequent but equally informative features.},
  langid = {english}
}

@article{conneau_unsupervised_2020,
  title = {Unsupervised {{Cross-lingual Representation Learning}} for {{Speech Recognition}}},
  author = {Conneau, Alexis and Baevski, Alexei and Collobert, Ronan and Mohamed, Abdelrahman and Auli, Michael},
  year = {2020},
  month = dec,
  journal = {arXiv:2006.13979 [cs, eess]},
  eprint = {2006.13979},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {This paper presents XLSR which learns cross-lingual speech representations by pretraining a single model from the raw waveform of speech in multiple languages. We build on wav2vec 2.0 which is trained by solving a contrastive task over masked latent speech representations and jointly learns a quantization of the latents shared across languages. The resulting model is fine-tuned on labeled data and experiments show that cross-lingual pretraining significantly outperforms monolingual pretraining. On the CommonVoice benchmark, XLSR shows a relative phoneme error rate reduction of 72\% compared to the best known results. On BABEL, our approach improves word error rate by 16\% relative compared to a comparable system. Our approach enables a single multilingual speech recognition model which is competitive to strong individual models. Analysis shows that the latent discrete speech representations are shared across languages with increased sharing for related languages. We hope to catalyze research in low-resource speech understanding by releasing XLSR-53, a large model pretrained in 53 languages.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@misc{conti_improving_2017,
  title = {Improving {{Exploration}} in {{Evolution Strategies}} for {{Deep Reinforcement Learning}} via a {{Population}} of {{Novelty-Seeking Agents}}},
  author = {Conti, Edoardo and Madhavan, Vashisht and Such, Felipe Petroski and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
  year = {2017},
  month = dec,
  eprint = {1712.06560},
  eprinttype = {arxiv},
  abstract = {Evolution strategies (ES) are a family of black-box optimization algorithms able to train deep neural networks roughly as well as Q-learning and policy gradient methods on challenging deep reinforcement learning (RL) problems, but are much faster (e.g. hours vs. days) because they parallelize better. However, many RL problems require directed exploration because they have reward functions that are sparse or deceptive (i.e. contain local optima), and it is not known how to encourage such exploration with ES. Here we show that algorithms that have been invented to promote directed exploration in small-scale evolved neural networks via populations of exploring agents, specifically novelty search (NS) and quality diversity (QD) algorithms, can be hybridized with ES to improve its performance on sparse or deceptive deep RL tasks, while retaining scalability. Our experiments confirm that the resultant new algorithms, NS-ES and a version of QD we call NSR-ES, avoid local optima encountered by ES to achieve higher performance on tasks ranging from playing Atari to simulated robots learning to walk around a deceptive trap. This paper thus introduces a family of fast, scalable algorithms for reinforcement learning that are capable of directed exploration. It also adds this new family of exploration algorithms to the RL toolbox and raises the interesting possibility that analogous algorithms with multiple simultaneous paths of exploration might also combine well with existing RL algorithms outside ES.},
  archiveprefix = {arXiv}
}

@article{cortes_support-vector_1995,
  title = {Support-{{Vector Networks}}},
  author = {Cortes, Corinna and Vapnik, Vladimir},
  year = {1995},
  journal = {Journal of Machine Learning},
  volume = {20},
  number = {3},
  pages = {273--297},
  abstract = {Oil/water partition coefficient (log P) is one of the key points for lead compound to be drug. In silico log P models based solely on chemical structures have become an important part of modern drug discovery. Here, we report support vector machines, radial basis function neural networks, and multiple linear regression methods to investigate the correlation between partition coefficient and physico-chemical descriptors for a large data set of compounds. The correlation coefficient r (2) between experimental and predicted log P for training and test sets by support vector machines, radial basis function neural networks, and multiple linear regression is 0.92, 0.90, and 0.88, respectively. The results show that non-linear support vector machines derives statistical models that have better prediction ability than those of radial basis function neural networks and multiple linear regression methods. This indicates that support vector machines can be used as an alternative modeling tool for quantitative structure-property/activity relationships studies.},
  keywords = {Efficient learning algorithms,Neural network,Pattern recognition,Polynomial classifiers,Radial basis function classifiers}
}

@misc{coucke_snips_2018,
  title = {Snips {{Voice Platform}}: An Embedded {{Spoken Language Understanding}} System for Private-by-Design Voice Interfaces},
  shorttitle = {Snips {{Voice Platform}}},
  author = {Coucke, Alice and Saade, Alaa and Ball, Adrien and Bluche, Th{\'e}odore and Caulier, Alexandre and Leroy, David and Doumouro, Cl{\'e}ment and Gisselbrecht, Thibault and Caltagirone, Francesco and Lavril, Thibaut and Primet, Ma{\"e}l and Dureau, Joseph},
  year = {2018},
  month = may,
  eprint = {1805.10190},
  eprinttype = {arxiv},
  abstract = {This paper presents the machine learning architecture of the Snips Voice Platform, a software solution to perform Spoken Language Understanding on microprocessors typical of IoT devices. The embedded inference is fast and accurate while enforcing privacy by design, as no personal user data is ever collected. Focusing on Automatic Speech Recognition and Natural Language Understanding, we detail our approach to training high-performance Machine Learning models that are small enough to run in real-time on small devices. Additionally, we describe a data generation procedure that provides sufficient, high-quality training data without compromising user privacy.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing}
}

@misc{courbariaux_binarized_2016,
  title = {Binarized {{Neural Networks}}: {{Training Deep Neural Networks}} with {{Weights}} and {{Activations Constrained}} to +1 or -1},
  author = {Courbariaux, Matthieu and Hubara, Itay and Soudry, Daniel and {El-Yaniv}, Ran and Bengio, Yoshua},
  year = {2016},
  month = feb,
  eprint = {1602.02830},
  eprinttype = {arxiv},
  abstract = {We introduce a method to train Binarized Neural Networks (BNNs) - neural networks with binary weights and activations at run-time. At training-time the binary weights and activations are used for computing the parameters gradients. During the forward pass, BNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations, which is expected to substantially improve power-efficiency. To validate the effectiveness of BNNs we conduct two sets of experiments on the Torch7 and Theano frameworks. On both, BNNs achieved nearly state-of-the-art results over the MNIST, CIFAR-10 and SVHN datasets. Last but not least, we wrote a binary matrix multiplication GPU kernel with which it is possible to run our MNIST BNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The code for training and running our BNNs is available on-line.},
  archiveprefix = {arXiv},
  isbn = {9781510829008},
  pmid = {23554596}
}

@misc{crabbe_explaining_2021,
  title = {Explaining {{Time Series Predictions}} with {{Dynamic Masks}}},
  author = {Crabb{\'e}, Jonathan and {van der Schaar}, Mihaela},
  year = {2021},
  month = jun,
  eprint = {2106.05303},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {How can we explain the predictions of a machine learning model? When the data is structured as a multivariate time series, this question induces additional difficulties such as the necessity for the explanation to embody the time dependency and the large number of inputs. To address these challenges, we propose dynamic masks (Dynamask). This method produces instance-wise importance scores for each feature at each time step by fitting a perturbation mask to the input sequence. In order to incorporate the time dependency of the data, Dynamask studies the effects of dynamic perturbation operators. In order to tackle the large number of inputs, we propose a scheme to make the feature selection parsimonious (to select no more feature than necessary) and legible (a notion that we detail by making a parallel with information theory). With synthetic and real-world data, we demonstrate that the dynamic underpinning of Dynamask, together with its parsimony, offer a neat improvement in the identification of feature importance over time. The modularity of Dynamask makes it ideal as a plug-in to increase the transparency of a wide range of machine learning models in areas such as medicine and finance, where time series are abundant.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@inproceedings{cremer_inference_2018,
  title = {Inference {{Suboptimality}} in {{Variational Autoencoders}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Cremer, Chris and Li, Xuechen and Duvenaud, David},
  editor = {Dy, Jennifer and Krause, Andreas},
  year = {2018},
  month = jul,
  series = {Proceedings of Machine Learning Research},
  volume = {80},
  pages = {1078--1086},
  publisher = {{PMLR}},
  address = {{Stockholmsm\"assan, Stockholm, Sweden}},
  abstract = {Amortized inference allows latent-variable models trained via variational learning to scale to large datasets. The quality of approximate inference is determined by two factors: a) the capacity of the variational distribution to match the true posterior and b) the ability of the recognition network to produce good variational parameters for each datapoint. We examine approximate inference in variational autoencoders in terms of these factors. We find that divergence from the true posterior is often due to imperfect recognition networks, rather than the limited complexity of the approximating distribution. We show that this is due partly to the generator learning to accommodate the choice of approximation. Furthermore, we show that the parameters used to increase the expressiveness of the approximation play a role in generalizing inference rather than simply improving the complexity of the approximation.}
}

@article{cremer_reinterpreting_2017,
  title = {Reinterpreting {{Importance-Weighted Autoencoders}}},
  author = {Cremer, Chris and Morris, Quaid and Duvenaud, David},
  year = {2017},
  pages = {6},
  abstract = {The standard interpretation of importance-weighted autoencoders is that they maximize a tighter lower bound on the marginal likelihood. We give an alternate interpretation of this procedure: that it optimizes the standard variational lower bound, but using a more complex distribution. We formally derive this result, and visualize the implicit importance-weighted approximate posterior.},
  langid = {english},
  keywords = {⛔ No DOI found}
}

@misc{cuayahuitl_strategic_2015,
  title = {Strategic {{Dialogue Management}} via {{Deep Reinforcement Learning}}},
  author = {Cuay{\'a}huitl, Heriberto and Keizer, Simon and Lemon, Oliver},
  year = {2015},
  month = nov,
  eprint = {1511.08099},
  eprinttype = {arxiv},
  abstract = {Artificially intelligent agents equipped with strategic skills that can negotiate during their interactions with other natural or artificial agents are still underdeveloped. This paper describes a successful application of Deep Reinforcement Learning (DRL) for training intelligent agents with strategic conversational skills, in a situated dialogue setting. Previous studies have modelled the behaviour of strategic agents using supervised learning and traditional reinforcement learning techniques, the latter using tabular representations or learning with linear function approximation. In this study, we apply DRL with a high-dimensional state space to the strategic board game of Settlers of Catan\textemdash where players can offer resources in exchange for others and they can also reply to offers made by other players. Our experimental results report that the DRL-based learnt policies significantly outperformed several baselines including random, rule-based, and supervised-based behaviours. The DRL-based policy has a 53\% win rate versus 3 automated players (`bots'), whereas a supervised player trained on a dialogue corpus in this setting achieved only 27\%, versus the same 3 bots. This result supports the claim that DRL is a promising framework for training dialogue systems, and strategic agents with negotiation abilities.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@article{dahl_context-dependent_2012,
  title = {Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition},
  author = {Dahl, George E. and Yu, Dong and Deng, Li and Acero, Alex},
  year = {2012},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  volume = {20},
  number = {1},
  pages = {30--42},
  publisher = {{IEEE}}
}

@misc{dai_semi-supervised_2015,
  title = {Semi-Supervised {{Sequence Learning}}},
  author = {Dai, Andrew M. and Le, Quoc V.},
  year = {2015},
  eprint = {1511.01432},
  eprinttype = {arxiv},
  abstract = {We present two approaches that use unlabeled data to improve sequence learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a "pretraining" step for a later supervised sequence learning algorithm. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after being pretrained with the two approaches are more stable and generalize better. With pretraining, we are able to train long short term memory recurrent networks up to a few hundred timesteps, thereby achieving strong performance in many text classification tasks, such as IMDB, DBpedia and 20 Newsgroups.},
  archiveprefix = {arXiv},
  pmid = {414454}
}

@misc{dai_transformer-xl_2019,
  title = {Transformer-{{XL}}: {{Attentive Language Models Beyond}} a {{Fixed-Length Context}}},
  shorttitle = {Transformer-{{XL}}},
  author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V. and Salakhutdinov, Ruslan},
  year = {2019},
  month = jun,
  eprint = {1901.02860},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, TransformerXL learns dependency that is 80\% longer than RNNs and 450\% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-ofthe-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch1.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{dai_very_2017,
  title = {Very Deep Convolutional Neural Networks for Raw Waveforms},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Dai, W and Dai, Chia and Qu, Shuhui and Li, Juncheng and Das, Samarjit},
  year = {2017},
  eprint = {1610.00087},
  eprinttype = {arxiv},
  pages = {421--425},
  issn = {19909772},
  abstract = {Learning acoustic models directly from the raw waveform data with minimal processing is challenging. Current waveform-based models have generally used very few (\textasciitilde 2) convolutional layers, which might be insufficient for building high-level discriminative features. In this work, we propose very deep convolutional neural networks (CNNs) that directly use time-domain waveforms as inputs. Our CNNs, with up to 34 weight layers, are efficient to optimize over very long sequences (e.g., vector of size 32000), necessary for processing acoustic waveforms. This is achieved through batch normalization, residual learning, and a careful design of down-sampling in the initial layers. Our networks are fully convolutional, without the use of fully connected layers and dropout, to maximize representation learning. We use a large receptive field in the first convolutional layer to mimic bandpass filters, but very small receptive fields subsequently to control the model capacity. We demonstrate the performance gains with the deeper models. Our evaluation shows that the CNN with 18 weight layers outperforms the CNN with 3 weight layers by over 15\% in absolute accuracy for an environmental sound recognition task and is competitive with the performance of models using log-mel features.},
  archiveprefix = {arXiv},
  isbn = {VO -},
  keywords = {Acoustic modeling,Automatic speech recognition,Band-pass filters,Convolutional neural networks,Raw waveform,Representation learning,Residual learning,Time-domain analysis}
}

@inproceedings{damianou_deep_2013,
  title = {Deep {{Gaussian Processes}}},
  booktitle = {Proceedings of the 16th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}} ({{AISTATS}})},
  author = {Damianou, Andreas and Lawrence, Neil},
  editor = {Carvalho, Carlos M. and Ravikumar, Pradeep},
  year = {2013},
  month = apr,
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {31},
  pages = {207--215},
  publisher = {{PMLR}},
  address = {{Scottsdale, Arizona, USA}},
  abstract = {In this paper we introduce deep Gaussian process (GP) models. Deep GPs are a deep belief network based on Gaussian process mappings. The data is modeled as the output of a multivariate GP. The inputs to that Gaussian process are then governed by another GP. A single layer model is equivalent to a standard GP or the GP latent variable model (GP-LVM). We perform inference in the model by approximate variational marginalization. This results in a strict lower bound on the marginal likelihood of the model which we use for model selection (number of layers and nodes per layer). Deep belief networks are typically applied to relatively large data sets using stochastic gradient descent for optimization. Our fully Bayesian treatment allows for the application of deep models even when data is scarce. Model selection by our variational bound shows that a five layer hierarchy is justified even when modelling a digit data set containing only 150 examples.}
}

@misc{damour_underspecification_2020,
  title = {Underspecification {{Presents Challenges}} for {{Credibility}} in {{Modern Machine Learning}}},
  author = {D'Amour, Alexander and Heller, Katherine and Moldovan, Dan and Adlam, Ben and Alipanahi, Babak and Beutel, Alex and Chen, Christina and Deaton, Jonathan and Eisenstein, Jacob and Hoffman, Matthew D. and Hormozdiari, Farhad and Houlsby, Neil and Hou, Shaobo and Jerfel, Ghassen and Karthikesalingam, Alan and Lucic, Mario and Ma, Yian and McLean, Cory and Mincu, Diana and Mitani, Akinori and Montanari, Andrea and Nado, Zachary and Natarajan, Vivek and Nielson, Christopher and Osborne, Thomas F. and Raman, Rajiv and Ramasamy, Kim and Sayres, Rory and Schrouff, Jessica and Seneviratne, Martin and Sequeira, Shannon and Suresh, Harini and Veitch, Victor and Vladymyrov, Max and Wang, Xuezhi and Webster, Kellie and Yadlowsky, Steve and Yun, Taedong and Zhai, Xiaohua and Sculley, D.},
  year = {2020},
  month = nov,
  eprint = {2011.03395},
  eprinttype = {arxiv},
  abstract = {ML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification as a key reason for these failures. An ML pipeline is underspecified when it can return many predictors with equivalently strong held-out performance in the training domain. Underspecification is common in modern ML pipelines, such as those based on deep learning. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{dauphin_identifying_2014,
  title = {Identifying and Attacking the Saddle Point Problem in High-Dimensional Non-Convex Optimization},
  author = {Dauphin, Yann and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
  year = {2014},
  eprint = {1406.2572},
  eprinttype = {arxiv},
  abstract = {A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance.},
  archiveprefix = {arXiv}
}

@misc{daxberger_bayesian_2020,
  title = {Bayesian {{Variational Autoencoders}} for {{Unsupervised Out-of-Distribution Detection}}},
  author = {Daxberger, Erik and {Hern{\'a}ndez-Lobato}, Jos{\'e} Miguel},
  year = {2020},
  month = jul,
  eprint = {1912.05651},
  eprinttype = {arxiv},
  abstract = {Despite their successes, deep neural networks may make unreliable predictions when faced with test data drawn from a distribution different to that of the training data, constituting a major problem for AI safety. While this has recently motivated the development of methods to detect such out-of-distribution (OoD) inputs, a robust solution is still lacking. We propose a new probabilistic, unsupervised approach to this problem based on a Bayesian variational autoencoder model, which estimates a full posterior distribution over the decoder parameters using stochastic gradient Markov chain Monte Carlo, instead of fitting a point estimate. We describe how information-theoretic measures based on this posterior can then be used to detect OoD inputs both in input space and in the model's latent space. We empirically demonstrate the effectiveness of our proposed approach.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{dayan_helmholtz_1995,
  title = {The {{Helmholtz Machine}}},
  author = {Dayan, Peter and Hinton, Geoffrey E. and Neal, Radford M. and Zemel, Richard S.},
  year = {1995},
  month = sep,
  journal = {Neural Comput.},
  volume = {7},
  number = {5},
  pages = {889--904},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA, USA}},
  issn = {0899-7667}
}

@article{DBLP:journals/corr/abs-1910-02760,
  title = {Negative Sampling in Variational Autoencoders},
  author = {Csisz{\'a}rik, Adri{\'a}n and Benko, Beatrix and Varga, D{\'a}niel},
  year = {2019},
  journal = {CoRR},
  volume = {abs/1910.02760},
  eprint = {1910.02760},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/journals/corr/abs-1910-02760.bib},
  timestamp = {Wed, 09 Oct 2019 14:07:58 +0200}
}

@article{de_jong_praat_2009,
  title = {Praat Script to Detect Syllable Nuclei and Measure Speech Rate Automatically},
  author = {{de Jong}, Nivja H. and Wempe, Ton},
  year = {2009},
  month = may,
  journal = {Behavior Research Methods},
  volume = {41},
  number = {2},
  pages = {385--390},
  issn = {1554-351X, 1554-3528},
  abstract = {In this article, we describe a method for automatically detecting syllable nuclei in order to measure speech rate without the need for a transcription. A script written in the software program Praat (Boersma \& Weenink, 2007) detects syllables in running speech. Peaks in intensity (dB) that are preceded and followed by dips in intensity are considered to be potential syllable nuclei. The script subsequently discards peaks that are not voiced. Testing the resulting syllable counts of this script on two corpora of spoken Dutch, we obtained high correlations between speech rate calculated from human syllable counts and speech rate calculated from automatically determined syllable counts. We conclude that a syllable count measured in this automatic fashion suffices to reliably assess and compare speech rates between participants and tasks.},
  langid = {english}
}

@misc{de_sa_representation_2018,
  title = {Representation {{Tradeoffs}} for {{Hyperbolic Embeddings}}},
  author = {De Sa, Christopher and Gu, Albert and R{\'e}, Christopher and Sala, Frederic},
  year = {2018},
  month = apr,
  eprint = {1804.03329},
  eprinttype = {arxiv},
  abstract = {Hyperbolic embeddings offer excellent quality with few dimensions when embedding hierarchical data structures like synonym or type hierarchies. Given a tree, we give a combinatorial construction that embeds the tree in hyperbolic space with arbitrarily low distortion without using optimization. On WordNet, our combinatorial embedding obtains a mean-average-precision of 0.989 with only two dimensions, while Nickel et al.'s recent construction obtains 0.87 using 200 dimensions. We provide upper and lower bounds that allow us to characterize the precision-dimensionality tradeoff inherent in any hyperbolic embedding. To embed general metric spaces, we propose a hyperbolic generalization of multidimensional scaling (h-MDS). We show how to perform exact recovery of hyperbolic points from distances, provide a perturbation analysis, and give a recovery result that allows us to reduce dimensionality. The h-MDS approach offers consistently low distortion even with few dimensions across several datasets. Finally, we extract lessons from the algorithms and theory above to design a PyTorch-based implementation that can handle incomplete information and is scalable.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Hyperbolic space,Statistics - Machine Learning}
}

@article{dehak_front-end_2011,
  title = {Front-{{End Factor Analysis}} for {{Speaker Verification}}},
  author = {Dehak, Najim and Kenny, Patrick J. and Dehak, R{\'e}da and Dumouchel, Pierre and Ouellet, Pierre},
  year = {2011},
  month = may,
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  volume = {19},
  number = {4},
  pages = {788--798},
  issn = {1558-7924},
  abstract = {This paper presents an extension of our previous work which proposes a new speaker representation for speaker verification. In this modeling, a new low-dimensional speaker- and channel-dependent space is defined using a simple factor analysis. This space is named the total variability space because it models both speaker and channel variabilities. Two speaker verification systems are proposed which use this new representation. The first system is a support vector machine-based system that uses the cosine kernel to estimate the similarity between the input data. The second system directly uses the cosine similarity as the final decision score. We tested three channel compensation techniques in the total variability space, which are within-class covariance normalization (WCCN), linear discriminate analysis (LDA), and nuisance attribute projection (NAP). We found that the best results are obtained when LDA is followed by WCCN. We achieved an equal error rate (EER) of 1.12\% and MinDCF of 0.0094 using the cosine distance scoring on the male English trials of the core condition of the NIST 2008 Speaker Recognition Evaluation dataset. We also obtained 4\% absolute EER improvement for both-gender trials on the 10 s-10 s condition compared to the classical joint factor analysis scoring.},
  keywords = {Context modeling,Cosine distance scoring,joint factor analysis (JFA),Kernel,Linear discriminant analysis,Natural languages,NIST,Permission,Speaker recognition,Speech analysis,Support vector machines,support vector machines (SVMs),Testing,total variability space}
}

@article{dehak_support_nodate,
  title = {Support {{Vector Machines}} versus {{Fast Scoring}} in the {{Low-Dimensional Total Variability Space}} for {{Speaker Verification}}},
  author = {Dehak, Najim and Dehak, Reda and Kenny, Patrick and Brummer, Niko and Ouellet, Pierre and Dumouchel, Pierre},
  pages = {4},
  abstract = {This paper presents a new speaker verification system architecture based on Joint Factor Analysis (JFA) as feature extractor. In this modeling, the JFA is used to define a new low-dimensional space named the total variability factor space, instead of both channel and speaker variability spaces for the classical JFA. The main contribution in this approach, is the use of the cosine kernel in the new total factor space to design two different systems: the first system is Support Vector Machines based, and the second one uses directly this kernel as a decision score. This last scoring method makes the process faster and less computation complex compared to others classical methods. We tested several intersession compensation methods in total factors, and we found that the combination of Linear Discriminate Analysis and Within Class Covariance Normalization achieved the best performance. We achieved a remarkable results using fast scoring method based only on cosine kernel especially for male trials, we yield an EER of 1.12\% and MinDCF of 0.0094 on the English trials of the NIST 2008 SRE dataset.},
  langid = {english}
}

@book{deisenroth_mathematics_nodate,
  title = {Mathematics for {{Machine Learning}}},
  author = {Deisenroth, Marc Peter and Faisal, A Aldo and Ong, Cheng Soon},
  publisher = {{Cambridge University Press}},
  keywords = {★}
}

@article{dekens_comparative_2007,
  title = {A {{Comparative Study}} of {{Speech Rate Estimation Techniques}}},
  author = {Dekens, Tomas and Demol, Mike and Verhelst, Werner and Verhoeve, Piet},
  year = {2007},
  journal = {Interspeech},
  pages = {4},
  abstract = {In this paper we evaluate the performance of 8 different speech rate estimators [1, 2, 3, 4, 5] previously described in the literature by applying them on a multilingual test database [6]. All the estimators show an underestimation at high speech rates and some also suffer from an overestimation at low speech rates. Overall the tested methods obtain high correlation coefficients with the reference speech rate. The Temporal Correlation and Selected Sub-band Correlation method (tcssbc), which uses sub-band and time domain correlation for detecting the number of vowels or diphthongs present in the speech signal, shows little errors and appears to be the most appropriate overall technique for speech rate estimation.},
  langid = {english}
}

@misc{deng_binary_2010,
  title = {Binary {{Coding}} of {{Speech Spectrograms Using}} a {{Deep Auto-encoder}}},
  author = {Deng, L. and Seltzer, M. and Yu, D. and Acero, A. and Mohamed, A. and Hinton, G.},
  year = {2010},
  abstract = {This paper reports our recent exploration of the layer-by-layer learning strategy for training a multi-layer generative model of patches of speech spectrograms. The top layer of the generative model learns binary codes that can be used for efficient compression of speech and could also be used for scalable speech recognition or rapid speech content retrieval. Each layer of the generative model is fully connected to the layer below and the weights on these connections are pretrained efficiently by using the contrastive divergence approximation to the log likelihood gradient. After layer-bylayer pre-training we ``unroll '' the generative model to form a deep auto-encoder, whose parameters are then fine-tuned using back-propagation. To reconstruct the full-length speech spectrogram, individual spectrogram segments predicted by their respective binary codes are combined using an overlapand-add method. Experimental results on speech spectrogram coding demonstrate that the binary codes produce a logspectral distortion that is approximately 2 dB lower than a subband vector quantization technique over the entire frequency range of wide-band speech.}
}

@inproceedings{deng_imagenet_2009,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CCVPR}})},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and {Fei-Fei}, Li},
  year = {2009},
  pages = {248--255},
  issn = {1063-6919},
  abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called \&\#x201C;ImageNet\&\#x201D;, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  isbn = {978-1-4244-3992-8},
  pmid = {21914436}
}

@article{denker_transforming_1991,
  title = {Transforming {{Neural-Net Output Levels}} to {{Probability Distributions}}},
  author = {Denker, John S. and LeCun, Yann A.},
  year = {1991},
  journal = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
  pages = {853--859},
  abstract = {(1) The outputs of a typical multi-output classification network do not satisfy the axioms of probability; probabilities should be positive and sum to one. This problem can be solved by treating the trained network as a preprocessor that produces a feature vector that can be further processed, for instance by classical statistical estimation techniques. (2) We present a method for computing the first two moments ofthe probability distribution indicating the range of outputs that are consistent with the input and the training data. It is particularly useful to combine these two ideas: we implement the ideas of section 1 using Parzen windows, where the shape and relative size of each window is computed using the ideas of section 2. This allows us to make contact between important theoretical ideas (e.g. the ensemble formalism) and practical techniques (e.g. back-prop). Our results also shed new light on and generalize the well-known "soft max" scheme.},
  arxiv = {11359-901120-05}
}

@inproceedings{desjardins_natural_2015,
  title = {Natural {{Neural Networks}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Neural Information Processing Systems}} ({{NIPS}})},
  author = {Desjardins, Guillaume and Simonyan, Karen and Pascanu, Razvan and Kavukcuoglu, Koray},
  year = {2015},
  eprint = {1507.00210},
  eprinttype = {arxiv},
  address = {{Montreal, Canada}},
  issn = {10495258},
  abstract = {We introduce Natural Neural Networks, a novel family of algorithms that speed up convergence by adapting their internal representation during training to improve conditioning of the Fisher matrix. In particular, we show a specific example that employs a simple and efficient reparametrization of the neural network weights by implicitly whitening the representation obtained at each layer, while preserving the feed-forward computation of the network. Such networks can be trained efficiently via the proposed Projected Natural Gradient Descent algorithm (PRONG), which amortizes the cost of these reparametrizations over many parameter updates and is closely related to the Mirror Descent online learning algorithm. We highlight the benefits of our method on both unsupervised and supervised learning tasks, and showcase its scalability by training on the large-scale ImageNet Challenge dataset.},
  archiveprefix = {arXiv}
}

@misc{detlefsen_reliable_2019,
  title = {Reliable Training and Estimation of Variance Networks},
  author = {Detlefsen, Nicki S. and J{\o}rgensen, Martin and Hauberg, S{\o}ren},
  year = {2019},
  month = nov,
  eprint = {1906.03260},
  eprinttype = {arxiv},
  abstract = {We propose and investigate new complementary methodologies for estimating predictive variance networks in regression neural networks. We derive a locally aware mini-batching scheme that results in sparse robust gradients, and we show how to make unbiased weight updates to a variance network. Further, we formulate a heuristic for robustly fitting both the mean and variance networks post hoc. Finally, we take inspiration from posterior Gaussian processes and propose a network architecture with similar extrapolation properties to Gaussian processes. The proposed methodologies are complementary, and improve upon baseline methods individually. Experimentally, we investigate the impact of predictive uncertainty on multiple datasets and tasks ranging from regression, active learning and generative modeling. Experiments consistently show significant improvements in predictive uncertainty estimation over state-of-the-art methods across tasks and datasets.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{devlin_bert:_2018,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2018},
  month = oct,
  eprint = {1810.04805},
  eprinttype = {arxiv},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT representations can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE benchmark to 80.4\% (7.6\% absolute improvement), MultiNLI accuracy to 86.7 (5.6\% absolute improvement) and the SQuAD v1.1 question answering Test F1 to 93.2 (1.5\% absolute improvement), outperforming human performance by 2.0\%.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{devries_learning_2018,
  title = {Learning {{Confidence}} for {{Out-of-Distribution Detection}} in {{Neural Networks}}},
  author = {DeVries, Terrance and Taylor, Graham W.},
  year = {2018},
  month = feb,
  eprint = {1802.04865},
  eprinttype = {arxiv},
  abstract = {Modern neural networks are very powerful predictive models, but they are often incapable of recognizing when their predictions may be wrong. Closely related to this is the task of out-of-distribution detection, where a network must determine whether or not an input is outside of the set on which it is expected to safely perform. To jointly address these issues, we propose a method of learning confidence estimates for neural networks that is simple to implement and produces intuitively interpretable outputs. We demonstrate that on the task of out-of-distribution detection, our technique surpasses recently proposed techniques which construct confidence based on the network's output distribution, without requiring any additional labels or access to out-of-distribution examples. Additionally, we address the problem of calibrating out-of-distribution detectors, where we demonstrate that misclassified in-distribution examples can be used as a proxy for out-of-distribution examples.},
  archiveprefix = {arXiv}
}

@book{dewancker_bayesian_2015,
  title = {Bayesian Optimization Primer},
  author = {Dewancker, Ian and McCourt, Michael and Clark, Scott},
  year = {2015},
  abstract = {SigOpt employs Bayesian optimization to help experts tune machine learning models and simulations. Instead of resorting to standard techniques like grid search, random search, or manual tuning, Bayesian optimization efficiently trades off exploration and exploitation of the parameter space to quickly guide the user into the configuration that best optimizes some overall evaluation criterion (OEC) like accuracy, AUC, or likelihood. In this short introduction we introduce Bayesian optimization and several techniques that SigOpt uses to optimize users models and simulations. For applications and examples of SigOpt using Bayesian optimization in real world problems please visit https://sigopt.com/research.},
  keywords = {Pattern recognition systems,Statistical decision}
}

@misc{dhingra_gated-attention_2016,
  title = {Gated-{{Attention Readers}} for {{Text Comprehension}}},
  author = {Dhingra, Bhuwan and Liu, Hanxiao and Yang, Zhilin and Cohen, William W and Salakhutdinov, Ruslan},
  year = {2016},
  eprint = {1606.01549},
  eprinttype = {arxiv},
  abstract = {In this paper we study the problem of answering cloze-style questions over documents. Our model, the Gated-Attention (GA) Reader, integrates a multi-hop architecture with a novel attention mechanism, which is based on multiplicative interactions between the query embedding and the intermediate states of a recurrent neural network document reader. This enables the reader to build query-specific representations of tokens in the document for accurate answer selection. The GA Reader obtains state-of-the-art results on three benchmarks for this task--the CNN \textbackslash\& Daily Mail news stories and the Who Did What dataset. The effectiveness of multiplicative interaction is demonstrated by an ablation study, and by comparing to alternative compositional operators for implementing the gated-attention. The code is available at https://github.com/bdhingra/ga-reader.},
  archiveprefix = {arXiv},
  isbn = {9781945626753}
}

@misc{dieleman_variable-rate_2021,
  title = {Variable-Rate Discrete Representation Learning},
  author = {Dieleman, Sander and Nash, Charlie and Engel, Jesse and Simonyan, Karen},
  year = {2021},
  month = mar,
  eprint = {2103.06089},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {Semantically meaningful information content in perceptual signals is usually unevenly distributed. In speech signals for example, there are often many silences, and the speed of pronunciation can vary considerably. In this work, we propose slow autoencoders (SlowAEs) for unsupervised learning of high-level variable-rate discrete representations of sequences, and apply them to speech. We show that the resulting event-based representations automatically grow or shrink depending on the density of salient information in the input signals, while still allowing for faithful signal reconstruction. We develop run-length Transformers (RLTs) for event-based representation modelling and use them to construct language models in the speech domain, which are able to generate grammatical and semantically coherent utterances and continuations. Samples can be found at https://vdrl.github.io/.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@inproceedings{dieng_avoiding_2019,
  title = {Avoiding Latent Variable Collapse with Generative Skip Models},
  booktitle = {Proceedings of the 22nd {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}} ({{AISTATS}})},
  author = {Dieng, Adji B. and Kim, Yoon and Rush, Alexander M. and Blei, David M.},
  editor = {Chaudhuri, Kamalika and Sugiyama, Masashi},
  year = {2019},
  volume = {89},
  pages = {2397--2405},
  publisher = {{PMLR}},
  address = {{Naha, Okinawa, Japan}},
  abstract = {Variational autoencoders (VAEs) learn distributions of high-dimensional data. They model data with a deep latent-variable model and then fit the model by maximizing a lower bound of the log marginal likelihood. VAEs can capture complex distributions, but they can also suffer from an issue known as "latent variable collapse," especially if the likelihood model is powerful. Specifically, the lower bound involves an approximate posterior of the latent variables; this posterior "collapses" when it is set equal to the prior, i.e., when the approximate posterior is independent of the data. While VAEs learn good generative models, latent variable collapse prevents them from learning useful representations. In this paper, we propose a simple new way to avoid latent variable collapse by including skip connections in our generative model; these connections enforce strong links between the latent variables and the likelihood function. We study generative skip models both theoretically and empirically. Theoretically, we prove that skip models increase the mutual information between the observations and the inferred latent variables. Empirically, we study images (MNIST and Omniglot) and text (Yahoo). Compared to existing VAE architectures, we show that generative skip models maintain similar predictive performance but lead to less collapse and provide more meaningful representations of the data.}
}

@article{ding_flowprior_nodate,
  title = {{{FlowPrior}}: {{Learning Expressive Priors}} for {{Latent Variable Sentence Models}}},
  author = {Ding, Xiaoan and Gimpel, Kevin},
  pages = {17},
  abstract = {Variational autoencoders (VAEs) are widely used for latent variable modeling of text. We focus on variations that learn expressive prior distributions over the latent variable. We find that existing training strategies are not effective for learning rich priors, so we add the importance-sampled log marginal likelihood as a second term to the standard VAE objective to help when learning the prior. Doing so improves results for all priors evaluated, including a novel choice for sentence VAEs based on normalizing flows (NF). Priors parameterized with NF are no longer constrained to a specific distribution family, allowing a more flexible way to encode the data distribution. Our model, which we call FlowPrior, shows a substantial improvement in language modeling tasks compared to strong baselines. We demonstrate that FlowPrior learns an expressive prior with analysis and several forms of evaluation involving generation.},
  langid = {english}
}

@inproceedings{dinh_density_2017,
  title = {Density Estimation Using {{Real NVP}}},
  booktitle = {5th {{International Conference}} on {{Learning Representations}}},
  author = {Dinh, Laurent and {Sohl-Dickstein}, Jascha and Bengio, Samy},
  year = {2017},
  month = apr,
  eprint = {1605.08803},
  eprinttype = {arxiv},
  address = {{Palais des Congr\`es Neptune, Toulon, France}},
  abstract = {Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful, stably invertible, and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact and efficient sampling, exact and efficient inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation, and latent variable manipulations.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
}

@inproceedings{dinh_nice_2015,
  title = {{{NICE}}: {{Non-linear Independent Components Estimation}}},
  shorttitle = {{{NICE}}},
  booktitle = {3rd {{International Conference}} on {{Learning Representations Workshop}}},
  author = {Dinh, Laurent and Krueger, David and Bengio, Yoshua},
  year = {2015},
  month = may,
  eprint = {1410.8516},
  eprinttype = {arxiv},
  address = {{San Diego, CA, USA}},
  abstract = {We propose a deep learning framework for modeling complex high-dimensional densities called Non-linear Independent Component Estimation (NICE). It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the determinant of the Jacobian and inverse Jacobian is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable. Unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning}
}

@misc{dodge_show_2019,
  title = {Show {{Your Work}}: {{Improved Reporting}} of {{Experimental Results}}},
  shorttitle = {Show {{Your Work}}},
  author = {Dodge, Jesse and Gururangan, Suchin and Card, Dallas and Schwartz, Roy and Smith, Noah A.},
  year = {2019},
  month = sep,
  eprint = {1909.03004},
  eprinttype = {arxiv},
  abstract = {Research in natural language processing proceeds, in part, by demonstrating that new models achieve superior performance (e.g., accuracy) on held-out test data, compared to previous results. In this paper, we demonstrate that test-set performance scores alone are insufficient for drawing accurate conclusions about which model performs best. We argue for reporting additional details, especially performance on validation data obtained during model development. We present a novel technique for doing so: expected validation performance of the best-found model as a function of computation budget (i.e., the number of hyperparameter search trials or the overall training time). Using our approach, we find multiple recent model comparisons where authors would have reached a different conclusion if they had used more (or less) computation. Our approach also allows us to estimate the amount of computation required to obtain a given accuracy; applying it to several recently published results yields massive variation across papers, from hours to weeks. We conclude with a set of best practices for reporting experimental results which allow for robust future comparisons, and provide code to allow researchers to use our technique.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology}
}

@misc{doersch_tutorial_2016,
  title = {Tutorial on {{Variational Autoencoders}}},
  author = {Doersch, Carl},
  year = {2016},
  eprint = {1606.05908},
  eprinttype = {arxiv},
  abstract = {In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
  archiveprefix = {arXiv},
  isbn = {1532-4435},
  pmid = {27148061}
}

@misc{doersch_tutorial_2021,
  title = {Tutorial on {{Variational Autoencoders}}},
  author = {Doersch, Carl},
  year = {2021},
  month = jan,
  eprint = {1606.05908},
  eprinttype = {arxiv},
  abstract = {In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits [1, 2], faces [1, 3, 4], house numbers [5, 6], CIFAR images [6], physical models of scenes [4], segmentation [7], and predicting the future from static images [8]. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{doersch_unsupervised_2016,
  title = {Unsupervised {{Visual Representation Learning}} by {{Context Prediction}}},
  author = {Doersch, Carl and Gupta, Abhinav and Efros, Alexei A.},
  year = {2016},
  month = jan,
  journal = {arXiv:1505.05192 [cs]},
  eprint = {1505.05192},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {This work explores the use of spatial context as a source of free and plentiful supervisory signal for training a rich visual representation. Given only a large, unlabeled image collection, we extract random pairs of patches from each image and train a convolutional neural net to predict the position of the second patch relative to the first. We argue that doing well on this task requires the model to learn to recognize objects and their parts. We demonstrate that the feature representation learned using this within-image context indeed captures visual similarity across images. For example, this representation allows us to perform unsupervised visual discovery of objects like cats, people, and even birds from the Pascal VOC 2011 detection dataset. Furthermore, we show that the learned ConvNet can be used in the R-CNN framework and provides a significant boost over a randomly-initialized ConvNet, resulting in state-of-the-art performance among algorithms which use only Pascal-provided training set annotations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{domingos_few_2012,
  title = {A Few Useful Things to Know about Machine Learning},
  author = {Domingos, Pedro},
  year = {2012},
  journal = {Communications of the ACM},
  volume = {55},
  number = {10},
  eprint = {cs/9605103},
  eprinttype = {arxiv},
  pages = {78},
  issn = {00010782},
  abstract = {MACHINE LEARNING SYSTEMS automatically learn programs from data. This is often a very attractive alternative to manually constructing them, and in the last decade the use of machine learning has spread rapidly throughout computer science and beyond. Machine learning is used in Web search, spam filters, recommender systems, ad placement, credit scoring, fraud detection, stock trading, drug design, and many other applications. A recent report from the McKinsey Global Institute asserts that machine learning (a.k.a. data mining or predictive analytics) will be the driver of the next big wave of innovation. Several fine textbooks are available to interested practitioners and researchers (for example, Mitchell and Witten et al.). However, much of the ``folk knowledge'' that is needed to successfully develop machine learning applications is not readily available in them. As a result, many machine learning projects take much longer than necessary or wind up producing less-than-ideal results. Yet much of this folk knowledge is fairly easy to communicate. This is the purpose of this article.},
  archiveprefix = {arXiv},
  isbn = {0001-0782},
  pmid = {1000183096},
  keywords = {★}
}

@inproceedings{domke_importance_2018,
  title = {Importance {{Weighting}} and {{Variational Inference}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Domke, Justin and Sheldon, Daniel R},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  volume = {31},
  pages = {4470--4479},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Recent work used importance sampling ideas for better variational bounds on likelihoods. We clarify the applicability of these ideas to pure probabilistic inference, by showing the resulting Importance Weighted Variational Inference (IWVI) technique is an instance of augmented variational inference, thus identifying the looseness in previous work. Experiments confirm IWVI's practicality for probabilistic inference. As a second contribution, we investigate inference with elliptical distributions, which improves accuracy in low dimensions, and convergence in high dimensions}
}

@inproceedings{donahue_decaf_2014,
  title = {{{DeCAF}}: {{A Deep Convolutional Activation Feature}} for {{Generic Visual Recognition}}.},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Donahue, Jeff and Jia, Yangqing and Vinyals, Oriol and Hoffman, Judy and Zhang, Ning and Tzeng, Eric and Darrell, Trevor},
  year = {2014},
  volume = {32},
  pages = {647--655}
}

@inproceedings{donahue_long-term_2015,
  title = {Long-Term Recurrent Convolutional Networks for Visual Recognition and Description},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CCVPR}})},
  author = {Donahue, Jeffrey and Anne Hendricks, Lisa and Guadarrama, Sergio and Rohrbach, Marcus and Venugopalan, Subhashini and Saenko, Kate and Darrell, Trevor},
  year = {2015},
  pages = {2625--2634}
}

@article{dong_image_2016,
  title = {Image Super-Resolution Using Deep Convolutional Networks},
  author = {Dong, Chao and Loy, Chen Change and He, Kaiming and Tang, Xiaoou},
  year = {2016},
  journal = {IEEE transactions on pattern analysis and machine intelligence},
  volume = {38},
  number = {2},
  pages = {295--307},
  publisher = {{IEEE}}
}

@article{dong_speech-transformer:_2018,
  title = {Speech-{{Transformer}}: {{A No-Recurrence Sequence-To-Sequence Model}} for {{Speech Recognition}}},
  author = {Dong, Linhao and Xu, Shuang and Xu, Bo},
  year = {2018},
  journal = {International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
  pages = {5884--5888},
  abstract = {Recurrent sequence-to-sequence models using encoder-decoder ar- chitecture have made great progress in speech recognition task. However, they suffer from the drawback of slow training speed because the internal recurrence limits the training parallelization. In this paper, we present the Speech-Transformer, a no-recurrence sequence-to-sequence model entirely relies on attention mechanism- s to learn the positional dependencies, which can be trained faster with more efficiency. We also propose a 2D-Attention mechanis- m, which can jointly attend to the time and frequency axes of the 2-dimensional speech inputs, thus providing more expressive repre- sentations for the Speech-Transformer. Evaluated on the Wall Street Journal (WSJ) speech recognition dataset, our best model achieves competitive word error rate (WER) of 10.9\%, while the whole train- ing process only takes 1.2 days on 1 GPU, significantly faster than the published results of recurrent sequence-to-sequence models.}
}

@misc{dosovitskiy_image_2021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year = {2021},
  month = jun,
  eprint = {2010.11929},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@incollection{doucet_introduction_2001,
  title = {An {{Introduction}} to {{Sequential Monte Carlo Methods}}},
  booktitle = {Sequential {{Monte Carlo Methods}} in {{Practice}}},
  author = {Doucet, Arnaud and {de Freitas}, Nando and Gordon, Neil},
  editor = {Doucet, Arnaud and {de Freitas}, Nando and Gordon, Neil},
  year = {2001},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  abstract = {Many real-world data analysis tasks involve estimating unknown quantities from some given observations. In most of these applications, prior knowledge about the phenomenon being modelled is available. This knowledge allows us to formulate Bayesian models, that is prior distributions for the unknown quantities and likelihood functions relating these quantities to the observations. Within this setting, all inference on the unknown quantities is based on the posterior distribution obtained from Bayes' theorem. Often, the observations arrive sequentially in time and one is interested in performing inference on-line. It is therefore necessary to update the posterior distribution as data become available. Examples include tracking an aircraft using radar measurements, estimating a digital communications signal using noisy measurements, or estimating the volatility of financial instruments using stock market data. Computational simplicity in the form of not having to store all the data might also be an additional motivating factor for sequential methods.},
  isbn = {978-1-4757-3437-9}
}

@book{downey_think_2012,
  title = {Think {{Bayes}} - {{Bayesian Statistics Made Simple}}},
  author = {Downey, Alan B.},
  year = {2012},
  publisher = {{Green Tea Press}}
}

@article{dozat_incorporating_2016,
  title = {Incorporating {{Nesterov Momentum}} into {{Adam}}},
  author = {Dozat, Timothy},
  year = {2016},
  journal = {International Conference on Learning Representations (ICLR) Workshop},
  number = {1},
  pages = {2013--2016},
  abstract = {This work aims to improve upon the recently proposed and rapidly popular-ized optimization algorithm Adam (Kingma \& Ba, 2014). Adam has two main components\textemdash a momentum component and an adaptive learning rate component. However, regular momentum can be shown conceptually and empirically to be in-ferior to a similar algorithm known as Nesterov's accelerated gradient (NAG). We show how to modify Adam's momentum component to take advantage of insights from NAG, and then we present preliminary evidence suggesting that making this substitution improves the speed of convergence and the quality of the learned mod-els.}
}

@inproceedings{dror_hitchhikers_2018,
  title = {The {{Hitchhiker}}'s {{Guide}} to {{Testing Statistical Significance}} in {{Natural Language Processing}}},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Dror, Rotem and Baumer, Gili and Shlomov, Segev and Reichart, Roi},
  year = {2018},
  pages = {1383--1392},
  publisher = {{Association for Computational Linguistics}},
  address = {{Melbourne, Australia}},
  abstract = {Statistical significance testing is a standard statistical tool designed to ensure that experimental results are not coincidental. In this opinion/theoretical paper we discuss the role of statistical significance testing in Natural Language Processing (NLP) research. We establish the fundamental concepts of significance testing and discuss the specific aspects of NLP tasks, experimental setups and evaluation measures that affect the choice of significance tests in NLP research. Based on this discussion, we propose a simple practical protocol for statistical significance test selection in NLP setups and accompany this protocol with a brief survey of the most relevant tests. We then survey recent empirical papers published in ACL and TACL during 2017 and show that while our community assigns great value to experimental results, statistical significance testing is often ignored or misused. We conclude with a brief discussion of open issues that should be properly addressed so that this important tool can be applied in NLP research in a statistically sound manner1.},
  langid = {english}
}

@misc{drusvyatskiy_optimal_2016,
  title = {An Optimal First Order Method Based on Optimal Quadratic Averaging},
  author = {Drusvyatskiy, Dmitriy and Fazel, Maryam and Roy, Scott},
  year = {2016},
  eprint = {1604.06543},
  eprinttype = {arxiv},
  abstract = {In a recent paper, Bubeck, Lee, and Singh introduced a new first order method for minimizing smooth strongly convex functions. Their geometric descent algorithm, largely inspired by the ellipsoid method, enjoys the optimal linear rate of convergence. We show that the same iterate sequence is generated by a scheme that in each iteration computes an optimal average of quadratic lower-models of the function. Indeed, the minimum of the averaged quadratic approaches the true minimum at an optimal rate. This intuitive viewpoint reveals clear connections to the original fast-gradient methods and cutting plane ideas, and leads to limited-memory extensions with improved performance.},
  archiveprefix = {arXiv}
}

@article{du_gradient_2018,
  title = {Gradient {{Descent Finds Global Minima}} of {{Deep Neural Networks}}},
  author = {Du, Simon S. and Lee, Jason D. and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  year = {2018},
  month = nov,
  langid = {english}
}

@article{duan_low_2014,
  title = {Low Rank Approximation of the Symmetric Positive Semidefinite Matrix},
  author = {Duan, Xuefeng and Li, Jiaofen and Wang, Qingwen and Zhang, Xinjun},
  year = {2014},
  journal = {Journal of Computational and Applied Mathematics},
  volume = {260},
  pages = {236--243},
  issn = {03770427},
  abstract = {In this paper, we consider the low rank approximation of the symmetric positive semidefinite matrix, which arises in machine learning, quantum chemistry and inverse problem. We first characterize the feasible set by X=YYT,Ya\^\^\^Rn\texttimes k, and then transform low rank approximation into an unconstrained optimization problem. Finally, we use the nonlinear conjugate gradient method with exact line search to compute the optimal low rank symmetric positive semidefinite approximation of the given matrix. Numerical examples show that the new method is feasible and effective. \textcopyright{} 2013 Elsevier Inc.},
  keywords = {Feasible set,Low rank approximation,Nonlinear conjugate gradient method,Symmetric positive semidefinite matrix,Unconstrained optimization}
}

@misc{duan_rl_2016,
  title = {{{RL}}\$\^2\$: {{Fast Reinforcement Learning}} via {{Slow Reinforcement Learning}}},
  author = {Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter L. and Sutskever, Ilya and Abbeel, Pieter},
  year = {2016},
  month = nov,
  eprint = {1611.02779},
  eprinttype = {arxiv},
  abstract = {Deep reinforcement learning (deep RL) has been successful in learning sophisticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, benefiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a "fast" reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, RL\$\^2\$, the algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose ("slow") RL algorithm. The RNN receives all information a typical RL algorithm would receive, including observations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the "fast" RL algorithm on the current (previously unseen) MDP. We evaluate RL\$\^2\$ experimentally on both small-scale and large-scale problems. On the small-scale side, we train it to solve randomly generated multi-arm bandit problems and finite MDPs. After RL\$\^2\$ is trained, its performance on new MDPs is close to human-designed algorithms with optimality guarantees. On the large-scale side, we test RL\$\^2\$ on a vision-based navigation task and show that it scales up to high-dimensional problems.},
  archiveprefix = {arXiv}
}

@article{duchi_adaptive_2011,
  title = {Adaptive {{Subgradient Methods}} for {{Online Learning}} and {{Stochastic Optimization}}},
  author = {Duchi, John and Hazan, Elad and Singer, Yoram},
  year = {2011},
  journal = {Journal of Machine Learning Research},
  volume = {12},
  pages = {2121--2159},
  abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
  keywords = {Online learning,Stochastic convex optimization,Subgradient methods}
}

@misc{dumoulin_guide_2016,
  title = {A {{Guide}} to {{Convolution Arithmetic}} for {{Deep Learning}}},
  author = {Dumoulin, Vincent and Visin, Francesco},
  year = {2016},
  month = mar,
  eprint = {1603.07285},
  eprinttype = {arxiv},
  abstract = {We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.},
  archiveprefix = {arXiv}
}

@inproceedings{durall_watch_2020,
  title = {Watch {{Your Up-Convolution}}: {{CNN Based Generative Deep Neural Networks Are Failing}} to {{Reproduce Spectral Distributions}}},
  booktitle = {Proceedings of the 2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Durall, R. and Keuper, M. and Keuper, J.},
  year = {2020},
  month = jun,
  pages = {7887--7896},
  issn = {2575-7075},
  abstract = {Generative convolutional deep neural networks, e.g. popular GAN architectures, are relying on convolution based up-sampling methods to produce non-scalar outputs like images or video sequences. In this paper, we show that common up-sampling methods, i.e. known as up-convolution or transposed convolution, are causing the inability of such models to reproduce spectral distributions of natural training data correctly. This effect is independent of the underlying architecture and we show that it can be used to easily detect generated data like deepfakes with up to 100\% accuracy on public benchmarks. To overcome this drawback of current generative models, we propose to add a novel spectral regularization term to the training optimization objective. We show that this approach not only allows to train spectral consistent GANs that are avoiding high frequency errors. Also, we show that a correct approximation of the frequency spectrum has positive effects on the training stability and output quality of generative networks.},
  keywords = {approximation theory,Convolution,convolution based up-sampling methods,convolutional neural nets,current generative models,Distortion,Gallium nitride,GAN architectures,Generative adversarial networks,generative deep neural networks,generative networks,learning (artificial intelligence),natural training data,Neural networks,nonscalar outputs,optimisation,output quality,reproduce spectral distributions,sampling methods,spectral consistent GANs,spectral regularization term,Training,training optimization objective,training stability,transposed convolution,underlying architecture,up-convolution,up-sampling methods,video sequences}
}

@article{duvenaud_automatic_nodate,
  title = {Automatic {{Model Construction}}  with {{Gaussian Processes}}},
  author = {Duvenaud, David Kristjanson},
  pages = {157},
  langid = {english}
}

@inproceedings{ebbers_hidden_2017,
  title = {Hidden {{Markov Model Variational Autoencoder}} for {{Acoustic Unit Discovery}}},
  booktitle = {Interspeech 2017},
  author = {Ebbers, Janek and Heymann, Jahn and Drude, Lukas and Glarner, Thomas and {Haeb-Umbach}, Reinhold and Raj, Bhiksha},
  year = {2017},
  month = aug,
  pages = {488--492},
  publisher = {{ISCA}},
  langid = {english}
}

@misc{edizel_misspelling_2019,
  title = {Misspelling {{Oblivious Word Embeddings}}},
  author = {Edizel, Bora and Piktus, Aleksandra and Bojanowski, Piotr and Ferreira, Rui and Grave, Edouard and Silvestri, Fabrizio},
  year = {2019},
  month = may,
  eprint = {1905.09755},
  eprinttype = {arxiv},
  abstract = {In this paper we present a method to learn word embeddings that are resilient to misspellings. Existing word embeddings have limited applicability to malformed texts, which contain a non-negligible amount of outof-vocabulary words. We propose a method combining FastText with subwords and a supervised task of learning misspelling patterns. In our method, misspellings of each word are embedded close to their correct variants. We train these embeddings on a new dataset we are releasing publicly. Finally, we experimentally show the advantages of this approach on both intrinsic and extrinsic NLP tasks using public test sets.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@inproceedings{edwards_towards_2017,
  title = {Towards a {{Neural Statistician}}},
  booktitle = {Proceedings of the 5th {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Edwards, Harrison and Storkey, Amos J.},
  year = {2017},
  address = {{Toulon, France}}
}

@article{ehrlinger_towards_nodate,
  title = {Towards a {{Definition}} of {{Knowledge Graphs}}},
  author = {Ehrlinger, Lisa and W{\"o}{\ss}, Wolfram},
  pages = {4},
  abstract = {Recently, the term knowledge graph has been used frequently in research and business, usually in close association with Semantic Web technologies, linked data, large-scale data analytics and cloud computing. Its popularity is clearly influenced by the introduction of Google's Knowledge Graph in 2012, and since then the term has been widely used without a definition. A large variety of interpretations has hampered the evolution of a common understanding of knowledge graphs. Numerous research papers refer to Google's Knowledge Graph, although no official documentation about the used methods exists. The prerequisite for widespread academic and commercial adoption of a concept or technology is a common understanding, based ideally on a definition that is free from ambiguity. We tackle this issue by discussing and defining the term knowledge graph, considering its history and diversity in interpretations and use. Our goal is to propose a definition of knowledge graphs that serves as basis for discussions on this topic and contributes to a common vision.},
  langid = {english},
  keywords = {Graphs,Knowledge,Representation}
}

@inproceedings{el_hihi_hierarchical_1995,
  title = {Hierarchical Recurrent Neural Networks for Long-Term Dependencies},
  booktitle = {Proceedings of the 8th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {El Hihi, Salah and Bengio, Yoshua},
  year = {1995},
  series = {{{NIPS}}'95},
  pages = {493--499},
  publisher = {{MIT Press}},
  address = {{Denver, CO, USA}},
  abstract = {We have already shown that extracting long-term dependencies from sequential data is difficult, both for determimstic dynamical systems such as recurrent networks, and probabilistic models such as hidden Markov models (HMMs) or input/output hidden Markov models (IOHMMs). In practice, to avoid this problem, researchers have used domain specific a-priori knowledge to give meaning to the hidden or state variables representing past context. In this paper, we propose to use a more general type of a-priori knowledge, namely that the temporal dependencies are structured hierarchically. This implies that long-term dependencies are represented by variables with a long time scale. This principle is applied to a recurrent network which includes delays and multiple time scales. Experiments confirm the advantages of such structures. A similar approach is proposed for HMMs and IOHMMs.}
}

@article{eloff_unsupervised_2019,
  title = {Unsupervised Acoustic Unit Discovery for Speech Synthesis Using Discrete Latent-Variable Neural Networks},
  author = {Eloff, Ryan and Nortje, Andr{\'e} and {van Niekerk}, Benjamin and Govender, Avashna and Nortje, Leanne and Pretorius, Arnu and {van Biljon}, Elan and {van der Westhuizen}, Ewald and {van Staden}, Lisa and Kamper, Herman},
  year = {2019},
  month = jun,
  journal = {arXiv:1904.07556 [cs, eess]},
  eprint = {1904.07556},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {For our submission to the ZeroSpeech 2019 challenge, we apply discrete latent-variable neural networks to unlabelled speech and use the discovered units for speech synthesis. Unsupervised discrete subword modelling could be useful for studies of phonetic category learning in infants or in low-resource speech technology requiring symbolic input. We use an autoencoder (AE) architecture with intermediate discretisation. We decouple acoustic unit discovery from speaker modelling by conditioning the AE's decoder on the training speaker identity. At test time, unit discovery is performed on speech from an unseen speaker, followed by unit decoding conditioned on a known target speaker to obtain reconstructed filterbanks. This output is fed to a neural vocoder to synthesise speech in the target speaker's voice. For discretisation, categorical variational autoencoders (CatVAEs), vector-quantised VAEs (VQ-VAEs) and straight-through estimation are compared at different compression levels on two languages. Our final model uses convolutional encoding, VQ-VAE discretisation, deconvolutional decoding and an FFTNet vocoder. We show that decoupled speaker conditioning intrinsically improves discrete acoustic representations, yielding competitive synthesis quality compared to the challenge baseline.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@inproceedings{engel_ddsp_2020,
  title = {{{DDSP}}: {{Differentiable}} Digital Signal Processing},
  booktitle = {Proceedings of the 8th {{International Conference}} on {{Learning Representations}} (Iclr)},
  author = {Engel, Jesse H. and Hantrakul, Lamtharn and Gu, Chenjie and Roberts, Adam},
  year = {2020},
  eprint = {2001.04643},
  eprinttype = {arxiv},
  publisher = {{OpenReview.net}},
  address = {{Addis Ababa, Ethiopia}},
  abstract = {Most generative models of audio directly generate samples in one of two domains: time or frequency. While sufficient to express any signal, these representations are inefficient, as they do not utilize existing knowledge of how sound is generated and perceived. A third approach (vocoders/synthesizers) successfully incorporates strong domain knowledge of signal processing and perception, but has been less actively researched due to limited expressivity and difficulty integrating with modern auto-differentiation-based machine learning methods. In this paper, we introduce the Differentiable Digital Signal Processing (DDSP) library, which enables direct integration of classic signal processing elements with deep learning methods. Focusing on audio synthesis, we achieve high-fidelity generation without the need for large autoregressive models or adversarial losses, demonstrating that DDSP enables utilizing strong inductive biases without losing the expressive power of neural networks. Further, we show that combining interpretable modules permits manipulation of each separate model component, with applications such as independent control of pitch and loudness, realistic extrapolation to pitches not seen during training, blind dereverberation of room acoustics, transfer of extracted room acoustics to new environments, and transformation of timbre between disparate sources. In short, DDSP enables an interpretable and modular approach to generative modeling, without sacrificing the benefits of deep learning. The library is publicly available at https://github.com/magenta/ddsp and we welcome further contributions from the community and domain experts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Electrical Engineering and Systems Science - Signal Processing,Statistics - Machine Learning}
}

@article{eslami_neural_2018,
  title = {Neural Scene Representation and Rendering},
  author = {Eslami, S M Ali and Rezende, Danilo Jimenez and Besse, Frederic and Viola, Fabio and Morcos, Ari S and Garnelo, Marta and Ruderman, Avraham and Rusu, Andrei A and Danihelka, Ivo and Gregor, Karol and Reichert, David P and Buesing, Lars and Weber, Theophane and Vinyals, Oriol and Rosenbaum, Dan and Rabinowitz, Neil and King, Helen and Hillier, Chloe and Botvinick, Matt},
  year = {2018},
  journal = {Science},
  volume = {360},
  number = {6394},
  eprint = {1312.6114},
  eprinttype = {arxiv},
  pages = {1204--1210},
  issn = {0036-8075},
  abstract = {Scene representation\textemdash the process of converting visual sensory data into concise descriptions\textemdash is a requirement for intelligent behavior. Recent work has shown that neural networks excel at this task when provided with large, labeled datasets. However, removing the reliance on human labeling remains an important open problem. To this end, we introduce the Generative Query Network (GQN), a framework within which machines learn to represent scenes using only their own sensors. The GQN takes as input images of a scene taken from different viewpoints, constructs an internal representation, and uses this representation to predict the appearance of that scene from previously unobserved viewpoints. The GQN demonstrates representation learning without human labels or domain knowledge, paving the way toward machines that autonomously learn to understand the world around them.},
  archiveprefix = {arXiv},
  isbn = {1312.6114v10},
  pmid = {29903970}
}

@misc{esser_taming_2021,
  title = {Taming {{Transformers}} for {{High-Resolution Image Synthesis}}},
  author = {Esser, Patrick and Rombach, Robin and Ommer, Bj{\"o}rn},
  year = {2021},
  month = feb,
  eprint = {2012.09841},
  eprinttype = {arxiv},
  abstract = {Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a contextrich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semanticallyguided synthesis of megapixel images with transformers. Project page at https://git.io/JLlvY.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@misc{everitt_reward_2019,
  title = {Reward {{Tampering Problems}} and {{Solutions}} in {{Reinforcement Learning}}},
  author = {Everitt, Tom and Hutter, Marcus},
  year = {2019},
  eprint = {1908.04734v2},
  eprinttype = {arxiv},
  abstract = {Can an arbitrarily intelligent reinforcement learning agent be kept under control by a human user? Or do agents with sufficient intelligence inevitably find ways to shortcut their reward signal? This question impacts how far reinforcement learning can be scaled, and whether alternative paradigms must be developed in order to build safe artificial general intelligence. In this paper, we use an intuitive yet precise graphical model called causal influence diagrams to formalize reward tampering problems. We also describe a number of tweaks to the reinforcement learning objective that prevent incentives for reward tampering. We verify the solutions using recently developed graphical criteria for inferring agent incentives from causal influence diagrams.},
  archiveprefix = {arXiv}
}

@misc{fabius_variational_2015,
  title = {Variational {{Recurrent Auto-Encoders}}},
  author = {Fabius, Otto and {van Amersfoort}, Joost R.},
  year = {2015},
  month = jun,
  eprint = {1412.6581},
  eprinttype = {arxiv},
  abstract = {In this paper we propose a model that combines the strengths of RNNs and SGVB: the Variational Recurrent Auto-Encoder (VRAE). Such a model can be used for efficient, large scale unsupervised learning on time series data, mapping the time series data to a latent vector representation. The model is generative, such that data can be generated from samples of the latent space. An important contribution of this work is that the model can make use of unlabeled data in order to facilitate supervised training of RNNs by initialising the weights and network state.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
}

@misc{fan_training_2020,
  title = {Training with {{Quantization Noise}} for {{Extreme Model Compression}}},
  author = {Fan, Angela and Stock, Pierre and Graham, Benjamin and Grave, Edouard and Gribonval, Remi and Jegou, Herve and Joulin, Armand},
  year = {2020},
  month = apr,
  eprint = {2004.07320},
  eprinttype = {arxiv},
  abstract = {We tackle the problem of producing compact models, maximizing their accuracy for a given model size. A standard solution is to train networks with Quantization Aware Training [1], where the weights are quantized during training and the gradients approximated with the Straight-Through Estimator [2]. In this paper, we extend this approach to work with extreme compression methods where the approximations introduced by STE are severe. Our proposal is to only quantize a different random subset of weights during each forward, allowing for unbiased gradients to flow through the other weights. Controlling the amount of noise and its form allows for extreme compression rates while maintaining the performance of the original model. As a result we establish new state-of-the-art compromises between accuracy and model size both in natural language processing and image classification. For example, applying our method to state-of-the-art Transformer and ConvNet architectures, we can achieve 82.5\% accuracy on MNLI by compressing RoBERTa to 14 MB and 80.0\% top-1 accuracy on ImageNet by compressing an EfficientNet-B3 to 3.3 MB.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{fang_discrete_2021,
  title = {Discrete {{Auto-regressive Variational Attention Models}} for {{Text Modeling}}},
  author = {Fang, Xianghong and Bai, Haoli and Li, Jian and Xu, Zenglin and Lyu, Michael and King, Irwin},
  year = {2021},
  month = jun,
  eprint = {2106.08571},
  eprinttype = {arxiv},
  abstract = {Variational autoencoders (VAEs) have been widely applied for text modeling. In practice, however, they are troubled by two challenges: information underrepresentation and posterior collapse. The former arises as only the last hidden state of LSTM encoder is transformed into the latent space, which is generally insufficient to summarize the data. The latter is a longstanding problem during the training of VAEs as the optimization is trapped to a disastrous local optimum. In this paper, we propose Discrete Auto-regressive Variational Attention Model (DAVAM) to address the challenges. Specifically, we introduce an auto-regressive variational attention approach to enrich the latent space by effectively capturing the semantic dependency from the input. We further design discrete latent space for the variational attention and mathematically show that our model is free from posterior collapse. Extensive experiments on language modeling tasks demonstrate the superiority of DAVAM against several VAE counterparts. Code will be released.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@inproceedings{fang_implicit_2019,
  title = {Implicit {{Deep Latent Variable Models}} for {{Text Generation}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  author = {Fang, Le and Li, Chunyuan and Gao, Jianfeng and Dong, Wen and Chen, Changyou},
  year = {2019},
  pages = {3944--3954},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  abstract = {Deep latent variable models (LVM) such as variational auto-encoder (VAE) have recently played an important role in text generation. One key factor is the exploitation of smooth latent structures to guide the generation. However, the representation power of VAEs is limited due to two reasons: (1) the Gaussian assumption is often made on the variational posteriors; and meanwhile (2) a notorious ``posterior collapse'' issue occurs. In this paper, we advocate sample-based representations of variational distributions for natural language, leading to implicit latent features, which can provide flexible representation power compared with Gaussian-based posteriors. We further develop an LVM to directly match the aggregated posterior to the prior. It can be viewed as a natural extension of VAEs with a regularization of maximizing mutual information, mitigating the ``posterior collapse'' issue. We demonstrate the effectiveness and versatility of our models in various text generation scenarios, including language modeling, unaligned style transfer, and dialog response generation. The source code to reproduce our experimental results is available on GitHub1.},
  langid = {english}
}

@article{farabet_learning_2013,
  title = {Learning Hierarchical Features for Scene Labeling},
  author = {Farabet, Clement and Couprie, Camille and Najman, Laurent and LeCun, Yann A.},
  year = {2013},
  journal = {IEEE transactions on pattern analysis and machine intelligence},
  volume = {35},
  number = {8},
  pages = {1915--1929},
  publisher = {{IEEE}}
}

@article{ferrucci_building_2010,
  title = {Building {{Watson}}: {{An Overview}} of the {{DeepQA Project}}},
  author = {Ferrucci, David},
  year = {2010},
  journal = {Ai Magazine},
  volume = {31},
  number = {3},
  issn = {07384602}
}

@misc{finn_model-agnostic_2017,
  title = {Model-{{Agnostic Meta-Learning}} for {{Fast Adaptation}} of {{Deep Networks}}},
  author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  year = {2017},
  month = jul,
  eprint = {1703.03400},
  eprinttype = {arxiv},
  abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two fewshot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@article{fisher_use_1936,
  title = {The {{Use}} of {{Multiple Measurements}} in {{Taxonomic Problems}}},
  author = {Fisher, Ronald A},
  year = {1936},
  journal = {Annals of eugenics},
  volume = {7},
  number = {2},
  pages = {179--188}
}

@misc{floto_exponentially_2022,
  title = {The {{Exponentially Tilted Gaussian Prior}} for {{Variational Autoencoders}}},
  author = {Floto, Griffin and Kremer, Stefan and Nica, Mihai},
  year = {2022},
  month = feb,
  eprint = {2111.15646},
  eprinttype = {arxiv},
  abstract = {An important property for deep neural networks is the ability to perform robust out-of-distribution detection on previously unseen data. This property is essential for safety purposes when deploying models for real world applications. Recent studies show that probabilistic generative models can perform poorly on this task, which is surprising given that they seek to estimate the likelihood of training data. To alleviate this issue, we propose the exponentially tilted Gaussian prior distribution for the Variational Autoencoder (VAE) which pulls points onto the surface of a hyper-sphere in latent space. This achieves state-of-the art results on the area under the curve-receiver operator characteristics metric using just the negative log-likelihood that the VAE naturally assigns. Because this prior is a simple modification of the traditional VAE prior, it is faster and easier to implement than competitive methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{fortuin_priors_2021,
  title = {Priors in {{Bayesian Deep Learning}}: {{A Review}}},
  shorttitle = {Priors in {{Bayesian Deep Learning}}},
  author = {Fortuin, Vincent},
  year = {2021},
  month = may,
  eprint = {2105.06868},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {While the choice of prior is one of the most critical parts of the Bayesian inference workflow, recent Bayesian deep learning models have often fallen back on uninformative priors, such as standard Gaussians. In this review, we highlight the importance of prior choices for Bayesian deep learning and present an overview of different priors that have been proposed for (deep) Gaussian processes, variational autoencoders, and Bayesian neural networks. We also outline different methods of learning priors for these models from data. We hope to motivate practitioners in Bayesian deep learning to think more carefully about the prior specification for their models and to provide them with some inspiration in this regard.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@phdthesis{fraccaro_deep_2018,
  title = {Deep {{Latent Variable Models}} for {{Sequential Data}}},
  author = {Fraccaro, Marco},
  year = {2018},
  address = {{Kongens Lyngby}},
  abstract = {Over the last few decades an ever-increasing amount of data is being collected in a wide range of applications. This has boosted the development of mathematical models that are able to analyze it and discover its underlying structure, and use the extracted information to solve a multitude of different tasks, such as for predictive modelling or pattern recognition. The available data is however often complex and high-dimensional, making traditional data analysis methods ineffective in many applications. In the recent years there has then been a big focus on the development of more powerful models, that need to be general enough to be able to handle many diverse applications and kinds of data. Some of the most interesting advancements in this research direction have been recently obtained combining ideas from probabilistic modelling and deep learning. Variational auto-encoders (VAEs), that belong to the broader family of deep latent variable models, are powerful and scalable models that can be used for unsupervised learning of complex high-dimensional data distributions. They achieve this by parameterizing expressive probability distributions over the latent variables of the model using deep neural networks. VAEs can be used in applications with static data, for example as a generative model of images, but they are not suitable to model temporal data such as the sequences of images that form a video. However, a major part of the data that is being collected has a sequential nature, and finding powerful architectures that are able to model it is therefore fundamental. In the first part of the thesis we will introduce a broad class of deep latent variable models for sequential data, that can be used for unsupervised learning of complex and high-dimensional sequential data distributions. We obtain these models by extending VAEs to the temporal setting, and further combining ideas from deep learning (e.g. deep and recurrent neural networks) and probabilistic modelling (e.g. state-space models) to define generative models for the data that use deep neural networks to parameterize very flexible probability distributions. This results in a family of powerful architectures that can model a wide range of complex temporal data, and can be trained in a scalable way using large unlabelled datasets. In the second part of the thesis we will then present in detail three architectures belonging to this family of models. First, we will introduce stochastic recurrent neural networks (Fraccaro et al., 2016c), that combine the expressiveness of recurrent neural networks and the ability of state-space models to model the uncertainty in the learned latent representation. We will then present Kalman variational auto-encoders (Fraccaro et al., 2017), that can learn from data disentangled and more interpretable visual and dynamic representations. Finally, we will show that to deal with temporal applications that require a high memory capacity we can combine deep latent variable models with external memory architectures, as in the generative temporal model with spatial memory of Fraccaro et al., (2018).},
  langid = {english},
  school = {Technical University of Denmark},
  keywords = {⛔ No DOI found}
}

@inproceedings{fraccaro_sequential_2016,
  title = {Sequential {{Neural Models}} with {{Stochastic Layers}}},
  booktitle = {Proceedings of the 30th {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Fraccaro, Marco and S{\o}nderby, S{\o}ren Kaae and Paquet, Ulrich and Winther, Ole},
  year = {2016},
  address = {{Barcelona, Spain}},
  abstract = {How can we efficiently propagate uncertainty in a latent state representation with recurrent neural networks? This paper introduces stochastic recurrent neural networks which glue a deterministic recurrent neural network and a state space model together to form a stochastic and sequential neural generative model. The clear separation of deterministic and stochastic layers allows a structured variational inference network to track the factorization of the model's posterior distribution. By retaining both the nonlinear recursive structure of a recurrent neural network and averaging over the uncertainty in a latent path, like a state space model, we improve the state of the art results on the Blizzard and TIMIT speech modeling data sets by a large margin, while achieving comparable performances to competing methods on polyphonic music modeling.},
  langid = {english},
  keywords = {⛔ No DOI found}
}

@article{fukushima_neocognitron_1980,
  title = {Neocognitron: {{A Self-organizing Neural Network Model}} for a {{Mechanism}} of {{Pattern Recognition Unaffected}} by {{Shift}} in {{Position}}},
  author = {Fukushima, Kunihiko},
  year = {1980},
  journal = {Biological Cybernetics},
  volume = {36},
  number = {4},
  pages = {193--202},
  abstract = {A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by ``learning without a teacher'', and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname ``neocognitron''. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of ``S-cells'', which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of ``C-cells'' similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any ``teacher'' during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.}
}

@article{gaetz_massive_2018,
  title = {Massive Cortical Reorganization Is Reversible Following Bilateral Transplants of the Hands: Evidence from the First Successful Bilateral Pediatric Hand Transplant Patient},
  author = {Gaetz, William and Kessler, Sudha K. and Roberts, Tim P. L. and Berman, Jeffrey I. and Levy, Todd J. and Hsia, Michelle and Humpl, Deborah and Schwartz, Erin S. and Amaral, Sandra and Chang, Ben and Levin, Lawrence Scott},
  year = {2018},
  month = dec,
  journal = {Annals of Clinical and Translational Neurology},
  volume = {5},
  number = {1},
  eprint = {1712.01815},
  eprinttype = {arxiv},
  pages = {92--97},
  issn = {23289503},
  abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
  archiveprefix = {arXiv},
  isbn = {3013372370},
  pmid = {29376095}
}

@misc{gal_dropout_2016,
  title = {Dropout as a {{Bayesian Approximation}}: {{Representing Model Uncertainty}} in {{Deep Learning}}},
  shorttitle = {Dropout as a {{Bayesian Approximation}}},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  year = {2016},
  month = oct,
  eprint = {1506.02142},
  eprinttype = {arxiv},
  abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs \textendash extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and nonlinearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{gal_dropout_nodate,
  title = {Dropout as a {{Bayesian Approximation}}: {{Insights}} and {{Applications}}},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  pages = {10},
  abstract = {Deep learning techniques are used more and more often, but they lack the ability to reason about uncertainty over the features. Features extracted from a dataset are given as point estimates, and do not capture how much the model is confident in its estimation. This is in contrast to probabilistic Bayesian models, which allow reasoning about model confidence, but often with the price of diminished performance.},
  langid = {english},
  keywords = {⛔ No DOI found}
}

@misc{gal_theoretically_2015,
  title = {A {{Theoretically Grounded Application}} of {{Dropout}} in {{Recurrent Neural Networks}}},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  year = {2015},
  eprint = {1512.05287},
  eprinttype = {arxiv},
  abstract = {Recurrent neural networks (RNNs) stand at the forefront of many recent developments in deep learning. Yet a major difficulty with these models is their tendency to overfit, with dropout shown to fail when applied to recurrent layers. Recent results at the intersection of Bayesian modelling and deep learning offer a Bayesian interpretation of common deep learning techniques such as dropout. This grounding of dropout in approximate Bayesian inference suggests an extension of the theoretical results, offering insights into the use of dropout with RNN models. We apply this new variational inference based dropout technique in LSTM and GRU models, assessing it on language modelling and sentiment analysis tasks. The new approach outperforms existing techniques, and to the best of our knowledge improves on the single model state-of-the-art in language modelling with the Penn Treebank (73.4 test perplexity). This extends our arsenal of variational tools in deep learning.},
  archiveprefix = {arXiv},
  isbn = {9789537619084},
  pmid = {21803542}
}

@phdthesis{gal_uncertainty_2016,
  title = {Uncertainty in {{Deep Learning}}},
  author = {Gal, Yarin},
  year = {2016},
  number = {October},
  issn = {18736246},
  abstract = {Deep learning has attracted tremendous attention from researchers in various fields of information engineering such as AI, computer vision, and language processing [Kalchbrenner and Blunsom, 2013; Krizhevsky et al., 2012; Mnih et al., 2013], but also from more traditional sciences such as physics, biology, and manufacturing [Anjos et al., 2015; Baldi et al., 2014; Bergmann et al., 2014]. Neural networks, image processing tools such as convolutional neural networks, sequence processing models such as recurrent neural networks, and regularisation tools such as dropout, are used extensively. However, fields such as physics, biology, and manufacturing are ones in which representing model uncertainty is of crucial importance [Ghahramani, 2015; Krzywinski and Altman, 2013]. With the recent shift in many of these fields towards the use of Bayesian uncertainty [Herzog and Ostwald, 2013; Nuzzo, 2014; Trafimow and Marks, 2015], new needs arise from deep learning. In this work we develop tools to obtain practical uncertainty estimates in deep learning, casting recent deep learning tools as Bayesian models without changing either the models or the optimisation. In the first part of this thesis we develop the theory for such tools, providing applications and illustrative examples. We tie approximate inference in Bayesian models to dropout and other stochastic regularisation techniques, and assess the approximations empirically. We give example applications arising from this connection between modern deep learning and Bayesian modelling such as active learning of image data and data efficient deep reinforcement learning. We further demonstrate the method's practicality through a survey of recent applications making use of the suggested tools in language applications, medical diagnostics, bioinformatics, image processing, and autonomous driving. In the second part of the thesis we explore its theoretical implications, and the insights stemming from the link between Bayesian modelling and deep learning. We discuss what determines model uncertainty properties, analyse the approximate inference analytically in the linear case, and theoretically examine various priors such as spike and slab priors.},
  pmid = {27178640},
  school = {University of Cambridge},
  keywords = {★}
}

@misc{gal_what_2015,
  title = {What My Deep Model Doesn't Know...},
  author = {Gal, Yarin},
  year = {2015},
  howpublished = {http://www.cs.ox.ac.uk/people/yarin.gal/website/blog\_3d801aa532c1ce.html},
  annotation = {Computer Science Department, University of Oxford}
}

@misc{gan_deep_2015,
  title = {Deep {{Temporal Sigmoid Belief Networks}} for {{Sequence Modeling}}},
  author = {Gan, Zhe and Li, Chunyuan and Henao, Ricardo and Carlson, David and Carin, Lawrence},
  year = {2015},
  month = sep,
  eprint = {1509.07087},
  eprinttype = {arxiv},
  abstract = {Deep dynamic generative models are developed to learn sequential dependencies in time-series data. The multi-layered model is designed by constructing a hierarchy of temporal sigmoid belief networks (TSBNs), defined as a sequential stack of sigmoid belief networks (SBNs). Each SBN has a contextual hidden state, inherited from the previous SBNs in the sequence, and is used to regulate its hidden bias. Scalable learning and inference algorithms are derived by introducing a recognition model that yields fast sampling from the variational posterior. This recognition model is trained jointly with the generative model, by maximizing its variational lower bound on the log-likelihood. Experimental results on bouncing balls, polyphonic music, motion capture, and text streams show that the proposed approach achieves state-of-the-art predictive performance, and has the capacity to synthesize various sequences.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{ganin_domain-adversarial_2016,
  title = {Domain-Adversarial Training of Neural Networks},
  author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c c}ois and Marchand, Mario and Lempitsky, Victor},
  year = {2016},
  journal = {Journal of Machine Learning Research},
  volume = {17},
  number = {59},
  pages = {1--35}
}

@article{ganin_unsupervised_2014,
  title = {Unsupervised {{Domain Adaptation}} by {{Backpropagation}}},
  author = {Ganin, Yaroslav and Lempitsky, Victor},
  year = {2014},
  month = sep,
  abstract = {Top-performing deep architectures are trained on massive amounts of labeled data. In the absence of labeled data for a certain task, domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain (e.g. synthetic images) are available. Here, we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain (no labeled target-domain data is necessary).  As the training progresses, the approach promotes the emergence of "deep" features that are (i) discriminative for the main learning task on the source domain and (ii) invariant with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a simple new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation.  Overall, the approach can be implemented with little effort using any of the deep-learning packages. The method performs very well in a series of image classification experiments, achieving adaptation effect in the presence of big domain shifts and outperforming previous state-of-the-art on Office datasets.},
  langid = {english},
  keywords = {Domain adaptation,Multitask learning,Semi-supervised learning,Supervised learning,Unsupervised learning}
}

@misc{gao_neural_2018,
  title = {Neural {{Approaches}} to {{Conversational AI}}},
  author = {Gao, Jianfeng and Galley, Michel and Li, Lihong},
  year = {2018},
  month = sep,
  eprint = {1809.08267},
  eprinttype = {arxiv},
  abstract = {The present paper surveys neural approaches to conversational AI that have been developed in the last few years. We group conversational systems into three categories: (1) question answering agents, (2) task-oriented dialogue agents, and (3) chatbots. For each category, we present a review of state-of-the-art neural approaches, draw the connection between them and traditional approaches, and discuss the progress that has been made and challenges still being faced, using specific systems and models as case studies.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language}
}

@misc{garipov_loss_2018,
  title = {Loss {{Surfaces}}, {{Mode Connectivity}}, and {{Fast Ensembling}} of {{DNNs}}},
  author = {Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry and Wilson, Andrew Gordon},
  year = {2018},
  eprint = {1802.10026},
  eprinttype = {arxiv},
  abstract = {The loss functions of deep neural networks are complex and their geometric properties are not well understood. We show that the optima of these complex loss functions are in fact connected by simple curves, such as a polygonal chain with only one bend, over which training and test accuracy are nearly constant. We introduce a training procedure to discover these high-accuracy pathways between modes. Inspired by this new geometric insight, we also propose a new ensembling method entitled Fast Geometric Ensembling (FGE). Using FGE we can train high-performing ensembles in the time required to train a single model. We achieve improved performance compared to the recent state-of-the-art Snapshot Ensembles, on CIFAR-10 and CIFAR-100, using state-of-the-art deep residual networks. On ImageNet we improve the top-1 error-rate of a pre-trained ResNet by 0.56\% by running FGE for just 5 epochs.},
  archiveprefix = {arXiv}
}

@misc{garofolo_timit_1993,
  title = {{{TIMIT Acoustic-Phonetic Continuous Speech Corpus LDC93S1}}},
  author = {Garofolo, John S.},
  year = {1993},
  publisher = {{Linguistic Data Consortium}},
  annotation = {Web Download}
}

@misc{gatys_neural_2015,
  title = {A {{Neural Algorithm}} of {{Artistic Style}}},
  author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
  year = {2015},
  eprint = {1508.06576},
  eprinttype = {arxiv},
  abstract = {In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks. Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.},
  archiveprefix = {arXiv},
  isbn = {2200000006},
  pmid = {1000200972}
}

@misc{ge_speaker_2017,
  title = {Speaker {{Change Detection Using Features}} through {{A Neural Network Speaker Classifier}}},
  author = {Ge, Zhenhao and Iyer, Ananth N. and Cheluvaraja, Srinath and Ganapathiraju, Aravind},
  year = {2017},
  month = feb,
  eprint = {1702.02285},
  eprinttype = {arxiv},
  abstract = {The mechanism proposed here is for real-time speaker change detection in conversations, which firstly trains a neural network text-independent speaker classifier using in-domain speaker data. Through the network, features of conversational speech from out-of-domain speakers are then converted into likelihood vectors, i.e. similarity scores comparing to the in-domain speakers. These transformed features demonstrate very distinctive patterns, which facilitates differentiating speakers and enable speaker change detection with some straight-forward distance metrics. The speaker classifier and the speaker change detector are trained/tested using speech of the first 200 (in-domain) and the remaining 126 (out-of-domain) male speakers in TIMIT respectively. For the speaker classification, 100\% accuracy at a 200 speaker size is achieved on any testing file, given the speech duration is at least 0.97 seconds. For the speaker change detection using speaker classification outputs, performance based on 0.5, 1, and 2 seconds of inspection intervals were evaluated in terms of error rate and F1 score, using synthesized data by concatenating speech from various speakers. It captures close to 97\% of the changes by comparing the current second of speech with the previous second, which is very competitive among literature using other methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Sound}
}

@article{gehring_convolutional_nodate,
  title = {A {{Convolutional Encoder Model}} for {{Neural Machine Translation}}},
  author = {Gehring, Jonas and Auli, Michael and Grangier, David and Dauphin, Yann N},
  abstract = {The prevalent approach to neural machine translation relies on bi-directional LSTMs to encode the source sentence. We present a faster and simpler architecture based on a succession of convolutional layers. This allows to encode the source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies. On WMT'16 English-Romanian translation we achieve competitive accuracy to the state-of-the-art and on WMT'15 English-German we outper-form several recently published results. Our models obtain almost the same accuracy as a very deep LSTM setup on WMT'14 English-French translation. We speed up CPU decoding by more than two times at the same or higher accuracy as a strong bi-directional LSTM. 1}
}

@book{gelman_bayesian_2013,
  title = {Bayesian Data Analysis, Third Edition},
  author = {Gelman, A. and Carlin, J.B. and Stern, H.S. and Dunson, D.B. and Vehtari, A. and Rubin, D.B.},
  year = {2013},
  series = {Chapman \& {{Hall}}/{{CRC}} Texts in Statistical Science},
  publisher = {{Taylor \& Francis}},
  isbn = {978-1-4398-4095-5},
  lccn = {2013039507}
}

@article{gelman_bayesian_nodate,
  title = {Bayesian {{Data Analysis Third}} Edition (with Errors Fixed as of 25 {{August}} 2020)},
  author = {Gelman, Andrew and Carlin, John B and Stern, Hal S and Dunson, David B and Vehtari, Aki and Rubin, Donald B},
  pages = {677},
  langid = {english}
}

@misc{gelman_what_2021,
  title = {What Are the Most Important Statistical Ideas of the Past 50 Years?},
  author = {Gelman, Andrew and Vehtari, Aki},
  year = {2021},
  month = jan,
  eprint = {2012.00174},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {We argue that the most important statistical ideas of the past half century are: counterfactual causal inference, bootstrapping and simulation-based inference, overparameterized models and regularization, multilevel models, generic computation algorithms, adaptive decision analysis, robust inference, and exploratory data analysis. We discuss common features of these ideas, how they relate to modern computing and big data, and how they might be developed and extended in future decades. The goal of this article is to provoke thought and discussion regarding the larger themes of research in statistics and data science.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Methodology}
}

@inproceedings{germain_made_2015,
  title = {{{MADE}}: {{Masked Autoencoder}} for {{Distribution Estimation}}},
  shorttitle = {{{MADE}}},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Machine Learning}}},
  author = {Germain, Mathieu and Gregor, Karol and Murray, Iain and Larochelle, Hugo},
  year = {2015},
  eprint = {1502.03509},
  eprinttype = {arxiv},
  address = {{Lille, France}},
  abstract = {There has been a lot of recent interest in designing neural network models to estimate a distribution from a set of examples. We introduce a simple modification for autoencoder neural networks that yields powerful generative models. Our method masks the autoencoder's parameters to respect autoregressive constraints: each input is reconstructed only from previous inputs in a given ordering. Constrained this way, the autoencoder outputs can be interpreted as a set of conditional probabilities, and their product, the full joint probability. We can also train a single network that can decompose the joint probability in multiple different orderings. Our simple framework can be applied to multiple architectures, including deep ones. Vectorized implementations, such as on GPUs, are simple and fast. Experiments demonstrate that this approach is competitive with stateof-the-art tractable distribution estimators. At test time, the method is significantly faster and scales better than other autoregressive estimators.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
}

@inproceedings{gers_recurrent_2000,
  title = {Recurrent Nets That Time and Count},
  booktitle = {Proceedings of the {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Gers, F.A. and Schmidhuber, J{\"u}rgen},
  year = {2000},
  pages = {189-194 vol.3},
  abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein-protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-{$\alpha$}-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD {$\leq$} 2.0 \AA{} for the interface backbone atoms) increased from 21\% with default Glide SP settings to 58\% with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63\% success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40\% of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.}
}

@article{geweke_antithetic_1988,
  title = {Antithetic Acceleration of {{Monte Carlo}} Integration in {{Bayesian}} Inference},
  author = {Geweke, John},
  year = {1988},
  journal = {Journal of Econometrics},
  volume = {38},
  number = {1-2},
  pages = {73--89},
  publisher = {{North-Holland}},
  issn = {03044076},
  abstract = {It is proposed to sample antithetically rather than randomly from the posterior density in Bayesian inference using Monte Carlo integration. Conditions are established under which the number of replications required with antithetic sampling relative to the number required with random sampling is inversely proportional to sample size, as sample size increases. The result is illustrated in an experiment using a bivariate vector autoregression. \textcopyright{} 1988.}
}

@article{geyer_practical_1992,
  title = {Practical {{Markov Chain Monte Carlo}}},
  author = {Geyer, Charles J.},
  year = {1992},
  month = nov,
  journal = {Statistical Science},
  volume = {7},
  number = {4},
  pages = {473--483},
  issn = {0883-4237},
  abstract = {Markov chain Monte Carlo using the Metropolis-Hastings algorithm is a general method for the simulation of stochastic processes having probability densities known up to a constant of proportionality. Despite recent advances in its theory, the practice has remained controversial. This article makes the case for basing all inference on one long run of the Markov chain and estimating the Monte Carlo error by standard nonparametric methods well-known in the time-series and operations research literature. In passing it touches on the Kipnis-Varadhan central limit theorem for reversible Markov chains, on some new variance estimators, on judging the relative efficiency of competing Monte Carlo schemes, on methods for constructing more rapidly mixing Markov chains and on diagnostics for Markov chain Monte Carlo.},
  langid = {english}
}

@article{ghahramani_probabilistic_2015,
  title = {Probabilistic Machine Learning and Artificial Intelligence},
  author = {Ghahramani, Zoubin},
  year = {2015},
  journal = {Nature},
  volume = {521},
  number = {7553},
  issn = {0028-0836},
  abstract = {How can a machine learn from experience? Probabilistic modelling provides a framework for understanding what learning is, and has therefore emerged as one of the principal theoretical and practical approaches for designing machines that learn from data acquired through experience. The probabilistic framework, which describes how to represent and manipulate uncertainty about models and predictions, has a central role in scientific data analysis, machine learning, robotics, cognitive science and artificial intelligence. This Review provides an introduction to this framework, and discusses some of the state-of-the-art advances in the field, namely, probabilistic programming, Bayesian optimization, data compression and automatic model discovery.},
  isbn = {0028-0836},
  pmid = {26017444}
}

@article{ghosh_deep_2021,
  title = {Deep {{Clustering For General-Purpose Audio Representations}}},
  author = {Ghosh, Sreyan and Katta, Sandesh V. and Seth, Ashish and Umesh, S.},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.08895 [cs, eess]},
  eprint = {2110.08895},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {We introduce DECAR, a self-supervised pre-training approach for learning general-purpose audio representations. Our system is based on clustering: it utilizes an offline clustering step to provide target labels that act as pseudo-labels for solving a prediction task. We develop on top of recent advances in self-supervised learning for computer vision and design a lightweight, easy-to-use self-supervised pre-training scheme. We pre-train DECAR embeddings on a balanced subset of the large-scale Audioset dataset and transfer those representations to 9 downstream classification tasks, including speech, music, animal sounds, and acoustic scenes. Furthermore, we conduct ablation studies identifying key design choices and also make all our code and pre-trained models publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@misc{gidney_how_2019,
  title = {How to Factor 2048 Bit {{RSA}} Integers in 8 Hours Using 20 Million Noisy Qubits},
  author = {Gidney, Craig and Eker{\aa}, Martin},
  year = {2019},
  month = may,
  eprint = {1905.09749},
  eprinttype = {arxiv},
  abstract = {We significantly reduce the cost of factoring integers and computing discrete logarithms over finite fields on a quantum computer by combining techniques from Griffiths-Niu 1996, Zalka 2006, Fowler 2012, Eker\{\textbackslash aa\}-H\{\textbackslash aa\}stad 2017, Eker\{\textbackslash aa\} 2017, Eker\{\textbackslash aa\} 2018, Gidney-Fowler 2019, Gidney 2019. We estimate the approximate cost of our construction using plausible physical assumptions for large-scale superconducting qubit platforms: a planar grid of qubits with nearest-neighbor connectivity, a characteristic physical gate error rate of \$10\^\{-3\}\$, a surface code cycle time of 1 microsecond, and a reaction time of 10 micro-seconds. We account for factors that are normally ignored such as noise, the need to make repeated attempts, and the spacetime layout of the computation. When factoring 2048 bit RSA integers, our construction's spacetime volume is a hundredfold less than comparable estimates from earlier works (Fowler et al. 2012, Gheorghiu et al. 2019). In the abstract circuit model (which ignores overheads from distillation, routing, and error correction) our construction uses \$3 n + 0.002 n \textbackslash lg n\$ logical qubits, \$0.3 n\^3 + 0.0005 n\^3 \textbackslash lg n\$ Toffolis, and \$500 n\^2 + n\^2 \textbackslash lg n\$ measurement depth to factor \$n\$-bit RSA integers. We quantify the cryptographic implications of our work, both for RSA and for schemes based on the DLP in finite fields.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Quantum Physics}
}

@article{gillman_car_1992,
  title = {The Car and the Goats},
  author = {Gillman, Leonard},
  year = {1992},
  journal = {The American Mathematical Monthly},
  volume = {99},
  number = {1},
  pages = {3--7},
  publisher = {{Mathematical Association of America}},
  issn = {00029890, 19300972}
}

@article{gilmartin_just_2018,
  title = {Just {{Talking}} - {{Modelling Casual Conversation}}},
  author = {Gilmartin, Emer and Saam, Christian and Vogel, Carl and Campbell, Nick and Wade, Vincent},
  year = {2018},
  pages = {9},
  abstract = {Casual conversation has become a focus for dialogue applications. Such talk is ubiquitous and its structure differs from that found in the task-based interactions that have been the focus of dialogue system design for many years. It is unlikely that such conversations can be modelled as an extension of task-based talk. We review theories of casual conversation, report on our studies of the structure of casual dialogue, and outline challenges we see for the development of spoken dialog systems capable of carrying on casual friendly conversation in addition to performing welldefined tasks.},
  langid = {english},
  keywords = {⛔ No DOI found}
}

@inproceedings{girshick_fast_2015,
  title = {Fast R-Cnn},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Girshick, Ross},
  year = {2015},
  pages = {1440--1448}
}

@article{girshick_region-based_2016,
  title = {Region-Based Convolutional Networks for Accurate Object Detection and Segmentation},
  author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  year = {2016},
  journal = {IEEE transactions on pattern analysis and machine intelligence},
  volume = {38},
  number = {1},
  pages = {142--158},
  publisher = {{IEEE}}
}

@inproceedings{girshick_rich_2014,
  title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CCVPR}})},
  author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  year = {2014},
  pages = {580--587}
}

@misc{giryes_deep_2015,
  title = {Deep {{Neural Networks}} with {{Random Gaussian Weights}}: {{A Universal Classification Strategy}}?},
  author = {Giryes, Raja and Sapiro, Guillermo and Bronstein, Alex M.},
  year = {2015},
  month = apr,
  eprint = {1504.08291},
  eprinttype = {arxiv},
  abstract = {Three important properties of a classification machinery are: (i) the system preserves the core information of the input data; (ii) the training examples convey information about unseen data; and (iii) the system is able to treat differently points from different classes. In this work we show that these fundamental properties are satisfied by the architecture of deep neural networks. We formally prove that these networks with random Gaussian weights perform a distance-preserving embedding of the data, with a special treatment for in-class and out-of-class data. Similar points at the input of the network are likely to have a similar output. The theoretical analysis of deep networks here presented exploits tools used in the compressed sensing and dictionary learning literature, thereby making a formal connection between these important topics. The derived results allow drawing conclusions on the metric learning properties of the network and their relation to its structure, as well as providing bounds on the required size of the training set such that the training examples would represent faithfully the unseen data. The results are validated with state-of-the-art trained networks.},
  archiveprefix = {arXiv}
}

@inproceedings{glarner_full_2018,
  title = {Full {{Bayesian Hidden Markov Model Variational Autoencoder}} for {{Acoustic Unit Discovery}}},
  booktitle = {19th {{Annual Conference}} of the {{International Speech Communication Association}} ({{Interspeech}})},
  author = {Glarner, Thomas and Hanebrink, Patrick and Ebbers, Janek and {Haeb-Umbach}, Reinhold},
  year = {2018},
  month = sep,
  pages = {2688--2692},
  publisher = {{ISCA}},
  address = {{Hyderabad, India}},
  abstract = {The invention of the Variational Autoencoder enables the application of Neural Networks to a wide range of tasks in unsupervised learning, including the field of Acoustic Unit Discovery (AUD). The recently proposed Hidden Markov Model Variational Autoencoder (HMMVAE) allows a joint training of a neural network based feature extractor and a structured prior for the latent space given by a Hidden Markov Model. It has been shown that the HMMVAE significantly outperforms pure GMM-HMM based systems on the AUD task. However, the HMMVAE cannot autonomously infer the number of acoustic units and thus relies on the GMM-HMM system for initialization. This paper introduces the Bayesian Hidden Markov Model Variational Autoencoder (BHMMVAE) which solves these issues by embedding the HMMVAE in a Bayesian framework with a Dirichlet Process Prior for the distribution of the acoustic units, and diagonal or full-covariance Gaussians as emission distributions. Experiments on TIMIT and Xitsonga show that the BHMMVAE is able to autonomously infer a reasonable number of acoustic units, can be initialized without supervision by a GMM-HMM system, achieves computationally efficient stochastic variational inference by using natural gradient descent, and, additionally, improves the AUD performance over the HMMVAE.},
  langid = {english}
}

@book{glasserman_monte_2003,
  title = {Monte {{Carlo Methods}} in {{Financial Engineering}}},
  author = {Glasserman, Paul},
  year = {2003},
  volume = {53},
  eprint = {1011.1669v3},
  eprinttype = {arxiv},
  issn = {14697688},
  abstract = {This book develops the use of Monte Carlo methods in finance and it also uses simulation as a vehicle for presenting models and ideas from financial engineering. It divides roughly into three parts.},
  archiveprefix = {arXiv},
  isbn = {978-1-4419-1822-2},
  pmid = {25246403}
}

@article{glorot_deep_2011,
  title = {Deep Sparse Rectifier Neural Networks},
  author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  year = {2011},
  journal = {Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)},
  volume = {15},
  eprint = {1502.03167},
  eprinttype = {arxiv},
  pages = {315--323},
  abstract = {While logistic sigmoid neurons are more biologically plausable that hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros, which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabelled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labelled data sets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised nueral networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training},
  archiveprefix = {arXiv}
}

@inproceedings{glorot_understanding_2010,
  title = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  booktitle = {Proceedings of the {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}} ({{AISTATS}})},
  author = {Glorot, Xavier and Bengio, Yoshua},
  year = {2010},
  volume = {9},
  pages = {249--256},
  abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
}

@article{glynn_likelihood_1990,
  title = {Likelihood Ratio Gradient Estimation for Stochastic Systems},
  author = {Glynn, Peter W.},
  year = {1990},
  month = oct,
  journal = {Communications of the ACM},
  volume = {33},
  number = {10},
  pages = {75--84},
  issn = {0001-0782},
  abstract = {Consider a computer system having a CPU that feeds jobs to two input/output (I/O) devices having different speeds. Let \texttheta{} be the fraction of jobs routed to the first I/O device, so that 1 - \texttheta{} is the fraction routed to the second. Suppose that {$\alpha$} = {$\alpha$}(\texttheta ) is the steady-sate amount of time that a job spends in the system. Given that \texttheta{} is a decision variable, a designer might wish to minimize {$\alpha$}(\texttheta ) over \texttheta. Since {$\alpha$}({$\cdot$}) is typically difficult to evaluate analytically, Monte Carlo optimization is an attractive methodology. By analogy with deterministic mathematical programming, efficient Monte Carlo gradient estimation is an important ingredient of simulation-based optimization algorithms. As a consequence, gradient estimation has recently attracted considerable attention in the simulation community. It is our goal, in this article, to describe one efficient method for estimating gradients in the Monte Carlo setting, namely the likelihood ratio method (also known as the efficient score method). This technique has been previously described (in less general settings than those developed in this article) in [6, 16, 18, 21]. An alternative gradient estimation procedure is infinitesimal perturbation analysis; see [11, 12] for an introduction. While it is typically more difficult to apply to a given application than the likelihood ratio technique of interest here, it often turns out to be statistically more accurate.In this article, we first describe two important problems which motivate our study of efficient gradient estimation algorithms. Next, we will present the likelihood ratio gradient estimator in a general setting in which the essential idea is most transparent. The section that follows then specializes the estimator to discrete-time stochastic processes. We derive likelihood-ratio-gradient estimators for both time-homogeneous and non-time homogeneous discrete-time Markov chains. Later, we discuss likelihood ratio gradient estimation in continuous time. As examples of our analysis, we present the gradient estimators for time-homogeneous continuous-time Markov chains; non-time homogeneous continuous-time Markov chains; semi-Markov processes; and generalized semi-Markov processes. (The analysis throughout these sections assumes the performance measure that defines {$\alpha$}(\texttheta ) corresponds to a terminating simulation.) Finally, we conclude the article with a brief discussion of the basic issues that arise in extending the likelihood ratio gradient estimator to steady-state performance measures.}
}

@article{gong_ssast_2021,
  title = {{{SSAST}}: {{Self-Supervised Audio Spectrogram Transformer}}},
  shorttitle = {{{SSAST}}},
  author = {Gong, Yuan and Lai, Cheng-I. Jeff and Chung, Yu-An and Glass, James},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.09784 [cs, eess]},
  eprint = {2110.09784},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {Recently, neural networks based purely on self-attention, such as the Vision Transformer (ViT), have been shown to outperform deep learning models constructed with convolutional neural networks (CNNs) on various vision tasks, thus extending the success of Transformers, which were originally developed for language processing, to the vision domain. A recent study showed that a similar methodology can also be applied to the audio domain. Specifically, the Audio Spectrogram Transformer (AST) achieves state-of-the-art results on various audio classification benchmarks. However, pure Transformer models tend to require more training data compared to CNNs, and the success of the AST relies on supervised pretraining that requires a large amount of labeled data and a complex training pipeline, thus limiting the practical usage of AST. This paper focuses on audio and speech classification, and aims to alleviate the data requirement issues with the AST by leveraging self-supervised learning using unlabeled data. Specifically, we propose to pretrain the AST model with joint discriminative and generative masked spectrogram patch modeling (MSPM) using unlabeled audio from AudioSet and Librispeech. We evaluate our pretrained models on both audio and speech classification tasks including audio event classification, keyword spotting, emotion recognition, and speaker identification. The proposed self-supervised framework significantly boosts AST performance on all tasks, with an average improvement of 60.9\%, leading to similar or even better results than a supervised pretrained AST. To the best of our knowledge, it is the first patch-based self-supervised learning framework in the audio and speech domain, and also the first self-supervised learning framework for AST.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@book{goodfellow_deep_2016,
  title = {Deep {{Learning}}},
  author = {Goodfellow, Ian J. and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  journal = {MIT Press},
  edition = {First},
  publisher = {{MIT Press}},
  address = {{Cambridge, Massachusetts}},
  abstract = {Deep learning draws upon many modeling formalisms that researchers can use to guide their design efforts and describe their algorithms. One of these formalisms is the idea of structured probabilistic models. We have already discussed structured probabilistic models briefly in Chapter 3.14. That brief presentation was sufficient to understand how to use structured probabilistic models as a language to describe some of the algorithms in part II of this book. Now, in part III, structured probabilistic models are a key ingredient of many of the most important research topics in deep learning. In order to prepare to discuss these research ideas, this chapter describes structured probabilistic models in much greater detail. This chapter is intended to be self-contained; the reader does not need to review the earlier introduction before continuing with this chapter. A structured probabilistic model is a way of describing a probability distribu-tion, using a graph to describe which random variables in the probability distri-bution interact with each other directly. Here we use " graph " in the graph theory sense\textendash a set of vertices connected to one another by a set of edges. Because the structure of the model is defined by a graph, these models are often also referred to as graphical models. The graphical models research community is large and has developed many different models, training algorithms, and inference algorithms. In this chap-ter, we provide basic background on some of the most central ideas of graphical models, with an emphasis on the concepts that have proven most useful to the deep learning research community. If you already have a strong background in graphical models, you may wish to skip most of this chapter. However, even a graphical model expert may benefit from reading the final section of this chap-ter, section 13.6, in which we highlight some of the unique ways that graphical 412 CHAPTER 13. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING models are used for deep learning algorithms. Deep learning practitioners tend to use very different model structures, learning algorithms, and inference procedures than are commonly used by the rest of the graphical models research community. In this chapter, we identify these differences in preferences and explain the reasons for them. In this chapter we first describe the challenges of building large-scale proba-bilistic models in section 13.1. Next, we describe how to use a graph to describe the structure of a probability distribution in section 13.2. We then revisit the challenges we described in section 13.1 and show how the structured approach to probabilistic modeling can overcome these challenges in section 13.3. One of the major difficulties in graphical modeling is understanding which variables need to be able to interact directly, i.e., which graph structures are most suitable for a given problem. We outline two approaches to resolving this difficulty by learning about the dependencies in section 13.4. Finally, we close with a discussion of the unique emphasis that deep learning practitioners place on specific approaches to graphical modeling in section 13.6. 13.1 The Challenge of Unstructured Modeling},
  isbn = {978-0-262-03561-3},
  keywords = {★,Deep learning,Machine learning}
}

@inproceedings{goodfellow_explaining_2015,
  title = {Explaining and Harnessing Adversarial Examples},
  booktitle = {Proceedings of the 3rd {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
  editor = {Bengio, Yoshua and LeCun, Yann},
  year = {2015},
  address = {{San Diego, CA, USA}}
}

@article{goodfellow_generative_2014,
  title = {Generative {{Adversarial Networks}}},
  author = {Goodfellow, Ian J. and {Pouget-Abadie}, Jean and Mirza, Mehdi and Xu, Bing and {Warde-Farley}, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  year = {2014},
  month = jun,
  journal = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
  eprint = {1406.2661},
  eprinttype = {arxiv},
  pages = {2672--2680},
  issn = {10495258},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  archiveprefix = {arXiv},
  isbn = {1406.2661},
  pmid = {1000183096}
}

@misc{goodfellow_maxout_2013,
  title = {Maxout {{Networks}}},
  author = {Goodfellow, Ian J. and {Warde-Farley}, David and Mirza, Mehdi and Courville, Aaron and Bengio, Yoshua},
  year = {2013},
  month = feb,
  eprint = {1302.4389},
  eprinttype = {arxiv},
  abstract = {We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.},
  archiveprefix = {arXiv}
}

@inproceedings{goodfellow_qualitatively_2015,
  title = {Qualitatively Characterizing Neural Network Optimization Problems},
  booktitle = {Proceedings of the {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Goodfellow, Ian J and Vinyals, Oriol and Saxe, Andrew M},
  year = {2015},
  eprint = {1412.6544v5},
  eprinttype = {arxiv},
  pages = {1--11},
  abstract = {Training neural networks involves solving large-scale non-convex optimization problems. This task has long been believed to be extremely difficult, with fear of local minima and other obstacles motivating a variety of schemes to improve opti-mization, such as unsupervised pretraining. However, modern neural networks are able to achieve negligible training error on complex tasks, using only direct train-ing with stochastic gradient descent. We introduce a simple analysis technique to look for evidence that such networks are overcoming local optima. We find that, in fact, on a straight path from initialization to solution, a variety of state of the art neural networks never encounter any significant obstacles.},
  archiveprefix = {arXiv}
}

@inproceedings{gorman_we_2019,
  title = {We Need to Talk about Standard Splits},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Gorman, Kyle and Bedrick, Steven},
  year = {2019},
  month = aug,
  address = {{Florence, Italy}},
  abstract = {It is standard practice in speech \& language technology to rank systems according to per- formance on a test set held out for evalua- tion. However, few researchers apply statis- tical tests to determine whether differences in performance are likely to arise by chance, and few examine the stability of system ranking across multiple training-testing splits. We con- duct replication and reproduction experiments with nine part-of-speech taggers published be- tween 2000 and 2018, each of which reports state-of-the-art performance on a widely-used ``standard split''. We fail to reliably repro- duce some rankings using randomly generated splits. We suggest that randomly generated splits should be used in system comparison.}
}

@inproceedings{goyal_z-forcing_2017,
  title = {Z-{{Forcing}}: {{Training Stochastic Recurrent Networks}}},
  shorttitle = {Z-{{Forcing}}},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Goyal, Anirudh and Sordoni, Alessandro and C{\^o}t{\'e}, Marc-Alexandre and Ke, Nan Rosemary and Bengio, Yoshua},
  year = {2017},
  month = nov,
  eprint = {1711.05411},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  address = {{Long Beach, CA, USA}},
  abstract = {Many efforts have been devoted to training generative latent variable models with autoregressive decoders, such as recurrent neural networks (RNN). Stochastic recurrent models have been successful in capturing the variability observed in natural sequential data such as speech. We unify successful ideas from recently proposed architectures into a stochastic recurrent model: each step in the sequence is associated with a latent variable that is used to condition the recurrent dynamics for future steps. Training is performed with amortized variational inference where the approximate posterior is augmented with a RNN that runs backward through the sequence. In addition to maximizing the variational lower bound, we ease training of the latent variables by adding an auxiliary cost which forces them to reconstruct the state of the backward recurrent network. This provides the latent variables with a task-independent objective that enhances the performance of the overall model. We found this strategy to perform better than alternative approaches such as KL annealing. Although being conceptually simple, our model achieves state-of-the-art results on standard speech benchmarks such as TIMIT and Blizzard and competitive performance on sequential MNIST. Finally, we apply our model to language modeling on the IMDB dataset where the auxiliary cost helps in learning interpretable latent variables.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{grace_when_2017,
  title = {When {{Will AI Exceed Human Performance}}? {{Evidence}} from {{AI Experts}}},
  author = {Grace, Katja and Salvatier, John and Dafoe, Allan and Zhang, Baobao and Evans, Owain},
  year = {2017},
  month = may,
  eprint = {1705.08807},
  eprinttype = {arxiv},
  abstract = {Advances in artificial intelligence (AI) will transform modern life by reshaping transportation, health, science, finance, and the military. To adapt public policy, we need to better anticipate these advances. Here we report the results from a large survey of machine learning researchers on their beliefs about progress in AI. Researchers predict AI will outperform humans in many activities in the next ten years, such as translating languages (by 2024), writing high-school essays (by 2026), driving a truck (by 2027), working in retail (by 2031), writing a bestselling book (by 2049), and working as a surgeon (by 2053). Researchers believe there is a 50\% chance of AI outperforming humans in all tasks in 45 years and of automating all human jobs in 120 years, with Asian respondents expecting these dates much sooner than North Americans. These results will inform discussion amongst researchers and policymakers about anticipating and managing trends in AI.},
  archiveprefix = {arXiv}
}

@misc{grathwohl_ffjord_2018,
  title = {{{FFJORD}}: {{Free-form Continuous Dynamics}} for {{Scalable Reversible Generative Models}}},
  shorttitle = {{{FFJORD}}},
  author = {Grathwohl, Will and Chen, Ricky T. Q. and Bettencourt, Jesse and Sutskever, Ilya and Duvenaud, David},
  year = {2018},
  month = oct,
  eprint = {1810.01367},
  eprinttype = {arxiv},
  abstract = {A promising class of generative models maps points from a simple distribution to a complex distribution through an invertible neural network. Likelihood-based training of these models requires restricting their architectures to allow cheap computation of Jacobian determinants. Alternatively, the Jacobian trace can be used if the transformation is specified by an ordinary differential equation. In this paper, we use Hutchinson's trace estimator to give a scalable unbiased estimate of the log-density. The result is a continuous-time invertible generative model with unbiased density estimation and one-pass sampling, while allowing unrestricted neural network architectures. We demonstrate our approach on high-dimensional density estimation, image generation, and variational inference, achieving the state-of-the-art among exact likelihood methods with efficient sampling.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{grathwohl_oops_2021,
  title = {Oops {{I Took A Gradient}}: {{Scalable Sampling}} for {{Discrete Distributions}}},
  shorttitle = {Oops {{I Took A Gradient}}},
  author = {Grathwohl, Will and Swersky, Kevin and Hashemi, Milad and Duvenaud, David and Maddison, Chris J.},
  year = {2021},
  month = feb,
  eprint = {2102.04509},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We propose a general and scalable approximate sampling strategy for probabilistic models with discrete variables. Our approach uses gradients of the likelihood function with respect to its discrete inputs to propose updates in a MetropolisHastings sampler. We show empirically that this approach outperforms generic samplers in a number of difficult settings including Ising models, Potts models, restricted Boltzmann machines, and factorial hidden Markov models. We also demonstrate the use of our improved sampler for training deep energy-based models on high dimensional discrete data. This approach outperforms variational auto-encoders and existing energy-based models. Finally, we give bounds showing that our approach is near-optimal in the class of samplers which propose local updates.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning}
}

@inproceedings{graves_connectionist_2006,
  title = {Connectionist {{Temporal Classification}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Graves, Alex and Fern{\'a}ndez, Santiago and Gomez, Faustino and Schmidhuber, J{\"u}rgen},
  year = {2006},
  eprint = {1512.02595},
  eprinttype = {arxiv},
  pages = {369--376},
  address = {{Pittsburgh, Pennsylvania, USA}},
  issn = {10987576},
  abstract = {Many real-world sequence learning tasks re- quire the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their out- puts into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label un- segmented sequences directly, thereby solv- ing both problems. An experiment on the TIMIT speech corpus demonstrates its ad- vantages over both a baseline HMM and a hybrid HMM-RNN.},
  archiveprefix = {arXiv},
  isbn = {1-59593-383-2},
  pmid = {1000285842},
  keywords = {★}
}

@misc{graves_generating_2013,
  title = {Generating Sequences with Recurrent Neural Networks},
  author = {Graves, Alex},
  year = {2013},
  eprint = {1308.0850},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@article{graves_hybrid_2016,
  title = {Hybrid Computing Using a Neural Network with Dynamic External Memory},
  author = {Graves, Alex and Wayne, Greg and Reynolds, Malcolm and Harley, Tim and Danihelka, Ivo and {Grabska-Barwi{\'n}ska}, Agnieszka and Colmenarejo, Sergio G{\'o}mez and Grefenstette, Edward and Ramalho, Tiago and Agapiou, John and Badia, Adri{\`a} Puigdom{\`e}nech and Hermann, Karl Moritz and Zwols, Yori and Ostrovski, Georg and Cain, Adam and King, Helen and Summerfield, Christopher and Blunsom, Phil and Kavukcuoglu, Koray and Hassabis, Demis},
  year = {2016},
  month = oct,
  journal = {Nature},
  volume = {538},
  number = {7626},
  pages = {471--476},
  issn = {1476-4687},
  abstract = {Artificial neural networks are remarkably adept at sensory processing, sequence learning and reinforcement learning, but are limited in their ability to represent variables and data structures and to store data over long timescales, owing to the lack of an external memory. Here we introduce a machine learning model called a differentiable neural computer (DNC), which consists of a neural network that can read from and write to an external memory matrix, analogous to the random-access memory in a conventional computer. Like a conventional computer, it can use its memory to represent and manipulate complex data structures, but, like a neural network, it can learn to do so from data. When trained with supervised learning, we demonstrate that a DNC can successfully answer synthetic questions designed to emulate reasoning and inference problems in natural language. We show that it can learn tasks such as finding the shortest path between specified points and inferring the missing links in randomly generated graphs, and then generalize these tasks to specific graphs such as transport networks and family trees. When trained with reinforcement learning, a DNC can complete a moving blocks puzzle in which changing goals are specified by sequences of symbols. Taken together, our results demonstrate that DNCs have the capacity to solve complex, structured tasks that are inaccessible to neural networks without external read\textendash write memory.}
}

@misc{graves_neural_2014,
  title = {Neural {{Turing Machines}}},
  author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  year = {2014},
  month = oct,
  eprint = {1410.5401},
  eprinttype = {arxiv},
  abstract = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
  archiveprefix = {arXiv},
  isbn = {0028-0836},
  pmid = {18958277}
}

@inproceedings{graves_practical_2011,
  title = {Practical {{Variational Inference}} for {{Neural Networks}}.},
  booktitle = {Proceedings of the {{International Conference}} on {{Neural Information Processing Systems}} ({{NIPS}})},
  author = {Graves, Alex},
  year = {2011},
  pages = {1--9},
  abstract = {Variational methods have been previously explored as a tractable approximation to Bayesian inference for neural networks. However the approaches proposed so far have only been applicable to a few simple network architectures. This paper introduces an easy-to-implement stochastic variational method (or equivalently, minimum description length loss function) that can be applied to most neural networks. Along the way it revisits several common regularisers from a variational perspective. It also provides a simple pruning heuristic that can both drastically reduce the number of network weights and lead to improved generalisation. Experimental results are provided for a hierarchical multidimensional recurrent neural network applied to the TIMIT speech corpus.},
  isbn = {978-1-61839-599-3}
}

@misc{graves_sequence_2012,
  title = {Sequence {{Transduction}} with {{Recurrent Neural Networks}}},
  author = {Graves, Alex},
  year = {2012},
  month = nov,
  eprint = {1211.3711},
  eprinttype = {arxiv},
  abstract = {Many machine learning tasks can be expressed as the transformation\textemdash or transduction\textemdash of input sequences into output sequences: speech recognition, machine translation, protein secondary structure prediction and text-to-speech to name but a few. One of the key challenges in sequence transduction is learning to represent both the input and output sequences in a way that is invariant to sequential distortions such as shrinking, stretching and translating. Recurrent neural networks (RNNs) are a powerful sequence learning architecture that has proven capable of learning such representations. However RNNs traditionally require a pre-defined alignment between the input and output sequences to perform transduction. This is a severe limitation since finding the alignment is the most difficult aspect of many sequence transduction problems. Indeed, even determining the length of the output sequence is often challenging. This paper introduces an end-to-end, probabilistic sequence transduction system, based entirely on RNNs, that is in principle able to transform any input sequence into any finite, discrete output sequence. Experimental results for phoneme recognition are provided on the TIMIT speech corpus.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
}

@inproceedings{graves_speech_2013,
  title = {Speech Recognition with Deep Recurrent Neural Networks},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey E.},
  year = {2013},
  pages = {6645--6649}
}

@book{graves_supervised_2012,
  title = {Supervised {{Sequence Labelling}} with {{Recurrent Neural Networks}}},
  author = {Graves, Alex},
  year = {2012},
  month = feb
}

@inproceedings{graves_towards_2014,
  title = {Towards {{End-To-End Speech Recognition}} with {{Recurrent Neural Networks}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Graves, Alex and Jaitly, Navdeep},
  year = {2014},
  month = jan,
  pages = {1764--1772},
  abstract = {This paper presents a speech recognition system that directly transcribes audio data with text, without requiring an intermediate phonetic representation. The system is based on a combination of th...},
  langid = {english}
}

@article{greff_lstm_2017,
  title = {{{LSTM}}: {{A Search Space Odyssey}}},
  author = {Greff, Klaus and Srivastava, Rupesh K. and Koutn{\'i}k, Jan and Steunebrink, Bas R. and Schmidhuber, J{\"u}rgen},
  year = {2017},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {28},
  number = {10},
  eprint = {1503.04069},
  eprinttype = {arxiv},
  pages = {2222--2232},
  issn = {21622388},
  abstract = {Several variants of the Long Short-Term Memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search, and their importance was assessed using the powerful fANOVA framework. In total, we summarize the results of 5400 experimental runs (\$\textbackslash approx 15\$ years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.},
  archiveprefix = {arXiv},
  isbn = {9788578110796},
  pmid = {25246403},
  keywords = {Long short-term memory (LSTM),Random search,recurrent neural networks,Sequence learning}
}

@misc{gregor_deep_2014,
  title = {Deep {{AutoRegressive Networks}}},
  author = {Gregor, Karol and Danihelka, Ivo and Mnih, Andriy and Blundell, Charles and Wierstra, Daan},
  year = {2014},
  month = may,
  eprint = {1310.8499},
  eprinttype = {arxiv},
  abstract = {We introduce a deep, generative autoencoder capable of learning hierarchies of distributed representations from data. Successive deep stochastic hidden layers are equipped with autoregressive connections, which enable the model to be sampled from quickly and exactly via ancestral sampling. We derive an efficient approximate parameter estimation method based on the minimum description length (MDL) principle, which can be seen as maximising a variational lower bound on the log-likelihood, with a feedforward neural network implementing approximate inference. We demonstrate state-of-the-art generative performance on a number of classic data sets: several UCI data sets, MNIST and Atari 2600 games.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{gregor_draw_2015,
  title = {{{DRAW}}: {{A}} Recurrent Neural Network for Image Generation},
  author = {Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo Jimenez and Wierstra, Daan},
  year = {2015},
  eprint = {1502.04623},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@misc{gregor_temporal_2019,
  title = {Temporal {{Difference Variational Auto-Encoder}}},
  author = {Gregor, Karol and Papamakarios, George and Besse, Frederic and Buesing, Lars and Weber, Theophane},
  year = {2019},
  month = jan,
  eprint = {1806.03107},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {To act and plan in complex environments, we posit that agents should have a mental simulator of the world with three characteristics: (a) it should build an abstract state representing the condition of the world; (b) it should form a belief which represents uncertainty on the world; (c) it should go beyond simple step-by-step simulation, and exhibit temporal abstraction. Motivated by the absence of a model satisfying all these requirements, we propose TD-VAE, a generative sequence model that learns representations containing explicit beliefs about states several steps into the future, and that can be rolled out directly without single-step transitions. TD-VAE is trained on pairs of temporally separated time points, using an analogue of temporal difference learning used in reinforcement learning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{gu_muprop_2016,
  title = {{{MuProp}}: {{Unbiased Backpropagation}} for {{Stochastic Neural Networks}}},
  shorttitle = {{{MuProp}}},
  author = {Gu, Shixiang and Levine, Sergey and Sutskever, Ilya and Mnih, Andriy},
  year = {2016},
  month = feb,
  eprint = {1511.05176},
  eprinttype = {arxiv},
  abstract = {Deep neural networks are powerful parametric models that can be trained efficiently using the backpropagation algorithm. Stochastic neural networks combine the power of large parametric functions with that of graphical models, which makes it possible to learn very complex distributions. However, as backpropagation is not directly applicable to stochastic networks that include discrete sampling operations within their computational graph, training such networks remains difficult. We present MuProp, an unbiased gradient estimator for stochastic networks, designed to make this task easier. MuProp improves on the likelihood-ratio estimator by reducing its variance using a control variate based on the first-order Taylor expansion of a mean-field network. Crucially, unlike prior attempts at using backpropagation for training stochastic networks, the resulting estimator is unbiased and well behaved. Our experiments on structured output prediction and discrete latent variable modeling demonstrate that MuProp yields consistently good performance across a range of difficult tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning}
}

@misc{gu_neural_2015,
  title = {Neural {{Adaptive Sequential Monte Carlo}}},
  author = {Gu, Shixiang and Ghahramani, Zoubin and Turner, Richard E.},
  year = {2015},
  month = nov,
  eprint = {1506.03338},
  eprinttype = {arxiv},
  abstract = {Sequential Monte Carlo (SMC), or particle filtering, is a popular class of methods for sampling from an intractable target distribution using a sequence of simpler intermediate distributions. Like other importance sampling-based methods, performance is critically dependent on the proposal distribution: a bad proposal can lead to arbitrarily inaccurate estimates of the target distribution. This paper presents a new method for automatically adapting the proposal using an approximation of the Kullback-Leibler divergence between the true posterior and the proposal distribution. The method is very flexible, applicable to any parameterized proposal distribution and it supports online and batch variants. We use the new framework to adapt powerful proposal distributions with rich parameterizations based upon neural networks leading to Neural Adaptive Sequential Monte Carlo (NASMC). Experiments indicate that NASMC significantly improves inference in a non-linear state space model outperforming adaptive proposal methods including the Extended Kalman and Unscented Particle Filters. Experiments also indicate that improved inference translates into improved parameter learning when NASMC is used as a subroutine of Particle Marginal Metropolis Hastings. Finally we show that NASMC is able to train a latent variable recurrent neural network (LV-RNN) achieving results that compete with the state-of-the-art for polymorphic music modelling. NASMC can be seen as bridging the gap between adaptive SMC methods and the recent work in scalable, black-box variational inference.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@book{gumbel_statistical_1954,
  title = {Statistical {{Theory}} of {{Extreme Values}} and {{Some Practical Applications}}. {{A Series}} of {{Lectures}}},
  author = {Gumbel, Emil Julius},
  year = {1954},
  volume = {33},
  publisher = {{National Bureau of Standards}},
  abstract = {This monograph is based on four lectures given at the National Bureau of Standards under the sponsorship of the Applied Mathematics Division. The aim of this publication is to make the statistical theory and techniques of extreme values readily available to scientists and engineers. This seems necessary because the original papers, partly written in foreign languages and published in remote journals, are not easy to find. The first lecture outlines some of the practical problems to which the theory pertains. The second lecture introduces certain new statistical tools necessary for the theory, which is developed in the third lecture, first in exact, then in asymptotic form. The fourth lecture shows a series of practical applications and gives all numerical details for enabling interested readers to apply the method to their own problems.}
}

@misc{guo_calibration_2017,
  title = {On {{Calibration}} of {{Modern Neural Networks}}},
  author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q.},
  year = {2017},
  month = jun,
  eprint = {1706.04599},
  eprinttype = {arxiv},
  abstract = {Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@inproceedings{hafner_learning_2019,
  title = {Learning Latent Dynamics for Planning from Pixels},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Hafner, Danijar and Lillicrap, Timothy and Fischer, Ian and Villegas, Ruben and Ha, David and Lee, Honglak and Davidson, James},
  year = {2019},
  month = jun,
  series = {Proceedings of Machine Learning Research},
  volume = {97},
  pages = {2555--2565},
  publisher = {{PMLR}},
  abstract = {Planning has been very successful for control tasks with known environment dynamics. To leverage planning in unknown environments, the agent needs to learn the dynamics from interactions with the world. However, learning dynamics models that are accurate enough for planning has been a long-standing challenge, especially in image-based domains. We propose the Deep Planning Network (PlaNet), a purely model-based agent that learns the environment dynamics from images and chooses actions through fast online planning in latent space. To achieve high performance, the dynamics model must accurately predict the rewards ahead for multiple time steps. We approach this using a latent dynamics model with both deterministic and stochastic transition components. Moreover, we propose a multi-step variational inference objective that we name latent overshooting. Using only pixel observations, our agent solves continuous control tasks with contact dynamics, partial observability, and sparse rewards, which exceed the difficulty of tasks that were previously solved by planning with learned models. PlaNet uses substantially fewer episodes and reaches final performance close to and sometimes higher than strong model-free algorithms.}
}

@article{hamilton_graph_2020,
  title = {Graph {{Representation Learning}}},
  author = {Hamilton, William L.},
  year = {2020},
  month = sep,
  journal = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
  volume = {14},
  number = {3},
  pages = {1--159},
  issn = {1939-4608, 1939-4616},
  langid = {english}
}

@inproceedings{han_eie_2016,
  title = {{{EIE}}: Efficient Inference Engine on Compressed Deep Neural Network},
  booktitle = {Proceedings of the 43rd {{International Symposium}} on {{Computer Architecture}}},
  author = {Han, Song and Liu, Xingyu and Mao, Huizi and Pu, Jing and Pedram, Ardavan and Horowitz, Mark A. and Dally, William J.},
  year = {2016},
  pages = {243--254}
}

@misc{hannun_deep_2014,
  title = {Deep {{Speech}}: {{Scaling}} up End-to-End Speech Recognition},
  shorttitle = {Deep {{Speech}}},
  author = {Hannun, Awni and Case, Carl and Casper, Jared and Catanzaro, Bryan and Diamos, Greg and Elsen, Erich and Prenger, Ryan and Satheesh, Sanjeev and Sengupta, Shubho and Coates, Adam and Ng, Andrew Y.},
  year = {2014},
  month = dec,
  eprint = {1412.5567},
  eprinttype = {arxiv},
  abstract = {We present a state-of-the-art speech recognition system developed using end-to-end deep learning. Our architecture is significantly simpler than traditional speech systems, which rely on laboriously engineered processing pipelines; these traditional systems also tend to perform poorly when used in noisy environments. In contrast, our system does not need hand-designed components to model background noise, reverberation, or speaker variation, but instead directly learns a function that is robust to such effects. We do not need a phoneme dictionary, nor even the concept of a "phoneme." Key to our approach is a well-optimized RNN training system that uses multiple GPUs, as well as a set of novel data synthesis techniques that allow us to efficiently obtain a large amount of varied data for training. Our system, called Deep Speech, outperforms previously published results on the widely studied Switchboard Hub5'00, achieving 16.0\% error on the full test set. Deep Speech also handles challenging noisy environments better than widely used, state-of-the-art commercial speech systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@misc{hannun_first-pass_2014,
  title = {First-{{Pass Large Vocabulary Continuous Speech Recognition}} Using {{Bi-Directional Recurrent DNNs}}},
  author = {Hannun, Awni Y. and Maas, Andrew L. and Jurafsky, Daniel and Ng, Andrew Y.},
  year = {2014},
  month = aug,
  eprint = {1408.2873},
  eprinttype = {arxiv},
  abstract = {We present a method to perform first-pass large vocabulary continuous speech recognition using only a neural network and language model. Deep neural network acoustic models are now commonplace in HMM-based speech recognition systems, but building such systems is a complex, domain-specific task. Recent work demonstrated the feasibility of discarding the HMM sequence modeling framework by directly predicting transcript text from audio. This paper extends this approach in two ways. First, we demonstrate that a straightforward recurrent neural network architecture can achieve a high level of accuracy. Second, we propose and evaluate a modified prefix-search decoding algorithm. This approach to decoding enables first-pass speech recognition with a language model, completely unaided by the cumbersome infrastructure of HMM-based systems. Experiments on the Wall Street Journal corpus demonstrate fairly competitive word error rates, and the importance of bi-directional network recurrence.},
  archiveprefix = {arXiv}
}

@misc{hansen_cma_2016,
  title = {The {{CMA Evolution Strategy}}: {{A Tutorial}}},
  author = {Hansen, Nikolaus},
  year = {2016},
  eprint = {1604.00772},
  eprinttype = {arxiv},
  abstract = {This tutorial introduces the CMA Evolution Strategy (ES), where CMA stands for Covariance Matrix Adaptation. The CMA-ES is a stochastic, or randomized, method for real-parameter (continuous domain) optimization of non-linear, non-convex functions. We try to motivate and derive the algorithm from intuitive concepts and from requirements of non-linear, non-convex search in continuous domain.},
  archiveprefix = {arXiv},
  isbn = {3540290060},
  pmid = {15003161}
}

@article{hansen_completely_2001,
  title = {Completely {{Derandomized Self-Adaptation}} in {{Evolution Strategies}}},
  author = {Hansen, Nikolaus and Ostermeier, Andreas},
  year = {2001},
  journal = {Evolutionary Computation},
  volume = {9},
  number = {2},
  pages = {159--195},
  issn = {1063-6560},
  abstract = {This paper puts forward two useful methods for self-adaptation of the mutation distribution - the concepts of derandomization and cumulation. Principle shortcomings of the concept of mutative strategy parameter control and two levels of derandomization are reviewed. Basic demands on the self-adaptation of arbitrary (normal) mutation distributions are developed. Applying arbitrary, normal mutation distributions is equivalent to applying a general, linear problem encoding. The underlying objective of mutative strategy parameter control is roughly to favor previously selected mutation steps in the future. If this objective is pursued rigorously, a completely derandomized self-adaptation scheme results, which adapts arbitrary normal mutation distributions. This scheme, called covariance matrix adaptation (CMA), meets the previously stated demands. It can still be considerably improved by cumulation - utilizing an evolution path rather than single search steps. Simulations on various test functions reveal local and global search properties of the evolution strategy with and without covariance matrix adaptation. Their performances are comparable only on perfectly scaled functions. On badly scaled, non-separable functions usually a speed up factor of several orders of magnitude is observed. On moderately mis-scaled functions a speed up factor of three to ten can be expected.},
  pmid = {11382355}
}

@article{hanson_comparing_1989,
  title = {Comparing {{Biases}} for {{Minimal Network Construction}} with {{Back-Propagation}}},
  author = {Hanson, Stephen Jos{\'e} and Pratt, Lorien},
  year = {1989},
  journal = {Advances in Neural Information Processing Systems},
  pages = {177--185},
  abstract = {Rumelhart (1987). has proposed a method for choosing minimal or "simple" representations during learning in Back-propagation networks. This approach can be used to (a) dynamically select the number of hidden units. (b) construct a representation that is appropriate for the problem and (c) thus improve the generalization ability of Back-propagation networks. The method Rumelhart suggests involves adding penalty terms to the usual error function. In this paper we introduce Rumelhart{$\cdot$}s minimal networks idea and compare two possible biases on the weight search space. These biases are compared in both simple counting problems and a speech recognition problem. In general. the constrained search does seem to minimize the number of hidden units required with an expected increase in local minima.},
  isbn = {1-558-60015-9}
}

@article{haque_audio-linguistic_2019,
  title = {Audio-{{Linguistic Embeddings}} for {{Spoken Sentences}}},
  author = {Haque, Albert and Guo, Michelle and Verma, Prateek and {Fei-Fei}, Li},
  year = {2019},
  month = feb,
  journal = {arXiv:1902.07817 [cs, eess]},
  eprint = {1902.07817},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {We propose spoken sentence embeddings which capture both acoustic and linguistic content. While existing works operate at the character, phoneme, or word level, our method learns long-term dependencies by modeling speech at the sentence level. Formulated as an audio-linguistic multitask learning problem, our encoder-decoder model simultaneously reconstructs acoustic and natural language features from audio. Our results show that spoken sentence embeddings outperform phoneme and word-level baselines on speech recognition and emotion recognition tasks. Ablation studies show that our embeddings can better model high-level acoustic concepts while retaining linguistic content. Overall, our work illustrates the viability of generic, multi-modal sentence embeddings for spoken language understanding.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@misc{hasanpour_lets_2016,
  title = {Lets Keep It Simple, {{Using}} Simple Architectures to Outperform Deeper and More Complex Architectures},
  author = {Hasanpour, Seyyed Hossein and Rouhani, Mohammad and Fayyaz, Mohsen and Sabokrou, Mohammad},
  year = {2016},
  eprint = {1608.06037},
  eprinttype = {arxiv},
  abstract = {Major winning Convolutional Neural Networks (CNNs), such as AlexNet, VGGNet, ResNet, GoogleNet, include tens to hundreds of millions of parameters, which impose considerable computation and memory overhead. This limits their practical use for training, optimization and memory efficiency. On the contrary, light-weight architectures, being proposed to address this issue, mainly suffer from low accuracy. These inefficiencies mostly stem from following an ad hoc procedure. We propose a simple architecture, called SimpleNet, based on a set of designing principles, with which we empirically show, a well-crafted yet simple and reasonably deep architecture can perform on par with deeper and more complex architectures. SimpleNet provides a good tradeoff between the computation/memory efficiency and the accuracy. Our simple 13-layer architecture outperforms most of the deeper and complex architectures to date such as VGGNet, ResNet, and GoogleNet on several well-known benchmarks while having 2 to 25 times fewer number of parameters and operations. This makes it very handy for embedded system or system with computational and memory limitations. We achieved state-of-the-art result on CIFAR10 outperforming several heavier architectures, near state of the art on MNIST and competitive results on CIFAR100 and SVHN. Models are made available at: https://github.com/Coderx7/SimpleNet},
  archiveprefix = {arXiv}
}

@misc{hashemi_learning_2018,
  title = {Learning {{Memory Access Patterns}}},
  author = {Hashemi, Milad and Swersky, Kevin and Smith, Jamie A. and Ayers, Grant and Litz, Heiner and Chang, Jichuan and Kozyrakis, Christos and Ranganathan, Parthasarathy},
  year = {2018},
  eprint = {1803.02329},
  eprinttype = {arxiv},
  abstract = {The explosion in workload complexity and the recent slow-down in Moore's law scaling call for new approaches towards efficient computing. Researchers are now beginning to use recent advances in machine learning in software optimizations, augmenting or replacing traditional heuristics and data structures. However, the space of machine learning for computer hardware architecture is only lightly explored. In this paper, we demonstrate the potential of deep learning to address the von Neumann bottleneck of memory performance. We focus on the critical problem of learning memory access patterns, with the goal of constructing accurate and efficient memory prefetchers. We relate contemporary prefetching strategies to n-gram models in natural language processing, and show how recurrent neural networks can serve as a drop-in replacement. On a suite of challenging benchmark datasets, we find that neural networks consistently demonstrate superior performance in terms of precision and recall. This work represents the first step towards practical neural-network based prefetching, and opens a wide range of exciting directions for machine learning in computer architecture research.},
  archiveprefix = {arXiv}
}

@book{hastie_elements_2009,
  title = {The {{Elements}} of {{Statistical Learning}}},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  year = {2009},
  journal = {Bayesian Forecasting and Dynamic Models},
  edition = {Second},
  publisher = {{Springer}},
  issn = {0172-7397},
  abstract = {During the past decade there has been an explosion in computation and information technology. With it has come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It should be a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting-the first comprehensive treatment of this topic in any book. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie wrote much of the statistical modeling software in S-PLUS and invented principal curves and surfaces. Tibshirani proposed the Lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, and projection pursuit. FROM THE REVIEWS: TECHNOMETRICS "This is a vast and complex book. Generally, it concentrates on explaining why and how the methods work, rather than how to use them. Examples and especially the visualizations are principle features...As a source for the methods of statistical learning...it will probably be a long time before there is a competitor to this book."},
  isbn = {978-0-387-84857-0},
  pmid = {12377617},
  keywords = {Machine learning,Optimization,Statistical learning,Statistics}
}

@inproceedings{hauberg_geometric_2012,
  title = {A {{Geometric}} Take on {{Metric Learning}}},
  booktitle = {Proceedings of the 26th {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Hauberg, S{\o}ren and Freifeld, Oren and Black, Michael J},
  year = {2012},
  pages = {2033--2041},
  address = {{Lake Tahoe, Nevada, USA}},
  abstract = {Multi-metric learning techniques learn local metric tensors in different parts of a feature space. With such an approach, even simple classifiers can be competitive with the state-of-the-art because the distance measure locally adapts to the structure of the data. The learned distance measure is, however, non-metric, which has prevented multi-metric learning from generalizing to tasks such as dimensionality reduction and regression in a principled way. We prove that, with appropriate changes, multi-metric learning corresponds to learning the structure of a Riemannian manifold. We then show that this structure gives us a principled way to perform dimensionality reduction and regression according to the learned metrics. Algorithmically, we provide the first practical algorithm for computing geodesics according to the learned metrics, as well as algorithms for computing exponential and logarithmic maps on the Riemannian manifold. Together, these tools let many Euclidean algorithms take advantage of multi-metric learning. We illustrate the approach on regression and dimensionality reduction tasks that involve predicting measurements of the human body from shape data.},
  langid = {english}
}

@article{havasi_training_2021,
  title = {Training {{Independent Subnetworks}} for {{Robust Prediction}}},
  author = {Havasi, Marton and Jenatton, Rodolphe and Fort, Stanislav},
  year = {2021},
  pages = {13},
  abstract = {Recent approaches to efficiently ensemble neural networks have shown that strong robustness and uncertainty performance can be achieved with a negligible gain in parameters over the original network. However, these methods still require multiple forward passes for prediction, leading to a significant computational cost. In this work, we show a surprising result: the benefits of using multiple predictions can be achieved `for free' under a single model's forward pass. In particular, we show that, using a multi-input multi-output (MIMO) configuration, one can utilize a single model's capacity to train multiple subnetworks that independently learn the task at hand. By ensembling the predictions made by the subnetworks, we improve model robustness without increasing compute. We observe a significant improvement in negative log-likelihood, accuracy, and calibration error on CIFAR10, CIFAR100, ImageNet, and their out-of-distribution variants compared to previous methods.},
  langid = {english}
}

@inproceedings{havtorn_hierarchical_2021,
  title = {Hierarchical {{VAEs Know What They Don}}'t {{Know}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Havtorn, Jakob D. and Frellsen, Jes and Hauberg, S{\o}ren and Maal{\o}e, Lars},
  year = {2021},
  month = jul,
  eprint = {2102.08248},
  eprinttype = {arxiv},
  pages = {4117--4128},
  publisher = {{PMLR}},
  address = {{Virtual}},
  abstract = {Deep generative models have been demonstrated as state-of-the-art density estimators. Yet, recent work has found that they often assign a higher likelihood to data from outside the training distribution. This seemingly paradoxical behavior has caused concerns over the quality of the attained density estimates. In the context of hierarchical variational autoencoders, we provide evidence to explain this behavior by out-of-distribution data having in-distribution low-level features. We argue that this is both expected and desirable behavior. With this insight in hand, we develop a fast, scalable and fully unsupervised likelihood-ratio score for OOD detection that requires data to be in-distribution across all feature-levels. We benchmark the method on a vast set of data and model combinations and achieve state-of-the-art results on out-of-distribution detection.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved}
}

@inproceedings{havtorn_multiqt_2020,
  title = {{{MultiQT}}: {{Multimodal}} Learning for Real-Time Question Tracking in Speech},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{ACL}})},
  author = {Havtorn, Jakob D. and Latko, Jan and Edin, Joakim and Borgholt, Lasse and Maal{\o}e, Lars and Belgrano, Lorenzo and Jakobsen, Nicolai Frost and Sdun, Regitze and Agic, Zeljko},
  year = {2020},
  eprint = {2005.00812},
  eprinttype = {arxiv},
  primaryclass = {cs, eees},
  publisher = {{Association for Computational Linguistics}},
  address = {{Virtual}},
  abstract = {We address a challenging and practical task of labeling questions in speech in real time during telephone calls to emergency medical services in English, which embeds within a broader decision support system for emergency call-takers. We propose a novel multimodal approach to real-time sequence labeling in speech. Our model treats speech and its own textual representation as two separate modalities or views, as it jointly learns from streamed audio and its noisy transcription into text via automatic speech recognition. Our results show significant gains of jointly learning from the two modalities when compared to text or audio only, under adverse noise and limited volume of training data. The results generalize to medical symptoms detection where we observe a similar pattern of improvements with multimodal learning.},
  archiveprefix = {arXiv}
}

@phdthesis{havtorn_physics_2014,
  type = {Second {{Year Project}}},
  title = {Physics {{Project Contact Resistance Measurement Rig}} for {{Thermoelectric Materials}}},
  author = {Havtorn, Jakob D. and Skovhus, Thorbj{\o}rn and Blaabjerg, Lasse},
  year = {2014},
  school = {Technical University of Denmark}
}

@mastersthesis{havtorn_variational_2018,
  title = {Variational {{Optimization}} of {{Neural Networks}}},
  author = {Havtorn, Jakob D.},
  year = {2018},
  month = jul,
  address = {{Kongens Lyngby}},
  abstract = {This thesis presents evolution strategies as a competitive alternative to classical methods for reinforcement learning and unifies previous work within black-box optimization with deep learning. It presents variational optimization as an encompassing mathematical framework, which maintains a search distribution over the optimized parameters during training, and describes how it can be efficiently applied for optimization of neural networks without the use of backpropagation. The natural gradient is derived and implemented as a way to update a search distribution subject to a similarity constraint w.r.t. the previous iterate. It is shown to be superior to using the regular gradient, especially when adapting the variance of a Gaussian search distribution. Antithetic sampling and the method of common random numbers are derived and applied to reduce the variance of the gradient. Experiments run primarily in the supervised setting on the MNIST dataset show that while antithetic sampling is rather efficient at achieving this goal, common random numbers is not. A novel approach for reduction of gradient variance as well as computation based on a local reparameterization of feedforward neural networks is presented and treated theoretically. Different search distributions based on the Gaussian are derived and implemented and the effect of adapting the mean and variance is compared to adapting only the mean which parameterizes the network parameters. It is found that while using an isotropic Gaussian with fixed variance provides good results, adapting the variance can lead to divergence. Separable Gaussians with a variance per network layer or per network weight are shown to perform similarly to using a fixed variance but not significantly better. These results are discussed and related to the geometry of the loss surface.},
  copyright = {All rights reserved},
  langid = {english},
  school = {Technical University of Denmark}
}

@article{he_deep_2015,
  title = {Deep Residual Learning for Image Recognition},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  month = dec,
  journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CCVPR)},
  eprint = {1512.03385},
  eprinttype = {arxiv},
  pages = {770--778},
  archiveprefix = {arXiv}
}

@article{he_delving_2015,
  title = {Delving {{Deep}} into {{Rectifiers}}: {{Surpassing Human-Level Performance}} on {{ImageNet Classification}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  journal = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  eprint = {1502.01852v1},
  eprinttype = {arxiv},
  pages = {1026--1034},
  abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra com-putational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94\% top-5 test error on the ImageNet 2012 clas-sification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\% [33]). To our knowledge, our result is the first 1 to surpass the reported human-level performance (5.1\%, [26]) on this dataset.},
  archiveprefix = {arXiv}
}

@inproceedings{he_identity_2016,
  title = {Identity Mappings in Deep Residual Networks},
  booktitle = {European {{Conference}} on {{Computer Vision}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  pages = {630--645}
}

@misc{he_lagging_2019,
  title = {Lagging {{Inference Networks}} and {{Posterior Collapse}} in {{Variational Autoencoders}}},
  author = {He, Junxian and Spokoyny, Daniel and Neubig, Graham and {Berg-Kirkpatrick}, Taylor},
  year = {2019},
  month = jan,
  eprint = {1901.05534},
  eprinttype = {arxiv},
  abstract = {The variational autoencoder (VAE) is a popular combination of deep latent variable model and accompanying variational learning technique. By using a neural inference network to approximate the model's posterior on latent variables, VAEs efficiently parameterize a lower bound on marginal data likelihood that can be optimized directly via gradient methods. In practice, however, VAE training often results in a degenerate local optimum known as "posterior collapse" where the model learns to ignore the latent variable and the approximate posterior mimics the prior. In this paper, we investigate posterior collapse from the perspective of training dynamics. We find that during the initial stages of training the inference network fails to approximate the model's true posterior, which is a moving target. As a result, the model is encouraged to ignore the latent encoding and posterior collapse occurs. Based on this observation, we propose an extremely simple modification to VAE training to reduce inference lag: depending on the model's current mutual information between latent variable and observation, we aggressively optimize the inference network before performing each model update. Despite introducing neither new model components nor significant complexity over basic VAE, our approach is able to avoid the problem of collapse that has plagued a large amount of previous work. Empirically, our approach outperforms strong autoregressive baselines on text and image benchmarks in terms of held-out likelihood, and is competitive with more complex techniques for avoiding collapse while being substantially faster.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{he_multi-view_2017,
  title = {Multi-View {{Recurrent Neural Acoustic Word Embeddings}}},
  author = {He, Wanjia and Wang, Weiran and Livescu, Karen},
  year = {2017},
  month = mar,
  journal = {arXiv:1611.04496 [cs]},
  eprint = {1611.04496},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Recent work has begun exploring neural acoustic word embeddings---fixed-dimensional vector representations of arbitrary-length speech segments corresponding to words. Such embeddings are applicable to speech retrieval and recognition tasks, where reasoning about whole words may make it possible to avoid ambiguous sub-word representations. The main idea is to map acoustic sequences to fixed-dimensional vectors such that examples of the same word are mapped to similar vectors, while different-word examples are mapped to very different vectors. In this work we take a multi-view approach to learning acoustic word embeddings, in which we jointly learn to embed acoustic sequences and their corresponding character sequences. We use deep bidirectional LSTM embedding models and multi-view contrastive losses. We study the effect of different loss variants, including fixed-margin and cost-sensitive losses. Our acoustic word embeddings improve over previous approaches for the task of word discrimination. We also present results on other tasks that are enabled by the multi-view approach, including cross-view word discrimination and word similarity.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@inproceedings{he_spatial_2014,
  title = {Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition},
  booktitle = {European {{Conference}} on {{Computer Vision}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2014},
  pages = {346--361}
}

@misc{he_streaming_2018,
  title = {Streaming {{End-to-end Speech Recognition For Mobile Devices}}},
  author = {He, Yanzhang and Sainath, Tara N. and Prabhavalkar, Rohit and McGraw, Ian and Alvarez, Raziel and Zhao, Ding and Rybach, David and Kannan, Anjuli and Wu, Yonghui and Pang, Ruoming and Liang, Qiao and Bhatia, Deepti and Shangguan, Yuan and Li, Bo and Pundak, Golan and Sim, Khe Chai and Bagby, Tom and Chang, Shuo-yiin and Rao, Kanishka and Gruenstein, Alexander},
  year = {2018},
  month = nov,
  eprint = {1811.06621},
  eprinttype = {arxiv},
  abstract = {End-to-end (E2E) models, which directly predict output character sequences given input speech, are good candidates for on-device speech recognition. E2E models, however, present numerous challenges: In order to be truly useful, such models must decode speech utterances in a streaming fashion, in real time; they must be robust to the long tail of use cases; they must be able to leverage user-specific context (e.g., contact lists); and above all, they must be extremely accurate. In this work, we describe our efforts at building an E2E speech recognizer using a recurrent neural network transducer. In experimental evaluations, we find that the proposed approach can outperform a conventional CTC-based model in terms of both latency and accuracy in a number of evaluation categories.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@inproceedings{heck_feature_2017,
  title = {Feature Optimized {{DPGMM}} Clustering for Unsupervised Subword Modeling: {{A}} Contribution to Zerospeech 2017},
  booktitle = {2017 {{IEEE}} Automatic Speech Recognition and Understanding Workshop, {{ASRU}} 2017, Okinawa, Japan, December 16-20, 2017},
  author = {Heck, Michael and Sakti, Sakriani and Nakamura, Satoshi},
  year = {2017},
  pages = {740--746}
}

@misc{heigold_end--end_2015,
  title = {End-to-{{End Text-Dependent Speaker Verification}}},
  author = {Heigold, Georg and Moreno, Ignacio and Bengio, Samy and Shazeer, Noam},
  year = {2015},
  month = sep,
  eprint = {1509.08062},
  eprinttype = {arxiv},
  abstract = {In this paper we present a data-driven, integrated approach to speaker verification, which maps a test utterance and a few reference utterances directly to a single score for verification and jointly optimizes the system's components using the same evaluation protocol and metric as at test time. Such an approach will result in simple and efficient systems, requiring little domain-specific knowledge and making few model assumptions. We implement the idea by formulating the problem as a single neural network architecture, including the estimation of a speaker model on only a few utterances, and evaluate it on our internal "Ok Google" benchmark for text-dependent speaker verification. The proposed approach appears to be very effective for big data applications like ours that require highly accurate, easy-to-maintain systems with a small footprint.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound}
}

@article{heinrich_estimating_2011,
  title = {Estimating {{Speaking Rate}} by {{Means}} of {{Rhythmicity Parameters}}},
  author = {Heinrich, Christian and Schiel, Florian},
  year = {2011},
  pages = {4},
  abstract = {In this paper we present a speech rate estimator based on so-called rhythmicity features derived from a modified version of the short-time energy envelope. To evaluate the new method, it is compared to a traditional speech rate estimator on the basis of semi-automatic segmentation. Speech material from the Alcohol Language Corpus (ALC) covering intoxicated and sober speech of different speech styles provides a statistically sound foundation to test upon. The proposed measure clearly correlates with the semi-automatically determined speech rate and seems to be robust across speech styles and speaker states.},
  langid = {english}
}

@misc{henaff_data-efficient_2019,
  title = {Data-{{Efficient Image Recognition}} with {{Contrastive Predictive Coding}}},
  author = {H{\'e}naff, Olivier J. and Razavi, Ali and Doersch, Carl and Eslami, S. M. Ali and van den Oord, A{\"a}ron},
  year = {2019},
  month = may,
  eprint = {1905.09272},
  eprinttype = {arxiv},
  abstract = {Large scale deep learning excels when labeled images are abundant, yet data-efficient learning remains a longstanding challenge. While biological vision is thought to leverage vast amounts of unlabeled data to solve classification problems with limited supervision, computer vision has so far not succeeded in this `semi-supervised' regime. Our work tackles this challenge with Contrastive Predictive Coding, an unsupervised objective which extracts stable structure from still images. The result is a representation which, equipped with a simple linear classifier, separates ImageNet categories better than all competing methods, and surpasses the performance of a fully-supervised AlexNet model. When given a small number of labeled images (as few as 13 per class), this representation retains a strong classification performance, outperforming state-of-the-art semi-supervised methods by 10\% Top-5 accuracy and supervised methods by 20\%. Finally, we find our unsupervised representation to serve as a useful substrate for image detection on the PASCAL-VOC 2007 dataset, approaching the performance of representations trained with a fully annotated ImageNet dataset. We expect these results to open the door to pipelines that use scalable unsupervised representations as a drop-in replacement for supervised ones for real-world vision tasks where labels are scarce.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@article{henderson_deep_2013,
  title = {Deep {{Neural Network Approach}} for the {{Dialog State Tracking Challenge}}},
  author = {Henderson, Matthew and Thomson, Blaise and Young, Steve},
  year = {2013},
  journal = {Proceedings of the SIGDIAL 2013 Conference},
  pages = {467--471},
  abstract = {While belief tracking is known to be im-portant in allowing statistical dialog sys-tems to manage dialogs in a highly robust manner, until recently little attention has been given to analysing the behaviour of belief tracking techniques. The Dialogue State Tracking Challenge has allowed for such an analysis, comparing multiple be-lief tracking approaches on a shared task. Recent success in using deep learning for speech research motivates the Deep Neu-ral Network approach presented here. The model parameters can be learnt by directly maximising the likelihood of the training data. The paper explores some aspects of the training, and the resulting tracker is found to perform competitively, particu-larly on a corpus of dialogs from a system not found in the training.},
  isbn = {9781937284954}
}

@inproceedings{hendrycks_baseline_2017,
  title = {A {{Baseline}} for {{Detecting Misclassified}} and {{Out-of-Distribution Examples}} in {{Neural Networks}}},
  booktitle = {Proceedings of the 5th {{International Conference}} on {{Learning Representations}} ({{ICRL}})},
  author = {Hendrycks, Dan and Gimpel, Kevin},
  year = {2017},
  address = {{Toulon, France}},
  abstract = {We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks.},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@inproceedings{hendrycks_deep_2019,
  title = {Deep Anomaly Detection with Outlier Exposure},
  booktitle = {Proceedings of the 7th {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Hendrycks, Dan and Mazeika, Mantas and Dietterich, Thomas G.},
  year = {2019},
  address = {{New Orleans, LA, USA}},
  abstract = {It is important to detect anomalous inputs when deploying machine learning systems. The use of larger and more complex inputs in deep learning magnifies the difficulty of distinguishing between anomalous and in-distribution examples. At the same time, diverse image and text data are available in enormous quantities. We propose leveraging these data to improve deep anomaly detection by training anomaly detectors against an auxiliary dataset of outliers, an approach we call Outlier Exposure (OE). This enables anomaly detectors to generalize and detect unseen anomalies. In extensive experiments on natural language processing and small- and large-scale vision tasks, we find that Outlier Exposure significantly improves detection performance. We also observe that cutting-edge generative models trained on CIFAR-10 may assign higher likelihoods to SVHN images than to CIFAR-10 images; we use OE to mitigate this issue. We also analyze the flexibility and robustness of Outlier Exposure, and identify characteristics of the auxiliary dataset that improve performance.}
}

@book{herlau_introduction_2016,
  title = {Introduction to {{Machine Learning}} and {{Data Mining}}},
  author = {Herlau, Tue and Schmidt, Mikkel N. and M{\o}rup, Morten},
  year = {2016},
  abstract = {Course notes/book for 02450 Introduction to Machine Learning and Data Mining}
}

@article{hermann_teaching_2015,
  title = {Teaching {{Machines}} to {{Read}} and {{Comprehend}}},
  author = {Hermann, Karl Moritz and Ko{\v c}isk{\'y}, Tom{\'a}{\v s} and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
  year = {2015},
  journal = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
  eprint = {1506.03340},
  eprinttype = {arxiv},
  pages = {1693--1701},
  issn = {10495258},
  abstract = {Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.},
  archiveprefix = {arXiv},
  isbn = {1045-9227},
  pmid = {18263409}
}

@article{hermansky_rasta_1994,
  title = {{{RASTA Processing}} of {{Speech}}},
  author = {Hermansky, Hynek and Morgan, Nelson},
  year = {1994},
  month = oct,
  journal = {IEEE Transactions on Speech and Audio Processing},
  volume = {2},
  number = {4},
  pages = {587--589}
}

@inproceedings{hernandez-lobato_black-box_2016,
  title = {Black-Box \$\textbackslash alpha\$-Divergence {{Minimization}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {{Hern{\'a}ndez-Lobato}, Jos{\'e} Miguel and Li, Yingzhen and Rowland, Mark and {Hern{\'a}ndez-Lobato}, Daniel and Bui, Thang and Turner, Richard E.},
  year = {2016},
  month = nov,
  eprint = {1511.03243},
  eprinttype = {arxiv},
  address = {{New York, NY, USA}},
  abstract = {Black-box alpha (BB-\$\textbackslash alpha\$) is a new approximate inference method based on the minimization of \$\textbackslash alpha\$-divergences. BB-\$\textbackslash alpha\$ scales to large datasets because it can be implemented using stochastic gradient descent. BB-\$\textbackslash alpha\$ can be applied to complex probabilistic models with little effort since it only requires as input the likelihood function and its gradients. These gradients can be easily obtained using automatic differentiation. By changing the divergence parameter \$\textbackslash alpha\$, the method is able to interpolate between variational Bayes (VB) (\$\textbackslash alpha \textbackslash rightarrow 0\$) and an algorithm similar to expectation propagation (EP) (\$\textbackslash alpha = 1\$). Experiments on probit regression and neural network regression and classification problems show that BB-\$\textbackslash alpha\$ with non-standard settings of \$\textbackslash alpha\$, such as \$\textbackslash alpha = 0.5\$, usually produces better predictions than with \$\textbackslash alpha \textbackslash rightarrow 0\$ (VB) or \$\textbackslash alpha = 1\$ (EP).},
  archiveprefix = {arXiv}
}

@misc{hernandez-lobato_probabilistic_2015,
  title = {Probabilistic {{Backpropagation}} for {{Scalable Learning}} of {{Bayesian Neural Networks}}},
  author = {{Hern{\'a}ndez-Lobato}, Jos{\'e} Miguel and Adams, Ryan P.},
  year = {2015},
  eprint = {1502.05336},
  eprinttype = {arxiv},
  abstract = {Large multilayer neural networks trained with backpropagation have recently achieved state-of-the-art results in a wide range of problems. However, using backprop for neural net learning still has some disadvantages, e.g., having to tune a large number of hyperparameters to the data, lack of calibrated probabilistic predictions, and a tendency to overfit the training data. In principle, the Bayesian approach to learning neural networks does not have these problems. However, existing Bayesian techniques lack scalability to large dataset and network sizes. In this work we present a novel scalable method for learning Bayesian neural networks, called probabilistic backpropagation (PBP). Similar to classical backpropagation, PBP works by computing a forward propagation of probabilities through the network and then doing a backward computation of gradients. A series of experiments on ten real-world datasets show that PBP is significantly faster than other techniques, while offering competitive predictive abilities. Our experiments also show that PBP provides accurate estimates of the posterior variance on the network weights.},
  archiveprefix = {arXiv},
  isbn = {9781510810587}
}

@inproceedings{hertel_deep_2015,
  title = {Deep Convolutional Neural Networks as Generic Feature Extractors},
  booktitle = {Proceedings of the {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Hertel, Lars and Barth, Erhardt and Kaster, Thomas and Martinetz, Thomas},
  year = {2015},
  volume = {2015-Septe},
  eprint = {1710.02286},
  eprinttype = {arxiv},
  issn = {2161-4393},
  abstract = {Recognizing objects in natural images is an intricate problem involving multiple conflicting objectives. Deep convolutional neural networks, trained on large datasets, achieve convincing results and are currently the state-of-the-art approach for this task. However, the long time needed to train such deep networks is a major drawback. We tackled this problem by reusing a previously trained network. For this purpose, we first trained a deep convolutional network on the ILSVRC2012 dataset. We then maintained the learned convolution kernels and only retrained the classification part on different datasets. Using this approach, we achieved an accuracy of 67.68 \% on CIFAR-100, compared to the previous state-of-the-art result of 65.43 \%. Furthermore, our findings indicate that convolutional networks are able to learn generic feature extractors that can be used for different tasks.},
  archiveprefix = {arXiv},
  isbn = {978-1-4799-1960-4},
  keywords = {Art,Convolution,Handwriting recognition,Kernel}
}

@inproceedings{heusel_gans_2017,
  title = {{{GANs Trained}} by a {{Two Time-Scale Update Rule Converge}} to a {{Local Nash Equilibrium}}},
  booktitle = {Proceedings of the {{Conference}} on {{Neural Information Processing Systems}} ({{NIPS}})},
  author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  year = {2017},
  eprint = {1706.08500},
  eprinttype = {arxiv},
  issn = {10495258},
  abstract = {Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the "Fr\textbackslash 'echet Inception Distance" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.},
  archiveprefix = {arXiv}
}

@inproceedings{hinton_autoencoders_1994,
  title = {Autoencoders, Minimum Description Length and Helmholtz Free Energy},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hinton, Geoffrey E and Zemel, Richard},
  editor = {Cowan, J. and Tesauro, G. and Alspector, J.},
  year = {1994},
  volume = {6},
  publisher = {{Morgan-Kaufmann}}
}

@article{hinton_deep_2012,
  title = {Deep Neural Networks for Acoustic Modeling in Speech Recognition: {{The}} Shared Views of Four Research Groups},
  author = {Hinton, G and Deng, Li and Yu, Dong and Dahl, GE},
  year = {2012},
  journal = {IEEE Signal},
  volume = {29},
  number = {6},
  pages = {82--97}
}

@techreport{hinton_development_1988,
  title = {The {{Development}} of {{Time-Delay Neural Network Architecture}} for {{Speech Recognition}}},
  author = {Hinton, Geoffrey E. and McClelland, James L.},
  year = {1988},
  institution = {{Carnegie Mellon University}},
  isbn = {0780374029}
}

@misc{hinton_distilling_2015,
  title = {Distilling the Knowledge in a Neural Network},
  author = {Hinton, Geoffrey E. and Vinyals, Oriol and Dean, Jeff},
  year = {2015},
  eprint = {1503.02531},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@misc{hinton_how_2021,
  title = {How to Represent Part-Whole Hierarchies in a Neural Network},
  author = {Hinton, Geoffrey},
  year = {2021},
  month = feb,
  eprint = {2102.12627},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {This paper does not describe a working system. Instead, it presents a single idea about representation which allows advances made by several different groups to be combined into an imaginary system called GLOM1. The advances include transformers, neural fields, contrastive representation learning, distillation and capsules. GLOM answers the question: How can a neural network with a fixed architecture parse an image into a partwhole hierarchy which has a different structure for each image? The idea is simply to use islands of identical vectors to represent the nodes in the parse tree. If GLOM can be made to work, it should significantly improve the interpretability of the representations produced by transformer-like systems when applied to vision or language.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,I.2.6,I.4.8}
}

@misc{hinton_improving_2012,
  title = {Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors},
  author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
  year = {2012},
  eprint = {1207.0580},
  eprinttype = {arxiv},
  abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
  archiveprefix = {arXiv}
}

@inproceedings{hinton_keeping_1993,
  title = {Keeping the Neural Networks Simple by Minimizing the Description Length of the Weights},
  booktitle = {Proceedings of {{Annual Conference}} on {{Computational Learning Theory}} ({{COLT}})},
  author = {Hinton, Geoffrey E. and {van Camp}, Drew},
  year = {1993},
  pages = {5--13},
  abstract = {Supervised neural networks generalize there is much less information learning, well if in the weights than there is in the output vectors of the train- ing cases. So during it is impor- tant to keep the weights simple by penaliz- ing the amount of information The amount of information be controlled they contain. in a weight can by adding Gaussian noise and the noise level can be adapted during learning to optimize a method of computing the trade-off between the expected squared error of the network and the amount of information in the noisy weights can be computed is required in the weights. We describe the derivatives work that contains a layer of non-linear time-consuming that of the expected squared error and of the amount of information in a net- hidden units. Provided the output units are linear, the exact derivatives without efficiently tions. The idea of minimizing information Monte Carlo simula- the amount of to communicate the weights of a neural network leads to a number of intereating schemes for encoding the weights.},
  isbn = {0-89791-611-5}
}

@misc{ho_denoising_2020,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  year = {2020},
  month = dec,
  eprint = {2006.11239},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{ho_flow_2019,
  title = {Flow++: {{Improving Flow-Based Generative Models}} with {{Variational Dequantization}} and {{Architecture Design}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Ho, Jonathan and Chen, Xi and Srinivas, Aravind and Duan, Yan and Abbeel, Pieter},
  year = {2019},
  pages = {9},
  address = {{Long Beach, CA, USA}},
  abstract = {Flow-based generative models are powerful exact likelihood models with efficient sampling and inference. Despite their computational efficiency, flow-based models generally have much worse density modeling performance compared to stateof-the-art autoregressive models. In this paper, we investigate and improve upon three limiting design choices employed by flow-based models in prior work: the use of uniform noise for dequantization, the use of inexpressive affine flows, and the use of purely convolutional conditioning networks in coupling layers. Based on our findings, we propose Flow++, a new flow-based model that is now the state-of-the-art non-autoregressive model for unconditional density estimation on standard image benchmarks. Our work has begun to close the significant performance gap that has so far existed between autoregressive models and flow-based models. Our implementation is available at: https://github.com/ aravindsrinivas/flowpp.},
  langid = {english},
  keywords = {⛔ No DOI found}
}

@article{hochreiter_long_1997,
  title = {Long {{Short-Term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  journal = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}
}

@misc{hoffer_norm_2018,
  title = {Norm Matters: Efficient and Accurate Normalization Schemes in Deep Networks},
  shorttitle = {Norm Matters},
  author = {Hoffer, Elad and Banner, Ron and Golan, Itay and Soudry, Daniel},
  year = {2018},
  month = mar,
  eprint = {1803.01814},
  eprinttype = {arxiv},
  abstract = {Over the past few years batch-normalization has been commonly used in deep networks, allowing faster training and high performance for a wide variety of applications. However, the reasons behind its merits remained unanswered, with several shortcomings that hindered its use for certain tasks. In this work we present a novel view on the purpose and function of normalization methods and weight-decay, as tools to decouple weights' norm from the underlying optimized objective. We also improve the use of weight-normalization and show the connection between practices such as normalization, weight decay and learning-rate adjustments. Finally, we suggest several alternatives to the widely used \$L\^2\$ batch-norm, using normalization in \$L\^1\$ and \$L\^\textbackslash infty\$ spaces that can substantially improve numerical stability in low-precision implementations as well as provide computational and memory benefits. We demonstrate that such methods enable the first batch-norm alternative to work for half-precision implementations.},
  archiveprefix = {arXiv},
  keywords = {Normalization,Optimization}
}

@inproceedings{hoffman_elbo_2016,
  title = {{{ELBO}} Surgery: Yet Another Way to Carve up the Variational Evidence Lower Bound},
  booktitle = {Workshop in {{Advances}} in {{Approximate Bayesian Inference}}},
  author = {Hoffman, Matthew D and Johnson, Matthew J},
  year = {2016},
  pages = {4},
  abstract = {We rewrite the variational evidence lower bound objective (ELBO) of variational autoencoders in a way that highlights the role of the encoded data distribution. This perspective suggests that to improve our variational bounds we should improve our priors and not just the encoder and decoder.},
  langid = {english},
  keywords = {⛔ No DOI found}
}

@article{hoffman_learning_nodate,
  title = {Learning {{Deep Latent Gaussian Models}} with {{Markov Chain Monte Carlo}}},
  author = {Hoffman, Matthew D},
  pages = {10},
  abstract = {Deep latent Gaussian models are powerful and popular probabilistic models of highdimensional data. These models are almost always fit using variational expectationmaximization, an approximation to true maximum-marginal-likelihood estimation. In this paper, we propose a different approach: rather than use a variational approximation (which produces biased gradient signals), we use Markov chain Monte Carlo (MCMC, which allows us to trade bias for computation). We find that our MCMC-based approach has several advantages: it yields higher held-out likelihoods, produces sharper images, and does not suffer from the variational overpruning effect. MCMC's additional computational overhead proves to be significant, but not prohibitive.},
  langid = {english}
}

@article{hoffman_stochastic_2012,
  title = {Stochastic {{Variational Inference}}},
  author = {Hoffman, Matt and Blei, David M. and Wang, Chong and Paisley, John},
  year = {2012},
  journal = {Journal of Machine Learning Research},
  eprint = {1206.7051},
  eprinttype = {arxiv},
  issn = {1532-4435},
  abstract = {We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets.},
  archiveprefix = {arXiv},
  isbn = {1532-4435},
  pmid = {19926898},
  keywords = {★}
}

@article{honkela_variational_2004,
  title = {Variational Learning and Bits-Back Coding: {{An}} Information-Theoretic View to {{Bayesian}} Learning},
  author = {Honkela, Antti and Valpola, Harri},
  year = {2004},
  journal = {IEEE Transactions on Neural Networks},
  volume = {15},
  number = {4},
  pages = {800--810},
  issn = {10459227},
  abstract = {The bits-back coding first introduced by Wallace in 1990 and later by Hinton and van Camp in 1993 provides an interesting link between Bayesian learning and information-theoretic minimum-description-length (MDL) learning approaches. The bits-back coding allows interpreting the cost function used in the variational Bayesian method called ensemble learning as a code length in addition to the Bayesian view of misfit of the posterior approximation and a lower bound of model evidence. Combining these two viewpoints provides interesting insights to the learning process and the functions of different parts of the model. In this paper, the problem of variational Bayesian learning of hierarchical latent variable models is used to demonstrate the benefits of the two views. The code-length interpretation provides new views to many parts of the problem such as model comparison and pruning and helps explain many phenomena occurring in learning.},
  pmid = {15461074}
}

@misc{hoogeboom_argmax_2021,
  title = {Argmax {{Flows}} and {{Multinomial Diffusion}}: {{Towards Non-Autoregressive Language Models}}},
  shorttitle = {Argmax {{Flows}} and {{Multinomial Diffusion}}},
  author = {Hoogeboom, Emiel and Nielsen, Didrik and Jaini, Priyank and Forr{\'e}, Patrick and Welling, Max},
  year = {2021},
  month = feb,
  eprint = {2102.05379},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The field of language modelling has been largely dominated by autoregressive models, for which sampling is inherently difficult to parallelize. This paper introduces two new classes of generative models for categorical data such as language or image segmentation: Argmax Flows and Multinomial Diffusion. Argmax Flows are defined by a composition of a continuous distribution (such as a normalizing flow), and an argmax function. To optimize this model, we learn a probabilistic inverse for the argmax that lifts the categorical data to a continuous space. Multinomial Diffusion gradually adds categorical noise in a diffusion process, for which the generative denoising process is learned. We demonstrate that our models perform competitively on language modelling and modelling of image segmentation maps.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{hospedales_meta-learning_2020,
  title = {Meta-{{Learning}} in {{Neural Networks}}: {{A Survey}}},
  shorttitle = {Meta-{{Learning}} in {{Neural Networks}}},
  author = {Hospedales, Timothy and Antoniou, Antreas and Micaelli, Paul and Storkey, Amos},
  year = {2020},
  month = apr,
  eprint = {2004.05439},
  eprinttype = {arxiv},
  abstract = {The field of meta-learning, or learning-to-learn, has seen a dramatic rise in interest in recent years. Contrary to conventional approaches to AI where a given task is solved from scratch using a fixed learning algorithm, meta-learning aims to improve the learning algorithm itself, given the experience of multiple learning episodes. This paradigm provides an opportunity to tackle many of the conventional challenges of deep learning, including data and computation bottlenecks, as well as the fundamental issue of generalization. In this survey we describe the contemporary meta-learning landscape. We first discuss definitions of meta-learning and position it with respect to related fields, such as transfer learning, multi-task learning, and hyperparameter optimization. We then propose a new taxonomy that provides a more comprehensive breakdown of the space of meta-learning methods today. We survey promising applications and successes of meta-learning including few-shot learning, reinforcement learning and architecture search. Finally, we discuss outstanding challenges and promising areas for future research.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{hsu_extracting_2018,
  title = {Extracting {{Domain Invariant Features}} by {{Unsupervised Learning}} for {{Robust Automatic Speech Recognition}}},
  author = {Hsu, Wei-Ning and Glass, James},
  year = {2018},
  month = mar,
  eprint = {1803.02551},
  eprinttype = {arxiv},
  primaryclass = {cs, eess, stat},
  abstract = {The performance of automatic speech recognition (ASR) systems can be significantly compromised by previously unseen conditions, which is typically due to a mismatch between training and testing distributions. In this paper, we address robustness by studying domain invariant features, such that domain information becomes transparent to ASR systems, resolving the mismatch problem. Specifically, we investigate a recent model, called the Factorized Hierarchical Variational Autoencoder (FHVAE). FHVAEs learn to factorize sequence-level and segment-level attributes into different latent variables without supervision. We argue that the set of latent variables that contain segment-level information is our desired domain invariant feature for ASR. Experiments are conducted on Aurora-4 and CHiME-4, which demonstrate 41\% and 27\% absolute word error rate reductions respectively on mismatched domains.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning}
}

@article{hsu_hubert_2021,
  title = {{{HuBERT}}: {{Self-Supervised Speech Representation Learning}} by {{Masked Prediction}} of {{Hidden Units}}},
  shorttitle = {{{HuBERT}}},
  author = {Hsu, Wei-Ning and Bolte, Benjamin and Tsai, Yao-Hung Hubert and Lakhotia, Kushal and Salakhutdinov, Ruslan and Mohamed, Abdelrahman},
  year = {2021},
  month = jun,
  abstract = {Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19\% and 13\% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.},
  langid = {english}
}

@inproceedings{hsu_hubert_2021-1,
  title = {Hubert: {{How Much Can}} a {{Bad Teacher Benefit ASR Pre-Training}}?},
  shorttitle = {Hubert},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Hsu, Wei-Ning and Tsai, Yao-Hung Hubert and Bolte, Benjamin and Salakhutdinov, Ruslan and Mohamed, Abdelrahman},
  year = {2021},
  month = jun,
  pages = {6533--6537},
  issn = {2379-190X},
  abstract = {Compared to vision and language applications, self-supervised pre-training approaches for ASR are challenged by three unique problems: (1) There are multiple sound units in each input utterance, (2) With audio-only pre-training, there is no lexicon of sound units, and (3) Sound units have variable lengths with no explicit segmentation. In this paper, we propose the Hidden-Unit BERT (HUBERT) model which utilizes a cheap k-means clustering step to provide aligned target labels for pre-training of a BERT model. A key ingredient of our approach is applying the predictive loss over the masked regions only. This allows the pre-training stage to benefit from the consistency of the unsupervised teacher rather that its intrinsic quality. Starting with a simple k-means teacher of 100 cluster, and using two iterations of clustering, the HUBERT model matches the state-of-the-art wav2vec 2.0 performance on the ultra low-resource Libri-light 10h, 1h, 10min supervised subsets.},
  keywords = {Acoustics,Bit error rate,Conferences,Data models,Feature extraction,pre-training,representation learning,Signal processing,Task analysis}
}

@misc{hsu_learning_2017,
  title = {Learning {{Latent Representations}} for {{Speech Generation}} and {{Transformation}}},
  author = {Hsu, Wei-Ning and Zhang, Yu and Glass, James},
  year = {2017},
  month = sep,
  eprint = {1704.04222},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {An ability to model a generative process and learn a latent representation for speech in an unsupervised fashion will be crucial to process vast quantities of unlabelled speech data. Recently, deep probabilistic generative models such as Variational Autoencoders (VAEs) have achieved tremendous success in modeling natural images. In this paper, we apply a convolutional VAE to model the generative process of natural speech. We derive latent space arithmetic operations to disentangle learned latent representations. We demonstrate the capability of our model to modify the phonetic content or the speaker identity for speech segments using the derived operations, without the need for parallel supervisory data.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{hsu_unsupervised_2017,
  title = {Unsupervised {{Learning}} of {{Disentangled}} and {{Interpretable Representations}} from {{Sequential Data}}},
  booktitle = {Proceedings of the 31st {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Hsu, Wei-Ning and Zhang, Yu and Glass, James},
  year = {2017},
  address = {{Long Beach, CA, USA}},
  abstract = {We present a factorized hierarchical variational autoencoder, which learns disentangled and interpretable representations from sequential data without supervision. Specifically, we exploit the multi-scale nature of information in sequential data by formulating it explicitly within a factorized hierarchical graphical model that imposes sequence-dependent priors and sequence-independent priors to different sets of latent variables. The model is evaluated on two speech corpora to demonstrate, qualitatively, its ability to transform speakers or linguistic content by manipulating different sets of latent variables; and quantitatively, its ability to outperform an i-vector baseline for speaker verification and reduce the word error rate by as much as 35\% in mismatched train/test scenarios for automatic speech recognition tasks.},
  langid = {english}
}

@misc{hsu_wg-wavenet_2020,
  title = {{{WG-WaveNet}}: {{Real-Time High-Fidelity Speech Synthesis}} without {{GPU}}},
  shorttitle = {{{WG-WaveNet}}},
  author = {Hsu, Po-chun and Lee, Hung-yi},
  year = {2020},
  month = aug,
  eprint = {2005.07412},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {In this paper, we propose WG-WaveNet, a fast, lightweight, and high-quality waveform generation model. WG-WaveNet is composed of a compact flow-based model and a post-filter. The two components are jointly trained by maximizing the likelihood of the training data and optimizing loss functions on the frequency domains. As we design a flow-based model that is heavily compressed, the proposed model requires much less computational resources compared to other waveform generation models during both training and inference time; even though the model is highly compressed, the post-filter maintains the quality of generated waveform. Our PyTorch implementation can be trained using less than 8 GB GPU memory and generates audio samples at a rate of more than 960 kHz on an NVIDIA 1080Ti GPU. Furthermore, even if synthesizing on a CPU, we show that the proposed method is capable of generating 44.1 kHz speech waveform 1.2 times faster than real-time. Experiments also show that the quality of generated audio is comparable to those of other methods. Audio samples are publicly available online.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@inproceedings{huang_hierarchical_2019,
  title = {Hierarchical {{Importance Weighted Autoencoders}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Huang, Chin-Wei and Sankaran, Kris and Dhekane, Eeshan and Lacoste, Alexandre and Courville, Aaron},
  editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  year = {2019},
  month = jun,
  series = {Proceedings of Machine Learning Research},
  volume = {97},
  pages = {2869--2878},
  publisher = {{PMLR}},
  abstract = {Importance weighted variational inference (Burda et al., 2015) uses multiple i.i.d. samples to have a tighter variational lower bound. We believe a joint proposal has the potential of reducing the number of redundant samples, and introduce a hierarchical structure to induce correlation. The hope is that the proposals would coordinate to make up for the error made by one another to reduce the variance of the importance estimator. Theoretically, we analyze the condition under which convergence of the estimator variance can be connected to convergence of the lower bound. Empirically, we confirm that maximization of the lower bound does implicitly minimize variance. Further analysis shows that this is a result of negative correlation induced by the proposed hierarchical meta sampling scheme, and performance of inference also improves when the number of samples increases.}
}

@misc{huang_lookup-table_2021,
  title = {Lookup-{{Table Recurrent Language Models}} for {{Long Tail Speech Recognition}}},
  author = {Huang, W. Ronny and Sainath, Tara N. and Peyser, Cal and Kumar, Shankar and Rybach, David and Strohman, Trevor},
  year = {2021},
  month = apr,
  eprint = {2104.04552},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {We introduce Lookup-Table Language Models (LookupLM), a method for scaling up the size of RNN language models with only a constant increase in the floating point operations, by increasing the expressivity of the embedding table. In particular, we instantiate an (additional) embedding table which embeds the previous n-gram token sequence, rather than a single token. This allows the embedding table to be scaled up arbitrarily\textemdash with a commensurate increase in performance\textemdash without changing the token vocabulary. Since embeddings are sparsely retrieved from the table via a lookup; increasing the size of the table adds neither extra operations to each forward pass nor extra parameters that need to be stored on limited GPU/TPU memory. We explore scaling n-gram embedding tables up to nearly a billion parameters. When trained on a 3-billion sentence corpus, we find that LookupLM improves long tail log perplexity by 2.44 and long tail WER by 23.4\% on a downstream speech recognition task over a standard RNN language model baseline, an improvement comparable to a scaling up the baseline by 6.2x the number of floating point operations.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@article{huici_speech_2016,
  title = {Speech Rate Estimation in Disordered Speech Based on Spectral Landmark Detection},
  author = {Huici, Hernandez-Diaz and Kairuz, Hector A. and Martens, Heidi and Van Nuffelen, Gwen and De Bodt, Marc},
  year = {2016},
  month = may,
  journal = {Biomedical Signal Processing and Control},
  volume = {27},
  pages = {1--6},
  issn = {17468094},
  abstract = {Speech rate (SR) plays an important role in the assessment of disordered speech. Clinicians rely primarily on manual or semi-automatic methods to determine SR. The reported algorithms are designed for normal speech and show many restrictions with respect to disordered speech that are predominantly characterized by slow SR. This research presents an algorithm that in addition to energy and pitch, relies on information regarding the spectral characteristics of the borders of the syllables (landmarks). Speech samples (three sentences per speaker) for 66 healthy and dysarthric speakers were analyzed with four algorithms (Mrate, robust SR estimation method, Praat's script and the proposed algorithm based on landmark detection). The landmark approach is demonstrated to be more accurate for speakers with slow SR. The Pearson correlation coefficient between the calculated SR and the reference remains over 0.84 for the 198 sentences analyzed, while the other algorithms' correlations are below the values reported in literature for fluent speech. In samples where SR is high, the algorithm shows similar limitations versus other algorithms due to the merging of syllables. The landmark-based algorithm is an adequate method for determining SR in disordered speech.},
  langid = {english}
}

@misc{huszar_evolution_2017,
  title = {Evolution {{Strategies}}, {{Variational Optimisation}} and {{Natural ES}}},
  author = {Husz{\'a}r, Ferenc},
  year = {2017},
  abstract = {In my last post I conveyed my enthusiasm about evolution strategies (ES), and particularly the highly scalable distributed version. I have to admit, this was the first time I had come across this particular formulation, and unexpectedly, people were quick to point out a whole body of research that I probably should have read or known about. Here, I want to highlight two such papers: - Staines and Barber (2013) Optimization by Variational Bounding ESANN - Wierstra et al (2014) Natural Evolution Strategies JMLR And I also highly recommend David's blog post. This post is just a summary of what I've learnt about ES, a bit of an addendum to last week's post. Thanks to David Barber, Olivier Grisel and Nando de Freitas for their comments and pointers.},
  howpublished = {http://www.inference.vc/evolution-strategies-variational-optimisation-and-natural-es-2/},
  keywords = {Deep learning,Evolutionary computation,Optimization,Reinforcement learning,Variational optimization}
}

@article{huszar_how_2015,
  title = {How (Not) to {{Train}} Your {{Generative Model}}: {{Scheduled Sampling}}, {{Likelihood}}, {{Adversary}}?},
  shorttitle = {How (Not) to {{Train}} Your {{Generative Model}}},
  author = {Husz{\'a}r, Ferenc},
  year = {2015},
  month = nov,
  langid = {english},
  keywords = {Kullbach-Leibler divergence,Maximum likelihood,Scheduled sampling}
}

@misc{huszar_is_2017,
  title = {Is {{Maximum Likelihood Useful}} for {{Representation Learning}}?},
  author = {Husz{\'a}r, Ferenc},
  year = {2017},
  month = may,
  journal = {inFERENCe},
  langid = {english}
}

@misc{iandola_squeezenet_2016,
  title = {{{SqueezeNet}}: {{AlexNet-level}} Accuracy with 50x Fewer Parameters And{$<$} 0.5 {{MB}} Model Size},
  author = {Iandola, Forrest N. and Han, Song and Moskewicz, Matthew W. and Ashraf, Khalid and Dally, William J. and Keutzer, Kurt},
  year = {2016},
  eprint = {1602.07360},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@misc{im_denoising_2015,
  title = {Denoising {{Criterion}} for {{Variational Auto-Encoding Framework}}},
  author = {Im, Daniel Jiwoong and Ahn, Sungjin and Memisevic, Roland and Bengio, Yoshua},
  year = {2015},
  month = nov,
  eprint = {1511.06406},
  eprinttype = {arxiv},
  abstract = {Denoising autoencoders (DAE) are trained to reconstruct their clean inputs with noise injected at the input level, while variational autoencoders (VAE) are trained with noise injected in their stochastic hidden layer, with a regularizer that encourages this noise injection. In this paper, we show that injecting noise both in input and in the stochastic hidden layer can be advantageous and we propose a modified variational lower bound as an improved objective function in this setup. When input is corrupted, then the standard VAE lower bound involves marginalizing the encoder conditional distribution over the input noise, which makes the training criterion intractable. Instead, we propose a modified training criterion which corresponds to a tractable bound when input is corrupted. Experimentally, we find that the proposed denoising variational autoencoder (DVAE) yields better average log-likelihood than the VAE and the importance weighted autoencoder on the MNIST and Frey Face datasets.},
  archiveprefix = {arXiv}
}

@inproceedings{ioffe_batch_2015,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Ioffe, Sergey and Szegedy, Christian},
  year = {2015},
  eprint = {1502.03167},
  eprinttype = {arxiv},
  address = {{Lille, France}},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
  archiveprefix = {arXiv},
  keywords = {Batch normalization,Deep learning,Regularization}
}

@inproceedings{ipsen_not-miwae_2021,
  title = {Not-{{MIWAE}}: {{Deep}} Generative Modelling with Missing Not at Random Data},
  booktitle = {Proceedings of the 9th {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Ipsen, Niels Bruun and Mattei, Pierre-Alexandre and Frellsen, Jes},
  year = {2021},
  address = {{Virtual}}
}

@misc{ishikawa_efficient_2020,
  title = {Efficient {{Debiased Variational Bayes}} by {{Multilevel Monte Carlo Methods}}},
  author = {Ishikawa, Kei and Goda, Takashi},
  year = {2020},
  month = jan,
  eprint = {2001.04676},
  eprinttype = {arxiv},
  abstract = {Variational Bayes is a method to find a good approximation of the posterior probability distribution of latent variables from a parametric family of distributions. The evidence lower bound (ELBO), which is nothing but the model evidence minus the KullbackLeibler divergence, has been commonly used as a quality measure in the optimization process. However, the model evidence itself has been considered computationally intractable since it is expressed as a nested expectation with an outer expectation with respect to the training dataset and an inner conditional expectation with respect to latent variables. Similarly, if the KullbackLeibler divergence is replaced with another divergence metric, the corresponding lower bound on the model evidence is often given by such a nested expectation. The standard (nested) Monte Carlo method can be used to estimate such quantities, whereas the resulting estimate is biased and the variance is often quite large. Recently the authors provided an unbiased estimator of the model evidence with small variance by applying the idea from multilevel Monte Carlo (MLMC) methods. In this article, we give more examples involving nested expectations in the context of variational Bayes where MLMC methods can help construct low-variance unbiased estimators, and provide numerical results which demonstrate the effectiveness of our proposed estimators.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning}
}

@misc{itu-t_recommendation_1988,
  title = {Recommendation {{G}}. 711. {{Pulse Code Modulation}} ({{PCM}}) of Voice Frequencies},
  author = {{ITU-T}},
  year = {1988}
}

@inproceedings{iyyer_deep_2015,
  title = {Deep Unordered Composition Rivals Syntactic Methods for Text Classification},
  booktitle = {Proceedings of the 53rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 7th {{International Joint Conference}} on {{Natural Language Processing}}},
  author = {Iyyer, Mohit and Manjunatha, Varun and {Boyd-Graber}, Jordan and Daum{\'e} III, Hal},
  year = {2015},
  month = jul,
  pages = {1681--1691},
  publisher = {{Association for Computational Linguistics}},
  address = {{Beijing, China}}
}

@article{izbicki_high-dimensional_2014,
  title = {High-Dimensional Density Ratio Estimation with Extensions to Approximate Likelihood Computation},
  author = {Izbicki, Rafael and Lee, Ann B. and Schafer, Chad M.},
  year = {2014},
  journal = {Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)},
  volume = {33},
  eprint = {1404.7063},
  eprinttype = {arxiv},
  pages = {420--429},
  issn = {15337928},
  abstract = {The ratio between two probability density functions is an important component of various tasks, including selection bias correction, novelty detection and classification. Recently, several estimators of this ratio have been proposed. Most of these methods fail if the sample space is high-dimensional, and hence require a dimension reduction step, the result of which can be a significant loss of information. Here we propose a simple-to-implement, fully nonparametric density ratio estimator that expands the ratio in terms of the eigenfunctions of a kernel-based operator; these functions reflect the underlying geometry of the data (e.g., submanifold structure), often leading to better estimates without an explicit dimension reduction step. We show how our general framework can be extended to address another important problem, the estimation of a likelihood function in situations where that function cannot be well-approximated by an analytical form. One is often faced with this situation when performing statistical inference with data from the sciences, due the complexity of the data and of the processes that generated those data. We emphasize applications where using existing likelihood-free methods of inference would be challenging due to the high dimensionality of the sample space, but where our spectral series method yields a reasonable estimate of the likelihood function. We provide theoretical guarantees and illustrate the effectiveness of our proposed method with numerical experiments.},
  archiveprefix = {arXiv}
}

@misc{izmailov_averaging_2018,
  title = {Averaging {{Weights Leads}} to {{Wider Optima}} and {{Better Generalization}}},
  author = {Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
  year = {2018},
  month = mar,
  eprint = {1803.05407},
  eprinttype = {arxiv},
  abstract = {Deep neural networks are typically trained by optimizing a loss function with an SGD variant, in conjunction with a decaying learning rate, until convergence. We show that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training. We also show that this Stochastic Weight Averaging (SWA) procedure finds much broader optima than SGD, and approximates the recent Fast Geometric Ensembling (FGE) approach with a single model. Using SWA we achieve notable improvement in test accuracy over conventional SGD training on a range of state-of-the-art residual networks, PyramidNets, DenseNets, and Shake-Shake networks on CIFAR-10, CIFAR-100, and ImageNet. In short, SWA is extremely easy to implement, improves generalization, and has almost no computational overhead.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{jaderberg_population_2017,
  title = {Population {{Based Training}} of {{Neural Networks}}},
  author = {Jaderberg, Max and Dalibard, Valentin and Osindero, Simon and Czarnecki, Wojciech M. and Donahue, Jeff and Razavi, Ali and Vinyals, Oriol and Green, Tim and Dunning, Iain and Simonyan, Karen and Fernando, Chrisantha and Kavukcuoglu, Koray},
  year = {2017},
  month = nov,
  eprint = {1711.09846},
  eprinttype = {arxiv},
  abstract = {Neural networks dominate the modern machine learning landscape, but their training and success still suffer from sensitivity to empirical choices of hyperparameters such as model architecture, loss function, and optimisation algorithm. In this work we present \textbackslash emph\{Population Based Training (PBT)\}, a simple asynchronous optimisation algorithm which effectively utilises a fixed computational budget to jointly optimise a population of models and their hyperparameters to maximise performance. Importantly, PBT discovers a schedule of hyperparameter settings rather than following the generally sub-optimal strategy of trying to find a single fixed set to use for the whole course of training. With just a small modification to a typical distributed hyperparameter training framework, our method allows robust and reliable training of models. We demonstrate the effectiveness of PBT on deep reinforcement learning problems, showing faster wall-clock convergence and higher final performance of agents by optimising over a suite of hyperparameters. In addition, we show the same method can be applied to supervised learning for machine translation, where PBT is used to maximise the BLEU score directly, and also to training of Generative Adversarial Networks to maximise the Inception score of generated images. In all cases PBT results in the automatic discovery of hyperparameter schedules and model selection which results in stable training and better final performance.},
  archiveprefix = {arXiv},
  keywords = {Asynchronous optimization,Hyperparameter tuning}
}

@misc{jaegle_perceiver_2021,
  title = {Perceiver: {{General Perception}} with {{Iterative Attention}}},
  shorttitle = {Perceiver},
  author = {Jaegle, Andrew and Gimeno, Felix and Brock, Andrew and Zisserman, Andrew and Vinyals, Oriol and Carreira, Joao},
  year = {2021},
  month = jun,
  eprint = {2103.03206},
  eprinttype = {arxiv},
  abstract = {Biological systems perceive the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domain-specific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver - a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture is competitive with or outperforms strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video, and video+audio. The Perceiver obtains performance comparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly attending to 50,000 pixels. It is also competitive in all modalities in AudioSet.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@misc{jain_attention_2019,
  title = {Attention Is Not {{Explanation}}},
  author = {Jain, Sarthak and Wallace, Byron C.},
  year = {2019},
  month = may,
  eprint = {1902.10186},
  eprinttype = {arxiv},
  abstract = {Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful ``explanations" for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do. Code to reproduce all experiments is available at https://github.com/successar/ AttentionExplanation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@inproceedings{jaitly_vocal_2013,
  title = {Vocal {{Tract Length Perturbation}} ({{VTLP}}) Improves Speech Recognition},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{Machine Learning}}},
  author = {Jaitly, Navdeep and Hinton, Geoffrey E},
  year = {2013},
  pages = {5},
  abstract = {Augmenting datasets by transforming inputs in a way that does not change the label is a crucial ingredient of the state of the art methods for object recognition using neural networks. However this approach has (to our knowledge) not been exploited successfully in speech recognition (with or without neural networks). In this paper we lay the foundation for this approach, and show one way of augmenting speech datasets by transforming spectrograms, using a random linear warping along the frequency dimension. In practice this can be achieved by using warping techniques that are used for vocal tract length normalization (VTLN) - with the difference that a warp factor is generated randomly each time, during training, rather than fitting a single warp factor to each training and test speaker (or utterance). At test time, a prediction is made by averaging the predictions over multiple warp factors. When this technique is applied to TIMIT using Deep Neural Networks (DNN) of different depths, the Phone Error Rate (PER) improved by an average of 0.65\% on the test set. For a Convolutional neural network (CNN) with convolutional layer in the bottom, a gain of 1.0\% was observed. These improvements were achieved without increasing the number of training epochs, and suggest that data transformations should be an important component of training neural networks for speech, especially for data limited projects.},
  langid = {english}
}

@misc{jang_categorical_2017,
  title = {Categorical {{Reparameterization}} with {{Gumbel-Softmax}}},
  author = {Jang, Eric and Gu, Shixiang and Poole, Ben},
  year = {2017},
  month = aug,
  eprint = {1611.01144},
  eprinttype = {arxiv},
  abstract = {Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Gumbel softmax,Statistics - Machine Learning}
}

@article{jansen_towards_nodate,
  title = {Towards {{Unsupervised Training}} of {{Speaker Independent Acoustic Models}}},
  author = {Jansen, Aren and Church, Kenneth},
  pages = {4},
  abstract = {Can we automatically discover speaker independent phonemelike subword units with zero resources in a surprise language? There have been a number of recent efforts to automatically discover repeated spoken terms without a recognizer. This paper investigates the feasibility of using these results as constraints for unsupervised acoustic model training. We start with a relatively small set of word types, as well as their locations in the speech. The training process assumes that repetitions of the same (unknown) word share the same (unknown) sequence of subword units. For each word type, we train a whole-word hidden Markov model with Gaussian mixture observation densities and collapse correlated states across the word types using spectral clustering. We find that the resulting state clusters align reasonably well along phonetic lines. In evaluating cross-speaker word similarity, the proposed techniques outperform both raw acoustic features and language-mismatched acoustic models.},
  langid = {english}
}

@inproceedings{jansen_unsupervised_2018,
  title = {Unsupervised {{Learning}} of {{Semantic Audio Representations}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Jansen, Aren and Plakal, Manoj and Pandya, Ratheet and Ellis, Daniel P. W. and Hershey, Shawn and Liu, Jiayang and Moore, R. Channing and Saurous, Rif A.},
  year = {2018},
  month = apr,
  pages = {126--130},
  issn = {2379-190X},
  abstract = {Even in the absence of any explicit semantic annotation, vast collections of audio recordings provide valuable information for learning the categorical structure of sounds. We consider several class-agnostic semantic constraints that apply to unlabeled nonspeech audio: (i) noise and translations in time do not change the underlying sound category, (ii) a mixture of two sound events inherits the categories of the constituents, and (iii) the categories of events in close temporal proximity are likely to be the same or related. Without labels to ground them, these constraints are incompatible with classification loss functions. However, they may still be leveraged to identify geometric inequalities needed for triplet loss-based training of convolutional neural networks. The result is low-dimensional embeddings of the input spectrograms that recover 41\% and 84\% of the performance of their fully-supervised counterparts when applied to downstream query-by-example sound retrieval and sound event classification tasks, respectively. Moreover, in limited-supervision settings, our unsupervised embeddings double the state-of-the-art classification performance.},
  keywords = {Computer architecture,Measurement,Microsoft Windows,Semantics,sound classification,Spectrogram,Standards,Training,triplet loss,Unsupervised learning}
}

@inproceedings{jansen_weak_2013,
  title = {Weak Top-down Constraints for Unsupervised Acoustic Model Training},
  booktitle = {{{IEEE}} International Conference on Acoustics, Speech and Signal Processing, {{ICASSP}} 2013, Vancouver, {{BC}}, Canada, May 26-31, 2013},
  author = {Jansen, Aren and Thomas, Samuel and Hermansky, Hynek},
  year = {2013},
  pages = {8091--8095}
}

@article{jati_neural_2019,
  title = {Neural {{Predictive Coding Using Convolutional Neural Networks Toward Unsupervised Learning}} of {{Speaker Characteristics}}},
  author = {Jati, Arindam and Georgiou, Panayiotis},
  year = {2019},
  month = oct,
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {27},
  number = {10},
  pages = {1577--1589},
  issn = {2329-9304},
  abstract = {Learning speaker-specific features is vital in many applications like speaker recognition, diarization, and speech recognition. This paper provides a novel approach, we term neural predictive coding (NPC), to learn speaker-specific characteristics in a completely unsupervised manner from large amounts of unlabeled training data that even contain many non-speech events and multi-speaker audio streams. The NPC framework exploits the proposed short-term active-speaker stationarity hypothesis which assumes two temporally close short speech segments belong to the same speaker, and thus a common representation that can encode the commonalities of both the segments, should capture the vocal characteristics of that speaker. We train a convolutional deep siamese network to produce ``speaker embeddings'' by learning to separate ``same'' versus ``different'' speaker pairs which are generated from an unlabeled data of audio streams. Two sets of experiments are done in different scenarios to evaluate the strength of NPC embeddings and compare with state-of-the-art in-domain supervised methods. First, two speaker identification experiments with different context lengths are performed in a scenario with comparatively limited within-speaker channel variability. NPC embeddings are found to perform the best at short duration experiment, and they provide complementary information to i-vectors for full utterance experiments. Second, a large-scale speaker verification task having a wide range of within-speaker channel variability is adopted as an upper-bound experiment where comparisons are drawn with in-domain supervised methods.},
  keywords = {Convolutional Neural Networks (CNN),Neural networks,Predictive coding,siamese network,speaker recognition,Speaker recognition,Speaker-specific characteristics,Speech processing,Speech recognition,Task analysis,Training,unsupervised learning}
}

@inproceedings{jati_speaker2vec_2017,
  title = {{{Speaker2Vec}}: {{Unsupervised Learning}} and {{Adaptation}} of a {{Speaker Manifold Using Deep Neural Networks}} with an {{Evaluation}} on {{Speaker Segmentation}}},
  shorttitle = {{{Speaker2Vec}}},
  booktitle = {Interspeech 2017},
  author = {Jati, Arindam and Georgiou, Panayiotis},
  year = {2017},
  month = aug,
  pages = {3567--3571},
  publisher = {{ISCA}},
  abstract = {This paper presents a novel approach, we term Speaker2Vec, to derive a speaker-characteristics manifold learned in an unsupervised manner. The proposed representation can be employed in different applications such as diarization, speaker identification or, as in our evaluation test case, speaker segmentation. Speaker2Vec exploits large amounts of unlabeled training data and the assumption of short-term active-speaker stationarity to derive a speaker embedding using Deep Neural Networks (DNN). We assume that temporally-near speech segments belong to the same speaker, and as such a joint representation connecting these nearby segments can encode their common information. Thus, this bottleneck representation will be capturing mainly speaker-specific information. Such training can take place in a completely unsupervised manner. For testing, our trained model generates the embeddings for the test audio, and applies a simple distance metric to detect speaker-change points. The paper also proposes a strategy for unsupervised adaptation of the DNN models to the application domain. The proposed method outperforms the state-of-the-art speaker segmentation algorithms and MFCC based baseline methods on four evaluation datasets, while it allows for further improvements by employing this embedding into supervised training methods.},
  langid = {english}
}

@article{jeffreys_invariant_1946,
  title = {An {{Invariant Form}} for the {{Prior Probability}} in {{Estimation Problems}}},
  author = {Jeffreys, H.},
  year = {1946},
  journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {186},
  number = {1007},
  pages = {453--461},
  issn = {1364-5021},
  abstract = {It is shown that a certain differential form depending on the values of the parameters in a law of chance is invariant for all transformations of the parameters when the law is differentiable with regard to all parameters. For laws containing a location and a scale parameter a form with a somewhat restricted type of invariance is found even when the law is not everywhere differentiable with regard to the parameters. This form has the properties required to give a general rule for stating the prior probability in a large class of estimation problems.},
  isbn = {1364-5021},
  pmid = {20998741}
}

@article{ji_3d_2013,
  title = {{{3D}} Convolutional Neural Networks for Human Action Recognition},
  author = {Ji, Shuiwang and Xu, Wei and Yang, Ming and Yu, Kai},
  year = {2013},
  journal = {IEEE transactions on pattern analysis and machine intelligence},
  volume = {35},
  number = {1},
  pages = {221--231},
  publisher = {{IEEE}}
}

@article{ji_blackout_2016,
  title = {{{BlackOut}}: {{Speeding}} up {{Recurrent Neural Network Language Models With Very Large Vocabularies}}},
  author = {Ji, Shihao and Vishwanathan, S. V. N. and Satish, Nadathur and Anderson, Michael J. and Dubey, Pradeep},
  year = {2016},
  journal = {Under review of ICLR},
  eprint = {1511.06909},
  eprinttype = {arxiv},
  pages = {1--12},
  abstract = {We propose BlackOut, an approximation algorithm to efficiently train massive recurrent neural network language models (RNNLMs) with million word vocabularies. BlackOut is motivated by using a discriminative loss, and we describe a new sampling strategy which significantly reduces computation while improving stability, sample efficiency, and rate of convergence. One way to understand BlackOut is to view it as an extension of the DropOut strategy to the output layer, wherein we use a discriminative training loss and a weighted sampling scheme. We also establish close connections between BlackOut, importance sampling, and noise contrastive estimation (NCE). Our experiments, on the recently released one billion word language modeling benchmark, demonstrate scalability and accuracy of BlackOut; we outperform the state-of-the art, and achieve the lowest perplexity scores on this dataset. Moreover, unlike other established methods which typically require GPUs or CPU clusters, we show that a carefully implemented version of BlackOut requires only 1-10 days on a single machine to train a RNNLM with a million word vocabulary and billions of parameters on one billion of words.},
  archiveprefix = {arXiv}
}

@misc{jia_direct_2019,
  title = {Direct Speech-to-Speech Translation with a Sequence-to-Sequence Model},
  author = {Jia, Ye and Weiss, Ron J. and Biadsy, Fadi and Macherey, Wolfgang and Johnson, Melvin and Chen, Zhifeng and Wu, Yonghui},
  year = {2019},
  month = apr,
  eprint = {1904.06037},
  eprinttype = {arxiv},
  abstract = {We present an attention-based sequence-to-sequence neural network which can directly translate speech from one language into speech in another language, without relying on an intermediate text representation. The network is trained end-to-end, learning to map speech spectrograms into target spectrograms in another language, corresponding to the translated content (in a different canonical voice). We further demonstrate the ability to synthesize translated speech using the voice of the source speaker. We conduct experiments on two Spanish-to-English speech translation datasets, and find that the proposed model slightly underperforms a baseline cascade of a direct speech-to-text translation model and a text-to-speech synthesis model, demonstrating the feasibility of the approach on this very challenging task.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@misc{jiang_fantastic_2019,
  title = {Fantastic {{Generalization Measures}} and {{Where}} to {{Find Them}}},
  author = {Jiang, Yiding and Neyshabur, Behnam and Mobahi, Hossein and Krishnan, Dilip and Bengio, Samy},
  year = {2019},
  month = dec,
  eprint = {1912.02178},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Generalization of deep networks has been of great interest in recent years, resulting in a number of theoretically and empirically motivated complexity measures. However, most papers proposing such measures study only a small set of models, leaving open the question of whether the conclusion drawn from those experiments would remain valid in other settings. We present the first large scale study of generalization in deep networks. We investigate more then 40 complexity measures taken from both theoretical bounds and empirical studies. We train over 10,000 convolutional networks by systematically varying commonly used hyperparameters. Hoping to uncover potentially causal relationships between each measure and generalization, we analyze carefully controlled experiments and show surprising failures of some measures as well as promising measures for further research.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{jiang_further_2020,
  title = {A {{Further Study}} of {{Unsupervised Pre-training}} for {{Transformer Based Speech Recognition}}},
  author = {Jiang, Dongwei and Li, Wubo and Zhang, Ruixiong and Cao, Miao and Luo, Ne and Han, Yang and Zou, Wei and Li, Xiangang},
  year = {2020},
  month = jun,
  journal = {arXiv:2005.09862 [cs, eess]},
  eprint = {2005.09862},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {Building a good speech recognition system usually requires large amounts of transcribed data, which is expensive to collect. To tackle this problem, many unsupervised pre-training methods have been proposed. Among these methods, Masked Predictive Coding achieved significant improvements on various speech recognition datasets with BERT-like Masked Reconstruction loss and Transformer backbone. However, many aspects of MPC have not been fully investigated. In this paper, we conduct a further study on MPC and focus on three important aspects: the effect of pre-training data speaking style, its extension on streaming model, and how to better transfer learned knowledge from pre-training stage to downstream tasks. Experiments reveled that pre-training data with matching speaking style is more useful on downstream recognition tasks. A unified training objective with APC and MPC provided 8.46\% relative error reduction on streaming model trained on HKUST. Also, the combination of target data adaption and layer-wise discriminative training helped the knowledge transfer of MPC, which achieved 3.99\% relative error reduction on AISHELL over a strong baseline.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@article{jiang_improving_2019,
  title = {Improving {{Transformer-based Speech Recognition Using Unsupervised Pre-training}}},
  author = {Jiang, Dongwei and Lei, Xiaoning and Li, Wubo and Luo, Ne and Hu, Yuxuan and Zou, Wei and Li, Xiangang},
  year = {2019},
  month = oct,
  journal = {arXiv:1910.09932 [cs, eess]},
  eprint = {1910.09932},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {Speech recognition technologies are gaining enormous popularity in various industrial applications. However, building a good speech recognition system usually requires large amounts of transcribed data, which is expensive to collect. To tackle this problem, an unsupervised pre-training method called Masked Predictive Coding is proposed, which can be applied for unsupervised pre-training with Transformer based model. Experiments on HKUST show that using the same training data, we can achieve CER 23.3\%, exceeding the best end-to-end model by over 0.2\% absolute CER. With more pre-training data, we can further reduce the CER to 21.0\%, or a 11.8\% relative CER reduction over baseline.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@inproceedings{jiao_online_2016,
  title = {Online Speaking Rate Estimation Using Recurrent Neural Networks},
  booktitle = {2016 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Jiao, Yishan and Tu, Ming and Berisha, Visar and Liss, Julie},
  year = {2016},
  month = mar,
  pages = {5245--5249},
  publisher = {{IEEE}},
  address = {{Shanghai}},
  abstract = {A reliable online speaking rate estimation tool is useful in many domains, including speech recognition, speech therapy intervention, speaker identification, etc. This paper proposes an online speaking rate estimation model based on recurrent neural networks (RNNs). Speaking rate is a long-term feature of speech, which depends on how many syllables were spoken over an extended time window (seconds). We posit that since RNNs can capture long-term dependencies through the memory of previous hidden states, they are a good match for the speaking rate estimation task. Here we train a long shortterm memory (LSTM) RNN on a set of speech features that are known to correlate with speech rhythm. An evaluation on spontaneous speech shows that the method yields a higher correlation between the estimated rate and the ground-truth rate when compared to the state-of-the-art alternatives. The evaluation on longitudinal pathological speech shows that the proposed method can capture long-term and short-term changes in speaking rate.},
  isbn = {978-1-4799-9988-0},
  langid = {english}
}

@article{johnson_accelerating_2013,
  title = {Accelerating {{Stochastic Gradient Descent}} Using {{Predictive Variance Reduction}}},
  author = {Johnson, Rie and Zhang, Tong},
  year = {2013},
  journal = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
  volume = {1},
  number = {3},
  pages = {315--323},
  issn = {10495258},
  abstract = {Stochastic gradient descent is popular for large scale optimization but has slow convergence asymptotically due to the inherent variance. To remedy this prob-lem, we introduce an explicit variance reduction method for stochastic gradient descent which we call stochastic variance reduced gradient (SVRG). For smooth and strongly convex functions, we prove that this method enjoys the same fast con-vergence rate as those of stochastic dual coordinate ascent (SDCA) and Stochastic Average Gradient (SAG). However, our analysis is significantly simpler and more intuitive. Moreover, unlike SDCA or SAG, our method does not require the stor-age of gradients, and thus is more easily applicable to complex problems such as some structured prediction problems and neural network learning.},
  pmid = {880145}
}

@inproceedings{johnson_composing_2016,
  title = {Composing Graphical Models with Neural Networks for Structured Representations and Fast Inference},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Johnson, Matthew J and Duvenaud, David K and Wiltschko, Alex and Adams, Ryan P and Datta, Sandeep R},
  editor = {Lee, D. and Sugiyama, M. and Luxburg, U. and Guyon, I. and Garnett, R.},
  year = {2016},
  volume = {29},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{jones_is_2020,
  title = {Is the Discrete {{VAE}}'s Power Stuck in Its Prior?},
  booktitle = {''{{I}} Can't Believe It's Not Better!'' {{NeurIPS}} 2020 Workshop},
  author = {Jones, Haydn Thomas and Moore, Juston},
  year = {2020}
}

@article{jones_taxonomy_nodate,
  title = {A {{Taxonomy}} of {{Global Optimization Methods Based}} on {{Response Surfaces}}},
  author = {Jones, Donald R},
  pages = {39},
  abstract = {This paper presents a taxonomy of existing approaches for using response surfaces for global optimization. Each method is illustrated with a simple numerical example that brings out its advantages and disadvantages. The central theme is that methods that seem quite reasonable often have non-obvious failure modes. Understanding these failure modes is essential for the development of practical algorithms that fulfill the intuitive promise of the response surface approach.},
  langid = {english},
  keywords = {⛔ No DOI found}
}

@article{jordan_introduction_1999,
  title = {An Introduction to Variational Methods for Graphical Models},
  author = {Jordan, Michael I. and Ghahramani, Zoubin and Jaakkola, Tommi S. and Saul, Lawrence K.},
  year = {1999},
  journal = {Machine Learning},
  volume = {37},
  number = {2},
  pages = {183--233}
}

@article{jorgensen_isometric_nodate,
  title = {Isometric {{Gaussian Process Latent Variable Model}}  for {{Dissimilarity Data}}},
  author = {J{\o}rgensen, Martin and Hauberg, S{\o}ren},
  pages = {10},
  abstract = {We present a probabilistic model where the latent variable respects both the distances and the topology of the modeled data. The model leverages the Riemannian geometry of the generated manifold to endow the latent space with a well-defined stochastic distance measure, which is modeled locally as Nakagami distributions. These stochastic distances are sought to be as similar as possible to observed distances along a neighborhood graph through a censoring process. The model is inferred by variational inference based on observations of pairwise distances. We demonstrate how the new model can encode invariances in the learned manifolds.},
  langid = {english}
}

@misc{joy_learning_2021,
  title = {Learning {{Multimodal VAEs}} through {{Mutual Supervision}}},
  author = {Joy, Tom and Shi, Yuge and Torr, Philip H. S. and Rainforth, Tom and Schmon, Sebastian M. and Siddharth, N.},
  year = {2021},
  month = jun,
  eprint = {2106.12570},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Multimodal variational autoencoders (VAEs) seek to model the joint distribution over heterogeneous data (e.g. vision, language), whilst also capturing a shared representation across such modalities. Prior work has typically combined information from the modalities by reconciling idiosyncratic representations directly in the recognition model through explicit products, mixtures, or other such factorisations. Here we introduce a novel alternative, the Mutually supErvised Multimodal VAE (MEME), that avoids such explicit combinations by repurposing semi-supervised VAEs to combine information between modalities implicitly through mutual supervision. This formulation naturally allows learning from partially-observed data where some modalities can be entirely missing\textemdash something that most existing approaches either cannot handle, or do so to a limited extent. We demonstrate that MEME outperforms baselines on standard metrics across both partial and complete observation schemes on the MNIST-SVHN (image\textendash image) and CUB (image\textendash text) datasets. We also contrast the quality of the representations learnt by mutual supervision against standard approaches and observe interesting trends in its ability to capture relatedness between data.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@article{jozefowicz_empirical_nodate,
  title = {An {{Empirical Exploration}} of {{Recurrent Network Architectures}}},
  author = {Jozefowicz, Rafal and Zaremba, Wojciech and Sutskever, Ilya},
  abstract = {The Recurrent Neural Network (RNN) is an ex-tremely powerful sequence model that is often difficult to train. The Long Short-Term Memory (LSTM) is a specific RNN architecture whose design makes it much easier to train. While wildly successful in practice, the LSTM's archi-tecture appears to be ad-hoc so it is not clear if it is optimal, and the significance of its individual components is unclear. In this work, we aim to determine whether the LSTM architecture is optimal or whether much better architectures exist. We conducted a thor-ough architecture search where we evaluated over ten thousand different RNN architectures, and identified an architecture that outperforms both the LSTM and the recently-introduced Gated Recurrent Unit (GRU) on some but not all tasks. We found that adding a bias of 1 to the LSTM's forget gate closes the gap between the LSTM and the GRU.}
}

@misc{jozefowicz_exploring_2016,
  title = {Exploring the Limits of Language Modeling},
  author = {Jozefowicz, Rafal and Vinyals, Oriol and Schuster, Mike and Shazeer, Noam and Wu, Yonghui},
  year = {2016},
  eprint = {1602.02410},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@article{jumper_highly_2021,
  title = {Highly Accurate Protein Structure Prediction with {{AlphaFold}}},
  author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v Z}{\'i}dek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and {Romera-Paredes}, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
  year = {2021},
  month = aug,
  journal = {Nature},
  volume = {596},
  number = {7873},
  pages = {583--589},
  issn = {0028-0836, 1476-4687},
  abstract = {Abstract                            Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort               1\textendash 4               , the structures of around 100,000 unique proteins have been determined               5               , but this represents a small fraction of the billions of known protein sequences               6,7               . Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence\textemdash the structure prediction component of the `protein folding problem'               8               \textemdash has been an important open research problem for more than 50~years               9               . Despite recent progress               10\textendash 14               , existing methods fall far~short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)               15               , demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
  langid = {english}
}

@article{jumper_supplementary_nodate,
  title = {Supplementary {{Information}} for: {{Highly}} Accurate Protein Structure Prediction with {{AlphaFold}}},
  author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v Z}{\'i}dek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, A A and Ballard, Andrew J and Cowie, Andrew and {Romera-Paredes}, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Senior, Andrew W and Kavukcuoglu, Koray and Kohli, Pushmeet},
  pages = {62},
  langid = {english}
}

@inproceedings{jun_distribution_2020,
  title = {Distribution {{Augmentation}} for {{Generative Modeling}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Jun, Heewoo and Child, Rewon and Chen, Mark and Schulman, John and Ramesh, Aditya and Radford, Alec and Sutskever, Ilya},
  year = {2020},
  series = {Proceedings of {{Machine Learning Research}}},
  pages = {5006--5019},
  publisher = {{PMLR}},
  address = {{Vienna, Austria}},
  abstract = {We present distribution augmentation (DistAug), a simple and powerful method of regularizing generative models. Our approach applies augmentation functions to data and, importantly, conditions the generative model on the specific function used. Unlike typical data augmentation, DistAug allows usage of functions which modify the target density, enabling aggressive augmentations more commonly seen in supervised and self-supervised learning. We demonstrate this is a more effective regularizer than standard methods, and use it to train a 152M parameter autoregressive model on CIFAR-10 to 2.56 bits per dim (relative to the state-of-the-art 2.80). Samples from this model attain FID 12.75 and IS 8.40, outperforming the majority of GANs. We further demonstrate the technique is broadly applicable across model architectures and problem domains.},
  langid = {english}
}

@article{jung_current_2014,
  title = {Current Status and Future Advances for Wind Speed and Power Forecasting},
  author = {Jung, Jaesung and Broadwater, Robert P.},
  year = {2014},
  journal = {Renewable and Sustainable Energy Reviews},
  volume = {31},
  pages = {762--777},
  issn = {13640321},
  abstract = {This paper presents an overview of existing research on wind speed and power forecasting. It first discusses state-of-the-art wind speed and power forecasting approaches. Then, forecasting accuracy is presented based on variable factors. Finally, potential techniques to improve the accuracy of forecasting models are reviewed. A full survey on all existing models is not presented, but attempts to highlight the most promising body of knowledge concerning wind speed and power forecasting. \textcopyright{} 2014 Elsevier Ltd.},
  isbn = {1364-0321},
  keywords = {Offshore forecasting,Probabilistic forecasting,Regional forecasting,Spatial correlation forecasting,Wind power,Wind speed}
}

@book{jurafsky_speech_2019,
  title = {Speech and {{Language Processing}} - {{An Introduction}} to {{Natural Language Processing}}, {{Computational Linguistics}}, and {{Speech Recognition}}},
  author = {Jurafsky, Daniel and Martin, James H.},
  year = {2019},
  edition = {Third},
  address = {{Boulder, Colorado}},
  keywords = {★}
}

@article{kaelbling_artificial_1998,
  title = {Artificial {{Intelligence Planning}} and Acting in Partially Observable Stochastic Domains},
  author = {Kaelbling, Leslie Pack and Littman, Michael L. and Cassandra ', Anthony R.},
  year = {1998},
  journal = {Artificial IntelligenceArti\$cial Intelligence},
  volume = {101},
  number = {101},
  pages = {99--134},
  abstract = {In this paper, we bring techniques from operations research to bear on the problem of choosing optimal actions in partially observable stochastic domains. We begin by introducing the theory of Markov decision processes (MDPS) and partially observable MDPS (POMDPS). We then outline a novel algorithm for solving POMDPS off line and show how, in some cases, a finite-memory controller can be extracted from the solution to a POMDP. We conclude with a discussion of how our approach relates to previous work, the complexity of finding exact solutions to POMDPS, and of some possibilities for finding approximate solutions. 0 1998 Consider the problem of a robot navigating in a large office building. The robot can move from hallway intersection to intersection and can make local observations of its world. Its actions are not completely reliable, however. Sometimes, when it intends to move, it stays where it is or goes too far; sometimes, when it intends to turn, it overshoots. It has similar problems with observation. Sometimes a corridor looks like a comer; sometimes a T-junction looks like an L-junction. How can such an error-plagued robot navigate, even given a map of the corridors? In general, the robot will have to remember something about its history of actions and observations and use this information, together with its knowledge of the underlying dynamics of the world (the map and other information), to maintain an estimate of its location. Many engineering applications follow this approach, using methods like the Kalman filter [26] to maintain a running estimate of the robot's spatial uncertainty, expressed as an ellipsoid or normal distribution in Cartesian space. This approach will not do for our robot, though. Its uncertainty may be discrete: it might be almost certain that it is in the northeast comer of either the fourth or the seventh floors, though it admits a chance that it is on the fifth floor, as well. Then, given an uncertain estimate of its location, the robot has to decide what actions to take. In some cases, it might be sufficient to ignore its uncertainty and take actions that would be appropriate for the most likely location. In other cases, it might be better for the robot to take actions for the purpose of gathering information, such as searching for a landmark or reading signs on the wall. In general, it will take actions that fulfill both purposes simultaneously.},
  keywords = {Partially observable Markov decision processes,Planning,Uncertainty}
}

@article{kaelbling_reinforcement_1996,
  title = {Reinforcement Learning: {{A Survey}}},
  author = {Kaelbling, Leslie Pack and Littman, Michael L. and Moore, Andrew W.},
  year = {1996},
  journal = {Journal of Artificial Intelligence Research},
  volume = {4},
  pages = {237--285},
  abstract = {This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word ``reinforcement.'' The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.}
}

@inproceedings{kahn_libri-light_2020,
  title = {Libri-Light: {{A}} Benchmark for {{ASR}} with Limited or No Supervision},
  booktitle = {Proceedings of the {{International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Kahn, J. and Riviere, M. and Zheng, W. and Kharitonov, E. and Xu, Q. and Mazare, P.E. and Karadayi, J. and Liptchinsky, V. and Collobert, R. and Fuegen, C. and {al.}, et},
  year = {2020},
  month = may,
  address = {{Virtual}}
}

@misc{kalatzis_variational_2020,
  title = {Variational {{Autoencoders}} with {{Riemannian Brownian Motion Priors}}},
  author = {Kalatzis, Dimitris and Eklund, David and Arvanitidis, Georgios and Hauberg, S{\o}ren},
  year = {2020},
  month = feb,
  eprint = {2002.05227},
  eprinttype = {arxiv},
  abstract = {Variational Autoencoders (VAEs) represent the given data in a low-dimensional latent space, which is generally assumed to be Euclidean. This assumption naturally leads to the common choice of a standard Gaussian prior over continuous latent variables. Recent work has, however, shown that this prior has a detrimental effect on model capacity, leading to subpar performance. We propose that the Euclidean assumption lies at the heart of this failure mode. To counter this, we assume a Riemannian structure over the latent space, which constitutes a more principled geometric view of the latent codes, and replace the standard Gaussian prior with a Riemannian Brownian motion prior. We propose an efficient inference scheme that does not rely on the unknown normalizing factor of this prior. Finally, we demonstrate that this prior significantly increases model capacity using only one additional scalar parameter.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{kalchbrenner_convolutional_2014,
  title = {A {{Convolutional Neural Network}} for {{Modelling Sentences}}},
  booktitle = {Proceedings of the 52nd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Kalchbrenner, Nal and Grefenstette, Edward and Blunsom, Phil},
  year = {2014},
  eprint = {1404.2188v1},
  eprinttype = {arxiv},
  pages = {655--665},
  issn = {03064573},
  abstract = {The ability to accurately represent sen- tences is central to language understand- ing. We describe a convolutional architec- ture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences. The network uses Dynamic k-Max Pool- ing, a global pooling operation over lin- ear sequences. The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations. The network does not rely on a parse tree and is easily ap- plicable to any language. We test the DCNN in four experiments: small scale binary and multi-class sentiment predic- tion, six-way question classification and Twitter sentiment prediction by distant su- pervision. The network achieves excellent performance in the first three tasks and a greater than 25\% error reduction in the last task with respect to the strongest baseline.},
  archiveprefix = {arXiv},
  isbn = {978-1-937284-72-5},
  pmid = {15003161}
}

@inproceedings{kamper_deep_2016,
  title = {Deep Convolutional Acoustic Word Embeddings Using Word-Pair Side Information},
  booktitle = {2016 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Kamper, Herman and Wang, Weiran and Livescu, Karen},
  year = {2016},
  month = mar,
  pages = {4950--4954},
  issn = {2379-190X},
  abstract = {Recent studies have been revisiting whole words as the basic modelling unit in speech recognition and query applications, instead of phonetic units. Such whole-word segmental systems rely on a function that maps a variable-length speech segment to a vector in a fixed-dimensional space; the resulting acoustic word embeddings need to allow for accurate discrimination between different word types, directly in the embedding space. We compare several old and new approaches in a word discrimination task. Our best approach uses side information in the form of known word pairs to train a Siamese convolutional neural network (CNN): a pair of tied networks that take two speech segments as input and produce their embeddings, trained with a hinge loss that separates same-word pairs and different-word pairs by some margin. A word classifier CNN performs similarly, but requires much stronger supervision. Both types of CNNs yield large improvements over the best previously published results on the word discrimination task.},
  keywords = {Acoustic word embeddings,Acoustics,Fasteners,fixed-dimensional representations,Lattices,Neural networks,query-by-example search,segmental acoustic models,Speech,Speech recognition,Training}
}

@inproceedings{kamper_embedded_2017,
  title = {An Embedded Segmental {{K-means}} Model for Unsupervised Segmentation and Clustering of Speech},
  booktitle = {2017 {{IEEE Automatic Speech Recognition}} and {{Understanding Workshop}} ({{ASRU}})},
  author = {Kamper, Herman and Livescu, Karen and Goldwater, Sharon},
  year = {2017},
  month = dec,
  pages = {719--726},
  abstract = {Unsupervised segmentation and clustering of unlabelled speech are core problems in zero-resource speech processing. Most approaches lie at methodological extremes: some use probabilistic Bayesian models with convergence guarantees, while others opt for more efficient heuristic techniques. Despite competitive performance in previous work, the full Bayesian approach is difficult to scale to large speech corpora. We introduce an approximation to a recent Bayesian model that still has a clear objective function but improves efficiency by using hard clustering and segmentation rather than full Bayesian inference. Like its Bayesian counterpart, this embedded segmental K-means model (ES-KMeans) represents arbitrary-length word segments as fixed-dimensional acoustic word embeddings. We first compare ES-KMeans to previous approaches on common English and Xitsonga data sets (5 and 2.5 hours of speech): ES-KMeans outperforms a leading heuristic method in word segmentation, giving similar scores to the Bayesian model while being 5 times faster with fewer hyperparameters. However, its clusters are less pure than those of the other models. We then show that ES-KMeans scales to larger corpora by applying it to the 5 languages of the Zero Resource Speech Challenge 2017 (up to 45 hours), where it performs competitively compared to the challenge baseline.},
  keywords = {Acoustics,Bayes methods,Clustering algorithms,language acquisition,Probabilistic logic,Speech,Speech processing,Standards,unsupervised learning,word segmentation,Zero-resource speech processing}
}

@article{kamper_segmental_2017,
  title = {A Segmental Framework for Fully-Unsupervised Large-Vocabulary Speech Recognition},
  author = {Kamper, Herman and Jansen, Aren and Goldwater, Sharon},
  year = {2017},
  month = nov,
  journal = {Computer Speech \& Language},
  volume = {46},
  pages = {154--174},
  issn = {0885-2308},
  abstract = {Zero-resource speech technology is a growing research area that aims to develop methods for speech processing in the absence of transcriptions, lexicons, or language modelling text. Early term discovery systems focused on identifying isolated recurring patterns in a corpus, while more recent full-coverage systems attempt to completely segment and cluster the audio into word-like units\textemdash effectively performing unsupervised speech recognition. This article presents the first attempt we are aware of to apply such a system to large-vocabulary multi-speaker data. Our system uses a Bayesian modelling framework with segmental word representations: each word segment is represented as a fixed-dimensional acoustic embedding obtained by mapping the sequence of feature frames to a single embedding vector. We compare our system on English and Xitsonga datasets to state-of-the-art baselines, using a variety of measures including word error rate (obtained by mapping the unsupervised output to ground truth transcriptions). Very high word error rates are reported\textemdash in the order of 70\textendash 80\% for speaker-dependent and 80\textendash 95\% for speaker-independent systems\textemdash highlighting the difficulty of this task. Nevertheless, in terms of cluster quality and word segmentation metrics, we show that by imposing a consistent top-down segmentation while also using bottom-up knowledge from detected syllable boundaries, both single-speaker and multi-speaker versions of our system outperform a purely bottom-up single-speaker syllable-based approach. We also show that the discovered clusters can be made less speaker- and gender-specific by using an unsupervised autoencoder-like feature extractor to learn better frame-level features (prior to embedding). Our system's discovered clusters are still less pure than those of unsupervised term discovery systems, but provide far greater coverage.},
  langid = {english},
  keywords = {Clustering,Language acquisition,Representation learning,Segmentation,Unsupervised speech processing}
}

@inproceedings{kamper_unsupervised_2015,
  title = {Unsupervised Neural Network Based Feature Extraction Using Weak Top-down Constraints},
  booktitle = {2015 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Kamper, Herman and Elsner, Micha and Jansen, Aren and Goldwater, Sharon},
  year = {2015},
  month = apr,
  pages = {5818--5822},
  issn = {2379-190X},
  abstract = {Deep neural networks (DNNs) have become a standard component in supervised ASR, used in both data-driven feature extraction and acoustic modelling. Supervision is typically obtained from a forced alignment that provides phone class targets, requiring transcriptions and pronunciations. We propose a novel unsupervised DNN-based feature extractor that can be trained without these resources in zero-resource settings. Using unsupervised term discovery, we find pairs of isolated word examples of the same unknown type; these provide weak top-down supervision. For each pair, dynamic programming is used to align the feature frames of the two words. Matching frames are presented as input-output pairs to a deep autoencoder (AE) neural network. Using this AE as feature extractor in a word discrimination task, we achieve 64\% relative improvement over a previous state-of-the-art system, 57\% improvement relative to a bottom-up trained deep AE, and come to within 23\% of a supervised system.},
  keywords = {Artificial neural networks,deep neural networks,Feature extraction,Gold,Speech,Standards,top-down constraints,Training,Unsupervised feature extraction,zero-resource speech processing}
}

@misc{kannan_large-scale_2019,
  title = {Large-{{Scale Multilingual Speech Recognition}} with a {{Streaming End-to-End Model}}},
  author = {Kannan, Anjuli and Datta, Arindrima and Sainath, Tara N. and Weinstein, Eugene and Ramabhadran, Bhuvana and Wu, Yonghui and Bapna, Ankur and Chen, Zhifeng and Lee, Seungji},
  year = {2019},
  month = sep,
  eprint = {1909.05330},
  eprinttype = {arxiv},
  abstract = {Multilingual end-to-end (E2E) models have shown great promise in expansion of automatic speech recognition (ASR) coverage of the world's languages. They have shown improvement over monolingual systems, and have simplified training and serving by eliminating language-specific acoustic, pronunciation, and language models. This work presents an E2E multilingual system which is equipped to operate in low-latency interactive applications, as well as handle a key challenge of real world data: the imbalance in training data across languages. Using nine Indic languages, we compare a variety of techniques, and find that a combination of conditioning on a language vector and training language-specific adapter layers produces the best model. The resulting E2E multilingual model achieves a lower word error rate (WER) than both monolingual E2E models (eight of nine languages) and monolingual conventional systems (all nine languages).},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning}
}

@inproceedings{karpathy_deep_2015,
  title = {Deep Visual-Semantic Alignments for Generating Image Descriptions},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CCVPR}})},
  author = {Karpathy, Andrej and {Fei-Fei}, Li},
  year = {2015},
  pages = {3128--3137}
}

@inproceedings{karpathy_large-scale_2014,
  title = {Large-Scale Video Classification with Convolutional Neural Networks},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CCVPR}})},
  author = {Karpathy, Andrej and Toderici, George and Shetty, Sanketh and Leung, Thomas and Sukthankar, Rahul and {Fei-Fei}, Li},
  year = {2014},
  pages = {1725--1732}
}

@misc{karpathy_unreasonable_2015,
  title = {The {{Unreasonable Effectiveness}} of {{Recurrent Neural Networks}}},
  author = {Karpathy, Andrej},
  year = {2015},
  howpublished = {http://karpathy.github.io/2015/05/21/rnn-effectiveness/}
}

@misc{karpathy_visualizing_2015,
  title = {Visualizing and {{Understanding Recurrent Networks}}},
  author = {Karpathy, Andrej and Johnson, Justin and {Fei-Fei}, Li},
  year = {2015},
  eprint = {1506.02078},
  eprinttype = {arxiv},
  abstract = {Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-Term Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing an analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, our comparative analysis with finite horizon n-gram models traces the source of the LSTM improvements to long-range structural dependencies. Finally, we provide analysis of the remaining errors and suggests areas for further study.},
  archiveprefix = {arXiv},
  isbn = {978-3-319-10589-5},
  pmid = {26353135}
}

@misc{karras_progressive_2018,
  title = {Progressive {{Growing}} of {{GANs}} for {{Improved Quality}}, {{Stability}}, and {{Variation}}},
  author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
  year = {2018},
  month = feb,
  eprint = {1710.10196},
  eprinttype = {arxiv},
  abstract = {We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CELEBA images at 10242. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CELEBA dataset.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
}

@misc{katharopoulos_biased_2017,
  title = {Biased {{Importance Sampling}} for {{Deep Neural Network Training}}},
  author = {Katharopoulos, Angelos and Fleuret, Fran{\c c}ois},
  year = {2017},
  month = may,
  eprint = {1706.00043},
  eprinttype = {arxiv},
  abstract = {Importance sampling has been successfully used to accelerate stochastic optimization in many convex problems. However, the lack of an efficient way to calculate the importance still hinders its application to Deep Learning. In this paper, we show that the loss value can be used as an alternative importance metric, and propose a way to efficiently approximate it for a deep model, using a small model trained for that purpose in parallel. This method allows in particular to utilize a biased gradient estimate that implicitly optimizes a soft max-loss, and leads to better generalization performance. While such method suffers from a prohibitively high variance of the gradient estimate when using a standard stochastic optimizer, we show that when it is combined with our sampling mechanism, it results in a reliable procedure. We showcase the generality of our method by testing it on both image classification and language modeling tasks using deep convolutional and recurrent neural networks. In particular, our method results in 30\% faster training of a CNN for CIFAR10 than when using uniform sampling.},
  archiveprefix = {arXiv}
}

@misc{katharopoulos_not_2018,
  title = {Not {{All Samples Are Created Equal}}: {{Deep Learning}} with {{Importance Sampling}}},
  author = {Katharopoulos, Angelos and Fleuret, Fran{\c c}ois},
  year = {2018},
  month = mar,
  eprint = {1803.00942},
  eprinttype = {arxiv},
  abstract = {Deep neural network training spends most of the computation on examples that are properly handled, and could be ignored. We propose to mitigate this phenomenon with a principled importance sampling scheme that focuses computation on "informative" examples, and reduces the variance of the stochastic gradients during training. Our contribution is twofold: first, we derive a tractable upper bound to the per-sample gradient norm, and second we derive an estimator of the variance reduction achieved with importance sampling, which enables us to switch it on when it will result in an actual speedup. The resulting scheme can be used by changing a few lines of code in a standard SGD procedure, and we demonstrate experimentally, on image classification, CNN fine-tuning, and RNN training, that for a fixed wall-clock time budget, it provides a reduction of the train losses of up to an order of magnitude and a relative improvement of test errors between 5\% and 17\%.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@article{kawakami_learning_2020,
  title = {Learning {{Robust}} and {{Multilingual Speech Representations}}},
  author = {Kawakami, Kazuya and Wang, Luyu and Dyer, Chris and Blunsom, Phil and van den Oord, Aaron},
  year = {2020},
  month = jan,
  journal = {arXiv:2001.11128 [cs, eess]},
  eprint = {2001.11128},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {Unsupervised speech representation learning has shown remarkable success at finding representations that correlate with phonetic structures and improve downstream speech recognition performance. However, most research has been focused on evaluating the representations in terms of their ability to improve the performance of speech recognition systems on read English (e.g. Wall Street Journal and LibriSpeech). This evaluation methodology overlooks two important desiderata that speech representations should have: robustness to domain shifts and transferability to other languages. In this paper we learn representations from up to 8000 hours of diverse and noisy speech data and evaluate the representations by looking at their robustness to domain shifts and their ability to improve recognition performance in many languages. We find that our representations confer significant robustness advantages to the resulting recognition systems: we see significant improvements in out-of-domain transfer relative to baseline feature sets and the features likewise provide improvements in 25 phonetically diverse languages including tonal languages and low-resource languages.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@article{kawakami_unsupervised_2019,
  title = {Unsupervised {{Learning}} of {{Efficient}} and {{Robust Speech Representations}}},
  author = {Kawakami, Kazuya and Wang, Luyu and Dyer, Chris and Blunsom, Phil and van den Oord, Aaron},
  year = {2019},
  month = sep,
  abstract = {We present an unsupervised method for learning speech representations based on a bidirectional contrastive predictive coding that implicitly discovers phonetic structure from large-scale corpora of...},
  langid = {english}
}

@misc{keng_variational_2017,
  title = {Variational {{Bayes}} and {{The Mean-Field Approximation}} | {{Bounded Rationality}}},
  author = {Keng, Brian},
  year = {2017},
  month = apr,
  howpublished = {https://bjlkeng.github.io/posts/variational-bayes-and-the-mean-field-approximation/}
}

@article{khan_fast_nodate,
  title = {Fast and {{Scalable Bayesian Deep Learning}} by {{Weight-Perturbation}} in {{Adam}}},
  author = {Khan, Mohammad Emtiyaz and Nielsen, Didrik and Tangkaratt, Voot and Lin, Wu and Gal, Yarin and Srivastava, Akash},
  abstract = {Uncertainty computation in deep learning is es-sential to design robust and reliable systems. Vari-ational inference (VI) is a promising approach for such computation, but requires more effort to implement and execute compared to maximum-likelihood methods. In this paper, we propose new natural-gradient algorithms to reduce such ef-forts for Gaussian mean-field VI. Our algorithms can be implemented within the Adam optimizer by perturbing the network weights during gradi-ent evaluations, and uncertainty estimates can be cheaply obtained by using the vector that adapts the learning rate. This requires lower memory, computation, and implementation effort than ex-isting VI methods, while obtaining uncertainty estimates of comparable quality. Our empirical results confirm this and further suggest that the weight-perturbation in our algorithm could be use-ful for exploration in reinforcement learning and stochastic optimization.}
}

@article{khandelwal_sharp_nodate,
  title = {Sharp {{Nearby}}, {{Fuzzy Far Away}}: {{How Neural Language Models Use Context}}},
  author = {Khandelwal, Urvashi and He, He and Qi, Peng and Jurafsky, Dan},
  abstract = {We know very little about how neural lan-guage models (LM) use prior linguistic context. In this paper, we investigate the role of context in an LSTM LM, through ablation studies. Specifically, we ana-lyze the increase in perplexity when prior context words are shuffled, replaced, or dropped. On two standard datasets, Penn Treebank and WikiText-2, we find that the model is capable of using about 200 to-kens of context on average, but sharply distinguishes nearby context (recent 50 to-kens) from the distant history. The model is highly sensitive to the order of words within the most recent sentence, but ig-nores word order in the long-range context (beyond 50 tokens), suggesting the distant past is modeled only as a rough seman-tic field or topic. We further find that the neural caching model (Grave et al., 2017b) especially helps the LSTM to copy words from within this distant context. Overall, our analysis not only provides a better un-derstanding of how neural LMs use their context, but also sheds light on recent suc-cess from cache-based models.}
}

@inproceedings{kharitonov_data_2021,
  title = {Data {{Augmenting Contrastive Learning}} of {{Speech Representations}} in the {{Time Domain}}},
  booktitle = {2021 {{IEEE Spoken Language Technology Workshop}} ({{SLT}})},
  author = {Kharitonov, Eugene and Rivi{\`e}re, Morgane and Synnaeve, Gabriel and Wolf, Lior and Mazar{\'e}, Pierre-Emmanuel and Douze, Matthijs and Dupoux, Emmanuel},
  year = {2021},
  month = jan,
  pages = {215--222},
  abstract = {Contrastive Predictive Coding (CPC), based on predicting future segments of speech from past segments is emerging as a powerful algorithm for representation learning of speech signal. However, it still under-performs compared to other methods on unsupervised evaluation benchmarks. Here, we intro-duce WavAugment, a time-domain data augmentation library which we adapt and optimize for the specificities of CPC (raw waveform input, contrastive loss, past versus future structure). We find that applying augmentation only to the segments from which the CPC prediction is performed yields better results than applying it also to future segments from which the samples (both positive and negative) of the contrastive loss are drawn. After selecting the best combination of pitch modification, additive noise and reverberation on unsupervised metrics on LibriSpeech (with a gain of 18-22\% relative on the ABX score), we apply this combination without any change to three new datasets in the Zero Resource Speech Benchmark 2017 and beat the state-of-the-art using out-of-domain training data. Finally, we show that the data-augmented pretrained features improve a downstream phone recognition task in the Libri-light semi-supervised setting (10 min, 1 h or 10 h of labelled data) reducing the PER by 15\% relative.},
  keywords = {Benchmark testing,contrastive predictive coding,data augmentation,Reverberation,Speech coding,Systematics,Task analysis,Time-domain analysis,Training data,unsupervised representation learning}
}

@misc{khurana_convolutional_2020,
  title = {A {{Convolutional Deep Markov Model}} for {{Unsupervised Speech Representation Learning}}},
  author = {Khurana, Sameer and Laurent, Antoine and Hsu, Wei-Ning and Chorowski, Jan and Lancucki, Adrian and Marxer, Ricard and Glass, James},
  year = {2020},
  month = jun,
  eprint = {2006.02547},
  eprinttype = {arxiv},
  abstract = {Probabilistic Latent Variable Models (LVMs) provide an alternative to self-supervised learning approaches for linguistic representation learning from speech. LVMs admit an intuitive probabilistic interpretation where the latent structure shapes the information extracted from the signal. Even though LVMs have recently seen a renewed interest due to the introduction of Variational Autoencoders (VAEs), their use for speech representation learning remains largely unexplored. In this work, we propose Convolutional Deep Markov Model (ConvDMM), a Gaussian state-space model with non-linear emission and transition functions modelled by deep neural networks. This unsupervised model is trained using black box variational inference. A deep convolutional neural network is used as an inference network for structured variational approximation. When trained on a large scale speech dataset (LibriSpeech), ConvDMM produces features that significantly outperform multiple self-supervised feature extracting methods on linear phone classification and recognition on the Wall Street Journal dataset. Furthermore, we found that ConvDMM complements self-supervised methods like Wav2Vec and PASE, improving on the results achieved with any of the methods alone. Lastly, we find that ConvDMM features enable learning better phone recognizers than any other features in an extreme low-resource regime with few labelled training examples.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@inproceedings{khurana_factorial_2019,
  title = {A {{Factorial Deep Markov Model}} for {{Unsupervised Disentangled Representation Learning}} from {{Speech}}},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Khurana, Sameer and Joty, Shafiq Rayhan and Ali, Ahmed and Glass, James},
  year = {2019},
  month = may,
  pages = {6540--6544},
  publisher = {{IEEE}},
  address = {{Brighton, United Kingdom}},
  abstract = {We present the Factorial Deep Markov Model (FDMM) for representation learning of speech. The FDMM learns disentangled, interpretable and lower dimensional latent representations from speech without supervision. We use a static and dynamic latent variable to exploit the fact that information in a speech signal evolves at different time scales. Latent representations learned by the FDMM outperform a baseline ivector system on speaker verification and dialect identification while also reducing the error rate of a phone recognition system in a domain mismatch scenario.},
  isbn = {978-1-4799-8131-1},
  langid = {english}
}

@article{khurana_magic_2021,
  title = {Magic Dust for Cross-Lingual Adaptation of Monolingual Wav2vec-2.0},
  author = {Khurana, Sameer and Laurent, Antoine and Glass, James},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.03560 [cs, eess]},
  eprint = {2110.03560},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {We propose a simple and effective cross-lingual transfer learning method to adapt monolingual wav2vec-2.0 models for Automatic Speech Recognition (ASR) in resource-scarce languages. We show that a monolingual wav2vec-2.0 is a good few-shot ASR learner in several languages. We improve its performance further via several iterations of Dropout Uncertainty-Driven Self-Training (DUST) by using a moderate-sized unlabeled speech dataset in the target language. A key finding of this work is that the adapted monolingual wav2vec-2.0 achieves similar performance as the topline multilingual XLSR model, which is trained on fifty-three languages, on the target language ASR task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@misc{kim_boosting_2019,
  title = {Boosting {{Vector Calculus}} with the {{Graphical Notation}}},
  author = {Kim, Joon-Hwi and Oh, Maverick S. H. and Kim, Keun-Young},
  year = {2019},
  month = nov,
  eprint = {1911.00892},
  eprinttype = {arxiv},
  abstract = {Learning vector calculus techniques is one of the major missions to be accomplished by physics undergraduates. However, beginners report various difficulties dealing with the index notation due to its bulkiness. Meanwhile, there have been graphical notations for tensor algebra that are intuitive and effective in calculations and can serve as a quick mnemonic for algebraic identities. Although they have been introduced and applied in vector algebra in the educational context, to the best of our knowledge, there have been no publications that employ the graphical notation to three-dimensional Euclidean vector \textbackslash textit\{calculus\}, involving differentiation and integration of vector fields. Aiming for physics students and educators, we introduce such ``graphical vector calculus,'' demonstrate its pedagogical advantages, and provide enough exercises containing both purely mathematical identities and practical calculations in physics. The graphical notation can readily be utilized in the educational environment to not only lower the barriers in learning and practicing vector calculus but also make students interested and self-motivated to manipulate the vector calculus syntax and heuristically comprehend the language of tensors by themselves.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Physics - Physics Education}
}

@misc{kim_convolutional_2014,
  title = {Convolutional {{Neural Networks}} for {{Sentence Classification}}},
  author = {Kim, Yoon},
  year = {2014},
  eprint = {1408.5882},
  eprinttype = {arxiv},
  abstract = {We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vec-tors for sentence-level classification tasks. We show that a simple CNN with lit-tle hyperparameter tuning and static vec-tors achieves excellent results on multi-ple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the ar-chitecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.},
  archiveprefix = {arXiv}
}

@misc{kim_disentangling_2019,
  title = {Disentangling by {{Factorising}}},
  author = {Kim, Hyunjik and Mnih, Andriy},
  year = {2019},
  month = jul,
  eprint = {1802.05983},
  eprinttype = {arxiv},
  abstract = {We define and address the problem of unsupervised learning of disentangled representations on data generated from independent factors of variation. We propose FactorVAE, a method that disentangles by encouraging the distribution of representations to be factorial and hence independent across the dimensions. We show that it improves upon {$\beta$}-VAE by providing a better trade-off between disentanglement and reconstruction quality. Moreover, we highlight the problems of a commonly used disentanglement metric and introduce a new metric that does not suffer from them.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{kim_variational_2019,
  title = {Variational {{Temporal Abstraction}}},
  author = {Kim, Taesup and Ahn, Sungjin and Bengio, Yoshua},
  year = {2019},
  month = oct,
  eprint = {1910.00775},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We introduce a variational approach to learning and inference of temporally hierarchical structure and representation for sequential data. We propose the Variational Temporal Abstraction (VTA), a hierarchical recurrent state space model that can infer the latent temporal structure and thus perform the stochastic state transition hierarchically. We also propose to apply this model to implement the jumpyimagination ability in imagination-augmented agent-learning in order to improve the efficiency of the imagination. In experiments, we demonstrate that our proposed method can model 2D and 3D visual sequence datasets with interpretable temporal structure discovery and that its application to jumpy imagination enables more efficient agent-learning in a 3D navigation task.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{kim_vilt_2021,
  title = {{{ViLT}}: {{Vision-and-Language Transformer Without Convolution}} or {{Region Supervision}}},
  shorttitle = {{{ViLT}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Kim, Wonjae and Son, Bokyung and Kim, Ildoo},
  year = {2021},
  month = jun,
  volume = {139},
  pages = {5583--5594},
  publisher = {{\{PMLR\}}},
  address = {{Virtual}},
  abstract = {Vision-and-Language Pre-training (VLP) has improved performance on various joint vision-and-language downstream tasks. Current approaches to VLP heavily rely on image feature extraction processes, most of which involve region supervision (e.g., object detection) and the convolutional architecture (e.g., ResNet). Although disregarded in the literature, we find it problematic in terms of both (1) efficiency/speed, that simply extracting input features requires much more computation than the multimodal interaction steps; and (2) expressive power, as it is upper bounded to the expressive power of the visual embedder and its predefined visual vocabulary. In this paper, we present a minimal VLP model, Vision-and-Language Transformer (ViLT), monolithic in the sense that the processing of visual inputs is drastically simplified to just the same convolution-free manner that we process textual inputs. We show that ViLT is up to tens of times faster than previous VLP models, yet with competitive or better downstream task performance. Our code and pre-trained weights are available at https://github.com/dandelin/vilt.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{kingma_adam_2015,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  booktitle = {Proceedings of the the 3rd {{International Conference}} for {{Learning Representations}} ({{ICLR}})},
  author = {Kingma, Diederik P. and Ba, Jimmy Lei},
  year = {2015},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  address = {{San Diego, CA, USA}},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  keywords = {★,Moments,Optimization}
}

@inproceedings{kingma_auto-encoding_2014,
  title = {Auto-{{Encoding Variational Bayes}}},
  booktitle = {Proceedings of the 2nd {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Kingma, Diederik P and Welling, Max},
  year = {2014},
  eprint = {1312.6114},
  eprinttype = {arxiv},
  address = {{Banff, AB, Canada}},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  archiveprefix = {arXiv},
  keywords = {★}
}

@inproceedings{kingma_glow_2018,
  title = {Glow: {{Generative Flow}} with {{Invertible}} 1\texttimes 1 {{Convolutions}}},
  booktitle = {Proceedings of the 32nd {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Kingma, Diederik P and Dhariwal, Prafulla},
  year = {2018},
  pages = {10},
  address = {{Montr\'eal, Canada}},
  abstract = {Flow-based generative models (Dinh et al., 2014) are conceptually attractive due to tractability of the exact log-likelihood, tractability of exact latent-variable inference, and parallelizability of both training and synthesis. In this paper we propose Glow, a simple type of generative flow using an invertible 1 \texttimes{} 1 convolution. Using our method we demonstrate a significant improvement in log-likelihood on standard benchmarks. Perhaps most strikingly, we demonstrate that a flow-based generative model optimized towards the plain log-likelihood objective is capable of efficient realistic-looking synthesis and manipulation of large images. The code for our model is available at https://github.com/openai/glow.},
  langid = {english},
  keywords = {⛔ No DOI found}
}

@inproceedings{kingma_improved_2016,
  title = {Improved {{Variational Inference}} with {{Inverse Autoregressive Flow}}},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Kingma, Diederik P. and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max},
  year = {2016},
  series = {{{NIPS}}'16},
  pages = {4743--4751},
  address = {{Barcelona, Spain}},
  abstract = {The framework of normalizing flows provides a general strategy for flexible variational inference of posteriors over latent variables. We propose a new type of normalizing flow, inverse autoregressive flow (IAF), that, in contrast to earlier published flows, scales well to high-dimensional latent spaces. The proposed flow consists of a chain of invertible transformations, where each transformation is based on an autoregressive neural network. In experiments, we show that IAF significantly improves upon diagonal Gaussian approximate posteriors. In addition, we demonstrate that a novel type of variational autoencoder, coupled with IAF, is competitive with neural autoregressive models in terms of attained log-likelihood on natural images, while allowing significantly faster synthesis.},
  isbn = {978-1-5108-3881-9}
}

@article{kingma_introduction_2019,
  title = {An {{Introduction}} to {{Variational Autoencoders}}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2019},
  journal = {Foundations and Trends\textregistered{} in Machine Learning},
  volume = {12},
  number = {4},
  eprint = {1906.02691},
  eprinttype = {arxiv},
  pages = {307--392},
  issn = {1935-8237, 1935-8245},
  abstract = {Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{kingma_semi-supervised_2014,
  title = {Semi-{{Supervised Learning}} with {{Deep Generative Models}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Kingma, Diederik P. and Rezende, Danilo J. and Mohamed, Shakir and Welling, Max},
  year = {2014},
  month = jun,
  eprint = {1406.5298},
  eprinttype = {arxiv},
  address = {{Montr\'eal, Quebec, Canada}},
  abstract = {The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.},
  archiveprefix = {arXiv}
}

@misc{kingma_variational_2015,
  title = {Variational {{Dropout}} and the {{Local Reparameterization Trick}}},
  author = {Kingma, Diederik P. and Salimans, Tim and Welling, Max},
  year = {2015},
  month = jun,
  eprint = {1506.02557},
  eprinttype = {arxiv},
  abstract = {We investigate a local reparameterizaton technique for greatly reducing the variance of stochastic gradients for variational Bayesian inference (SGVB) of a posterior over model parameters, while retaining parallelizability. This local reparameterization translates uncertainty about global parameters into local noise that is independent across datapoints in the minibatch. Such parameterizations can be trivially parallelized and have variance that is inversely proportional to the minibatch size, generally leading to much faster convergence. Additionally, we explore a connection with dropout: Gaussian dropout objectives correspond to SGVB with local reparameterization, a scale-invariant prior and proportionally fixed posterior variance. Our method allows inference of more flexibly parameterized posteriors; specifically, we propose variational dropout, a generalization of Gaussian dropout where the dropout rates are learned, often leading to better models. The method is demonstrated through several experiments.},
  archiveprefix = {arXiv}
}

@misc{kingma_variational_2021,
  title = {Variational {{Diffusion Models}}},
  author = {Kingma, Diederik P. and Salimans, Tim and Poole, Ben and Ho, Jonathan},
  year = {2021},
  month = jul,
  eprint = {2107.00630},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Diffusion-based generative models have demonstrated a capacity for perceptually impressive synthesis, but can they also be great likelihood-based models? We answer this in the affirmative, and introduce a family of diffusion-based generative models that obtain state-of-the-art likelihoods on standard image density estimation benchmarks. Unlike other diffusion-based models, our method allows for efficient optimization of the noise schedule jointly with the rest of the model. We show that the variational lower bound (VLB) simplifies to a remarkably short expression in terms of the signal-to-noise ratio of the diffused data, thereby improving our theoretical understanding of this model class. Using this insight, we prove an equivalence between several models proposed in the literature. In addition, we show that the continuous-time VLB is invariant to the noise schedule, except for the signal-to-noise ratio at its endpoints. This enables us to learn a noise schedule that minimizes the variance of the resulting VLB estimator, leading to faster optimization. Combining these advances with architectural improvements, we obtain state-of-the-art likelihoods on image density estimation benchmarks, outperforming autoregressive models that have dominated these benchmarks for many years, with often significantly faster optimization. In addition, we show how to turn the model into a bits-back compression scheme, and demonstrate lossless compression rates close to the theoretical optimum.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{kipf_variational_2016,
  title = {Variational {{Graph Auto-Encoders}}},
  author = {Kipf, Thomas N. and Welling, Max},
  year = {2016},
  month = nov,
  eprint = {1611.07308},
  eprinttype = {arxiv},
  abstract = {We introduce the variational graph auto-encoder (VGAE), a framework for unsupervised learning on graph-structured data based on the variational auto-encoder (VAE). This model makes use of latent variables and is capable of learning interpretable latent representations for undirected graphs. We demonstrate this model using a graph convolutional network (GCN) encoder and a simple inner product decoder. Our model achieves competitive results on a link prediction task in citation networks. In contrast to most existing models for unsupervised learning on graph-structured data and link prediction, our model can naturally incorporate node features, which significantly improves predictive performance on a number of benchmark datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{kirichenko_why_2020,
  title = {Why {{Normalizing Flows Fail}} to {{Detect Out-of-Distribution Data}}},
  author = {Kirichenko, Polina and Izmailov, Pavel and Wilson, Andrew Gordon},
  year = {2020},
  month = jun,
  eprint = {2006.08545},
  eprinttype = {arxiv},
  abstract = {Detecting out-of-distribution (OOD) data is crucial for robust machine learning systems. Normalizing flows are flexible deep generative models that often surprisingly fail to distinguish between in- and out-of-distribution data: a flow trained on pictures of clothing assigns higher likelihood to handwritten digits. We investigate why normalizing flows perform poorly for OOD detection. We demonstrate that flows learn local pixel correlations and generic image-to-latentspace transformations which are not specific to the target image dataset. We show that by modifying the architecture of flow coupling layers we can bias the flow towards learning the semantic structure of the target data, improving OOD detection. Our investigation reveals that properties that enable flows to generate high-fidelity images can have a detrimental effect on OOD detection.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{kitaev_reformer_2020,
  title = {Reformer: {{The Efficient Transformer}}},
  author = {Kitaev, Nikita and Kaiser, L. and Levskaya, Anselm},
  year = {2020},
  eprint = {2001.04451},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@misc{klambauer_self-normalizing_2017,
  title = {Self-{{Normalizing Neural Networks}}},
  author = {Klambauer, G{\"u}nter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
  year = {2017},
  eprint = {1706.02515},
  eprinttype = {arxiv},
  abstract = {Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are "scaled exponential linear units" (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance -- even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs and other machine learning methods such as random forests and support vector machines. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: github.com/bioinf-jku/SNNs.},
  archiveprefix = {arXiv}
}

@article{ko_audio_nodate,
  title = {Audio {{Augmentation}} for {{Speech Recognition}}},
  author = {Ko, Tom and Peddinti, Vijayaditya and Povey, Daniel and Khudanpur, Sanjeev},
  pages = {4},
  abstract = {Data augmentation is a common strategy adopted to increase the quantity of training data, avoid overfitting and improve robustness of the models. In this paper, we investigate audio-level speech augmentation methods which directly process the raw signal. The method we particularly recommend is to change the speed of the audio signal, producing 3 versions of the original signal with speed factors of 0.9, 1.0 and 1.1. The proposed technique has a low implementation cost, making it easy to adopt. We present results on 4 different LVCSR tasks with training data ranging from 100 hours to 1000 hours, to examine the effectiveness of audio augmentation in a variety of data scenarios. An average relative improvement of 4.3\% was observed across the 4 tasks.},
  langid = {english},
  keywords = {Data augmentation,Speech augmentation}
}

@article{koenecke_racial_2020,
  title = {Racial Disparities in Automated Speech Recognition},
  author = {Koenecke, Allison and Nam, Andrew and Lake, Emily and Nudell, Joe and Quartey, Minnie and Mengesha, Zion and Toups, Connor and Rickford, John R. and Jurafsky, Dan and Goel, Sharad},
  year = {2020},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {117},
  number = {14},
  pages = {7684--7689}
}

@article{kolmogorov_tables_1963,
  title = {On Tables of Random Numbers},
  author = {Kolmogorov, A. N.},
  year = {1963},
  journal = {Sankhy\=a: The Indian Journal of Statistics, Series A (1961-2002)},
  volume = {25},
  number = {4},
  pages = {369--376},
  issn = {0581572X}
}

@article{komarek_making_2005,
  title = {Making {{Logistic Regression A Core Data Mining Tool}}},
  author = {Komarek, Paul and Moore, Andrew},
  year = {2005},
  abstract = {Binary classification is a core data mining task. For large datasets or real-time applications, desirable classifiers are accurate, fast, and automatic (i.e. no parameter tuning). Naive Bayes and decision trees are fast and parameter-free, but their accuracy is often below state-of-the-art. Linear support vector machines (SVM) are fast and have good accuracy, but current implementations are sensitive to the capacity parameter. SVMs with radial basis function kernels are accurate but slow, and have multiple parameters that require tuning. In this paper we demonstrate that a very simple parameter-free implementation of logistic regression (LR) is suffi-ciently accurate and fast to compete with state-of-the-art binary classifiers on large real-world datasets. The accuracy is comparable to per-dataset tuned linear SVMs and, in higher dimensions, to tuned RBF SVMs. A combination of reg-ularization, truncated-Newton methods, and iteratively re-weighted least squares make this implementation faster than SVMs and relatively insensitive to parameters. Our fitting procedure, TR-IRLS, appears to outperform several common LR fitting procedures in our experiments. TR-IRLS is robust to linear dependencies and scaling problems in the data, and no data preprocessing is necessary. TR-IRLS is easy to implement and can be used anywhere that IRLS is used. Convergence guarantees can be stated for generalized linear models with canonical links.}
}

@misc{kopp_differentiable_2016,
  title = {A {{Differentiable Transition Between Additive}} and {{Multiplicative Neurons}}},
  author = {K{\"o}pp, Wiebke and {van der Smagt}, Patrick and Urban, Sebastian},
  year = {2016},
  month = apr,
  eprint = {1604.03736},
  eprinttype = {arxiv},
  abstract = {Existing approaches to combine both additive and multiplicative neural units either use a fixed assignment of operations or require discrete optimization to determine what function a neuron should perform. However, this leads to an extensive increase in the computational complexity of the training procedure. We present a novel, parameterizable transfer function based on the mathematical concept of non-integer functional iteration that allows the operation each neuron performs to be smoothly and, most importantly, differentiablely adjusted between addition and multiplication. This allows the decision between addition and multiplication to be integrated into the standard backpropagation training procedure.},
  archiveprefix = {arXiv}
}

@inproceedings{kornblith_similarity_2019,
  title = {Similarity of {{Neural Network Representations Revisited}}},
  booktitle = {36th {{International Conference}} on {{Machine Learning}} ({{ICML}} 2019)},
  author = {Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
  year = {2019},
  pages = {20},
  address = {{Long Beach, CA, USA}},
  abstract = {Recent work has sought to understand the behavior of neural networks by comparing representations between layers and between different trained models. We examine methods for comparing neural network representations based on canonical correlation analysis (CCA). We show that CCA belongs to a family of statistics for measuring multivariate similarity, but that neither CCA nor any other statistic that is invariant to invertible linear transformation can measure meaningful similarities between representations of higher dimension than the number of data points. We introduce a similarity index that measures the relationship between representational similarity matrices and does not suffer from this limitation. This similarity index is equivalent to centered kernel alignment (CKA) and is also closely connected to CCA. Unlike CCA, CKA can reliably identify correspondences between representations in networks trained from different initializations.},
  langid = {english}
}

@misc{kosiorek_adam_what_2018,
  title = {What Is Wrong with {{VAEs}}?},
  author = {Kosiorek, Adam},
  year = {2018},
  month = mar,
  howpublished = {http://akosiorek.github.io/ml/2018/03/14/what\_is\_wrong\_with\_vaes.html}
}

@misc{kosiorek_attention_2017,
  title = {Attention in {{Neural Networks}} and {{How}} to {{Use It}}},
  author = {Kosiorek, Adam},
  year = {2017},
  abstract = {Attention mechanisms in neural networks, otherwise known as neural attention or just attention, have recently attracted a lot of attention (pun intended). In this post, I will try to find a common denominator for different mechanisms and use-cases and I will describe (and implement!) two mechanisms of soft visual attention.},
  howpublished = {http://akosiorek.github.io/ml/2017/10/14/visual-attention.html}
}

@misc{koutnik_clockwork_2014,
  title = {A {{Clockwork RNN}}},
  author = {Koutn{\'i}k, Jan and Greff, Klaus and Gomez, Faustino and Schmidhuber, J{\"u}rgen},
  year = {2014},
  month = feb,
  eprint = {1402.3511},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Sequence prediction and classification are ubiquitous and challenging problems in machine learning that can require identifying complex dependencies between temporally distant inputs. Recurrent Neural Networks (RNNs) have the ability, in theory, to cope with these temporal dependencies by virtue of the short-term memory implemented by their recurrent (feedback) connections. However, in practice they are difficult to train successfully when the long-term memory is required. This paper introduces a simple, yet powerful modification to the standard RNN architecture, the Clockwork RNN (CW-RNN), in which the hidden layer is partitioned into separate modules, each processing inputs at its own temporal granularity, making computations only at its prescribed clock rate. Rather than making the standard RNN models more complex, CW-RNN reduces the number of RNN parameters, improves the performance significantly in the tasks tested, and speeds up the network evaluation. The network is demonstrated in preliminary experiments involving two tasks: audio signal generation and TIMIT spoken word classification, where it outperforms both RNN and LSTM networks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@inproceedings{koutnik_wavelet-based_2016,
  title = {A {{Wavelet-based Encoding}} for {{Neuroevolution}}},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference}} ({{GECCO}})},
  author = {Koutn{\'i}k, Jan and Driessens, Kurt},
  year = {2016},
  pages = {517--524},
  address = {{Denver, CO, USA}},
  abstract = {A new indirect scheme for encoding neural network connec-tion weights as sets of wavelet-domain coefficients is pro-posed in this paper. It exploits spatial regularities in the weight-space to reduce the gene-space dimension by con-sidering the low-frequency wavelet coefficients only. The wavelet-based encoding builds on top of a frequency-domain encoding, but unlike when using a Fourier-type transform, it offers gene locality while preserving continuity of the genotype-phenotype mapping. We argue that this added property al-lows for more efficient evolutionary search and demonstrate this on the octopus-arm control task, where superior solu-tions were found in fewer generations. The scalability of the wavelet-based encoding is shown by evolving networks with many parameters to control game-playing agents in the Arcade Learning Environment.},
  isbn = {978-1-4503-4206-3},
  keywords = {Indirect encoding,Wavelets}
}

@misc{krahenbuhl_data-dependent_2016,
  title = {Data-Dependent {{Initializations}} of {{Convolutional Neural Networks}}},
  author = {Kr{\"a}henb{\"u}hl, Philipp and Doersch, Carl and Donahue, Jeff and Darrell, Trevor},
  year = {2016},
  month = sep,
  eprint = {1511.06856},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Convolutional Neural Networks spread through computer vision like a wildfire, impacting almost all visual tasks imaginable. Despite this, few researchers dare to train their models from scratch. Most work builds on one of a handful of ImageNet pre-trained models, and fine-tunes or adapts these for specific tasks. This is in large part due to the difficulty of properly initializing these networks from scratch. A small miscalibration of the initial weights leads to vanishing or exploding gradients, as well as poor convergence properties. In this work we present a fast and simple data-dependent initialization procedure, that sets the weights of a network such that all units in the network train at roughly the same rate, avoiding vanishing or exploding gradients. Our initialization matches the current state-of-the-art unsupervised or self-supervised pre-training methods on standard computer vision tasks, such as image classification and object detection, while reducing the pre-training time by three orders of magnitude. When combined with pre-training methods, our initialization significantly outperforms prior work, narrowing the gap between supervised and unsupervised pre-training.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@misc{krishnan_deep_2015,
  title = {Deep {{Kalman Filters}}},
  author = {Krishnan, Rahul G. and Shalit, Uri and Sontag, David},
  year = {2015},
  month = nov,
  eprint = {1511.05121},
  eprinttype = {arxiv},
  abstract = {Kalman Filters are one of the most influential models of time-varying phenomena. They admit an intuitive probabilistic interpretation, have a simple functional form, and enjoy widespread adoption in a variety of disciplines. Motivated by recent variational methods for learning deep generative models, we introduce a unified algorithm to efficiently learn a broad spectrum of Kalman filters. Of particular interest is the use of temporal generative models for counterfactual inference. We investigate the efficacy of such models for counterfactual inference, and to that end we introduce the ``Healing MNIST'' dataset where long-term structure, noise and actions are applied to sequences of digits. We show the efficacy of our method for modeling this dataset. We further show how our model can be used for counterfactual inference for patients, based on electronic health record data of 8,000 patients over 4.5 years.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{krishnan_structured_2017,
  title = {Structured Inference Networks for Nonlinear State Space Models},
  booktitle = {Proceedings of the Thirty-First {{AAAI}} Conference on Artificial Intelligence},
  author = {Krishnan, Rahul G. and Shalit, Uri and Sontag, David},
  year = {2017},
  series = {{{AAAI}}'17},
  pages = {2101--2109},
  publisher = {{AAAI Press}},
  address = {{San Francisco, California, USA}},
  abstract = {Gaussian state space models have been used for decades as generative models of sequential data. They admit an intuitive probabilistic interpretation, have a simple functional form, and enjoy widespread adoption. We introduce a unified algorithm to efficiently learn a broad class of linear and non-linear state space models, including variants where the emission and transition distributions are modeled by deep neural networks. Our learning algorithm simultaneously learns a compiled inference network and the generative model, leveraging a structured variational approximation parameterized by recurrent neural networks to mimic the posterior distribution. We apply the learning algorithm to both synthetic and real-world datasets, demonstrating its scalability and versatility. We find that using the structured approximation to the posterior results in models with significantly higher held-out likelihood.}
}

@article{krizhevsky_imagenet_2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  year = {2012},
  journal = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
  eprint = {1102.0183},
  eprinttype = {arxiv},
  pages = {1--9},
  issn = {10495258},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  archiveprefix = {arXiv},
  isbn = {9781627480031},
  pmid = {7491034}
}

@phdthesis{krizhevsky_learning_2009,
  title = {Learning {{Multiple Layers}} of {{Features}} from {{Tiny Images}}},
  author = {Krizhevsky, Alex},
  year = {2009},
  eprint = {1011.1669v3},
  eprinttype = {arxiv},
  issn = {1098-6596},
  abstract = {Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it difficult to learn a good set of filters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is significantly improved by pre-training a layer of features on a large set of unlabeled tiny images.},
  archiveprefix = {arXiv},
  isbn = {9788578110796},
  pmid = {25246403},
  school = {University of Toronto}
}

@article{krizhevsky_one_2014,
  title = {One Weird Trick for Parallelizing Convolutional Neural Networks},
  author = {Krizhevsky, Alex},
  year = {2014},
  abstract = {I present a new way to parallelize the training of convolutional neural networks across multiple GPUs. The method scales significantly better than all alternatives when applied to modern convolutional neural networks.}
}

@article{kuleshov_calibrated_2015,
  title = {Calibrated {{Structured Prediction}}},
  author = {Kuleshov, Volodymyr and Liang, Percy S},
  year = {2015},
  journal = {Advances in Neural Information Processing Systems 28}
}

@article{kullback_information_1951,
  title = {On {{Information}} and {{Sufficiency}}},
  author = {Kullback, S. and Leibler, R. A.},
  year = {1951},
  journal = {The Annals of Mathematical Statistics},
  volume = {22},
  number = {1},
  eprint = {1511.00860},
  eprinttype = {arxiv},
  pages = {79--86},
  issn = {0003-4851},
  abstract = {This note generalizes to the abstract case Shannon's definition of information.},
  archiveprefix = {arXiv},
  isbn = {00034851},
  pmid = {10896709}
}

@inproceedings{kumar_ask_2016,
  title = {Ask Me Anything: {{Dynamic}} Memory Networks for Natural Language Processing},
  booktitle = {Proceedings of the 33rd {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Kumar, Ankit and Irsoy, Ozan and Ondruska, Peter and Iyyer, Mohit and Bradbury, James and Gulrajani, Ishaan and Zhong, Victor and Paulus, Romain and Socher, Richard},
  editor = {Balcan, Maria Florina and Weinberger, Kilian Q.},
  year = {2016},
  month = jun,
  series = {Proceedings of Machine Learning Research},
  volume = {48},
  pages = {1378--1387},
  publisher = {{PMLR}},
  address = {{New York, New York, USA}},
  abstract = {Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a neural network architecture which processes input sequences and questions, forms episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the inputs and the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The DMN can be trained end-to-end and obtains state-of-the-art results on several types of tasks and datasets: question answering (Facebook's bAbI dataset), text classification for sentiment analysis (Stanford Sentiment Treebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). The training for these different tasks relies exclusively on trained word vector representations and input-question-answer triplets.}
}

@misc{kurata_language_2017,
  title = {Language {{Modeling}} with {{Highway LSTM}}},
  author = {Kurata, Gakuto and Ramabhadran, Bhuvana and Saon, George and Sethy, Abhinav},
  year = {2017},
  month = sep,
  eprint = {1709.06436},
  eprinttype = {arxiv},
  abstract = {Language models (LMs) based on Long Short Term Memory (LSTM) have shown good gains in many automatic speech recognition tasks. In this paper, we extend an LSTM by adding highway networks inside an LSTM and use the resulting Highway LSTM (HW-LSTM) model for language modeling. The added highway networks increase the depth in the time dimension. Since a typical LSTM has two internal states, a memory cell and a hidden state, we compare various types of HW-LSTM by adding highway networks onto the memory cell and/or the hidden state. Experimental results on English broadcast news and conversational telephone speech recognition show that the proposed HW-LSTM LM improves speech recognition accuracy on top of a strong LSTM LM baseline. We report 5.1\% and 9.9\% on the Switchboard and CallHome subsets of the Hub5 2000 evaluation, which reaches the best performance numbers reported on these tasks to date.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language}
}

@misc{kuzina_diagnosing_2021,
  title = {Diagnosing {{Vulnerability}} of {{Variational Auto-Encoders}} to {{Adversarial Attacks}}},
  author = {Kuzina, Anna and Welling, Max and Tomczak, Jakub M.},
  year = {2021},
  month = mar,
  eprint = {2103.06701},
  eprinttype = {arxiv},
  abstract = {In this work, we explore adversarial attacks on the Variational Autoencoders (VAE). We show how to modify data point to obtain a prescribed latent code (supervised attack) or just get a drastically different code (unsupervised attack). We examine the influence of model modifications ({$\beta$}-VAE, NVAE) on the robustness of VAEs and suggest metrics to quantify it.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{lai_race:_2017,
  title = {{{RACE}}: {{Large-scale ReAding Comprehension Dataset From Examinations}}},
  shorttitle = {{{RACE}}},
  author = {Lai, Guokun and Xie, Qizhe and Liu, Hanxiao and Yang, Yiming and Hovy, Eduard},
  year = {2017},
  month = apr,
  eprint = {1704.04683},
  eprinttype = {arxiv},
  abstract = {We present RACE, a new dataset for benchmark evaluation of methods in the reading comprehension task. Collected from the English exams for middle and high school Chinese students in the age range between 12 to 18, RACE consists of near 28,000 passages and near 100,000 questions generated by human experts (English instructors), and covers a variety of topics which are carefully designed for evaluating the students' ability in understanding and reasoning. In particular, the proportion of questions that requires reasoning is much larger in RACE than that in other benchmark datasets for reading comprehension, and there is a significant gap between the performance of the state-of-the-art models (43\%) and the ceiling human performance (95\%). We hope this new dataset can serve as a valuable resource for research and evaluation in machine comprehension. The dataset is freely available at http://www.cs.cmu.edu/\textasciitilde glai1/data/race/ and the code is available at https://github.com/qizhex/RACE\_AR\_baselines.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@inproceedings{lai_re-examination_2019,
  title = {Re-Examination of the Role of Latent Variables in Sequence Modeling},
  booktitle = {Proceedings of the 32nd {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Lai, Guokun and Dai, Zihang and Yang, Yiming and Yoo, Shinjae},
  year = {2019},
  address = {{Vancouver, BC, Canada}}
}

@misc{lai_stochastic_2018,
  title = {Stochastic {{WaveNet}}: {{A Generative Latent Variable Model}} for {{Sequential Data}}},
  shorttitle = {Stochastic {{WaveNet}}},
  author = {Lai, Guokun and Li, Bohan and Zheng, Guoqing and Yang, Yiming},
  year = {2018},
  month = jun,
  eprint = {1806.06116},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {How to model distribution of sequential data, including but not limited to speech and human motions, is an important ongoing research problem. It has been demonstrated that model capacity can be significantly enhanced by introducing stochastic latent variables in the hidden states of recurrent neural networks. Simultaneously, WaveNet, equipped with dilated convolutions, achieves astonishing empirical performance in natural speech generation task. In this paper, we combine the ideas from both stochastic latent variables and dilated convolutions, and propose a new architecture to model sequential data, termed as Stochastic WaveNet, where stochastic latent variables are injected into the WaveNet structure. We argue that Stochastic WaveNet enjoys powerful distribution modeling capacity and the advantage of parallel training from dilated convolutions. In order to efficiently infer the posterior distribution of the latent variables, a novel inference network structure is designed based on the characteristics of WaveNet architecture. State-of-the-art performances on benchmark datasets are obtained by Stochastic WaveNet on natural speech modeling and high quality human handwriting samples can be generated as well.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{lake_human-level_2015,
  title = {Human-Level Concept Learning through Probabilistic Program Induction},
  author = {Lake, Brenden M. and Salakhutdinov, Ruslan and Tenenbaum, Joshua B.},
  year = {2015},
  journal = {Science},
  volume = {350},
  number = {6266},
  pages = {1332--1338},
  issn = {0036-8075},
  abstract = {Not only do children learn effortlessly, they do so quickly and with a remarkable ability to use what they have learned as the raw material for creating new stuff. Lake et al. describe a computational model that learns in a similar fashion and does so better than current deep learning algorithms. The model classifies, parses, and recreates handwritten characters, and can generate new letters of the alphabet that look ``right'' as judged by Turing-like tests of the model's output in comparison to what real humans produce.Science, this issue p. 1332People learning new concepts can often generalize successfully from just a single example, yet machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy. People can also use learned concepts in richer ways than conventional algorithms\textemdash for action, imagination, and explanation. We present a computational model that captures these human learning abilities for a large class of simple visual concepts: handwritten characters from the world's alphabets. The model represents concepts as simple programs that best explain observed examples under a Bayesian criterion. On a challenging one-shot classification task, the model achieves human-level performance while outperforming recent deep learning approaches. We also present several ``visual Turing tests'' probing the model's creative generalization abilities, which in many cases are indistinguishable from human behavior.}
}

@inproceedings{lakshminarayanan_simple_2017,
  title = {Simple and {{Scalable Predictive Uncertainty Estimation}} Using {{Deep Ensembles}}},
  booktitle = {In {{Proceddings}} of the 31st {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
  year = {2017},
  address = {{Long Beach, CA, USA}},
  abstract = {Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{lamb_professor_2016,
  title = {Professor {{Forcing}}: {{A New Algorithm}} for {{Training Recurrent Networks}}},
  shorttitle = {Professor {{Forcing}}},
  author = {Lamb, Alex and Goyal, Anirudh and Zhang, Ying and Zhang, Saizheng and Courville, Aaron and Bengio, Yoshua},
  year = {2016},
  month = oct,
  langid = {english},
  keywords = {Scheduled sampling}
}

@inproceedings{lample_large_2019,
  title = {Large Memory Layers with Product Keys},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32: {{Annual Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Lample, Guillaume and Sablayrolles, Alexandre and Ranzato, Marc'Aurelio and Denoyer, Ludovic and J{\'e}gou, Herv{\'e}},
  year = {2019},
  pages = {8546--8557},
  address = {{Vancouver, Canada}},
  keywords = {Memory}
}

@misc{lample_phrase-based_2018,
  title = {Phrase-{{Based}} \& {{Neural Unsupervised Machine Translation}}},
  author = {Lample, Guillaume and Ott, Myle and Conneau, Alexis and Denoyer, Ludovic and Ranzato, Marc'Aurelio},
  year = {2018},
  eprint = {1804.07755},
  eprinttype = {arxiv},
  abstract = {Machine translation systems achieve near human-level performance on some languages, yet their effectiveness strongly relies on the availability of large amounts of bitexts, which hinders their applicability to the majority of language pairs. This work investigates how to learn to translate when having access to only large monolingual corpora in each language. We propose two model variants, a neural and a phrase-based model. Both versions leverage automatic generation of parallel data by backtranslating with a backward model operating in the other direction, and the denoising effect of a language model trained on the target side. These models are significantly better than methods from the literature, while being simpler and having fewer hyper-parameters. On the widely used WMT14 English-French and WMT16 German-English benchmarks, our models respectively obtain 27.1 and 23.6 BLEU points without using a single parallel sentence, outperforming the state of the art by more than 11 BLEU points.},
  archiveprefix = {arXiv}
}

@inproceedings{larochelle_classification_2008,
  title = {Classification Using Discriminative Restricted {{Boltzmann}} Machines},
  booktitle = {Proceedings of the 25th International Conference on {{Machine}} Learning - {{ICML}} '08},
  author = {Larochelle, Hugo and Bengio, Yoshua},
  year = {2008},
  pages = {536--543},
  publisher = {{ACM Press}},
  address = {{Helsinki, Finland}},
  abstract = {Recently, many applications for Restricted Boltzmann Machines (RBMs) have been developed for a large variety of learning problems. However, RBMs are usually used as feature extractors for another learning algorithm or to provide a good initialization for deep feed-forward neural network classifiers, and are not considered as a standalone solution to classification problems. In this paper, we argue that RBMs provide a self-contained framework for deriving competitive non-linear classifiers. We present an evaluation of different learning algorithms for RBMs which aim at introducing a discriminative component to RBM training and improve their performance as classifiers. This approach is simple in that RBMs are used directly to build a classifier, rather than as a stepping stone. Finally, we demonstrate how discriminative RBMs can also be successfully employed in a semi-supervised setting.},
  isbn = {978-1-60558-205-4},
  langid = {english}
}

@inproceedings{larochelle_neural_2011,
  title = {The {{Neural Autoregressive Distribution Estimator}}},
  booktitle = {Proceedings of the 14th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}} ({{AISTATS}})},
  author = {Larochelle, Hugo and Murray, Iain},
  year = {2011},
  volume = {15},
  pages = {9},
  publisher = {{Journal of Machine Learning}},
  address = {{Fort Lauderdale, FL, USA}},
  abstract = {We describe a new approach for modeling the distribution of high-dimensional vectors of discrete variables. This model is inspired by the restricted Boltzmann machine (RBM), which has been shown to be a powerful model of such distributions. However, an RBM typically does not provide a tractable distribution estimator, since evaluating the probability it assigns to some given observation requires the computation of the so-called partition function, which itself is intractable for RBMs of even moderate size. Our model circumvents this difficulty by decomposing the joint distribution of observations into tractable conditional distributions and modeling each conditional using a non-linear function similar to a conditional of an RBM. Our model can also be interpreted as an autoencoder wired such that its output can be used to assign valid probabilities to observations. We show that this new model outperforms other multivariate binary distribution estimators on several datasets and performs similarly to a large (but intractable) RBM.},
  langid = {english},
  keywords = {⛔ No DOI found}
}

@misc{larsen_autoencoding_2016,
  title = {Autoencoding {{Beyond Pixels Using}} a {{Learned Similarity Metric}}},
  author = {Larsen, Anders Boesen Lindbo and S{\o}nderby, S{\o}ren Kaae and Larochelle, Hugo and Winther, Ole},
  year = {2016},
  month = feb,
  eprint = {1512.09300},
  eprinttype = {arxiv},
  abstract = {We present an autoencoder that leverages learned representations to better measure similarities in data space. By combining a variational autoencoder with a generative adversarial network we can use learned feature representations in the GAN discriminator as basis for the VAE reconstruction objective. Thereby, we replace element-wise errors with feature-wise errors to better capture the data distribution while offering invariance towards e.g. translation. We apply our method to images of faces and show that it outperforms VAEs with element-wise similarity measures in terms of visual fidelity. Moreover, we show that the method learns an embedding in which high-level abstract visual features (e.g. wearing glasses) can be modified using simple arithmetic.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{latif_variational_2020,
  title = {Variational {{Autoencoders}} for {{Learning Latent Representations}} of {{Speech Emotion}}: {{A Preliminary Study}}},
  shorttitle = {Variational {{Autoencoders}} for {{Learning Latent Representations}} of {{Speech Emotion}}},
  author = {Latif, Siddique and Rana, Rajib and Qadir, Junaid and Epps, Julien},
  year = {2020},
  month = jul,
  journal = {arXiv:1712.08708 [cs, eess, stat]},
  eprint = {1712.08708},
  eprinttype = {arxiv},
  primaryclass = {cs, eess, stat},
  abstract = {Learning the latent representation of data in unsupervised fashion is a very interesting process that provides relevant features for enhancing the performance of a classifier. For speech emotion recognition tasks, generating effective features is crucial. Currently, handcrafted features are mostly used for speech emotion recognition, however, features learned automatically using deep learning have shown strong success in many problems, especially in image processing. In particular, deep generative models such as Variational Autoencoders (VAEs) have gained enormous success for generating features for natural images. Inspired by this, we propose VAEs for deriving the latent representation of speech signals and use this representation to classify emotions. To the best of our knowledge, we are the first to propose VAEs for speech emotion classification. Evaluations on the IEMOCAP dataset demonstrate that features learned by VAEs can produce state-of-the-art results for speech emotion classification.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning}
}

@article{lawrence_gaussian_2004,
  title = {Gaussian {{Process Latent Variable Models}} for {{Visualisation}} of {{High Dimensional Data}}},
  author = {Lawrence, Neil D.},
  year = {2004},
  journal = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
  abstract = {In this paper we introduce a new underlying probabilistic model for prin-cipal component analysis (PCA). Our formulation interprets PCA as a particular Gaussian process prior on a mapping from a latent space to the observed data-space. We show that if the prior's covariance func-tion constrains the mappings to be linear the model is equivalent to PCA, we then extend the model by considering less restrictive covariance functions which allow non-linear mappings. This more general Gaussian pro-cess latent variable model (GPLVM) is then evaluated as an approach to the visualisation of high dimensional data for three different data-sets. Additionally our non-linear algorithm can be further kernelised leading to 'twin kernel PCA' in which a mapping between feature spaces occurs.}
}

@inproceedings{le_auto-encoding_2018,
  title = {Auto-{{Encoding Sequential Monte Carlo}}},
  booktitle = {6th {{International Conference}} on {{Learning Representations}} ({{ICLR}} 2018)},
  author = {Le, Tuan Anh and Igl, Maximilian and Rainforth, Tom and Jin, Tom and Wood, Frank},
  year = {2018},
  publisher = {{OpenReview.net}},
  address = {{Vancouver, Canada}}
}

@inproceedings{le_building_2013,
  title = {Building High-Level Features Using Large Scale Unsupervised Learning},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Le, Quoc V.},
  year = {2013},
  pages = {8595--8598}
}

@inproceedings{le_distributed_2014,
  title = {Distributed {{Representations}} of {{Sentences}} and {{Documents}}.},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Le, Quoc V. and Mikolov, Tomas},
  year = {2014},
  volume = {14},
  pages = {1188--1196}
}

@misc{le_non-autoregressive_2020,
  title = {Non-{{Autoregressive Dialog State Tracking}}},
  author = {Le, Hung and Socher, Richard and Hoi, Steven C. H.},
  year = {2020},
  month = feb,
  eprint = {2002.08024},
  eprinttype = {arxiv},
  abstract = {Recent efforts in Dialogue State Tracking (DST) for task-oriented dialogues have progressed toward open-vocabulary or generation-based approaches where the models can generate slot value candidates from the dialogue history itself. These approaches have shown good performance gain, especially in complicated dialogue domains with dynamic slot values. However, they fall short in two aspects: (1) they do not allow models to explicitly learn signals across domains and slots to detect potential dependencies among (domain, slot) pairs; and (2) existing models follow auto-regressive approaches which incur high time cost when the dialogue evolves over multiple domains and multiple turns. In this paper, we propose a novel framework of Non-Autoregressive Dialog State Tracking (NADST) which can factor in potential dependencies among domains and slots to optimize the models towards better prediction of dialogue states as a complete set rather than separate slots. In particular, the non-autoregressive nature of our method not only enables decoding in parallel to significantly reduce the latency of DST for realtime dialogue response generation, but also detect dependencies among slots at token level in addition to slot and domain level. Our empirical results show that our model achieves the state-of-the-art joint accuracy across all domains on the MultiWOZ 2.1 corpus, and the latency of our model is an order of magnitude lower than the previous state of the art as the dialogue history extends over time.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@article{lecun_backpropagation_1989,
  title = {Backpropagation {{Applied}} to {{Handwritten Zip Code Recognition}}},
  author = {LeCun, Yann A. and Boser, B. and Denker, John S. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
  year = {1989},
  journal = {Neural Computation},
  number = {1},
  pages = {541--551},
  abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.}
}

@article{lecun_efficient_2012,
  title = {Efficient Backprop},
  author = {LeCun, Yann A. and Bottou, L{\'e}on and Orr, Genevieve B. and M{\"u}ller, Klaus Robert},
  year = {2012},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  volume = {7700 LECTU},
  eprint = {gr-qc/9809069v1},
  eprinttype = {arxiv},
  pages = {9--48},
  issn = {03029743},
  abstract = {The convergence of back-propagation learning is analyzed so as to explain common phenomenon observedb y practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposedin serious technical publications. This paper gives some of those tricks, ando.ers explanations of why they work. Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most ``classical'' second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations.},
  archiveprefix = {arXiv},
  isbn = {9783642352881},
  pmid = {15003161}
}

@article{lecun_gradient-based_1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {LeCun, Yann A. and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  year = {1998},
  journal = {Proceedings of the IEEE},
  volume = {86},
  number = {11},
  pages = {2278--2323},
  abstract = {Multilayer neural networks trained with the back-propagation\textbackslash nalgorithm constitute the best example of a successful gradient based\textbackslash nlearning technique. Given an appropriate network architecture,\textbackslash ngradient-based learning algorithms can be used to synthesize a complex\textbackslash ndecision surface that can classify high-dimensional patterns, such as\textbackslash nhandwritten characters, with minimal preprocessing. This paper reviews\textbackslash nvarious methods applied to handwritten character recognition and\textbackslash ncompares them on a standard handwritten digit recognition task.\textbackslash nConvolutional neural networks, which are specifically designed to deal\textbackslash nwith the variability of 2D shapes, are shown to outperform all other\textbackslash ntechniques. Real-life document recognition systems are composed of\textbackslash nmultiple modules including field extraction, segmentation recognition,\textbackslash nand language modeling. A new learning paradigm, called graph transformer\textbackslash nnetworks (GTN), allows such multimodule systems to be trained globally\textbackslash nusing gradient-based methods so as to minimize an overall performance\textbackslash nmeasure. Two systems for online handwriting recognition are described.\textbackslash nExperiments demonstrate the advantage of global training, and the\textbackslash nflexibility of graph transformer networks. A graph transformer network\textbackslash nfor reading a bank cheque is also described. It uses convolutional\textbackslash nneural network character recognizers combined with global training\textbackslash ntechniques to provide record accuracy on business and personal cheques.\textbackslash nIt is deployed commercially and reads several million cheques per day\textbackslash n},
  keywords = {Convolutional neural networks,Document recognition,Finite state transducers,Gradient-based learning,Graph transformer networks,Machine learning,Neural network,Optical character recognition (OCR)}
}

@article{lecun_handwritten_1989,
  title = {Handwritten {{Digit Recognition}}: {{Applications}} of {{Neural Network Chips}} and {{Automatic Learning}}},
  author = {LeCun, Yann A. and Jackel, L. D. and Boser, B. and Denker, J. S. and Graf, H. P. and Guyon, I. and Henderson, D. and Howard, R. E. and Hubbard, W.},
  year = {1989},
  journal = {IEEE Communications Magazine},
  volume = {27},
  number = {11},
  pages = {41--46},
  abstract = {Two novel methods for achieving handwritten digit recognition are described. The first method is based on a neural network chip that performs line thinning and feature extraction using local template matching. The second method is implemented on a digital signal processor and makes extensive use of constrained automatic learning. Experimental results obtained using isolated handwritten digits taken from postal zip codes, a rather difficult data set, are reported and discussed.}
}

@inproceedings{lecun_learning_2004,
  title = {Learning Methods for Generic Object Recognition with Invariance to Pose and Lighting},
  booktitle = {{{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}} 2004)},
  author = {LeCun, Y. and Huang, F. and Bottou, L.},
  year = {2004},
  volume = {2},
  pages = {II-104 Vol.2}
}

@inproceedings{lecuyer_randomized_2016,
  title = {Randomized {{Quasi-Monte Carlo}}: {{An Introduction}} for {{Practitioners}}},
  booktitle = {International {{Conference}} on {{Monte Carlo}} and {{Quasi-Monte Carlo Methods}} in {{Scientific Computing}} ({{MCQMC}})},
  author = {L'Ecuyer, Pierre},
  year = {2016},
  abstract = {We survey basic ideas and results on randomized quasi-Monte Carlo (RQMC) methods, discuss their practical aspects, and give numerical illustrations. RQMC can improve accuracy compared with standard Monte Carlo (MC) when estimating an integral interpreted as a mathematical expectation. RQMC estimators are unbiased and their variance converges at a faster rate (under certain conditions) than MC estimators, as a function of the sample size. Variants of RQMC also work for the simulation of Markov chains, for function approximation and optimization, for solving partial differential equations, etc. In this introductory survey, we look at how RQMC point sets and sequences are constructed, how we measure their uniformity, why they can work for high-dimensional integrals, and how can they work when simulating Markov chains over a large number of steps.}
}

@article{lee_bidirectional_2021,
  title = {Bidirectional {{Variational Inference}} for {{Non-Autoregressive Text-to-Speech}}},
  author = {Lee, Yoonhyung and Shin, Joongbo and Jung, Kyomin},
  year = {2021},
  pages = {19},
  abstract = {Although early text-to-speech (TTS) models such as Tacotron 2 have succeeded in generating human-like speech, their autoregressive architectures have several limitations: (1) They require a lot of time to generate a mel-spectrogram consisting of hundreds of steps. (2) The autoregressive speech generation lacks robustness due to its error propagation property. In this paper, we propose a novel non-autoregressive TTS model called BVAE-TTS, which eliminates the architectural limitations and generates a mel-spectrogram in parallel. BVAE-TTS adopts a bidirectional-inference variational autoencoder (BVAE) that learns hierarchical latent representations using both bottom-up and top-down paths to increase its expressiveness. To apply BVAE to TTS, we design our model to utilize text information via an attention mechanism. By using attention maps that BVAE-TTS generates, we train a duration predictor so that the model uses the predicted duration of each phoneme at inference. In experiments conducted on LJSpeech dataset, we show that our model generates a mel-spectrogram 27 times faster than Tacotron 2 with similar speech quality. Furthermore, our BVAE-TTS outperforms GlowTTS, which is one of the state-of-the-art non-autoregressive TTS models, in terms of both speech quality and inference speed while having 58\% fewer parameters.},
  langid = {english}
}

@article{lee_efficient_2006,
  title = {Efficient {{Sparse}} Coding Algorithms},
  author = {Lee, Honglak and Battle, Alexis and Raina, Rajat and Ng, Andrew Y.},
  year = {2006},
  journal = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
  volume = {19},
  number = {2},
  eprint = {1506.03733v1},
  eprinttype = {arxiv},
  pages = {801--808},
  issn = {10495258},
  abstract = {Sparse coding provides a class of algorithms for finding succinct representations of stimuli; given only unlabeled input data, it discovers basis functions that cap- ture higher-level features in the data. However, finding sparse codes remains a very difficult computational problem. In this paper, we present efficient sparse coding algorithms that are based on iteratively solving two convex optimization problems: an L1-regularized least squares problem and an L2-constrained least squares problem. We propose novel algorithms to solve both of these optimiza- tion problems. Our algorithms result in a significant speedup for sparse coding, allowing us to learn larger sparse codes than possible with previously described algorithms. We apply these algorithms to natural images and demonstrate that the inferred sparse codes exhibit end-stopping and non-classical receptive field sur- round suppression and, therefore, may provide a partial explanation for these two phenomena in V1 neurons. 1},
  archiveprefix = {arXiv},
  isbn = {0262195682},
  pmid = {17051527},
  keywords = {Stanford University}
}

@misc{lee_end--end_2017,
  title = {End-to-End {{Neural Coreference Resolution}}},
  author = {Lee, Kenton and He, Luheng and Lewis, Mike and Zettlemoyer, Luke},
  year = {2017},
  eprint = {1707.07045},
  eprinttype = {arxiv},
  abstract = {We introduce the first end-to-end coreference resolution model and show that it significantly outperforms all previous work without using a syntactic parser or hand-engineered mention detector. The key idea is to directly consider all spans in a document as potential mentions and learn distributions over possible antecedents for each. The model computes span embeddings that combine context-dependent boundary representations with a head-finding attention mechanism. It is trained to maximize the marginal likelihood of gold antecedent spans from coreference clusters and is factored to enable aggressive pruning of potential mentions. Experiments demonstrate state-of-the-art performance, with a gain of 1.5 F1 on the OntoNotes benchmark and by 3.1 F1 using a 5-model ensemble, despite the fact that this is the first approach to be successfully trained with no external resources.},
  archiveprefix = {arXiv}
}

@misc{lee_fully_2016,
  title = {Fully {{Character-Level Neural Machine Translation}} without {{Explicit Segmentation}}},
  author = {Lee, Jason and Cho, Kyunghyun and Hofmann, Thomas},
  year = {2016},
  month = mar,
  eprint = {1610.03017},
  eprinttype = {arxiv},
  abstract = {Most existing machine translation systems operate at the level of words, relying on explicit segmentation to extract tokens. We introduce a neural machine translation (NMT) model that maps a source character sequence to a target character sequence without any segmentation. We employ a character-level convolutional network with max-pooling at the encoder to reduce the length of source representation, allowing the model to be trained at a speed comparable to subword-level models while capturing local regularities. Our character-to-character model outperforms a recently proposed baseline with a subword-level encoder on WMT'15 DE-EN and CS-EN, and gives comparable performance on FI-EN and RU-EN. We then demonstrate that it is possible to share a single character-level encoder across multiple languages by training a model on a many-to-one translation task. In this multilingual setting, the character-level encoder significantly outperforms the subword-level encoder on all the language pairs. We observe that on CS-EN, FI-EN and RU-EN, the quality of the multilingual character-level translation even surpasses the models specifically trained on that language pair alone, both in terms of BLEU score and human judgment.},
  archiveprefix = {arXiv},
  isbn = {9781510827585}
}

@inproceedings{lee_nonparametric_2012,
  title = {A {{Nonparametric Bayesian Approach}} to {{Acoustic Model Discovery}}},
  booktitle = {Proceedings of the 50th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Lee, Chia-ying and Glass, James},
  year = {2012},
  month = jul,
  pages = {40--49},
  publisher = {{Association for Computational Linguistics}},
  address = {{Jeju Island, Korea}}
}

@inproceedings{lee_simple_2018,
  title = {A {{Simple Unified Framework}} for {{Detecting Out-of-Distribution Samples}} and {{Adversarial Attacks}}},
  booktitle = {Proceedings of the 32nd {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Lee, Kimin and Lee, Kibok and Lee, Honglak and Shin, Jinwoo},
  year = {2018},
  pages = {11},
  address = {{Montr\'eal, Quebec, Canada}},
  abstract = {Detecting test samples drawn sufficiently far away from the training distribution statistically or adversarially is a fundamental requirement for deploying a good classifier in many real-world machine learning applications. However, deep neural networks with the softmax classifier are known to produce highly overconfident posterior distributions even for such abnormal samples. In this paper, we propose a simple yet effective method for detecting any abnormal samples, which is applicable to any pre-trained softmax neural classifier. We obtain the class conditional Gaussian distributions with respect to (low- and upper-level) features of the deep models under Gaussian discriminant analysis, which result in a confidence score based on the Mahalanobis distance. While most prior methods have been evaluated for detecting either out-of-distribution or adversarial samples, but not both, the proposed method achieves the state-of-the-art performances for both cases in our experiments. Moreover, we found that our proposed method is more robust in harsh cases, e.g., when the training dataset has noisy labels or small number of samples. Finally, we show that the proposed method enjoys broader usage by applying it to class-incremental learning: whenever out-of-distribution samples are detected, our classification rule can incorporate new classes well without further training deep models.}
}

@inproceedings{lee_training_2018,
  title = {Training {{Confidence-calibrated Classifiers}} for {{Detecting Out-of-Distribution Samples}}},
  booktitle = {Proceedings of the 6th {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Lee, Kimin and Lee, Honglak and Lee, Kibok and Shin, Jinwoo},
  year = {2018},
  abstract = {The problem of detecting whether a test sample is from in-distribution (i.e., training distribution by a classifier) or out-of-distribution sufficiently different from it arises in many real-world machine learning applications. However, the state-of-art deep neural networks are known to be highly overconfident in their predictions, i.e., do not distinguish in- and out-of-distributions. Recently, to handle this issue, several threshold-based detectors have been proposed given pre-trained neural classifiers. However, the performance of prior works highly depends on how to train the classifiers since they only focus on improving inference procedures. In this paper, we develop a novel training method for classifiers so that such inference algorithms can work better. In particular, we suggest two additional terms added to the original loss (e.g., cross entropy). The first one forces samples from out-of-distribution less confident by the classifier and the second one is for (implicitly) generating most effective training samples for the first one. In essence, our method jointly trains both classification and generative neural networks for out-of-distribution. We demonstrate its effectiveness using deep convolutional neural networks on various popular image datasets.},
  annotation = {http://arxiv.org/abs/1711.09325}
}

@inproceedings{lee_unsupervised_2009,
  title = {Unsupervised Feature Learning for Audio Classification Using Convolutional Deep Belief Networks},
  booktitle = {Proceedings of the 22nd {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Lee, Honglak and Largman, Yan and Pham, Peter and Ng, Andrew Y.},
  year = {2009},
  series = {{{NIPS}}'09},
  pages = {1096--1104},
  address = {{Vancouver, BC, Canada}},
  abstract = {In recent years, deep learning approaches have gained significant interest as a way of building hierarchical representations from unlabeled data. However, to our knowledge, these deep learning approaches have not been extensively studied for auditory data. In this paper, we apply convolutional deep belief networks to audio data and empirically evaluate them on various audio classification tasks. In the case of speech data, we show that the learned features correspond to phones/phonemes. In addition, our feature representations learned from unlabeled audio data show very good performance for multiple audio classification tasks. We hope that this paper will inspire more research on deep learning approaches applied to a wide range of audio recognition tasks.},
  isbn = {978-1-61567-911-9}
}

@article{lee_wide_2020,
  title = {Wide {{Neural Networks}} of {{Any Depth Evolve}} as {{Linear Models Under Gradient Descent}}},
  author = {Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel S. and Bahri, Yasaman and Novak, Roman and {Sohl-Dickstein}, Jascha and Pennington, Jeffrey},
  year = {2020},
  month = dec,
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  volume = {2020},
  number = {12},
  eprint = {1902.06720},
  eprinttype = {arxiv},
  pages = {124002},
  issn = {1742-5468},
  abstract = {A longstanding goal in deep learning research has been to precisely characterize training and generalization. However, the often complex loss landscapes of neural networks have made a theory of learning dynamics elusive. In this work, we show that for wide neural networks the learning dynamics simplify considerably and that, in the infinite width limit, they are governed by a linear model obtained from the first-order Taylor expansion of the network around its initial parameters. Furthermore, mirroring the correspondence between wide Bayesian neural networks and Gaussian processes, gradient-based training of wide neural networks with a squared loss produces test set predictions drawn from a Gaussian process with a particular compositional kernel. While these theoretical results are only exact in the infinite width limit, we nevertheless find excellent empirical agreement between the predictions of the original network and those of the linearized version even for finite practically-sized networks. This agreement is robust across different architectures, optimization methods, and loss functions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{lee-thorp_fnet_2021,
  title = {{{FNet}}: {{Mixing Tokens}} with {{Fourier Transforms}}},
  shorttitle = {{{FNet}}},
  author = {{Lee-Thorp}, James and Ainslie, Joshua and Eckstein, Ilya and Ontanon, Santiago},
  year = {2021},
  month = may,
  eprint = {2105.03824},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We show that Transformer encoder architectures can be massively sped up, with limited accuracy costs, by replacing the self-attention sublayers with simple linear transformations that ``mix'' input tokens. These linear transformations, along with simple nonlinearities in feed-forward layers, are sufficient to model semantic relationships in several text classification tasks. Perhaps most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder with a standard, unparameterized Fourier Transform achieves 92\% of the accuracy of BERT on the GLUE benchmark, but pre-trains and runs up to seven times faster on GPUs and twice as fast on TPUs. The resulting model, which we name FNet, scales very efficiently to long inputs, matching the accuracy of the most accurate ``efficient'' Transformers on the Long Range Arena benchmark, but training and running faster across all sequence lengths on GPUs and relatively shorter sequence lengths on TPUs. Finally, FNet has a light memory footprint and is particularly efficient at smaller model sizes: for a fixed speed and accuracy budget, small FNet models outperform Transformer counterparts.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{lehman_es_2017,
  title = {{{ES Is More Than Just}} a {{Traditional Finite-Difference Approximator}}},
  author = {Lehman, Joel and Chen, Jay and Clune, Jeff and Stanley, Kenneth O.},
  year = {2017},
  eprint = {1712.06568},
  eprinttype = {arxiv},
  abstract = {An evolution strategy (ES) variant recently attracted significant attention due to its surprisingly good performance at optimizing neural networks in challenging deep reinforcement learning domains. It searches directly in the parameter space of neural networks by generating perturbations to the current set of parameters, checking their performance, and moving in the direction of higher reward. The resemblance of this algorithm to a traditional finite-difference approximation of the reward gradient in parameter space naturally leads to the assumption that it is just that. However, this assumption is incorrect. The aim of this paper is to definitively demonstrate this point empirically. ES is a gradient approximator, but optimizes for a different gradient than just reward (especially when the magnitude of candidate perturbations is high). Instead, it optimizes for the average reward of the entire population, often also promoting parameters that are robust to perturbation. This difference can channel ES into significantly different areas of the search space than gradient descent in parameter space, and also consequently to networks with significantly different properties. This unique robustness-seeking property, and its consequences for optimization, are demonstrated in several domains. They include humanoid locomotion, where networks from policy gradient-based reinforcement learning are far less robust to parameter perturbation than ES-based policies that solve the same task. While the implications of such robustness and robustness-seeking remain open to further study, the main contribution of this work is to highlight that such differences indeed exist and deserve attention.},
  archiveprefix = {arXiv}
}

@misc{lehman_safe_2017,
  title = {Safe {{Mutations}} for {{Deep}} and {{Recurrent Neural Networks}} through {{Output Gradients}}},
  author = {Lehman, Joel and Chen, Jay and Clune, Jeff and Stanley, Kenneth O.},
  year = {2017},
  eprint = {1712.06563},
  eprinttype = {arxiv},
  abstract = {While neuroevolution (evolving neural networks) has a successful track record across a variety of domains from reinforcement learning to artificial life, it is rarely applied to large, deep neural networks. A central reason is that while random mutation generally works in low dimensions, a random perturbation of thousands or millions of weights is likely to break existing functionality, providing no learning signal even if some individual weight changes were beneficial. This paper proposes a solution by introducing a family of safe mutation (SM) operators that aim within the mutation operator itself to find a degree of change that does not alter network behavior too much, but still facilitates exploration. Importantly, these SM operators do not require any additional interactions with the environment. The most effective SM variant capitalizes on the intriguing opportunity to scale the degree of mutation of each individual weight according to the sensitivity of the network's outputs to that weight, which requires computing the gradient of outputs with respect to the weights (instead of the gradient of error, as in conventional deep learning). This safe mutation through gradients (SM-G) operator dramatically increases the ability of a simple genetic algorithm-based neuroevolution method to find solutions in high-dimensional domains that require deep and/or recurrent neural networks (which tend to be particularly brittle to mutation), including domains that require processing raw pixels. By improving our ability to evolve deep neural networks, this new safer approach to mutation expands the scope of domains amenable to neuroevolution.},
  archiveprefix = {arXiv}
}

@misc{lei_rationalizing_2016,
  title = {Rationalizing {{Neural Predictions}}},
  author = {Lei, Tao and Barzilay, Regina and Jaakkola, Tommi},
  year = {2016},
  month = nov,
  eprint = {1606.04155},
  eprinttype = {arxiv},
  abstract = {Prediction without justification has limited applicability. As a remedy, we learn to extract pieces of input text as justifications -- rationales -- that are tailored to be short and coherent, yet sufficient for making the same prediction. Our approach combines two modular components, generator and encoder, which are trained to operate well together. The generator specifies a distribution over text fragments as candidate rationales and these are passed through the encoder for prediction. Rationales are never given during training. Instead, the model is regularized by desiderata for rationales. We evaluate the approach on multi-aspect sentiment analysis against manually annotated test cases. Our approach outperforms attention-based baseline by a significant margin. We also successfully illustrate the method on the question retrieval task.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing}
}

@incollection{lemarechal_nondifferentiable_1989,
  title = {Nondifferentiable Optimization},
  booktitle = {Handbooks in {{Operations Research}} and {{Management Science}}},
  author = {Lemar{\'e}chal, Claude},
  year = {1989},
  volume = {1},
  pages = {529--572},
  issn = {09270507},
  abstract = {This chapter discusses the nondifferentiable optimization (NDO). Nondifferentiable optimization or nonsmooth optimization (NSO) deals with the situations in operations research where a function that fails to have derivatives for some values of the variables has to be optimized. For this situation, new tools are required to replace standard differential calculus, and these new tools come from convex analysis. Functions with discontinuous derivatives are frequent in operations research. Sometimes they arise when modeling the problem, sometimes they are introduced artificially during the solution procedure. The chapter discusses the necessary concepts and the basic properties and some examples of practical problems motivating the use of NSO. It is shown how and why classical methods fail. The chapter also discusses some possibilities that can be used when a special structure exists in the nonsmooth problem. It also presents subgradient methods and more recent methods and also covers some orientations for future research. \textcopyright{} 1989 Elsevier Science Publishers B.V.},
  isbn = {978-0-444-87284-5}
}

@article{lenz_deep_2015,
  title = {Deep Learning for Detecting Robotic Grasps},
  author = {Lenz, Ian and Lee, Honglak and Saxena, Ashutosh},
  year = {2015},
  journal = {The International Journal of Robotics Research},
  volume = {34},
  number = {4-5},
  pages = {705--724},
  publisher = {{SAGE Publications Sage UK: London, England}}
}

@article{leung_machine_2016,
  title = {Machine Learning in Genomic Medicine: {{A}} Review of Computational Problems and Data Sets},
  author = {Leung, Michael K. K. and Delong, Andrew and Alipanahi, Babak and Frey, Brendan J.},
  year = {2016},
  journal = {Proceedings of the IEEE},
  volume = {104},
  number = {1},
  issn = {00189219},
  abstract = {In this paper, we provide an introduction to machine learning tasks that address important problems in genomic medicine. One of the goals of genomic medicine is to determine how variations in the DNA of individuals can affect the risk of different diseases, and to find causal explanations so that targeted therapies can be designed. Here we focus on how machine learning can help to model the relationship between DNA and the quantities of key molecules in the cell, with the premise that these quantities, which we refer to as cell variables, may be associated with disease risks. Modern biology allows high-throughput measurement of many such cell variables, including gene expression, splicing, and proteins binding to nucleic acids, which can all be treated as training targets for predictive models. With the growing availability of large-scale data sets and advanced computational techniques such as deep learning, researchers can help to usher in a new era of effective genomic medicine.},
  isbn = {0018-9219 VO - 104}
}

@inproceedings{levin_fixed-dimensional_2013,
  title = {Fixed-Dimensional Acoustic Embeddings of Variable-Length Segments in Low-Resource Settings},
  booktitle = {2013 {{IEEE Workshop}} on {{Automatic Speech Recognition}} and {{Understanding}}},
  author = {Levin, Keith and Henry, Katharine and Jansen, Aren and Livescu, Karen},
  year = {2013},
  month = dec,
  pages = {410--415},
  abstract = {Measures of acoustic similarity between words or other units are critical for segmental exemplar-based acoustic models, spoken term discovery, and query-by-example search. Dynamic time warping (DTW) alignment cost has been the most commonly used measure, but it has well-known inadequacies. Some recently proposed alternatives require large amounts of training data. In the interest of finding more efficient, accurate, and low-resource alternatives, we consider the problem of embedding speech segments of arbitrary length into fixed-dimensional spaces in which simple distances (such as cosine or Euclidean) serve as a proxy for linguistically meaningful (phonetic, lexical, etc.) dissimilarities. Such embeddings would enable efficient audio indexing and permit application of standard distance learning techniques to segmental acoustic modeling. In this paper, we explore several supervised and unsupervised approaches to this problem and evaluate them on an acoustic word discrimination task. We identify several embedding algorithms that match or improve upon the DTW baseline in low-resource settings.},
  keywords = {Acoustics,Fixed-dimensional embedding,Laplace equations,Principal component analysis,query-by-example search,segmental acoustic modeling,Speech,speech indexing,Time series analysis,Training,Vectors}
}

@inproceedings{levin_segmental_2015,
  title = {Segmental Acoustic Indexing for Zero Resource Keyword Search},
  booktitle = {2015 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Levin, Keith and Jansen, Aren and Van Durme, Benjamin},
  year = {2015},
  month = apr,
  pages = {5828--5832},
  issn = {2379-190X},
  abstract = {The task of zero resource query-by-example keyword search has received much attention in recent years as the speech technology needs of the developing world grow. These systems traditionally rely upon dynamic time warping (DTW) based retrieval algorithms with runtimes that are linear in the size of the search collection. As a result, their scalability substantially lags that of their supervised counterparts, which take advantage of efficient word-based indices. In this paper, we present a novel audio indexing approach called Segmental Randomized Acoustic Indexing and Logarithmic-time Search (S-RAILS). S-RAILS generalizes the original frame-based RAILS methodology to word-scale segments by exploiting a recently proposed acoustic segment embedding technique. By indexing word-scale segments directly, we avoid higher cost frame-based processing of RAILS while taking advantage of the improved lexical discrimination of the embeddings. Using the same conversational telephone speech benchmark, we demonstrate major improvements in both speed and accuracy over the original RAILS system.},
  keywords = {Acoustics,Approximation methods,fixed-dimensional embedding,Indexing,Laplace equations,query-by-example search,Rails,Speech,speech indexing,Zero resource}
}

@article{levine_end--end_2016,
  title = {End-to-End Training of Deep Visuomotor Policies},
  author = {Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
  year = {2016},
  journal = {Journal of Machine Learning Research},
  volume = {17},
  number = {39},
  pages = {1--40}
}

@misc{levine_learning_2016,
  title = {Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection},
  author = {Levine, Sergey and Pastor, Peter and Krizhevsky, Alex and Quillen, Deirdre},
  year = {2016},
  eprint = {1603.02199},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@misc{li_advancing_2018,
  title = {Advancing {{Acoustic-to-Word CTC Model}}},
  author = {Li, Jinyu and Ye, Guoli and Das, Amit and Zhao, Rui and Gong, Yifan},
  year = {2018},
  eprint = {1803.05566},
  eprinttype = {arxiv},
  abstract = {The acoustic-to-word model based on the connectionist temporal classification (CTC) criterion was shown as a natural end-to-end (E2E) model directly targeting words as output units. However, the word-based CTC model suffers from the out-of-vocabulary (OOV) issue as it can only model limited number of words in the output layer and maps all the remaining words into an OOV output node. Hence, such a word-based CTC model can only recognize the frequent words modeled by the network output nodes. Our first attempt to improve the acoustic-to-word model is a hybrid CTC model which consults a letter-based CTC when the word-based CTC model emits OOV tokens during testing time. Then, we propose a much better solution by training a mixed-unit CTC model which decomposes all the OOV words into sequences of frequent words and multi-letter units. Evaluated on a 3400 hours Microsoft Cortana voice assistant task, the final acoustic-to-word solution improves the baseline word-based CTC by relative 12.09\% word error rate (WER) reduction when combined with our proposed attention CTC. Such an E2E model without using any language model (LM) or complex decoder outperforms the traditional context-dependent phoneme CTC which has strong LM and decoder by relative 6.79\%.},
  archiveprefix = {arXiv},
  isbn = {9781538646588}
}

@article{li_applications_2012,
  title = {Applications of {{Bayesian}} Methods in Wind Energy Conversion Systems},
  author = {Li, Gong and Shi, Jing},
  year = {2012},
  journal = {Renewable Energy},
  volume = {43},
  pages = {1--8},
  abstract = {The fast growth of wind power is in urgent need of more accurate, reliable, and adaptive modeling and data analysis methods for the characterization and prediction of wind resource and wind power, as well as reliability evaluation of wind energy conversion systems. Bayesian methods have shown unique advantages in statistical modeling and data analysis for the quantity of interest with uncertainty and variability. The adoption of Bayesian methods carries great potentials for various aspects in wind energy conversion systems such as improving the accuracy and reliability of wind resource estimation and short-term forecasts. This paper summarizes the basic theories of several Bayesian methods, and extensively reviews the literature addressing the applications of Bayesian methods in wind energy conversion systems. Based on the state-of-the-art review, the prospects of Bayesian methods in wind energy conversion systems are discussed on how to develop new applications and enhance the methods for existing applications. It is believed that Bayesian methods will be gaining more momentum in wind energy applications in the near future.}
}

@inproceedings{li_deep_2016,
  title = {Deep {{Reinforcement Learning}} for {{Dialogue Generation}}},
  booktitle = {Proceedings of the {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Li, Jiwei and Monroe, Will and Ritter, Alan and Jurafsky, Dan and Galley, Michel and Gao, Jianfeng},
  year = {2016},
  eprint = {1606.01541},
  eprinttype = {arxiv},
  pages = {1192--1202},
  issn = {24699969},
  abstract = {Recent neural models of dialogue generation offer great promise for generating responses for conversational agents, but tend to be shortsighted, predicting utterances one at a time while ignoring their influence on future outcomes. Modeling the future direction of a dialogue is crucial to generating coherent, interesting dialogues, a need which led traditional NLP models of dialogue to draw on reinforcement learning. In this paper, we show how to integrate these goals, applying deep reinforcement learning to model future reward in chatbot dialogue. The model simulates dialogues between two virtual agents, using policy gradient methods to reward sequences that display three useful conversational properties: informativity (non-repetitive turns), coherence, and ease of answering (related to forward-looking function). We evaluate our model on diversity, length as well as with human judges, showing that the proposed algorithm generates more interactive responses and manages to foster a more sustained conversation in dialogue simulation. This work marks a first step towards learning a neural conversational model based on the long-term success of dialogues.},
  archiveprefix = {arXiv},
  isbn = {978-1-5090-6182-2}
}

@misc{li_deep_2017,
  title = {Deep {{Reinforcement Learning}}: {{An Overview}}},
  author = {Li, Yuxi},
  year = {2017},
  month = jan,
  eprint = {1701.07274},
  eprinttype = {arxiv},
  abstract = {We give an overview of recent exciting achievements of deep reinforcement learning (RL). We discuss six core elements, six important mechanisms, and twelve applications. We start with background of machine learning, deep learning and reinforcement learning. Next we discuss core RL elements, including value function, in particular, Deep Q-Network (DQN), policy, reward, model, planning, and exploration. After that, we discuss important mechanisms for RL, including attention and memory, unsupervised learning, transfer learning, multi-agent RL, hierarchical RL, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, natural language processing, including dialogue systems, machine translation, and text generation, computer vision, neural architecture design, business management, finance, healthcare, Industry 4.0, smart grid, intelligent transportation systems, and computer systems. We mention topics not reviewed yet, and list a collection of RL resources. After presenting a brief summary, we close with discussions.},
  archiveprefix = {arXiv},
  isbn = {9781509011216},
  pmid = {15040217},
  keywords = {Deep learning,Overview,Reinforcement learning,Review,Survey}
}

@inproceedings{li_improving_2020,
  title = {Improving {{Transformer-Based Speech Recognition}} with {{Unsupervised Pre-Training}} and {{Multi-Task Semantic Knowledge Learning}}},
  booktitle = {Interspeech 2020},
  author = {Li, Song and Li, Lin and Hong, Qingyang and Liu, Lingling},
  year = {2020},
  month = oct,
  pages = {5006--5010},
  publisher = {{ISCA}},
  abstract = {Recently, the Transformer-based end-to-end speech recognition system has become a state-of-the-art technology. However, one prominent problem with current end-to-end speech recognition systems is that an extensive amount of paired data are required to achieve better recognition performance. In order to grapple with such an issue, we propose two unsupervised pre-training strategies for the encoder and the decoder of Transformer respectively, which make full use of unpaired data for training. In addition, we propose a new semi-supervised fine-tuning method named multi-task semantic knowledge learning to strengthen the Transformer's ability to learn about semantic knowledge, thereby improving the system performance. We achieve the best CER with our proposed methods on AISHELL-1 test set: 5.9\%, which exceeds the best end-to-end model by 10.6\% relative CER. Moreover, relative CER reduction of 20.3\% and 17.8\% are obtained for low-resource Mandarin and English data sets, respectively.},
  langid = {english}
}

@misc{li_jasper:_2019,
  title = {Jasper: {{An End-to-End Convolutional Neural Acoustic Model}}},
  shorttitle = {Jasper},
  author = {Li, Jason and Lavrukhin, Vitaly and Ginsburg, Boris and Leary, Ryan and Kuchaiev, Oleksii and Cohen, Jonathan M. and Nguyen, Huyen and Gadde, Ravi Teja},
  year = {2019},
  month = apr,
  eprint = {1904.03288},
  eprinttype = {arxiv},
  abstract = {In this paper, we report state-of-the-art results on LibriSpeech among end-to-end speech recognition models without any external training data. Our model, Jasper, uses only 1D convolutions, batch normalization, ReLU, dropout, and residual connections. To improve training, we further introduce a new layer-wise optimizer called NovoGrad. Through experiments, we demonstrate that the proposed deep architecture performs as well or better than more complex choices. Our deepest Jasper variant uses 54 convolutional layers. With this architecture, we achieve 2.95\% WER using beam-search decoder with an external neural language model and 3.86\% WER with a greedy decoder on LibriSpeech test-clean. We also report competitive results on the Wall Street Journal and the Hub5'00 conversational evaluation datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@inproceedings{li_learning_2016,
  title = {Learning to Generate with Memory},
  booktitle = {Proceedings of the 33nd {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Li, Chongxuan and Zhu, Jun and Zhang, Bo},
  year = {2016},
  pages = {1177--1186},
  address = {{New York, NY, USA}},
  keywords = {Memory}
}

@inproceedings{li_measuring_2018,
  title = {Measuring the {{Intrinsic Dimension}} of {{Objective Landscapes}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Li, Chunyuan and Farkhoor, Heerad and Liu, Rosanne and Yosinski, Jason},
  year = {2018},
  eprint = {1804.08838},
  eprinttype = {arxiv},
  address = {{Vancouver, Canada}},
  abstract = {Many recently trained neural networks employ large numbers of parameters to achieve good performance. One may intuitively use the number of parameters required as a rough gauge of the difficulty of a problem. But how accurate are such notions? How many parameters are really needed? In this paper we attempt to answer this question by training networks not in their native parameter space, but instead in a smaller, randomly oriented subspace. We slowly increase the dimension of this subspace, note at which dimension solutions first appear, and define this to be the intrinsic dimension of the objective landscape. The approach is simple to implement, computationally tractable, and produces several suggestive conclusions. Many problems have smaller intrinsic dimensions than one might suspect, and the intrinsic dimension for a given dataset varies little across a family of models with vastly different sizes. This latter result has the profound implication that once a parameter space is large enough to solve a problem, extra parameters serve directly to increase the dimensionality of the solution manifold. Intrinsic dimension allows some quantitative comparison of problem difficulty across supervised, reinforcement, and other types of learning where we conclude, for example, that solving the inverted pendulum problem is 100 times easier than classifying digits from MNIST, and playing Atari Pong from pixels is about as hard as classifying CIFAR-10. In addition to providing new cartography of the objective landscapes wandered by parameterized models, the method is a simple technique for constructively obtaining an upper bound on the minimum description length of a solution. A byproduct of this construction is a simple approach for compressing networks, in some cases by more than 100 times.},
  archiveprefix = {arXiv},
  keywords = {★}
}

@misc{li_renyi_2016,
  title = {R\'enyi {{Divergence Variational Inference}}},
  author = {Li, Yingzhen and Turner, Richard E.},
  year = {2016},
  eprint = {1602.02311},
  eprinttype = {arxiv},
  abstract = {This paper introduces the variational R\textbackslash 'enyi bound (VR) that extends traditional variational inference to R\textbackslash 'enyi's alpha-divergences. This new family of variational methods unifies a number of existing approaches, and enables a smooth interpolation from the evidence lower-bound to the log (marginal) likelihood that is controlled by the value of alpha that parametrises the divergence. The reparameterization trick, Monte Carlo approximation and stochastic optimisation methods are deployed to obtain a tractable and unified framework for optimisation. We further consider negative alpha values and propose a novel variational inference method as a new special case in the proposed framework. Experiments on Bayesian neural networks and variational auto-encoders demonstrate the wide applicability of the VR bound.},
  archiveprefix = {arXiv}
}

@misc{li_understanding_2018,
  title = {Understanding the {{Disharmony}} between {{Dropout}} and {{Batch Normalization}} by {{Variance Shift}}},
  author = {Li, Xiang and Chen, Shuo and Hu, Xiaolin and Yang, Jian},
  year = {2018},
  eprint = {1801.05134},
  eprinttype = {arxiv},
  abstract = {This paper first answers the question "why do the two most powerful techniques Dropout and Batch Normalization (BN) often lead to a worse performance when they are combined together?" in both theoretical and statistical aspects. Theoretically, we find that Dropout would shift the variance of a specific neural unit when we transfer the state of that network from train to test. However, BN would maintain its statistical variance, which is accumulated from the entire learning procedure, in the test phase. The inconsistency of that variance (we name this scheme as "variance shift") causes the unstable numerical behavior in inference that leads to more erroneous predictions finally, when applying Dropout before BN. Thorough experiments on DenseNet, ResNet, ResNeXt and Wide ResNet confirm our findings. According to the uncovered mechanism, we next explore several strategies that modifies Dropout and try to overcome the limitations of their combination by avoiding the variance shift risks.},
  archiveprefix = {arXiv}
}

@inproceedings{liang_enhancing_2018,
  title = {Enhancing the Reliability of Out-of-Distribution Image Detection in Neural Networks},
  booktitle = {Proceedings of the 6th {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Liang, Shiyu and Li, Yixuan and Srikant, R.},
  year = {2018},
  address = {{Vancouver, Canada}}
}

@misc{liang_learning_2018,
  title = {Learning {{Noise-Invariant Representations}} for {{Robust Speech Recognition}}},
  author = {Liang, Davis and Huang, Zhiheng and Lipton, Zachary C.},
  year = {2018},
  month = jul,
  eprint = {1807.06610},
  eprinttype = {arxiv},
  abstract = {Despite rapid advances in speech recognition, current models remain brittle to superficial perturbations to their inputs. Small amounts of noise can destroy the performance of an otherwise state-of-the-art model. To harden models against background noise, practitioners often perform data augmentation, adding artificially-noised examples to the training set, carrying over the original label. In this paper, we hypothesize that a clean example and its superficially perturbed counterparts shouldn't merely map to the same class --- they should map to the same representation. We propose invariant-representation-learning (IRL): At each training iteration, for each training example,we sample a noisy counterpart. We then apply a penalty term to coerce matched representations at each layer (above some chosen layer). Our key results, demonstrated on the Librispeech dataset are the following: (i) IRL significantly reduces character error rates (CER) on both 'clean' (3.3\% vs 6.5\%) and 'other' (11.0\% vs 18.1\%) test sets; (ii) on several out-of-domain noise settings (different from those seen during training), IRL's benefits are even more pronounced. Careful ablations confirm that our results are not simply due to shrinking activations at the chosen layers.},
  archiveprefix = {arXiv},
  keywords = {Automatic speech recognition,Computer Science - Computation and Language,Computer Science - Sound,Invariant representations,Natural Language Processing}
}

@misc{liang_learning_2019,
  title = {Learning {{Representations}} from {{Imperfect Time Series Data}} via {{Tensor Rank Regularization}}},
  author = {Liang, Paul Pu and Liu, Zhun and Tsai, Yao-Hung Hubert and Zhao, Qibin and Salakhutdinov, Ruslan and Morency, Louis-Philippe},
  year = {2019},
  month = jul,
  eprint = {1907.01011},
  eprinttype = {arxiv},
  abstract = {There has been an increased interest in multimodal language processing including multimodal dialog, question answering, sentiment analysis, and speech recognition. However, naturally occurring multimodal data is often imperfect as a result of imperfect modalities, missing entries or noise corruption. To address these concerns, we present a regularization method based on tensor rank minimization. Our method is based on the observation that high-dimensional multimodal time series data often exhibit correlations across time and modalities which leads to low-rank tensor representations. However, the presence of noise or incomplete values breaks these correlations and results in tensor representations of higher rank. We design a model to learn such tensor representations and effectively regularize their rank. Experiments on multimodal language data show that our model achieves good results across various levels of imperfection.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{liao_sharpening_2017,
  title = {Sharpening {{Jensen}}'s {{Inequality}}},
  author = {Liao, J. and Berg, Arthur},
  year = {2017},
  month = jul,
  journal = {The American Statistician},
  volume = {73},
  eprint = {1707.08644},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@misc{lievin_optimal_2020,
  title = {Optimal {{Variance Control}} of the {{Score Function Gradient Estimator}} for {{Importance Weighted Bounds}}},
  author = {Li{\'e}vin, Valentin and Dittadi, Andrea and Christensen, Anders and Winther, Ole},
  year = {2020},
  month = aug,
  eprint = {2008.01998},
  eprinttype = {arxiv},
  abstract = {This paper introduces novel results for the score function gradient estimator of the importance weighted variational bound (IWAE). We prove that in the limit of large K (number of importance samples) one can choose th{$\surd$}e control variate such that the Signal-to-Noise ratio (SNR) of the estimator grows as K. This is in{$\surd$}contrast to the standard pathwise gradient estimator where the SNR decreases as 1/ K. Based on our theoretical findings we develop a novel control variate that extends on VIMCO. Empirically, for the training of both continuous and discrete generative models, the proposed method yields superior variance reduction, resulting in an SNR for IWAE that increases with K without relying on the reparameterization trick. The novel estimator is competitive with state-of-the-art reparameterization-free gradient estimators such as Reweighted Wake-Sleep (RWS) and the thermodynamic variational objective (TVO) when training generative models.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{lievin_towards_2019,
  title = {Towards {{Hierarchical Discrete Variational Autoencoders}}},
  booktitle = {2nd {{Symposium}} on {{Advances}} in {{Approximate Bayesian Inference}}},
  author = {Lievin, Valentin and Dittadi, Andrea and Maal{\o}e, Lars and Winther, Ole},
  year = {2019},
  pages = {16},
  abstract = {Variational Autoencoders (VAEs) have proven to be powerful latent variable models. However, the form of the approximate posterior can limit the expressiveness of the model. Categorical distributions are flexible and useful building blocks for example in neural memory layers. We introduce the Hierarchical Discrete Variational Autoencoder (HD-VAE): a hierarchy of variational memory layers. The Concrete/Gumbel-Softmax relaxation allows maximizing a surrogate of the Evidence Lower Bound by stochastic gradient ascent. We show that, when using a limited number of latent variables, HD-VAE outperforms the Gaussian baseline on modelling multiple binary image datasets. Training very deep HD-VAE remains a challenge due to the relaxation bias that is induced by the use of a surrogate objective. We introduce a formal definition and conduct a preliminary theoretical and empirical study of the bias.},
  langid = {english},
  keywords = {Discrete representation,Gumbel softmax,Hierarchical VAE,Memory,Probabilistic Latent Variable Models}
}

@techreport{lighthill_lighthill_1973,
  title = {Lighthill {{Report}}: {{Artificial Intelligence}}: {{A Paper Symposium}}},
  author = {Lighthill, James},
  year = {1973},
  address = {{London, United Kingdom}},
  institution = {{Science Research Council}},
  abstract = {Lighthill's report was commissioned by the Science Research Council (SRC) to give an unbiased view of the state of AI research primarily in the UK in 1973. The two main research groups were at Sussex and Edinburgh. There was pressure from Edinburgh to buy a US machine, the Digital Equipment Corporation DEC10 which was used by most US researchers. AI research was funded by the Engineering Board of SRC as part of its Computer Science funding. The Lighthill Report was published early in 1973. Although it supported AI research related to automation and to computer simulation of neurophysiological and psychological processes, it was highly critical of basic research in the foundation areas such as robotics and language processing. Lighthill's report provoked a massive loss of confidence in AI by the academic establishment in the UK including the funding body. It persisted for almost a decade. AI research continued but the next attempt to mount a major activity in the area did not come until the September 1982 Research Area Review Meeting on Intelligent Knowledge-Based Systems. The findings of which were accepted by SERC (Science and Engineering Research Council, a change of name) and became the IKBS part of the Alvey Programme.}
}

@misc{lillicrap_continuous_2015,
  title = {Continuous Control with Deep Reinforcement Learning},
  author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  year = {2015},
  eprint = {1509.02971},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@misc{lin_network_2013,
  title = {Network {{In Network}}},
  author = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
  year = {2013},
  month = dec,
  eprint = {1312.4400},
  eprinttype = {arxiv},
  abstract = {We propose a novel deep network structure called "Network In Network" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.},
  archiveprefix = {arXiv}
}

@article{lin_why_2017,
  title = {Why Does Deep and Cheap Learning Work so Well?},
  author = {Lin, Henry W. and Tegmark, Max and Rolnick, David},
  year = {2017},
  month = sep,
  journal = {Journal of Statistical Physics},
  volume = {168},
  number = {6},
  pages = {1223--1247},
  issn = {0022-4715, 1572-9613},
  abstract = {We show how the success of deep learning could depend not only on mathematics but also on physics: although well-known mathematical theorems guarantee that neural networks can approximate arbitrary functions well, the class of functions of practical interest can frequently be approximated through "cheap learning" with exponentially fewer parameters than generic ones. We explore how properties frequently encountered in physics such as symmetry, locality, compositionality, and polynomial log-probability translate into exceptionally simple neural networks. We further argue that when the statistical process generating the data is of a certain hierarchical form prevalent in physics and machine-learning, a deep neural network can be more efficient than a shallow one. We formalize these claims using information theory and discuss the relation to the renormalization group. We prove various "no-flattening theorems" showing when efficient linear deep networks cannot be accurately approximated by shallow ones without efficiency loss, for example, we show that \$n\$ variables cannot be multiplied using fewer than 2\^n neurons in a single hidden layer.},
  langid = {english},
  keywords = {★,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Condensed Matter - Disordered Systems and Neural Networks,Statistics - Machine Learning}
}

@article{ling_decoar_2020,
  title = {{{DeCoAR}} 2.0: {{Deep Contextualized Acoustic Representations}} with {{Vector Quantization}}},
  shorttitle = {{{DeCoAR}} 2.0},
  author = {Ling, Shaoshi and Liu, Yuzong},
  year = {2020},
  month = dec,
  journal = {arXiv:2012.06659 [cs, eess]},
  eprint = {2012.06659},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {Recent success in speech representation learning enables a new way to leverage unlabeled data to train speech recognition model. In speech representation learning, a large amount of unlabeled data is used in a self-supervised manner to learn a feature representation. Then a smaller amount of labeled data is used to train a downstream ASR system using the new feature representations. Based on our previous work DeCoAR and inspirations from other speech representation learning, we propose DeCoAR 2.0, a Deep Contextualized Acoustic Representation with vector quantization. We introduce several modifications over the DeCoAR: first, we use Transformers in encoding module instead of LSTMs; second, we introduce a vector quantization layer between encoder and reconstruction modules; third, we propose an objective that combines the reconstructive loss with vector quantization diversity loss to train speech representations. Our experiments show consistent improvements over other speech representations in different data-sparse scenarios. Without fine-tuning, a light-weight ASR model trained on 10 hours of LibriSpeech labeled data with DeCoAR 2.0 features outperforms the model trained on the full 960-hour dataset with filterbank features.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@inproceedings{ling_deep_2020,
  title = {Deep {{Contextualized Acoustic Representations}} for {{Semi-Supervised Speech Recognition}}},
  booktitle = {{{ICASSP}} 2020 - 2020 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Ling, Shaoshi and Liu, Yuzong and Salazar, Julian and Kirchhoff, Katrin},
  year = {2020},
  month = may,
  pages = {6429--6433},
  issn = {2379-190X},
  abstract = {We propose a novel approach to semi-supervised automatic speech recognition (ASR). We first exploit a large amount of unlabeled audio data via representation learning, where we reconstruct a temporal slice of filterbank features from past and future context frames. The resulting deep contextualized acoustic representations (DeCoAR) are then used to train a CTC-based end-to-end ASR system using a smaller amount of labeled audio data. In our experiments, we show that systems trained on DeCoAR consistently outperform ones trained on conventional filterbank features, giving 42\% and 19\% relative improvement over the baseline on WSJ eval92 and LibriSpeech test-clean, respectively. Our approach can drastically reduce the amount of labeled data required; unsupervised training on LibriSpeech then supervision with 100 hours of labeled data achieves performance on par with training on all 960 hours directly.},
  keywords = {acoustic representation learning,Acoustics,Automatic speech recognition,Filter banks,semi-supervised learning,Semisupervised learning,Signal processing,Speech processing,speech recognition,Training}
}

@article{lippe_categorical_2021,
  title = {Categorical {{Normalizing Flows}} via {{Continuous Transformations}}},
  author = {Lippe, Phillip and Gavves, Efstratios},
  year = {2021},
  pages = {27},
  abstract = {Despite their popularity, to date, the application of normalizing flows on categorical data stays limited. The current practice of using dequantization to map discrete data to a continuous space is inapplicable as categorical data has no intrinsic order. Instead, categorical data have complex and latent relations that must be inferred, like the synonymy between words. In this paper, we investigate Categorical Normalizing Flows, that is normalizing flows for categorical data. By casting the encoding of categorical data in continuous space as a variational inference problem, we jointly optimize the continuous representation and the model likelihood. Using a factorized decoder, we introduce an inductive bias to model any interactions in the normalizing flow. As a consequence, we do not only simplify the optimization compared to having a joint decoder, but also make it possible to scale up to a large number of categories that is currently impossible with discrete normalizing flows. Based on Categorical Normalizing Flows, we propose GraphCNF a permutationinvariant generative model on graphs. GraphCNF implements a three-step approach modeling the nodes, edges, and adjacency matrix stepwise to increase efficiency. On molecule generation, GraphCNF outperforms both one-shot and autoregressive flow-based state-of-the-art.},
  langid = {english}
}

@inproceedings{liu_deep_2015,
  title = {Deep {{Learning Face Attributes}} in the {{Wild}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
  year = {2015}
}

@inproceedings{liu_efficient_2018,
  title = {Efficient {{Low-rank Multimodal Fusion With Modality-Specific Factors}}},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Liu, Zhun and Shen, Ying and Lakshminarasimhan, Varun Bharadhwaj and Liang, Paul Pu and Bagher Zadeh, AmirAli and Morency, Louis-Philippe},
  year = {2018},
  pages = {2247--2256},
  publisher = {{Association for Computational Linguistics}},
  address = {{Melbourne, Australia}},
  abstract = {Multimodal research is an emerging field of artificial intelligence, and one of the main research problems in this field is multimodal fusion. The fusion of multimodal data is the process of integrating multiple unimodal representations into one compact multimodal representation. Previous research in this field has exploited the expressiveness of tensors for multimodal representation. However, these methods often suffer from exponential increase in dimensions and in computational complexity introduced by transformation of input into tensor. In this paper, we propose the Lowrank Multimodal Fusion method, which performs multimodal fusion using low-rank tensors to improve efficiency. We evaluate our model on three different tasks: multimodal sentiment analysis, speaker trait analysis, and emotion recognition. Our model achieves competitive results on all these tasks while drastically reducing computational complexity. Additional experiments also show that our model can perform robustly for a wide range of low-rank settings, and is indeed much more efficient in both training and inference compared to other methods that utilize tensor representations.},
  langid = {english}
}

@misc{liu_energy-based_2020,
  title = {Energy-Based {{Out-of-distribution Detection}}},
  author = {Liu, Weitang and Wang, Xiaoyun and Owens, John D. and Li, Yixuan},
  year = {2020},
  month = oct,
  eprint = {2010.03759},
  eprinttype = {arxiv},
  abstract = {Determining whether inputs are out-of-distribution (OOD) is an essential building block for safely deploying machine learning models in the open world. However, previous methods relying on the softmax confidence score suffer from overconfident posterior distributions for OOD data. We propose a unified framework for OOD detection that uses an energy score. We show that energy scores better distinguish in- and out-of-distribution samples than the traditional approach using the softmax scores. Unlike softmax confidence scores, energy scores are theoretically aligned with the probability density of the inputs and are less susceptible to the overconfidence issue. Within this framework, energy can be flexibly used as a scoring function for any pre-trained neural classifier as well as a trainable cost function to shape the energy surface explicitly for OOD detection. On a CIFAR-10 pre-trained WideResNet, using the energy score reduces the average FPR (at TPR 95\%) by 18.03\% compared to the softmax confidence score. With energy-based training, our method outperforms the state-of-the-art on common benchmarks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@article{liu_gated_2017,
  title = {Gated {{End-to-End Memory Networks}}},
  author = {Liu, Fei and Perez, Julien},
  year = {2017},
  journal = {Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics (EACL)},
  volume = {1},
  eprint = {1610.04211},
  eprinttype = {arxiv},
  pages = {1--10},
  abstract = {Machine reading using differentiable rea-soning models has recently shown re-markable progress. In this context, End-to-End trainable Memory Networks (MemN2N) have demonstrated promising performance on simple natural language based reasoning tasks such as factual rea-soning and basic deduction. However, other tasks, namely multi-fact question-answering, positional reasoning or dialog related tasks, remain challenging particu-larly due to the necessity of more com-plex interactions between the memory and controller modules composing this family of models. In this paper, we introduce a novel end-to-end memory access regu-lation mechanism inspired by the current progress on the connection short-cutting principle in the field of computer vision. Concretely, we develop a Gated End-to-End trainable Memory Network architec-ture (GMemN2N). From the machine learn-ing perspective, this new capability is learned in an end-to-end fashion without the use of any additional supervision sig-nal which is, as far as our knowledge goes, the first of its kind. Our experi-ments show significant improvements on the most challenging tasks in the 20 bAbI dataset, without the use of any domain knowledge. Then, we show improvements on the Dialog bAbI tasks including the real human-bot conversion-based Di-alog State Tracking Challenge (DSTC-2) dataset. On these two datasets, our model sets the new state of the art.},
  archiveprefix = {arXiv},
  isbn = {0896-6273},
  pmid = {26774160},
  keywords = {Memory networks,Natural Language Processing,Question answering}
}

@misc{liu_gram-ctc:_2017,
  title = {Gram-{{CTC}}: {{Automatic Unit Selection}} and {{Target Decomposition}} for {{Sequence Labelling}}},
  shorttitle = {Gram-{{CTC}}},
  author = {Liu, Hairong and Zhu, Zhenyao and Li, Xiangang and Satheesh, Sanjeev},
  year = {2017},
  month = feb,
  eprint = {1703.00096},
  eprinttype = {arxiv},
  abstract = {Most existing sequence labelling models rely on a fixed decomposition of a target sequence into a sequence of basic units. These methods suffer from two major drawbacks: 1) the set of basic units is fixed, such as the set of words, characters or phonemes in speech recognition, and 2) the decomposition of target sequences is fixed. These drawbacks usually result in sub-optimal performance of modeling sequences. In this pa- per, we extend the popular CTC loss criterion to alleviate these limitations, and propose a new loss function called Gram-CTC. While preserving the advantages of CTC, Gram-CTC automatically learns the best set of basic units (grams), as well as the most suitable decomposition of tar- get sequences. Unlike CTC, Gram-CTC allows the model to output variable number of characters at each time step, which enables the model to capture longer term dependency and improves the computational efficiency. We demonstrate that the proposed Gram-CTC improves CTC in terms of both performance and efficiency on the large vocabulary speech recognition task at multiple scales of data, and that with Gram-CTC we can outperform the state-of-the-art on a standard speech benchmark.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@article{liu_masked_2020,
  title = {Masked {{Pre-trained Encoder}} Base on {{Joint CTC-Transformer}}},
  author = {Liu, Lu and Huang, Yiheng},
  year = {2020},
  month = aug,
  journal = {arXiv:2005.11978 [eess]},
  eprint = {2005.11978},
  eprinttype = {arxiv},
  primaryclass = {eess},
  abstract = {This study (The work was accomplished during the internship in Tencent AI lab) addresses semi-supervised acoustic modeling, i.e. attaining high-level representations from unsupervised audio data and fine-tuning the parameters of pre-trained model with supervised data. The proposed approach adopts a two-stage training framework, consisting of masked pre-trained encoder (MPE) and Joint CTC-Transformer (JCT). In the MPE framework, part of input frames are masked and reconstructed after the encoder with massive unsupervised data. In JCT framework, compared with original Transformer, acoustic features are applied as input instead of plain text. CTC loss performs as the prediction target on top of the encoder, and decoder blocks remain unchanged. This paper presents a comparison between two-stage training method and the fully supervised JCT. In addition, this paper investigates the our approach's robustness against different volumns of training data. Experiments on the two-stage training method deliver much better performance than fully supervised model. The word error rate (WER) with two-stage training which only exploits 30\textbackslash\% of WSJ labeled data achieves 17\textbackslash\% reduction than which trained by 50\textbackslash\% of WSJ in a fully supervised way. Moreover, increasing unlabeled data for MPE from WSJ (81h) to Librispeech (960h) attains about 22\textbackslash\% WER reduction.},
  archiveprefix = {arXiv},
  keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@inproceedings{liu_mockingjay_2020,
  title = {Mockingjay: {{Unsupervised Speech Representation Learning}} with {{Deep Bidirectional Transformer Encoders}}},
  shorttitle = {Mockingjay},
  booktitle = {{{ICASSP}} 2020 - 2020 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Liu, Andy T. and Yang, Shu-wen and Chi, Po-Han and Hsu, Po-chun and Lee, Hung-yi},
  year = {2020},
  month = may,
  pages = {6419--6423},
  issn = {2379-190X},
  abstract = {We present Mockingjay as a new speech representation learning approach, where bidirectional Transformer encoders are pre-trained on a large amount of unlabeled speech. Previous speech representation methods learn through conditioning on past frames and predicting information about future frames. Whereas Mockingjay is designed to predict the current frame through jointly conditioning on both past and future contexts. The Mockingjay representation improves performance for a wide range of downstream tasks, including phoneme classification, speaker recognition, and sentiment classification on spoken content, while outperforming other approaches. Mockingjay is empirically powerful and can be fine-tuned with downstream models, with only 2 epochs we further improve performance dramatically. In a low resource setting with only 0.1\% of labeled data, we outperform the result of Mel-features that uses all 100\% labeled data.},
  keywords = {Acoustics,Conferences,low resource,Phonetics,Signal processing,Speaker recognition,Speech processing,speech representation learning,Task analysis,transformer encoders,unsupervised training}
}

@article{liu_non-autoregressive_2020,
  title = {Non-{{Autoregressive Predictive Coding}} for {{Learning Speech Representations}} from {{Local Dependencies}}},
  author = {Liu, Alexander H. and Chung, Yu-An and Glass, James},
  year = {2020},
  month = oct,
  journal = {arXiv:2011.00406 [cs]},
  eprint = {2011.00406},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Self-supervised speech representations have been shown to be effective in a variety of speech applications. However, existing representation learning methods generally rely on the autoregressive model and/or observed global dependencies while generating the representation. In this work, we propose Non-Autoregressive Predictive Coding (NPC), a self-supervised method, to learn a speech representation in a non-autoregressive manner by relying only on local dependencies of speech. NPC has a conceptually simple objective and can be implemented easily with the introduced Masked Convolution Blocks. NPC offers a significant speedup for inference since it is parallelizable in time and has a fixed inference time for each time step regardless of the input sequence length. We discuss and verify the effectiveness of NPC by theoretically and empirically comparing it with other methods. We show that the NPC representation is comparable to other methods in speech experiments on phonetic and speaker classification while being more efficient.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{liu_pay_2021,
  title = {Pay {{Attention}} to {{MLPs}}},
  author = {Liu, Hanxiao and Dai, Zihang and So, David R. and Le, Quoc V.},
  year = {2021},
  month = may,
  eprint = {2105.08050},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Transformers [1] have become one of the most important architectural innovations in deep learning and have enabled many breakthroughs over the past few years. Here we propose a simple attention-free network architecture, gMLP, based solely on MLPs with gating, and show that it can perform as well as Transformers in key language and vision applications. Our comparisons show that self-attention is not critical for Vision Transformers, as gMLP can achieve the same accuracy. For BERT, our model achieves parity with Transformers on pretraining perplexity and is better on some downstream tasks. On finetuning tasks where gMLP performs worse, making the gMLP model substantially larger can close the gap with Transformers. In general, our experiments show that gMLP can scale as well as Transformers over increased data and compute.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@misc{liu_roberta:_2019,
  title = {{{RoBERTa}}: {{A Robustly Optimized BERT Pretraining Approach}}},
  shorttitle = {{{RoBERTa}}},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2019},
  month = jul,
  eprint = {1907.11692},
  eprinttype = {arxiv},
  abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language}
}

@inproceedings{liu_ssd:_2016,
  title = {{{SSD}}: {{Single}} Shot Multibox Detector},
  booktitle = {European {{Conference}} on {{Computer Vision}}},
  author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
  year = {2016},
  pages = {21--37}
}

@article{liu_tera_2021,
  title = {{{TERA}}: {{Self-Supervised Learning}} of {{Transformer Encoder Representation}} for {{Speech}}},
  shorttitle = {{{TERA}}},
  author = {Liu, Andy T. and Li, Shang-Wen and Lee, Hung-yi},
  year = {2021},
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {2351--2366},
  issn = {2329-9304},
  abstract = {We introduce a self-supervised speech pre-training method called TERA, which stands for Transformer Encoder Representations from Alteration. Recent approaches often learn by using a single auxiliary task like contrastive prediction, autoregressive prediction, or masked reconstruction. Unlike previous methods, we use alteration along three orthogonal axes to pre-train Transformer Encoders on a large amount of unlabeled speech. The model learns through the reconstruction of acoustic frames from their altered counterpart, where we use a stochastic policy to alter along various dimensions: time, frequency, and magnitude. TERA can be used for speech representations extraction or fine-tuning with downstream models. We evaluate TERA on several downstream tasks, including phoneme classification, keyword spotting, speaker recognition, and speech recognition. We present a large-scale comparison of various self-supervised models. TERA achieves strong performance in the comparison by improving upon surface features and outperforming previous models. In our experiments, we study the effect of applying different alteration techniques, pre-training on more data, and pre-training on various features. We analyze different model sizes and find that smaller models are strong representation learners than larger models, while larger models are more effective for downstream fine-tuning than smaller models. Furthermore, we show the proposed method is transferable to downstream datasets not used in pre-training.},
  keywords = {Acoustics,Bit error rate,Data models,pre-training,Predictive models,representation,Self-supervised,Speech processing,Task analysis,Training}
}

@inproceedings{liu_towards_2020,
  title = {Towards {{Unsupervised Speech Recognition}} and {{Synthesis}} with {{Quantized Speech Representation Learning}}},
  booktitle = {{{ICASSP}} 2020 - 2020 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Liu, Alexander H. and Tu, Tao and Lee, Hung-yi and Lee, Lin-shan},
  year = {2020},
  month = may,
  pages = {7259--7263},
  issn = {2379-190X},
  abstract = {In this paper we propose a Sequential Representation Quantization AutoEncoder (SeqRQ-AE) to learn from primarily unpaired audio data and produce sequences of representations very close to phoneme sequences of speech utterances. This is achieved by proper temporal segmentation to make the representations phoneme-synchronized, and proper phonetic clustering to have total number of distinct representations close to the number of phonemes. Mapping between the distinct representations and phonemes is learned from a small amount of annotated paired data. Preliminary experiments on LJSpeech demonstrated the learned representations for vowels have relative locations in latent space in good parallel to that shown in the IPA vowel chart defined by linguistics experts. With less than 20 minutes of annotated speech, our method outperformed existing methods on phoneme recognition and is able to synthesize intelligible speech that beats our baseline model.},
  keywords = {Acoustics,Conferences,Phonetics,Quantization (signal),representation quantization,Speech processing,speech recognition,Speech recognition,speech representation,speech synthesis}
}

@misc{liu_variance_2019,
  title = {On the {{Variance}} of the {{Adaptive Learning Rate}} and {{Beyond}}},
  author = {Liu, Liyuan and Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Han, Jiawei},
  year = {2019},
  month = aug,
  eprint = {1908.03265},
  eprinttype = {arxiv},
  abstract = {The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Here, we study its mechanism in details. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate (i.e., it has problematically large variance in the early stage), suggest warmup works as a variance reduction technique, and provide both empirical and theoretical evidence to verify our hypothesis. We further propose RAdam, a new variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Extensive experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the effectiveness and robustness of our proposed method. All implementations are available at: https://github.com/LiyuanLucasLiu/RAdam.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{loaiza-ganem_continuous_2019,
  title = {The Continuous {{Bernoulli}}: Fixing a Pervasive Error in Variational Autoencoders},
  shorttitle = {The Continuous {{Bernoulli}}},
  author = {{Loaiza-Ganem}, Gabriel and Cunningham, John P.},
  year = {2019},
  month = dec,
  eprint = {1907.06845},
  eprinttype = {arxiv},
  abstract = {Variational autoencoders (VAE) have quickly become a central tool in machine learning, applicable to a broad range of data types and latent variable models. By far the most common first step, taken by seminal papers and by core software libraries alike, is to model MNIST data using a deep network parameterizing a Bernoulli likelihood. This practice contains what appears to be and what is often set aside as a minor inconvenience: the pixel data is [0,1] valued, not \{0,1\} as supported by the Bernoulli likelihood. Here we show that, far from being a triviality or nuisance that is convenient to ignore, this error has profound importance to VAE, both qualitative and quantitative. We introduce and fully characterize a new [0,1]-supported, single parameter distribution: the continuous Bernoulli, which patches this pervasive bug in VAE. This distribution is not nitpicking; it produces meaningful performance improvements across a range of metrics and datasets, including sharper image samples, and suggests a broader class of performant VAE.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{long_fully_2015,
  title = {Fully {{Convolutional Networks}} for {{Semantic Segmentation}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CCVPR}})},
  author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  year = {2015},
  eprint = {1411.4038},
  eprinttype = {arxiv},
  pages = {3431--3440},
  issn = {10636919},
  abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
  archiveprefix = {arXiv},
  isbn = {978-1-4673-6964-0},
  pmid = {16190471}
}

@misc{loshchilov_fixing_2017,
  title = {Fixing {{Weight Decay Regularization}} in {{Adam}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  year = {2017},
  month = nov,
  eprint = {1711.05101},
  eprinttype = {arxiv},
  abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \textbackslash emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common deep learning frameworks of these algorithms implement L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by decoupling the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam, and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). We also propose a version of Adam with warm restarts (AdamWR) that has strong anytime performance while achieving state-of-the-art results on CIFAR-10 and ImageNet32x32. Our source code is available at https://github.com/loshchil/AdamW-and-SGDW},
  archiveprefix = {arXiv},
  keywords = {★}
}

@misc{lucas_analyzing_2021,
  title = {Analyzing {{Monotonic Linear Interpolation}} in {{Neural Network Loss Landscapes}}},
  author = {Lucas, James and Bae, Juhan and Zhang, Michael R. and Fort, Stanislav and Zemel, Richard and Grosse, Roger},
  year = {2021},
  month = apr,
  eprint = {2104.11044},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Linear interpolation between initial neural network parameters and converged parameters after training with stochastic gradient descent (SGD) typically leads to a monotonic decrease in the training objective. This Monotonic Linear Interpolation (MLI) property, first observed by Goodfellow et al. (2014), persists in spite of the non-convex objectives and highly non-linear training dynamics of neural networks. Extending this work, we evaluate several hypotheses for this property that, to our knowledge, have not yet been explored. Using tools from differential geometry, we draw connections between the interpolated paths in function space and the monotonicity of the network \textemdash{} providing sufficient conditions for the MLI property under mean squared error. While the MLI property holds under various settings (e.g. network architectures and learning problems), we show in practice that networks violating the MLI property can be produced systematically, by encouraging the weights to move far from initialization. The MLI property raises important questions about the loss landscape geometry of neural networks and highlights the need to further study their global properties.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{lucas_dont_nodate,
  title = {Don't {{Blame}} the {{ELBO}}! {{A Linear VAE Perspective}} on {{Posterior Collapse}}},
  author = {Lucas, James and Tucker, George and Grosse, Roger B and Norouzi, Mohammad},
  pages = {11},
  abstract = {Posterior collapse in Variational Autoencoders (VAEs) arises when the variational posterior distribution closely matches the prior for a subset of latent variables. This paper presents a simple and intuitive explanation for posterior collapse through the analysis of linear VAEs and their direct correspondence with Probabilistic PCA (pPCA). We explain how posterior collapse may occur in pPCA due to local maxima in the log marginal likelihood. Unexpectedly, we prove that the ELBO objective for the linear VAE does not introduce additional spurious local maxima relative to log marginal likelihood. We show further that training a linear VAE with exact variational inference recovers an identifiable global maximum corresponding to the principal component directions. Empirically, we find that our linear analysis is predictive even for high-capacity, non-linear VAEs and helps explain the relationship between the observation noise, local maxima, and posterior collapse in deep Gaussian VAEs.},
  langid = {english}
}

@article{lundberg_unified_nodate,
  title = {A {{Unified Approach}} to {{Interpreting Model Predictions}}},
  author = {Lundberg, Scott M and Lee, Su-In},
  pages = {10},
  abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
  langid = {english}
}

@article{luo_conv-tasnet_2019,
  title = {Conv-{{TasNet}}: {{Surpassing Ideal Time-Frequency Magnitude Masking}} for {{Speech Separation}}},
  author = {Luo, Yi and Mesgarani, Nima},
  year = {2019},
  journal = {IEEE ACM Trans. Audio Speech Lang. Process.},
  volume = {27},
  number = {8},
  pages = {1256--1266}
}

@article{luo_towards_2018,
  title = {Towards {{Understanding Regularization}} in {{Batch Normalization}}},
  author = {Luo, Ping and Wang, Xinjiang and Shao, Wenqi and Peng, Zhanglin},
  year = {2018},
  month = sep,
  abstract = {Batch Normalization (BN) improves both convergence and generalization in training neural networks. This work understands these phenomena theoretically. We analyze BN by using a basic block of neural networks, consisting of a kernel layer, a BN layer, and a nonlinear activation function. This basic network helps us understand the impacts of BN in three aspects. First, by viewing BN as an implicit regularizer, BN can be decomposed into population normalization (PN) and gamma decay as an explicit regularization. Second, learning dynamics of BN and the regularization show that training converged with large maximum and effective learning rate. Third, generalization of BN is explored by using statistical mechanics. Experiments demonstrate that BN in convolutional neural networks share the same traits of regularization as the above analyses.},
  langid = {english}
}

@misc{luong_achieving_2016,
  title = {Achieving {{Open Vocabulary Neural Machine Translation}} with {{Hybrid Word-Character Models}}},
  author = {Luong, Minh-Thang and Manning, Christopher D.},
  year = {2016},
  eprint = {1604.00788},
  eprinttype = {arxiv},
  abstract = {Nearly all previous work on neural machine translation (NMT) has used quite restricted vocabularies, perhaps with a subsequent method to patch in unknown words. This paper presents a novel word-character solution to achieving open vocabulary NMT. We build hybrid systems that translate mostly at the word level and consult the character components for rare words. Our character-level recurrent neural networks compute source word representations and recover unknown target words when needed. The twofold advantage of such a hybrid approach is that it is much faster and easier to train than character-based ones; at the same time, it never produces unknown words as in the case of word-based models. On the WMT'15 English to Czech translation task, this hybrid approach offers an addition boost of +2.1-11.4 BLEU points over models that already handle unknown words. Our best system achieves a new state-of-the-art result with 20.7 BLEU score. We demonstrate that our character models can successfully learn to not only generate well-formed words for Czech, a highly-inflected language with a very complex vocabulary, but also build correct representations for English source words.},
  archiveprefix = {arXiv},
  isbn = {9781510827585}
}

@misc{luong_effective_2015,
  title = {Effective Approaches to Attention-Based Neural Machine Translation},
  author = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D.},
  year = {2015},
  eprint = {1508.04025},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@misc{ly_tutorial_2017,
  title = {A {{Tutorial}} on {{Fisher Information}}},
  author = {Ly, Alexander and Marsman, Maarten and Verhagen, Josine and Grasman, Raoul and Wagenmakers, Eric-Jan},
  year = {2017},
  month = oct,
  eprint = {1705.01064},
  eprinttype = {arxiv},
  abstract = {In many statistical applications that concern mathematical psychologists, the concept of Fisher information plays an important role. In this tutorial we clarify the concept of Fisher information as it manifests itself across three different statistical paradigms. First, in the frequentist paradigm, Fisher information is used to construct hypothesis tests and confidence intervals using maximum likelihood estimators; second, in the Bayesian paradigm, Fisher information is used to define a default prior; lastly, in the minimum description length paradigm, Fisher information is used to measure model complexity.},
  archiveprefix = {arXiv},
  keywords = {Confidence intervals,Hypothesis testing,Jeffreys's prior,Minimum description length,Model complexity,model selec-tion,Statistical modeling}
}

@article{lydia_linear_2016,
  title = {Linear and Non-Linear Autoregressive Models for Short-Term Wind Speed Forecasting},
  author = {Lydia, M. and Suresh Kumar, S. and Immanuel Selvakumar, A. and Edwin Prem Kumar, G.},
  year = {2016},
  journal = {Energy Conversion and Management},
  volume = {112},
  pages = {115--124},
  issn = {01968904},
  abstract = {Wind speed forecasting aids in estimating the energy produced from wind farms. The soaring energy demands of the world and minimal availability of conventional energy sources have significantly increased the role of non-conventional sources of energy like solar, wind, etc. Development of models for wind speed forecasting with higher reliability and greater accuracy is the need of the hour. In this paper, models for predicting wind speed at 10-min intervals up to 1 h have been built based on linear and non-linear autoregressive moving average models with and without external variables. The autoregressive moving average models based on wind direction and annual trends have been built using data obtained from Sotavento Galicia Plc. and autoregressive moving average models based on wind direction, wind shear and temperature have been built on data obtained from Centre for Wind Energy Technology, Chennai, India. While the parameters of the linear models are obtained using the Gauss-Newton algorithm, the non-linear autoregressive models are developed using three different data mining algorithms. The accuracy of the models has been measured using three performance metrics namely, the Mean Absolute Error, Root Mean Squared Error and Mean Absolute Percentage Error.},
  keywords = {Auto-regressive moving average,Data mining,Multivariate models,Time-series forecasting,Wind shear}
}

@inproceedings{maaloe_auxiliary_2016,
  title = {Auxiliary Deep Generative Models},
  booktitle = {Proceedings of the 33rd {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Maal{\o}e, Lars and S{\o}nderby, Casper Kaae and S{\o}nderby, S{\o}ren Kaae and Winther, Ole},
  editor = {Balcan, Maria Florina and Weinberger, Kilian Q.},
  year = {2016},
  month = jun,
  series = {Proceedings of Machine Learning Research},
  volume = {48},
  pages = {1445--1453},
  publisher = {{PMLR}},
  address = {{New York, New York, USA}},
  abstract = {Deep generative models parameterized by neural networks have recently achieved state-of-the-art performance in unsupervised and semi-supervised learning. We extend deep generative models with auxiliary variables which improves the variational approximation. The auxiliary variables leave the generative model unchanged but make the variational distribution more expressive. Inspired by the structure of the auxiliary variable we also propose a model with two stochastic layers and skip connections. Our findings suggest that more expressive and properly specified deep generative models converge faster with better results. We show state-of-the-art performance within semi-supervised learning on MNIST, SVHN and NORB datasets.}
}

@inproceedings{maaloe_biva_2019,
  title = {{{BIVA}}: {{A Very Deep Hierarchy}} of {{Latent Variables}} for {{Generative Modeling}}},
  shorttitle = {{{BIVA}}},
  booktitle = {Proceedings of the 32nd {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Maal{\o}e, Lars and Fraccaro, Marco and Li{\'e}vin, Valentin and Winther, Ole},
  year = {2019},
  month = feb,
  pages = {6548--6558},
  address = {{Vancouver, Canada}},
  abstract = {With the introduction of the variational autoencoder (VAE), probabilistic latent variable models have received renewed attention as powerful generative models. However, their performance in terms of test likelihood and quality of generated samples has been surpassed by autoregressive models without stochastic units. Furthermore, flow-based models have recently been shown to be an attractive alternative that scales well to high-dimensional data. In this paper we close the performance gap by constructing VAE models that can effectively utilize a deep hierarchy of stochastic variables and model complex covariance structures. We introduce the Bidirectional-Inference Variational Autoencoder (BIVA), characterized by a skip-connected generative model and an inference network formed by a bidirectional stochastic inference path. We show that BIVA reaches state-of-the-art test likelihoods, generates sharp and coherent natural images, and uses the hierarchy of latent variables to capture different aspects of the data distribution. We observe that BIVA, in contrast to recent results, can be used for anomaly detection. We attribute this to the hierarchy of latent variables which is able to extract high-level semantic features. Finally, we extend BIVA to semi-supervised classification tasks and show that it performs comparably to state-of-the-art results by generative adversarial networks.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{maaloe_semi-supervised_2017,
  title = {Semi-{{Supervised Generation}} with {{Cluster-aware Generative Models}}},
  author = {Maal{\o}e, Lars and Fraccaro, Marco and Winther, Ole},
  year = {2017},
  month = apr,
  eprint = {1704.00637},
  eprinttype = {arxiv},
  abstract = {Deep generative models trained with large amounts of unlabelled data have proven to be powerful within the domain of unsupervised learning. Many real life data sets contain a small amount of labelled data points, that are typically disregarded when training generative models. We propose the Cluster-aware Generative Model, that uses unlabelled information to infer a latent representation that models the natural clustering of the data, and additional labelled data points to refine this clustering. The generative performances of the model significantly improve when labelled information is exploited, obtaining a log-likelihood of -79.38 nats on permutation invariant MNIST, while also achieving competitive semi-supervised classification accuracies. The model can also be trained fully unsupervised, and still improve the log-likelihood performance with respect to related methods.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@book{mackay_information_2003,
  title = {Information Theory, Inference, and Learning Algorithms},
  author = {MacKay, David J. C.},
  year = {2003},
  edition = {First},
  publisher = {{Cambridge University Press}},
  abstract = {Information theory and inference, often taught separately, are here united in one entertaining textbook. These topics lie at the heart of many exciting areas of contemporary science and engineering -- communication, signal processing, data mining, machine learning, pattern recognition, computational neuroscience, bioinformatics, and cryptography. This textbook introduces theory in tandem with applications. Information theory is taught alongside practical communication systems, such as arithmetic coding for data compression and sparse-graph codes for error-correction. A toolbox of inference techniques, including message-passing algorithms, Monte Carlo methods, and variational approximations, are developed alongside applications of these tools to clustering, convolutional codes, independent component analysis, and neural networks. The final part of the book describes the state of the art in error-correcting codes, including low-density parity-check codes, turbo codes, and digital fountain codes -- the twentyfirst-century standards for satellite communications, disk drives, and data broadcast. Richly illustrated, filled with worked examples and over 400 exercises, some with detailed solutions, the book is ideal for self-learning and for undergraduate or graduate courses. Interludes on crosswords, evolution, and sex provide entertainment along the way. The result is a textbook on information, communication, and coding for a new generation of students, and an unparalleled entry point into these subjects for professionals in areas as diverse as computational biology, financial engineering, and machine learning.},
  isbn = {978-0-521-64298-9}
}

@article{mackay_practical_1992,
  title = {A {{Practical Bayesian Framework}} for {{Backpropagation Networks}}},
  author = {MacKay, David J. C.},
  year = {1992},
  journal = {Neural Computation},
  volume = {4},
  number = {3},
  pages = {448--472},
  issn = {0899-7667},
  abstract = {A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks. The framework makes possible (1) objective comparisons between solutions using alternative network architectures, (2) objective stopping rules for network pruning or growing procedures, (3) objective choice of magnitude and type of weight decay terms or additive regularizers (for penalizing large weights, etc.), (4) a measure of the effective number of well-determined parameters in a model, (5) quantified estimates of the error bars on network parameters and on network output, and (6) objective comparisons with alternative learning and interpolation models such as splines and radial basis functions. The Bayesian "evidence" automatically embodies "Occam's razor," penalizing overflexible and overcomplex models. The Bayesian approach helps detect poor underlying assumptions in learning models. For learning models well matched to a problem, a good correlation between generalization ability and the Bayesian evidence is obtained.},
  isbn = {0899-7667}
}

@inproceedings{maddison_concrete_2017,
  title = {The Concrete Distribution: {{A}} Continuous Relaxation of Discrete Random Variables},
  booktitle = {Proceedings of the 5th {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Maddison, Chris J. and Mnih, Andriy and Teh, Yee Whye},
  year = {2017},
  address = {{Toulon, France}}
}

@misc{maddison_hamiltonian_2018,
  title = {Hamiltonian {{Descent Methods}}},
  author = {Maddison, Chris J. and Paulin, Daniel and Teh, Yee Whye and O'Donoghue, Brendan and Doucet, Arnaud},
  year = {2018},
  month = sep,
  eprint = {1809.05042},
  eprinttype = {arxiv},
  abstract = {We propose a family of optimization methods that achieve linear convergence using first-order gradient information and constant step sizes on a class of convex functions much larger than the smooth and strongly convex ones. This larger class includes functions whose second derivatives may be singular or unbounded at their minima. Our methods are discretizations of conformal Hamiltonian dynamics, which generalize the classical momentum method to model the motion of a particle with non-standard kinetic energy exposed to a dissipative force and the gradient field of the function of interest. They are first-order in the sense that they require only gradient computation. Yet, crucially the kinetic gradient map can be designed to incorporate information about the convex conjugate in a fashion that allows for linear convergence on convex functions that may be non-smooth or non-strongly convex. We study in detail one implicit and two explicit methods. For one explicit method, we provide conditions under which it converges to stationary points of non-convex functions. For all, we provide conditions on the convex function and kinetic energy pair that guarantee linear convergence, and show that these conditions can be satisfied by functions with power growth. In sum, these methods expand the class of convex functions on which linear convergence is possible with first-order computation.},
  archiveprefix = {arXiv},
  keywords = {First order methods,Gradient descent,Hamiltonian}
}

@misc{maddox_simple_2019,
  title = {A {{Simple Baseline}} for {{Bayesian Uncertainty}} in {{Deep Learning}}},
  author = {Maddox, Wesley and Garipov, Timur and Izmailov, Pavel and Vetrov, Dmitry and Wilson, Andrew Gordon},
  year = {2019},
  month = dec,
  eprint = {1902.02476},
  eprinttype = {arxiv},
  abstract = {We propose SWA-Gaussian (SWAG), a simple, scalable, and general purpose approach for uncertainty representation and calibration in deep learning. Stochastic Weight Averaging (SWA), which computes the first moment of stochastic gradient descent (SGD) iterates with a modified learning rate schedule, has recently been shown to improve generalization in deep learning. With SWAG, we fit a Gaussian using the SWA solution as the first moment and a low rank plus diagonal covariance also derived from the SGD iterates, forming an approximate posterior distribution over neural network weights; we then sample from this Gaussian distribution to perform Bayesian model averaging. We empirically find that SWAG approximates the shape of the true posterior, in accordance with results describing the stationary distribution of SGD iterates. Moreover, we demonstrate that SWAG performs well on a wide variety of tasks, including out of sample detection, calibration, and transfer learning, in comparison to many popular alternatives including MC dropout, KFAC Laplace, SGLD, and temperature scaling.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@book{madsen_time_2008,
  title = {Time {{Series Analysis}}},
  author = {Madsen, Henrik},
  year = {2008},
  edition = {First},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Copenhagen}},
  isbn = {978-1-4200-5967-0},
  keywords = {Time series analysis}
}

@article{magdon-ismail_approximating_2010,
  title = {Approximating the {{Covariance Matrix}} with {{Low-rank Perturbations}}},
  author = {{Magdon-Ismail}, Malik and Purnell, Jonathan T.},
  year = {2010},
  journal = {Intelligent Data Engineering and Automated Learning (IDEAL)},
  pages = {300--307},
  abstract = {Covariance matrices capture correlations that are invaluable in mod-eling real-life datasets. Using all d 2 elements of the covariance (in d dimensions) is costly and could result in over-fitting; and the simple diagonal approximation can be over-restrictive. We present an algorithm that improves upon the diagonal matrix by allowing a low rank perturbation. The efficiency is comparable to the diagonal approximation, yet one can capture correlations among the dimensions. We show that this method outperforms the diagonal when training GMMs on both synthetic and real-world data.},
  keywords = {EM,Gaussian mixture models,Maximum likelihood}
}

@article{magnus_matrix_1985,
  title = {Matrix Differential Calculus with Applications to Simple, Hadamard, and Kronecker Products},
  author = {Magnus, Jan R. and Neudecker, H.},
  year = {1985},
  month = dec,
  journal = {Journal of Mathematical Psychology},
  volume = {29},
  number = {4},
  pages = {474--492},
  issn = {00222496},
  langid = {english}
}

@misc{makhzani_adversarial_2015,
  title = {Adversarial {{Autoencoders}}},
  author = {Makhzani, Alireza and Shlens, Jonathon and Jaitly, Navdeep and Goodfellow, Ian and Frey, Brendan},
  year = {2015},
  month = nov,
  eprint = {1511.05644},
  eprinttype = {arxiv},
  abstract = {In this paper, we propose the "adversarial autoencoder" (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how the adversarial autoencoder can be used in applications such as semi-supervised classification, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks.},
  archiveprefix = {arXiv},
  isbn = {9781509008063}
}

@article{malykh_robust_2018,
  title = {Robust {{Word Vectors}}: {{Context-Informed Embeddings}} for {{Noisy Texts}}},
  author = {Malykh, Valentin and Khakhulin, Taras and Logacheva, Varvara},
  year = {2018},
  pages = {10},
  abstract = {We suggest a new language-independent architecture of robust word vectors (RoVe). It is designed to alleviate the issue of typos, which are common in almost any user-generated content, and hinder automatic text processing. Our model is morphologically motivated, which allows it to deal with unseen word forms in morphologically rich languages. We present the results on a number of Natural Language Processing (NLP) tasks and languages for the variety of related architectures and show that proposed architecture is typo-proof.},
  langid = {english}
}

@misc{mania_simple_2018,
  title = {Simple Random Search Provides a Competitive Approach to Reinforcement Learning},
  author = {Mania, Horia and Guy, Aurelia and Recht, Benjamin},
  year = {2018},
  eprint = {1803.07055},
  eprinttype = {arxiv},
  abstract = {A common belief in model-free reinforcement learning is that methods based on random search in the parameter space of policies exhibit significantly worse sample complexity than those that explore the space of actions. We dispel such beliefs by introducing a random search method for training static, linear policies for continuous control problems, matching state-of-the-art sample efficiency on the benchmark MuJoCo locomotion tasks. Our method also finds a nearly optimal controller for a challenging instance of the Linear Quadratic Regulator, a classi-cal problem in control theory, when the dynamics are not known. Computationally, our random search algorithm is at least 15 times more efficient than the fastest competing model-free meth-ods on these benchmarks. We take advantage of this computational efficiency to evaluate the performance of our method over hundreds of random seeds and many different hyperparam-eter configurations for each benchmark task. Our simulations highlight a high variability in performance in these benchmark tasks, suggesting that commonly used estimations of sample efficiency do not adequately evaluate the performance of RL algorithms.},
  archiveprefix = {arXiv}
}

@article{mann_test_1947,
  title = {On a {{Test}} of {{Whether}} One of {{Two Random Variables}} Is {{Stochastically Larger}} than the {{Other}}},
  author = {Mann, H. B. and Whitney, D. R.},
  year = {1947},
  journal = {The Annals of Mathematical Statistics},
  volume = {18},
  number = {1},
  pages = {50--60},
  issn = {0003-4851},
  abstract = {The object of this note is to point out and discuss a simple transformation of an absolutely continuous k-variate distribution in the uniform distribution on the k-dimensional hypercube.},
  pmid = {10226}
}

@misc{mao_deep_2014,
  title = {Deep {{Captioning}} with {{Multimodal Recurrent Neural Networks}} (m-{{RNN}})},
  author = {Mao, Junhua and Xu, Wei and Yang, Yi and Wang, Jiang and Huang, Zhiheng and Yuille, Alan L.},
  year = {2014},
  eprint = {1412.6632},
  eprinttype = {arxiv},
  abstract = {In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel image captions. It directly models the probability distribution of generating a word given previous words and an image. Image captions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on four benchmark datasets (IAPR TC-12, Flickr 8K, Flickr 30K and MS COCO). Our model outperforms the state-of-the-art methods. In addition, we apply the m-RNN model to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval. The project page of this work is: www.stat.ucla.edu/\textasciitilde junhua.mao/m-RNN.html .},
  archiveprefix = {arXiv},
  isbn = {1558608044}
}

@misc{marcus_deep_2018,
  title = {Deep {{Learning}}: {{A Critical Appraisal}}},
  author = {Marcus, Gary},
  year = {2018},
  eprint = {1801.00631},
  eprinttype = {arxiv},
  abstract = {Although deep learning has historical roots going back decades, neither the term "deep learning" nor the approach was popular just over five years ago, when the field was reignited by papers such as Krizhevsky, Sutskever and Hinton's now classic (2012) deep network model of Imagenet. What has the field discovered in the five subsequent years? Against a background of considerable progress in areas such as speech recognition, image recognition, and game playing, and considerable enthusiasm in the popular press, I present ten concerns for deep learning, and suggest that deep learning must be supplemented by other techniques if we are to reach artificial general intelligence.},
  archiveprefix = {arXiv}
}

@inproceedings{masegosa_learning_2020,
  title = {Learning under Model Misspecification: {{Applications}} to Variational and Ensemble Methods},
  booktitle = {Advances in Neural Information Processing Systems 33: {{Annual}} Conference on Neural Information Processing Systems 2020, {{NeurIPS}} 2020, December 6-12, 2020, Virtual},
  author = {Masegosa, Andr{\'e}s R.},
  editor = {Larochelle, Hugo and Ranzato, Marc'Aurelio and Hadsell, Raia and Balcan, Maria-Florina and Lin, Hsuan-Tien},
  year = {2020}
}

@misc{masters_revisiting_2018,
  title = {Revisiting {{Small Batch Training}} for {{Deep Neural Networks}}},
  author = {Masters, Dominic and Luschi, Carlo},
  year = {2018},
  eprint = {1804.07612},
  eprinttype = {arxiv},
  abstract = {Modern deep neural network training is typically based on mini-batch stochastic gradient optimization. While the use of large mini-batches increases the avail-able computational parallelism, small batch training has been shown to provide improved generalization performance and allows a significantly smaller memory footprint, which might also be exploited to improve machine throughput. In this paper, we review common assumptions on learning rate scaling and train-ing duration, as a basis for an experimental comparison of test performance for different mini-batch sizes. We adopt a learning rate that corresponds to a constant average weight update per gradient calculation (i.e., per unit cost of computation), and point out that this results in a variance of the weight updates that increases linearly with the mini-batch size m. The collected experimental results for the CIFAR-10, CIFAR-100 and ImageNet datasets show that increasing the mini-batch size progressively reduces the range of learning rates that provide stable convergence and acceptable test performance. On the other hand, small mini-batch sizes provide more up-to-date gradient cal-culations, which yields more stable and reliable training. The best performance has been consistently obtained for mini-batch sizes between m = 2 and m = 32, which contrasts with recent work advocating the use of mini-batch sizes in the thousands.},
  archiveprefix = {arXiv},
  keywords = {★,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{mathieu_disentangling_2019,
  title = {Disentangling {{Disentanglement}} in {{Variational Autoencoders}}},
  author = {Mathieu, Emile and Rainforth, Tom and Siddharth, N. and Teh, Yee Whye},
  year = {2019},
  month = jun,
  eprint = {1812.02833},
  eprinttype = {arxiv},
  abstract = {We develop a generalisation of disentanglement in variational autoencoders (VAEs)\textemdash decomposition of the latent representation\textemdash characterising it as the fulfilment of two factors: a) the latent encodings of the data having an appropriate level of overlap, and b) the aggregate encoding of the data conforming to a desired structure, represented through the prior. Decomposition permits disentanglement, i.e. explicit independence between latents, as a special case, but also allows for a much richer class of properties to be imposed on the learnt representation, such as sparsity, clustering, independent subspaces, or even intricate hierarchical dependency relationships. We show that the {$\beta$}-VAE varies from the standard VAE predominantly in its control of latent overlap and that for the standard choice of an isotropic Gaussian prior, its objective is invariant to rotations of the latent representation. Viewed from the decomposition perspective, breaking this invariance with simple manipulations of the prior can yield better disentanglement with little or no detriment to reconstructions. We further demonstrate how other choices of prior can assist in producing different decompositions and introduce an alternative training objective that allows the control of both decomposition factors in a principled manner.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{mattei_leveraging_2018,
  title = {Leveraging the Exact Likelihood of Deep Latent Variable Models},
  booktitle = {Proceedings of the 32nd {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Mattei, Pierre-Alexandre and Frellsen, Jes},
  year = {2018},
  pages = {3859--3870},
  address = {{Montreal, Canada}}
}

@inproceedings{mattei_miwae_2019,
  title = {{{MIWAE}}: {{Deep}} Generative Modelling and Imputation of Incomplete Data Sets},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Mattei, Pierre-Alexandre and Frellsen, Jes},
  year = {2019},
  pages = {4413--4423},
  address = {{Long Beach, CA, USA}}
}

@inproceedings{mattei_refit_2018,
  title = {Refit Your Encoder When New Data Comes By},
  booktitle = {3rd {{NeurIPS Workshop}} on {{Bayesian Deep Learning}}},
  author = {Mattei, Pierre-Alexandre and Frellsen, Jes},
  year = {2018}
}

@article{mazidi_infusing_2016,
  title = {Infusing {{NLU}} into {{Automatic Question Generation}}},
  author = {Mazidi, Karen and Tarau, Paul},
  year = {2016},
  journal = {Proceedings of the International Conference on Natural Language Generation (INLG)},
  number = {2011},
  pages = {51--60},
  abstract = {We present a fresh approach to automatic question generation that significantly in-creases the percentage of acceptable questions compared to prior state-of-the-art systems. In our evaluation of the top 20 questions, our sys-tem generated 71\% more acceptable questions by informing the generation process with Nat-ural Language Understanding techniques. The system also introduces our DeconStructure al-gorithm which creates an intuitive and prac-tical structure for easily accessing sentence functional constituents in NLP applications.}
}

@inproceedings{mcallester_formal_2020,
  title = {Formal Limitations on the Measurement of Mutual Information},
  booktitle = {Proceedings of the 23rd {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}} (Aistats)},
  author = {McAllester, David and Stratos, Karl},
  editor = {Chiappa, Silvia and Calandra, Roberto},
  year = {2020},
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {108},
  pages = {875--884},
  publisher = {{PMLR}},
  address = {{Palermo, Sicily, Italy}}
}

@article{mcauley_image-based_2015,
  title = {Image-Based {{Recommendations}} on {{Styles}} and {{Substitutes}}},
  author = {McAuley, Julian},
  year = {2015}
}

@article{mcauley_inferring_2015,
  title = {Inferring {{Networks}} of {{Substitutable}} and {{Complementary Products}}},
  author = {McAuley, Julian},
  year = {2015}
}

@misc{mccann_learned_2017,
  title = {Learned in {{Translation}}: {{Contextualized Word Vectors}}},
  author = {McCann, Bryan and Bradbury, James and Xiong, Caiming and Socher, Richard},
  year = {2017},
  eprint = {1708.00107},
  eprinttype = {arxiv},
  abstract = {Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art.},
  archiveprefix = {arXiv}
}

@article{mcclelland_why_1995,
  title = {Why There Are Complementary Learning Systems in the Hippocampus and Neocortex: {{Insights}} from the Successes and Failures of Connectionist Models of Learning and Memory},
  author = {McClelland, James L. and McNaughton, Bruce L. and O'Reilly, Randall C.},
  year = {1995},
  journal = {Psychological Review},
  volume = {102},
  number = {3},
  pages = {419--457},
  issn = {0033295X},
  abstract = {Damage to the hippocampal system disrupts recent memory but leaves remote memory intact. The account presented here suggests that memories are first stored via synaptic changes in the hippocampal system, that these changes support reinstatement of recent memories in the neocortex, that neocortical synapses change a little on each reinstatement, and that remote memory is based on accumulated neocortical changes. Models that learn via changes to connections help explain this organization. These models discover the structure in ensembles of items if learning of each item is gradual and interleaved with learning about other items. This suggests that the neocortex learns slowly to discover the structure in ensembles of experiences. The hippocampal system permits rapid learning of new items without disrupting this structure, and reinstatement of new memories interleaves them with others to integrate them into structured neocortical memory systems.},
  isbn = {0033-295X (Print)\textbackslash r0033-295X (Linking)},
  pmid = {7624455},
  keywords = {Neuroscience}
}

@article{mcculloch_logical_1943,
  title = {A {{Logical Calculus}} of the {{Idea Immanent}} in {{Nervous Activity}}},
  author = {McCulloch, Warren S. and Pitts, W.},
  year = {1943},
  journal = {Bulletin of Mathematical Biophysics},
  volume = {5},
  number = {4},
  pages = {115--133},
  issn = {0007-4985},
  abstract = {Because of the 'all-or-none' character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behaviour of every net can be described in these terms, with the addition of more complicated logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which nehaves under the other and gives the same results, algthough perhaps not in the same time. Various applications of the calculus are discussed.},
  pmid = {2185863}
}

@inproceedings{mei_imputing_2019,
  title = {Imputing {{Missing Events}} in {{Continuous-Time Event Streams}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Mei, Hongyuan and Qin, Guanghui and Eisner, Jason},
  editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  year = {2019},
  month = jun,
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {97},
  pages = {4475--4485},
  publisher = {{PMLR}},
  address = {{Long Beach, CA, USA}},
  abstract = {Events in the world may be caused by other, unobserved events. We consider sequences of events in continuous time. Given a probability model of complete sequences, we propose particle smoothing\textemdash a form of sequential importance sampling\textemdash to impute the missing events in an incomplete sequence. We develop a trainable family of proposal distributions based on a type of bidirectional continuous-time LSTM: Bidirectionality lets the proposals condition on future observations, not just on the past as in particle filtering. Our method can sample an ensemble of possible complete sequences (particles), from which we form a single consensus prediction that has low Bayes risk under our chosen loss metric. We experiment in multiple synthetic and real domains, using different missingness mechanisms, and modeling the complete sequences in each domain with a neural Hawkes process (Mei \&amp; Eisner 2017). On held-out incomplete sequences, our method is effective at inferring the ground-truth unobserved events, with particle smoothing consistently improving upon particle filtering.}
}

@misc{melis_state_2017,
  title = {On the {{State}} of the {{Art}} of {{Evaluation}} in {{Neural Language Models}}},
  author = {Melis, G{\'a}bor and Dyer, Chris and Blunsom, Phil},
  year = {2017},
  month = jul,
  eprint = {1707.05589},
  eprinttype = {arxiv},
  abstract = {Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing code bases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.},
  archiveprefix = {arXiv},
  isbn = {9781604562170}
}

@misc{mescheder_adversarial_2017,
  title = {Adversarial {{Variational Bayes}}: {{Unifying Variational Autoencoders}} and {{Generative Adversarial Networks}}},
  author = {Mescheder, Lars and Nowozin, Sebastian and Geiger, Andreas},
  year = {2017},
  month = jan,
  eprint = {1701.04722},
  eprinttype = {arxiv},
  abstract = {Variational Autoencoders (VAEs) are expressive latent variable models that can be used to learn complex probability distributions from training data. However, the quality of the resulting model crucially relies on the expressiveness of the inference model. We introduce Adversarial Variational Bayes (AVB), a technique for training Variational Autoencoders with arbitrarily expressive inference models. We achieve this by introducing an auxiliary discriminative network that allows to rephrase the maximum-likelihood-problem as a two-player game, hence establishing a principled connection between VAEs and Generative Adversarial Networks (GANs). We show that in the nonparametric limit our method yields an exact maximum-likelihood assignment for the parameters of the generative model, as well as the exact posterior distribution over the latent variables given an observation. Contrary to competing approaches which combine VAEs with GANs, our approach has a clear theoretical justification, retains most advantages of standard Variational Autoencoders and is easy to implement.},
  archiveprefix = {arXiv}
}

@article{meyer_unsupervised_2017,
  title = {Unsupervised {{Feature Learning}} for {{Audio Analysis}}},
  author = {Meyer, Matthias and Beutel, Jan and Thiele, Lothar},
  year = {2017},
  month = dec,
  journal = {arXiv:1712.03835 [cs]},
  eprint = {1712.03835},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Identifying acoustic events from a continuously streaming audio source is of interest for many applications including environmental monitoring for basic research. In this scenario neither different event classes are known nor what distinguishes one class from another. Therefore, an unsupervised feature learning method for exploration of audio data is presented in this paper. It incorporates the two following novel contributions: First, an audio frame predictor based on a Convolutional LSTM autoencoder is demonstrated, which is used for unsupervised feature extraction. Second, a training method for autoencoders is presented, which leads to distinct features by amplifying event similarities. In comparison to standard approaches, the features extracted from the audio frame predictor trained with the novel approach show 13 \% better results when used with a classifier and 36 \% better results when used for clustering.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@misc{miao_language_2016,
  title = {Language as a {{Latent Variable}}: {{Discrete Generative Models}} for {{Sentence Compression}}},
  shorttitle = {Language as a {{Latent Variable}}},
  author = {Miao, Yishu and Blunsom, Phil},
  year = {2016},
  month = oct,
  eprint = {1609.07317},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In this work we explore deep generative models of text in which the latent representation of a document is itself drawn from a discrete language model distribution. We formulate a variational auto-encoder for inference in this model and apply it to the task of compressing sentences. In this application the generative model first draws a latent summary sentence from a background language model, and then subsequently draws the observed sentence conditioned on this latent summary. In our empirical evaluation we show that generative formulations of both abstractive and extractive compression yield state-of-the-art results when trained on a large amount of supervised data. Further, we explore semi-supervised compression scenarios where we show that it is possible to achieve performance competitive with previously proposed supervised models while training on a fraction of the supervised data.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@misc{michelis_linear_2021,
  title = {On {{Linear Interpolation}} in the {{Latent Space}} of {{Deep Generative Models}}},
  author = {Michelis, Mike Yan and Becker, Quentin},
  year = {2021},
  month = may,
  eprint = {2105.03663},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The underlying geometrical structure of the latent space in deep generative models is in most cases not Euclidean, which may lead to biases when comparing interpolation capabilities of two models. Smoothness and plausibility of linear interpolations in latent space are associated with the quality of the underlying generative model. In this paper, we show that not all such interpolations are comparable as they can deviate arbitrarily from the shortest interpolation curve given by the geodesic. This deviation is revealed by computing curve lengths with the pull-back metric of the generative model, finding shorter curves than the straight line between endpoints, and measuring a non-zero relative length improvement on this straight line. This leads to a strategy to compare linear interpolations across two generative models. We also show the effect and importance of choosing an appropriate output space for computing shorter curves. For this computation we derive an extension of the pull-back metric.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning}
}

@article{mikolov_distributed_2013,
  title = {Distributed {{Representations}} of {{Words}} and {{Phrases}} and Their {{Compositionality}}},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = {2013},
  journal = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
  eprint = {1310.4546},
  eprinttype = {arxiv},
  pages = {3111--3119},
  issn = {10495258},
  abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
  archiveprefix = {arXiv},
  isbn = {2150-8097},
  pmid = {903}
}

@misc{mikolov_efficient_2013,
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = {2013},
  eprint = {1301.3781},
  eprinttype = {arxiv},
  primaryclass = {cs.CL},
  abstract = {We propose two novel model architectures for computing continuous vector repre-sentations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previ-ously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art perfor-mance on our test set for measuring syntactic and semantic word similarities.},
  archiveprefix = {arXiv}
}

@article{mikolov_subword_nodate,
  title = {Subword {{Language Modelling}} with {{Neural Networks}}},
  author = {Mikolov, Tomas{\textasciicaron} and Sutskever, Ilya and Deoras, Anoop and Le, Hai-Son and Kombrink, Stefan},
  pages = {4},
  abstract = {We explore the performance of several types of language models on the word-level and the character-level language modeling tasks. This includes two recently proposed recurrent neural network architectures, a feedforward neural network model, a maximum entropy model and the usual smoothed n-gram models. We then propose a simple technique for learning sub-word level units from the data, and show that it combines advantages of both character and wordlevel models. Finally, we show that neural network based language models can be order of magnitude smaller than compressed n-gram models, at the same level of performance when applied to a Broadcast news RT04 speech recognition task. By using sub-word units, the size can be reduced even more.},
  langid = {english}
}

@article{milde_unspeech_2018,
  title = {Unspeech: {{Unsupervised Speech Context Embeddings}}},
  shorttitle = {Unspeech},
  author = {Milde, Benjamin and Biemann, Chris},
  year = {2018},
  month = aug,
  journal = {arXiv:1804.06775 [cs, eess]},
  eprint = {1804.06775},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {We introduce "Unspeech" embeddings, which are based on unsupervised learning of context feature representations for spoken language. The embeddings were trained on up to 9500 hours of crawled English speech data without transcriptions or speaker information, by using a straightforward learning objective based on context and non-context discrimination with negative sampling. We use a Siamese convolutional neural network architecture to train Unspeech embeddings and evaluate them on speaker comparison, utterance clustering and as a context feature in TDNN-HMM acoustic models trained on TED-LIUM, comparing it to i-vector baselines. Particularly decoding out-of-domain speech data from the recently released Common Voice corpus shows consistent WER reductions. We release our source code and pre-trained Unspeech models under a permissive open source license.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@book{minsky_introduction_1969,
  title = {An {{Introduction}} to {{Computational Geometry}}},
  author = {Minsky, Marvin and Papert, Seymour A.},
  year = {1969},
  journal = {MIT Press},
  publisher = {{MIT Press}},
  abstract = {I. Algebraic theory of linear parallel predicates -- 1. Theory of linear Boolean inequalities -- 2. Group invariance of Boolean inequalities -- 3. Parity and one-in-a-box predicates -- 4. The "and/or" theorem -- II. Geometric theory of linear inequalities -- 5. {$\Psi$}connected: a geometric property with unbounded order -- 6. Geometric patterns of small order: spectra and context -- 7. Stratification and normalization -- 8. The diameter-limited perceptron -- 9. Geometric predicates and serial algorithms -- III. Learning theory -- 10. Magnitude of the coefficients -- 11. Learning -- 12. Linear separation and learning -- 13. Perceptrons and pattern recognition.},
  isbn = {0-262-63022-2}
}

@misc{mishkin_all_2016,
  title = {All You Need Is a Good Init},
  author = {Mishkin, Dmytro and Matas, Jiri},
  year = {2016},
  month = feb,
  eprint = {1511.06422},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Layer-sequential unit-variance (LSUV) initialization \textendash{} a simple method for weight initialization for deep net learning \textendash{} is proposed. The method consists of the two steps. First, pre-initialize weights of each convolution or inner-product layer with orthonormal matrices. Second, proceed from the first to the final layer, normalizing the variance of the output of each layer to be equal to one.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning}
}

@misc{mitchell_why_2021,
  title = {Why {{AI}} Is {{Harder Than We Think}}},
  author = {Mitchell, Melanie},
  year = {2021},
  month = apr,
  eprint = {2104.12871},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Since its beginning in the 1950s, the field of artificial intelligence has cycled several times between periods of optimistic predictions and massive investment (``AI spring'') and periods of disappointment, loss of confidence, and reduced funding (``AI winter''). Even with today's seemingly fast pace of AI breakthroughs, the development of long-promised technologies such as self-driving cars, housekeeping robots, and conversational companions has turned out to be much harder than many people expected. One reason for these repeating cycles is our limited understanding of the nature and complexity of intelligence itself. In this paper I describe four fallacies in common assumptions made by AI researchers, which can lead to overconfident predictions about the field. I conclude by discussing the open questions spurred by these fallacies, including the age-old challenge of imbuing machines with humanlike common sense.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence}
}

@misc{miyato_adversarial_2017,
  title = {{{ADVERSARIAL TRAINING METHODS FOR SEMI-SUPERVISED TEXT CLASSIFICATION}}},
  author = {Miyato, Takeru and Dai, Andrew M. and Goodfellow, Ian J.},
  year = {2017},
  eprint = {1605.07725v3},
  eprinttype = {arxiv},
  abstract = {Adversarial training provides a means of regularizing supervised learning algo-rithms while virtual adversarial training is able to extend supervised learning al-gorithms to the semi-supervised setting. However, both methods require making small perturbations to numerous entries of the input vector, which is inappropri-ate for sparse high-dimensional inputs such as one-hot word representations. We extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself. The proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks. We provide visualizations and analysis showing that the learned word embeddings have im-proved in quality and that while training, the model is less prone to overfitting.},
  archiveprefix = {arXiv}
}

@article{mnih_asynchronous_2016,
  title = {Asynchronous {{Methods}} for {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Badia, Adri{\`a} Puigdom{\`e}nech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  year = {2016},
  journal = {Proceedings of the International Conference on Machine Learning (ICML)},
  eprint = {1602.01783},
  eprinttype = {arxiv},
  issn = {1938-7228},
  abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
  archiveprefix = {arXiv},
  isbn = {9781510829008},
  pmid = {1000272564}
}

@article{mnih_human-level_2015,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  year = {2015},
  journal = {Nature},
  volume = {518},
  number = {7540},
  pages = {529--533},
  issn = {14764687},
  abstract = {The theory of reinforcement learning provides a normative account 1 , deeply rooted in psychological 2 and neuroscientific 3 perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory pro-cessing systems 4,5 , the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopa-minergic neurons and temporal difference reinforcement learning algorithms 3 . While reinforcement learning agents have achieved some successes in a variety of domains 6\textendash 8 , their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks 9\textendash 11 to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games 12},
  pmid = {25719670}
}

@article{mnih_learning_2014,
  title = {Learning Word Embeddings Efficiently with Noise-Contrastive},
  author = {Mnih, Andriy},
  year = {2014}
}

@misc{mnih_neural_2014,
  title = {Neural {{Variational Inference}} and {{Learning}} in {{Belief Networks}}},
  author = {Mnih, Andriy and Gregor, Karol and Deepmind, Google},
  year = {2014},
  eprint = {1402.0030v2},
  eprinttype = {arxiv},
  abstract = {Highly expressive directed latent variable models , such as sigmoid belief networks, are difficult to train on large datasets because exact inference in them is intractable and none of the approximate inference methods that have been applied to them scale well. We propose a fast non-iterative approximate inference method that uses a feedforward network to implement efficient exact sampling from the variational posterior. The model and this inference network are trained jointly by maximizing a variational lower bound on the log-likelihood. Although the naive estimator of the inference network gradient is too high-variance to be useful, we make it practical by applying several straightforward model-independent variance reduction techniques. Applying our approach to training sigmoid belief networks and deep autoregressive networks, we show that it outperforms the wake-sleep algorithm on MNIST and achieves state-of-the-art results on the Reuters RCV1 document dataset.},
  archiveprefix = {arXiv},
  keywords = {Belief networks,Deep learning,Variational inference}
}

@misc{mnih_playing_2013,
  title = {Playing {{Atari}} with {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  year = {2013},
  eprint = {1312.5602},
  eprinttype = {arxiv},
  abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  archiveprefix = {arXiv},
  isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
  pmid = {25719670}
}

@inproceedings{mnih_variational_2016,
  title = {Variational {{Inference}} for {{Monte Carlo Objectives}}},
  booktitle = {Proceedings of the 33rd {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Mnih, Andriy and Rezende, Danilo Jimenez},
  year = {2016},
  pages = {2188--2196},
  address = {{New York, NY, USA}}
}

@misc{mogren_crnngan_2016,
  title = {C-{{RNN-GAN}}: {{Continuous}} Recurrent Neural Networks with Adversarial Training},
  author = {Mogren, Olof},
  year = {2016},
  eprint = {1611.09904},
  eprinttype = {arxiv},
  abstract = {Generative adversarial networks have been proposed as a way of efficiently training deep generative neural networks. We propose a generative adversarial model that works on continuous sequential data, and apply it by training it on a collection of classical music. We conclude that it generates music that sounds better and better as the model is trained, report statistics on generated music, and let the reader judge the quality by downloading the generated songs.},
  archiveprefix = {arXiv}
}

@article{mohamed_acoustic_2012,
  title = {Acoustic Modeling Using Deep Belief Networks},
  author = {Mohamed, Abdel-rahman and Dahl, George E. and Hinton, Geoffrey E.},
  year = {2012},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  volume = {20},
  number = {1},
  pages = {14--22},
  publisher = {{IEEE}}
}

@misc{mohamed_monte_2019,
  title = {Monte {{Carlo Gradient Estimation}} in {{Machine Learning}}},
  author = {Mohamed, Shakir and Rosca, Mihaela and Figurnov, Michael and Mnih, Andriy},
  year = {2019},
  month = jun,
  eprint = {1906.10652},
  eprinttype = {arxiv},
  abstract = {This paper is a broad and accessible survey of the methods we have at our disposal for Monte Carlo gradient estimation in machine learning and across the statistical sciences: the problem of computing the gradient of an expectation of a function with respect to parameters defining the distribution that is integrated; the problem of sensitivity analysis. In machine learning research, this gradient problem lies at the core of many learning problems, in supervised, unsupervised and reinforcement learning. We will generally seek to rewrite such gradients in a form that allows for Monte Carlo estimation, allowing them to be easily and efficiently used and analysed. We explore three strategies\textemdash the pathwise, score function, and measure-valued gradient estimators\textemdash exploring their historical developments, derivation, and underlying assumptions. We describe their use in other fields, show how they are related and can be combined, and expand on their possible generalisations. Wherever Monte Carlo gradient estimators have been derived and deployed in the past, important advances have followed. A deeper and more widely-held understanding of this problem will lead to further advances, and it is these advances that we wish to support.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning}
}

@book{mohri_foundations_2012,
  title = {Foundations of {{Machine Learning}}},
  author = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
  year = {2012},
  series = {Adaptive Computation and Machine Learning Series},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA}},
  isbn = {978-0-262-01825-8},
  langid = {english},
  lccn = {Q325.5 .M64 2012},
  keywords = {★,Computer algorithms,Machine learning}
}

@misc{molchanov_variational_2017,
  title = {Variational {{Dropout Sparsifies Deep Neural Networks}}},
  author = {Molchanov, Dmitry and Ashukha, Arsenii and Vetrov, Dmitry},
  year = {2017},
  eprint = {1701.05369},
  eprinttype = {arxiv},
  abstract = {We explore a recently proposed Variational Dropout technique that provided an elegant Bayesian interpretation to Gaussian Dropout. We extend Variational Dropout to the case when dropout rates are unbounded, propose a way to reduce the variance of the gradient estimator and report first experimental results with individual dropout rates per weight. Interestingly, it leads to extremely sparse solutions both in fully-connected and convolutional layers. This effect is similar to automatic relevance determination effect in empirical Bayes but has a number of advantages. We reduce the number of parameters up to 280 times on LeNet architectures and up to 68 times on VGG-like networks with a negligible decrease of accuracy.},
  archiveprefix = {arXiv}
}

@incollection{montavon_practical_2012,
  title = {A {{Practical Guide}} to {{Training Restricted Boltzmann Machines}}},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}},
  author = {Hinton, Geoffrey E.},
  editor = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  year = {2012},
  volume = {7700},
  pages = {599--619},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  isbn = {978-3-642-35288-1 978-3-642-35289-8},
  langid = {english}
}

@article{montufar_number_nodate,
  title = {On the {{Number}} of {{Linear Regions}} of {{Deep Neural Networks}}},
  author = {Mont{\'u}far, Guido and Cho, Kyunghyun and Bengio, Yoshua},
  abstract = {We study the complexity of functions computable by deep feedforward neural networks with piecewise linear activations in terms of the symmetries and the number of linear regions that they have. Deep networks are able to sequentially map portions of each layer's input-space to the same output. In this way, deep models compute functions that react equally to complicated patterns of different inputs. The compositional structure of these functions enables them to re-use pieces of computation exponentially often in terms of the network's depth. This paper investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piecewise linear activation functions. In particular, our analysis is not specific to a single family of models, and as an example, we employ it for rectifier and maxout networks. We improve complexity bounds from pre-existing work and investigate the behavior of units in higher layers.},
  keywords = {Deep learning,Input space partition,Maxout,Neural network,Rectifier}
}

@article{morningstar_density_nodate,
  title = {Density of {{States Estimation}} for {{Out-of-Distribution Detection}}},
  author = {Morningstar, Warren R and Ham, Cusuh and Gallagher, Andrew G and Lakshminarayanan, Balaji and Alemi, Alexander A and Dillon, Joshua V},
  pages = {10},
  abstract = {Perhaps surprisingly, recent studies have shown probabilistic model likelihoods have poor specificity for out-of-distribution (OOD) detection and often assign higher likelihoods to OOD data than in-distribution data. To ameliorate this issue we propose DoSE, the Density of States Estimator. Drawing on the statistical physics notion of ``density of states,'' the DoSE decision rule avoids direct comparison of model probabilities, and instead utilizes the ``probability of the model probability,'' or indeed the frequency of any reasonable statistic. The frequency is calculated using nonparametric density estimators (e.g., KDE and one-class SVM) which measure the typicality of various model statistics given the training data and from which we can flag test points with low typicality as anomalous. Unlike many other methods, DoSE requires neither labeled data nor OOD examples. DoSE is modular and can be trivially applied to any existing, trained model. We demonstrate DoSE's state-of-the-art performance against other unsupervised OOD detectors on previously established ``hard'' benchmarks.},
  langid = {english}
}

@article{morokoff_quasi-monte_1995,
  title = {Quasi-{{Monte Carlo Integration}}},
  author = {Morokoff, William J. and Caflisch, Russel E.},
  year = {1995},
  month = dec,
  journal = {Journal of Computational Physics},
  volume = {122},
  number = {2},
  pages = {218--230},
  publisher = {{Academic Press}},
  issn = {00219991},
  abstract = {The standard Monte Carlo approach to evaluating multidimensional integrals using (pseudo)-random integration nodes is frequently used when quadrature methods are too difficult or expensive to implement. As an alternative to the random methods, it has been suggested that lower error and improved convergence may be obtained by replacing the pseudo-random sequences with more uniformly distributed sequences known as quasi-random. In this paper quasi-random (Halton, Sobol', and Faure) and pseudo-random sequences are compared in computational experiments designed to determine the effects on convergence of certain properties of the integrand, including variance, variation, smoothness, and dimension. The results show that variation, which plays an important role in the theoretical upper bound given by the Koksma-Hlawka inequality, does not affect convergence, while variance, the determining factor in random Monte Carlo, is shown to provide a rough upper bound, but does not accurately predict performance. In general, quasi-Monte Carlo methods are superior to random Monte Carlo, but the advantage may be slight, particularly in high dimensions or for integrands that are not smooth. For discontinuous integrands, we derive a bound which shows that the exponent for algebraic decay of the integration error from quasi-Monte Carlo is only slightly larger than 1/2 in high dimensions. \textcopyright{} 1995 Academic Press. All rights reserved.}
}

@inproceedings{morse_simple_2016,
  title = {Simple {{Evolutionary Optimization Can Rival Stochastic Gradient Descent}} in {{Neural Networks}}},
  booktitle = {Proceedings of the {{Conference}} on {{Genetic}} and {{Evolutionary Computation}} ({{GECCO}})},
  author = {Morse, Gregory and Stanley, Kenneth O.},
  year = {2016},
  pages = {477--484},
  abstract = {While evolutionary algorithms (EAs) have long offered an alternative approach to optimization, in recent years backpropagation through stochastic gradient descent (SGD) has come to dominate the fi elds of neural network optimization and deep learning. One hypothesis for the absence of EAs in deep learning is that modern neural networks have become so high dimensional that evolution with its inexact gradient cannot match the exact gradient calculations of backpropagation. Furthermore, the evaluation of a single individual in evolution on the big data sets now prevalent in deep learning would present a prohibitive obstacle towards effi cient optimization. This paper challenges these views, suggesting that EAs can be made to run signi cantly faster than previously thought by evaluating individuals only on a small number of training examples per generation. Surprisingly, using this approach with only a simple EA (called the limited evaluation EA or LEEA) is competitive with the performance of the state-of-the-art SGD variant RMSProp on several benchmarks with neural networks with over 1,000 weights. More investigation is warranted, but these initial results suggest the possibility that EAs could be the fi rst viable training alternative for deep learning outside of SGD, thereby opening up deep learning to all the tools of evolutionary computation.},
  isbn = {978-1-4503-4206-3}
}

@misc{mostafazadeh_generating_2016,
  title = {Generating {{Natural Questions About}} an {{Image}}},
  author = {Mostafazadeh, Nasrin and Misra, Ishan and Devlin, Jacob and Mitchell, Margaret and He, Xiaodong and Vanderwende, Lucy},
  year = {2016},
  month = mar,
  eprint = {1603.06059},
  eprinttype = {arxiv},
  abstract = {There has been an explosion of work in the vision \& language community during the past few years from image captioning to video transcription, and answering questions about images. These tasks have focused on literal descriptions of the image. To move beyond the literal, we choose to explore how questions about an image are often directed at commonsense inference and the abstract events evoked by objects in the image. In this paper, we introduce the novel task of Visual Question Generation (VQG), where the system is tasked with asking a natural and engaging question when shown an image. We provide three datasets which cover a variety of images from object-centric to event-centric, with considerably more abstract training data than provided to state-of-the-art captioning systems thus far. We train and test several generative and retrieval models to tackle the task of VQG. Evaluation results show that while such models ask reasonable questions for a variety of images, there is still a wide gap with human performance which motivates further work on connecting images with commonsense knowledge and pragmatics. Our proposed task offers a new challenge to the community which we hope furthers interest in exploring deeper connections between vision \& language.},
  archiveprefix = {arXiv},
  isbn = {9781510827585}
}

@misc{mozannar_consistent_2020,
  title = {Consistent {{Estimators}} for {{Learning}} to {{Defer}} to an {{Expert}}},
  author = {Mozannar, Hussein and Sontag, David},
  year = {2020},
  month = jun,
  eprint = {2006.01862},
  eprinttype = {arxiv},
  abstract = {Learning algorithms are often used in conjunction with expert decision makers in practical scenarios, however this fact is largely ignored when designing these algorithms. In this paper we explore how to learn predictors that can either predict or choose to defer the decision to a downstream expert. Given only samples of the expert's decisions, we give a procedure based on learning a classifier and a rejector and analyze it theoretically. Our approach is based on a novel reduction to cost sensitive learning where we give a consistent surrogate loss for cost sensitive learning that generalizes the cross entropy loss. We show the effectiveness of our approach on a variety of experimental tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{mrksic_fully_2018,
  title = {Fully {{Statistical Neural Belief Tracking}}},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  author = {Mrk{\v s}i{\'c}, Nikola and Vuli{\'c}, Ivan},
  year = {2018},
  pages = {108--113},
  publisher = {{Association for Computational Linguistics}},
  address = {{Melbourne, Australia}},
  abstract = {This paper proposes an improvement to the existing data-driven Neural Belief Tracking (NBT) framework for Dialogue State Tracking (DST). The existing NBT model uses a hand-crafted belief state update mechanism which involves an expensive manual retuning step whenever the model is deployed to a new dialogue domain. We show that this update mechanism can be learned jointly with the semantic decoding and context modelling parts of the NBT model, eliminating the last rule-based module from this DST framework. We propose two different statistical update mechanisms and show that dialogue dynamics can be modelled with a very small number of additional model parameters. In our DST evaluation over three languages, we show that this model achieves competitive performance and provides a robust framework for building resource-light DST models.},
  langid = {english}
}

@inproceedings{mrksic_neural_2017,
  title = {Neural {{Belief Tracker}}: {{Data-Driven Dialogue State Tracking}}},
  shorttitle = {Neural {{Belief Tracker}}},
  booktitle = {Proceedings of the 55th {{Annual Meeting}} of the {{Association}} for           {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Mrk{\v s}i{\'c}, Nikola and {\'O} S{\'e}aghdha, Diarmuid and Wen, Tsung-Hsien and Thomson, Blaise and Young, Steve},
  year = {2017},
  pages = {1777--1788},
  publisher = {{Association for Computational Linguistics}},
  address = {{Vancouver, Canada}},
  abstract = {One of the core components of modern spoken dialogue systems is the belief tracker, which estimates the user's goal at every step of the dialogue. However, most current approaches have difficulty scaling to larger, more complex dialogue domains. This is due to their dependency on either: a) Spoken Language Understanding models that require large amounts of annotated training data; or b) hand-crafted lexicons for capturing some of the linguistic variation in users' language. We propose a novel Neural Belief Tracking (NBT) framework which overcomes these problems by building on recent advances in representation learning. NBT models reason over pre-trained word vectors, learning to compose them into distributed representations of user utterances and dialogue context. Our evaluation on two datasets shows that this approach surpasses past limitations, matching the performance of state-of-the-art models which rely on hand-crafted semantic lexicons and outperforming them when such lexicons are not provided.},
  langid = {english}
}

@book{muirhead_aspects_2009,
  title = {Aspects of Multivariate Statistical Theory},
  author = {Muirhead, Robb J},
  year = {2009},
  volume = {197},
  publisher = {{John Wiley \& Sons}}
}

@book{murphy_machine_2012,
  title = {Machine {{Learning}}},
  author = {Murphy, Kevin P.},
  year = {2012},
  edition = {First},
  publisher = {{MIT Press}},
  issn = {0262018020},
  abstract = {Some of the most remarkable issues related to interharmonics observed from a probabilistic perspective are presented. Attention is firstly devoted to interharmonic frequency and amplitude variability. Starting from the basic mathematical and computational aspects of probabilistic harmonic models, the difficulties to include interharmonics are discussed with particular attention to the problem of the frequency resolution and of the computational burden. Then, simulation and measurement aspects are discussed, also showing some numerical and experimental results.},
  isbn = {978-0-262-01802-9},
  pmid = {20236947},
  keywords = {★}
}

@article{nadarajah_making_2011,
  title = {Making the {{Cauchy}} Work},
  author = {Nadarajah, Saralees},
  year = {2011},
  journal = {Brazilian Journal of Probability and Statistics},
  volume = {25},
  number = {1},
  pages = {99--120},
  issn = {01030752},
  abstract = {A truncated version of the Cauchy distribution is introduced. Un-like the Cauchy distribution, this possesses finite moments of all orders and could therefore be a better model for certain practical situations. More than 10 practical situations where the truncated distribution could be applied are dis-cussed. Explicit expressions are derived for the moments, L moments, mean deviations, moment generating function, characteristic function, convolution properties, Bonferroni curve, Lorenz curve, entropies, order statistics and the asymptotic distribution of the extreme order statistics. Estimation procedures are detailed by the method of moments and the method of maximum likeli-hood and expressions derived for the associated Fisher information matrix. Simulation issues are discussed. Finally, an application is illustrated for con-sumer price indices from the six major economics.},
  keywords = {Cauchy distribution,Convolution,Estimation,Moments,Order statistics,Truncated cauchy distribution}
}

@misc{nagano_wrapped_2019,
  title = {A {{Wrapped Normal Distribution}} on {{Hyperbolic Space}} for {{Gradient-Based Learning}}},
  author = {Nagano, Yoshihiro and Yamaguchi, Shoichiro and Fujita, Yasuhiro and Koyama, Masanori},
  year = {2019},
  month = may,
  eprint = {1902.02992},
  eprinttype = {arxiv},
  abstract = {Hyperbolic space is a geometry that is known to be well-suited for representation learning of data with an underlying hierarchical structure. In this paper, we present a novel hyperbolic distribution called pseudo-hyperbolic Gaussian, a Gaussianlike distribution on hyperbolic space whose density can be evaluated analytically and differentiated with respect to the parameters. Our distribution enables the gradient-based learning of the probabilistic models on hyperbolic space that could never have been considered before. Also, we can sample from this hyperbolic probability distribution without resorting to auxiliary means like rejection sampling. As applications of our distribution, we develop a hyperbolic-analog of variational autoencoder and a method of probabilistic word embedding on hyperbolic space. We demonstrate the efficacy of our distribution on various datasets including MNIST, Atari 2600 Breakout, and WordNet.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Hyperbolic space,Statistics - Machine Learning}
}

@misc{nagel_adaptive_2020,
  title = {Up or {{Down}}? {{Adaptive Rounding}} for {{Post-Training Quantization}}},
  shorttitle = {Up or {{Down}}?},
  author = {Nagel, Markus and Amjad, Rana Ali and {van Baalen}, Mart and Louizos, Christos and Blankevoort, Tijmen},
  year = {2020},
  month = jun,
  eprint = {2004.10568},
  eprinttype = {arxiv},
  abstract = {When quantizing neural networks, assigning each floating-point weight to its nearest fixed-point value is the predominant approach. We find that, perhaps surprisingly, this is not the best we can do. In this paper, we propose AdaRound, a better weight-rounding mechanism for post-training quantization that adapts to the data and the task loss. AdaRound is fast, does not require fine-tuning of the network, and only uses a small amount of unlabelled data. We start by theoretically analyzing the rounding problem for a pre-trained neural network. By approximating the task loss with a Taylor series expansion, the rounding task is posed as a quadratic unconstrained binary optimization problem. We simplify this to a layer-wise local loss and propose to optimize this loss with a soft relaxation. AdaRound not only outperforms rounding-to-nearest by a significant margin but also establishes a new state-of-the-art for post-training quantization on several networks and tasks. Without fine-tuning, we can quantize the weights of Resnet18 and Resnet50 to 4 bits while staying within an accuracy loss of 1\%.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{nagesh_robust_2016,
  title = {A Robust Speech Rate Estimation Based on the Activation Profile from the Selected Acoustic Unit Dictionary},
  booktitle = {2016 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Nagesh, S. and Yarra, C. and Deshmukh, O. D. and Ghosh, P. K.},
  year = {2016},
  month = mar,
  pages = {5400--5404},
  abstract = {A typical solution for the speech rate estimation consists of two stages, which involves first computing a short-time feature contour such that most of peaks of the contour correspond to the syllable nuclei followed by the detection of the peaks of the contour corresponding to the syllable nuclei. Temporal correlation selected subband correlation (TCSSBC) is often used as a feature contour for the speech rate estimation in which correlation within and across a few selected sub-band energies are computed. In this work, instead of a fixed set of sub-bands, we learn them in a data-driven manner using a dictionary learning approach. Similarly, instead of the energy contours, we use the activation profile from the learned dictionary elements. We found that the peaks detected from the data-driven approach significantly improve the speech rate estimation when combined with the traditional TCSSBC approach using a proposed peak-merging strategy. Experiments are performed separately using Switchboard, TIMIT and CTIMIT corpora. Except Switchboard, the correlation coefficient for the speech rate estimation using the proposed approach is found to be higher than those by the TCSSBC technique - 3.1\% and 5.2\% (relative) improvements for TIMIT and CTIMIT respectively.},
  keywords = {Acoustics,activation profile,Correlation,Dictionaries,dictionary learning approach,Estimation,estimation theory,robust speech rate estimation,selected acoustic unit dictionary,signal processing,Sparse matrices,Speech,speech recognition,Speech recognition,syllable nuclei,temporal correlation selected subband correlation}
}

@inproceedings{nair_rectified_2010,
  title = {Rectified Linear Units Improve Restricted Boltzmann Machines},
  booktitle = {Proceedings of the 27th {{International Conference}} on {{Machine Learning}} ({{ICML}} 2021)},
  author = {Nair, Vinod and Hinton, Geoffrey E.},
  year = {2010},
  pages = {807--814},
  address = {{Haifa, Israel}}
}

@misc{nakkiran_deep_2019,
  title = {Deep {{Double Descent}}: {{Where Bigger Models}} and {{More Data Hurt}}},
  shorttitle = {Deep {{Double Descent}}},
  author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
  year = {2019},
  month = dec,
  eprint = {1912.02292},
  eprinttype = {arxiv},
  abstract = {We show that a variety of modern deep learning tasks exhibit a ``double-descent'' phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
}

@inproceedings{nalisnick_deep_2019,
  title = {Do {{Deep Generative Models Know What They Don}}'t {{Know}}?},
  booktitle = {Proceedings of the 7th {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Nalisnick, Eric and Matsukawa, Akihiro and Teh, Yee Whye and Gorur, Dilan and Lakshminarayanan, Balaji},
  year = {2019},
  eprint = {1810.09136},
  eprinttype = {arxiv},
  address = {{New Orleans, LA, USA}},
  abstract = {A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data. A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong. Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel, out-of-distribution inputs. In this paper we challenge this assumption. We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former. Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN. To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood. We find such behavior persists even when we restrict the flows to constant-volume transformations. These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{nalisnick_detecting_2019,
  title = {Detecting {{Out-of-Distribution Inputs}} to {{Deep Generative Models Using Typicality}}},
  author = {Nalisnick, Eric and Matsukawa, Akihiro and Teh, Yee Whye and Lakshminarayanan, Balaji},
  year = {2019},
  eprint = {1906.02994},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found}
}

@inproceedings{narayanan_speech_2005,
  title = {Speech {{Rate Estimation}} via {{Temporal Correlation}} and {{Selected Sub-Band Correlation}}},
  booktitle = {Proceedings. ({{ICASSP}} '05). {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}, 2005.},
  author = {Narayanan, S. and Wang, D.},
  year = {2005},
  volume = {1},
  pages = {413--416},
  publisher = {{IEEE}},
  address = {{Philadelphia, Pennsylvania, USA}},
  abstract = {In this paper, we propose a novel method for speech rate estimation without requiring automatic speech recognition. It extends the methods of spectral subband correlation by including temporal correlation and the use of selecting prominent spectral subbands for correlation. Further more, to address some of the practical issues in previously published methods, we introduce some novel components into the algorithm such as the use of pitch confidence, magnifying window, relative peak measure and relative threshold. By selecting the parameters and thresholds from realistic development sets, this method achieves a 0.972 correlation coefficient on syllable number estimation and a 0.706 correlation on speech rate estimation. This result is about 6.9\% improvement than current best single estimator and 3.5\% improvement than current multi-estimator evaluated on the same switchboard database.},
  isbn = {978-0-7803-8874-1},
  langid = {english}
}

@article{neal_bayesian_1993,
  title = {Bayesian Learning via Stochastic Dynamics},
  author = {Neal, Radford M.},
  year = {1993},
  journal = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
  pages = {475},
  abstract = {The attempt to find a single "optimal" weight vector in conven-tional network training can lead to overfitting and poor generaliza-tion. Bayesian methods avoid this, without the need for a valida-tion set, by averaging the outputs of many networks with weights sampled from the posterior distribution given the training data. This sample can be obtained by simulating a stochastic dynamical system that has the posterior as its stationary distribution.}
}

@phdthesis{neal_bayesian_1995,
  title = {Bayesian {{Learning}} for {{Neural Networks}}},
  author = {Neal, Radford M.},
  year = {1995},
  abstract = {Artificial ``neural networks'' are now widely used as flexible models for regression and classification applications, but questions remain regarding what these models mean, and how they can safely be used when training data is limited. Bayesian Learning for Neural Networks shows that Bayesian methods allow complex neural network models to be used without fear of the ``overfitting'' that can occur with traditional neural network learning methods. Insight into the nature of these complex Bayesian models is provided by a theoretical investigation of the priors over functions that underlie them. Use of these models in practice is made possible using Markov chain Monte Carlo techniques. Both the theoretical and computational aspects of this work are of wider statistical interest, as they contribute to a better understanding of how Bayesian methods can be applied to complex problems. Presupposing only basic knowledge of probability and statistics, this book should be of interest to many researchers in Statistics, Engineering, and Artificial Intelligence. Software for Unix systems that implements the methods described is freely available over the Internet.},
  school = {University of Toronto}
}

@article{neal_connectionist_1992,
  title = {Connectionist Learning of Belief Networks},
  author = {Neal, Radford M.},
  year = {1992},
  journal = {Artificial Intelligence},
  volume = {56},
  number = {1},
  pages = {71--113},
  issn = {00043702},
  abstract = {Connectionist learning procedures are presented for "sigmoid" and "noisy-OR" varieties of probabilistic belief networks. These networks have previously been seen primarily as a means of representing knowledge derived from experts. Here it is shown that the "Gibbs sampling" simulation procedure for such networks can support maximum-likelihood learning from empirical data through local gradient ascent. This learning procedure resembles that used for "Boltzmann machines", and like it, allows the use of "hidden" variables to model correlations between visible variables. Due to the directed nature of the connections in a belief network, however, the "negative phase" of Boltzmann machine learning is unnecessary. Experimental results show that, as a result, learning in a sigmoid belief network can be faster than in a Boltzmann machine. These networks have other advantages over Boltzmann machines in pattern classification and decision making applications, are naturally applicable to unsupervised learning problems, and provide a link between work on connectionist learning and work on the representation of expert knowledge. \textcopyright{} 1992.},
  isbn = {0004-3702}
}

@misc{neklyudov_involutive_2020,
  title = {Involutive {{MCMC}}: A {{Unifying Framework}}},
  shorttitle = {Involutive {{MCMC}}},
  author = {Neklyudov, Kirill and Welling, Max and Egorov, Evgenii and Vetrov, Dmitry},
  year = {2020},
  month = jun,
  eprint = {2006.16653},
  eprinttype = {arxiv},
  abstract = {Markov Chain Monte Carlo (MCMC) is a computational approach to fundamental problems such as inference, integration, optimization, and simulation. The field has developed a broad spectrum of algorithms, varying in the way they are motivated, the way they are applied and how efficiently they sample. Despite all the differences, many of them share the same core principle, which we unify as the Involutive MCMC (iMCMC) framework. Building upon this, we describe a wide range of MCMC algorithms in terms of iMCMC, and formulate a number of ``tricks'' which one can use as design principles for developing new MCMC algorithms. Thus, iMCMC provides a unified view of many known MCMC algorithms, which facilitates the derivation of powerful extensions. We demonstrate the latter with two examples where we transform known reversible MCMC algorithms into more efficient irreversible ones.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology}
}

@misc{neklyudov_orbital_2020,
  title = {Orbital {{MCMC}}},
  author = {Neklyudov, Kirill and Welling, Max},
  year = {2020},
  month = oct,
  eprint = {2010.08047},
  eprinttype = {arxiv},
  abstract = {Markov Chain Monte Carlo (MCMC) is a computational approach to fundamental problems such as inference, integration, optimization, and simulation. Recently, the framework (Involutive MCMC) was proposed describing a large body of MCMC algorithms via two components: a stochastic acceptance test and an involutive deterministic function. This paper demonstrates that this framework is a special case of a larger family of algorithms operating on orbits of continuous deterministic bijections. We describe this family by deriving a novel MCMC kernel, which we call orbital MCMC (oMCMC). We provide a theoretical analysis and illustrate its utility using simple examples.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Computation}
}

@article{nesterov_method_1983,
  title = {A {{Method}} of {{Solving A Convex Programming Problem With Convergence}} Rate {{O}}(1/K\^2)},
  author = {Nesterov, Yurii},
  year = {1983},
  journal = {Soviet Mathematics Doklady},
  volume = {27},
  number = {2},
  pages = {372--376}
}

@article{nesterov_random_2017,
  title = {Random {{Gradient-Free Minimization}} of {{Convex Functions}}},
  author = {Nesterov, Yurii and Spokoiny, Vladimir},
  year = {2017},
  journal = {Foundations of Computational Mathematics},
  volume = {17},
  number = {2},
  pages = {527--566},
  issn = {16153383},
  abstract = {In this paper, we prove new complexity bounds for methods of convex optimization based only on computation of the function value. The search directions of our schemes are normally distributed random Gaussian vectors. It appears that such methods usually need at most n times more iterations than the standard gradient methods, where n is the dimension of the space of variables. This conclusion is true for both nonsmooth and smooth problems. For the latter class, we present also an accelerated scheme with the expected rate of convergence O(n\^2/k\^2), where k is the iteration counter. For stochastic optimization, we propose a zero-order scheme and justify its expected rate of convergence O(n/k\^1/2).We give also some bounds for the rate of convergence of the random gradient-free methods to stationary points of nonconvex functions, for both smooth and nonsmooth cases. Our theoretical results are supported by preliminary computational experiments.},
  keywords = {Complexity bounds,Convex optimization,Derivative-free methods,Random methods,Stochastic optimization}
}

@inproceedings{netzer_reading_2011,
  title = {Reading {{Digits}} in {{Natural Images}} with {{Unsupervised Feature Learning}}},
  booktitle = {{{NeurIPS Workshop}} on {{Deep Learning}} and {{Unsupervised Feature Learning}} 2011},
  author = {Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew Y.},
  year = {2011},
  abstract = {Detecting and reading text from natural images is a hard computer vision task that is central to a variety of emerging applications. Related problems like document character recognition have been widely studied by computer vision and machine learning researchers and are virtually solved for practical applications like reading handwritten digits. Reliably recognizing characters in more complex scenes like photographs, however, is far more difficult: the best existing methods lag well behind human performance on the same tasks. In this paper we attack the problem of recognizing digits in a real application using unsupervised feature learning methods: reading house numbers from street level photos. To this end, we introduce a new benchmark dataset for research use containing over 600,000 labeled digits cropped from Street View images. We then demonstrate the difficulty of recognizing these digits when the problem is approached with hand-designed features. Finally, we employ variants of two recently proposed unsupervised feature learning methods and find that they are convincingly superior on our benchmarks.}
}

@article{ngiam_multimodal_nodate,
  title = {Multimodal {{Deep Learning}}},
  author = {Ngiam, Jiquan and Khosla, Aditya and Kim, Mingyu and Nam, Juhan and Lee, Honglak and Ng, Andrew Y},
  pages = {8},
  abstract = {Deep networks have been successfully applied to unsupervised feature learning for single modalities (e.g., text, images or audio). In this work, we propose a novel application of deep networks to learn features over multiple modalities. We present a series of tasks for multimodal learning and show how to train deep networks that learn features to address these tasks. In particular, we demonstrate cross modality feature learning, where better features for one modality (e.g., video) can be learned if multiple modalities (e.g., audio and video) are present at feature learning time. Furthermore, we show how to learn a shared representation between modalities and evaluate it on a unique task, where the classifier is trained with audio-only data but tested with video-only data and vice-versa. Our models are validated on the CUAVE and AVLetters datasets on audio-visual speech classification, demonstrating best published visual speech classification on AVLetters and effective shared representation learning.},
  langid = {english},
  keywords = {⛔ No DOI found}
}

@inproceedings{nguyen_deep_2015,
  title = {Deep Neural Networks Are Easily Fooled: {{High}} Confidence Predictions for Unrecognizable Images},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CCVPR}})},
  author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
  year = {2015},
  volume = {07-12-June},
  pages = {427--436},
  abstract = {Deep neural networks (DNNs) have recently been achieving state-of-the-art performance on a variety of pattern-recognition tasks, most notably visual classification problems. Given that DNNs are now able to classify objects in images with near-human-level performance, questions naturally arise as to what differences remain between computer and human vision. A recent study revealed that changing an image (e.g. of a lion) in a way imperceptible to humans can cause a DNN to label the image as something else entirely (e.g. mislabeling a lion a library). Here we show a related result: it is easy to produce images that are completely unrecognizable to humans, but that state-of-the-art DNNs believe to be recognizable objects with 99.99\% confidence (e.g. labeling with certainty that white noise static is a lion). Specifically, we take convolutional neural networks trained to perform well on either the ImageNet or MNIST datasets and then find images with evolutionary algorithms or gradient ascent that DNNs label with high confidence as belonging to each dataset class. It is possible to produce images totally unrecognizable to human eyes that DNNs believe with near certainty are familiar objects, which we call "fooling images" (more generally, fooling examples). Our results shed light on interesting differences between human vision and current DNNs, and raise questions about the generality of DNN computer vision.},
  isbn = {978-1-4673-6964-0}
}

@misc{nguyen_loss_2017,
  title = {The Loss Surface and Expressivity of Deep Convolutional Neural Networks},
  author = {Nguyen, Quynh and Hein, Matthias},
  year = {2017},
  month = oct,
  eprint = {1710.10928},
  eprinttype = {arxiv},
  abstract = {We analyze the expressiveness and loss surface of practical deep convolutional neural networks (CNNs) with shared weights and max pooling layers. We show that such CNNs produce linearly independent features at a "wide" layer which has more neurons than the number of training samples. This condition holds e.g. for the VGG network. Furthermore, we provide for such wide CNNs necessary and sufficient conditions for global minima with zero training error. For the case where the wide layer is followed by a fully connected layer, we show that almost every critical point of the empirical loss is a global minimum with zero training error. Our analysis suggests that both depth and width are very important in deep learning. While depth brings more representational power and allows the network to learn high level features, width smoothes the optimization landscape of the loss function in the sense that a sufficiently wide network has a well-behaved loss surface with potentially no bad local minima.},
  archiveprefix = {arXiv}
}

@misc{niculae_sparsemap_2018,
  title = {{{SparseMAP}}: {{Differentiable Sparse Structured Inference}}},
  shorttitle = {{{SparseMAP}}},
  author = {Niculae, Vlad and Martins, Andr{\'e} F. T. and Blondel, Mathieu and Cardie, Claire},
  year = {2018},
  month = jun,
  eprint = {1802.04223},
  eprinttype = {arxiv},
  abstract = {Structured prediction requires searching over a combinatorial number of structures. To tackle it, we introduce SparseMAP: a new method for sparse structured inference, and its natural loss function. SparseMAP automatically selects only a few global structures: it is situated between MAP inference, which picks a single structure, and marginal inference, which assigns nonzero probability to all structures, including implausible ones. SparseMAP can be computed using only calls to a MAP oracle, making it applicable to problems with intractable marginal inference, e.g., linear assignment. Sparsity makes gradient backpropagation efficient regardless of the structure, enabling us to augment deep neural networks with generic and sparse structured hidden layers. Experiments in dependency parsing and natural language inference reveal competitive accuracy, improved interpretability, and the ability to capture natural language ambiguities, which is attractive for pipeline systems.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {68T50,Computer Science - Computation and Language,Computer Science - Machine Learning,I.2.6,Statistics - Machine Learning}
}

@misc{nielsen_neural_2015,
  title = {Neural {{Networks}} and {{Deep Learning}}},
  author = {Nielsen, Michael},
  year = {2015},
  journal = {Determination Press},
  abstract = {Neural Networks and Deep Learning is a free online book. The book will teach you about: Neural networks, a beautiful biologically-inspired programming paradigm which enables a computer to learn from observational data Deep learning, a powerful set of techniques for learning in neural networks Neural networks and deep learning currently provide the best solutions to many problems in image recognition, speech recognition, and natural language processing. This book will teach you many of the core concepts behind neural networks and deep learning.},
  howpublished = {http://neuralnetworksanddeeplearning.com/index.html}
}

@inproceedings{nielsen_survae_2020,
  title = {{{SurVAE Flows}}: {{Surjections}} to {{Bridge}} the {{Gap}} between {{VAEs}} and {{Flows}}},
  booktitle = {34th {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}} 2020)},
  author = {Nielsen, Didrik and Jaini, Priyank and Hoogeboom, Emiel and Winther, Ole and Welling, Max},
  year = {2020},
  pages = {12},
  address = {{Vancouver, Canada}},
  abstract = {Normalizing flows and variational autoencoders are powerful generative models that can represent complicated density functions. However, they both impose constraints on the models: Normalizing flows use bijective transformations to model densities whereas VAEs learn stochastic transformations that are non-invertible and thus typically do not provide tractable estimates of the marginal likelihood. In this paper, we introduce SurVAE Flows: A modular framework of composable transformations that encompasses VAEs and normalizing flows. SurVAE Flows bridge the gap between normalizing flows and VAEs with surjective transformations, wherein the transformations are deterministic in one direction \textendash{} thereby allowing exact likelihood computation, and stochastic in the reverse direction \textendash{} hence providing a lower bound on the corresponding likelihood. We show that several recently proposed methods, including dequantization and augmented normalizing flows, can be expressed as SurVAE Flows. Finally, we introduce common operations such as the max value, the absolute value, sorting and stochastic permutation as composable layers in SurVAE Flows.},
  langid = {english}
}

@article{ning_spectral_nodate,
  title = {A {{Spectral Clustering Approach}} to {{Speaker Diarization}}},
  author = {Ning, Huazhong and Liu, Ming and Tang, Hao and Huang, Thomas},
  pages = {4},
  abstract = {In this paper, we present a spectral clustering approach to explore the possibility of discovering structure from audio data. To apply the Ng-Jordan-Weiss (NJW) spectral clustering algorithm to speaker diarization, we propose some domain specific solutions to the open issues of this algorithm: choice of metric; selection of scaling parameter; estimation of the number of clusters. Then, a postprocessing step \textendash{} ``Cross EM refinement'' \textendash{} is conducted to further improve the performance of spectral learning. In experiments, this approach has performance very similar to the traditional hierarchical clustering on the audio data of Japanese Parliament Panel Discussions, but it runs much faster than the latter.},
  langid = {english},
  keywords = {⛔ No DOI found}
}

@inproceedings{NIPS2014_2b38c2df,
  title = {Tree-Structured Gaussian Process Approximations},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bui, Thang D and Turner, Richard E},
  editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. and Weinberger, K. Q.},
  year = {2014},
  volume = {27},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{nix_estimating_1994,
  title = {Estimating the Mean and Variance of the Target Probability Distribution},
  booktitle = {Proceedings of 1994 {{IEEE}} International Conference on Neural Networks ({{ICNN}}'94)},
  author = {Nix, D.A. and Weigend, A.S.},
  year = {1994},
  volume = {1},
  pages = {55-60 vol.1}
}

@misc{noauthor_adversarial_nodate,
  title = {Adversarial {{Multi-task Learning}} of {{Deep Neural Networks}} for {{Robust Speech Recognition}} - {{Google-s\o gning}}},
  howpublished = {https://www.google.dk/search?q=Adversarial+Multi-task+Learning+of+Deep+Neural+Networks+for+Robust+Speech+Recognition\&oq=Adversarial+Multi-task+Learning+of+Deep+Neural+Networks+for+Robust+Speech+Recognition\&aqs=chrome..69i57.144j0j7\&sourceid=chrome\&ie=UTF-8}
}

@misc{noauthor_frontiers_nodate,
  title = {Frontiers | {{On Assessing Trustworthy AI}} in {{Healthcare}}. {{Machine Learning}} as a {{Supportive Tool}} to {{Recognize Cardiac Arrest}} in {{Emergency Calls}} | {{Human Dynamics}}},
  howpublished = {https://www.frontiersin.org/articles/10.3389/fhumd.2021.673104/full?utm\_source=F-NTF\&utm\_medium=EMLX\&utm\_campaign=PRD\_FEOPS\_20170000\_ARTICLE}
}

@inproceedings{noauthor_mask-based_2022,
  title = {Mask-Based {{Latent Reconstruction}} for {{Reinforcement Learning}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  year = {2022},
  abstract = {For deep reinforcement learning (RL) from pixels, learning effective state representations is crucial for achieving high performance. However, in practice, limited experience and high-dimensional input prevent effective representation learning. To address this, motivated by the success of masked modeling in other research fields, we introduce mask-based reconstruction to promote state representation learning in RL. Specifically, we propose a simple yet effective self-supervised method, Mask-based Latent Reconstruction (MLR), to predict the complete state representations in the latent space from the observations with spatially and temporally masked pixels. MLR enables the better use of context information when learning state representations to make them more informative, which facilitates RL agent training. Extensive experiments show that our MLR significantly improves the sample efficiency in RL and outperforms the state-of-the-art sample-efficient RL methods on multiple continuous benchmark environments.}
}

@misc{noauthor_roberts_nodate,
  title = {Roberts , {{Gelman}} , {{Gilks}} : {{Weak}} Convergence and Optimal Scaling of Random Walk {{Metropolis}} Algorithms},
  howpublished = {https://projecteuclid.org/euclid.aoap/1034625254}
}

@misc{noauthor_twelve-factor_nodate,
  title = {The {{Twelve-Factor App}}},
  howpublished = {https://12factor.net/}
}

@misc{noauthor_vaes_nodate,
  title = {{{VAEs}} in the {{Presence}} of {{Missing Data}}}
}

@book{nocedal_numerical_2006,
  title = {Numerical Optimization},
  author = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  publisher = {{Springer}},
  isbn = {0-387-30303-0},
  keywords = {★}
}

@article{norouzi_exemplar_nodate,
  title = {Exemplar {{VAE}}: {{Linking Generative Models}}, {{Nearest Neighbor Retrieval}}, and {{Data Augmentation}}},
  author = {Norouzi, Sajad and Fleet, David J and Norouzi, Mohamamd},
  pages = {12},
  abstract = {We introduce Exemplar VAEs, a family of generative models that bridge the gap between parametric and non-parametric, exemplar based generative models. Exemplar VAE is a variant of VAE with a non-parametric prior in the latent space based on a Parzen window estimator. To sample from it, one first draws a random exemplar from a training set, then stochastically transforms that exemplar into a latent code and a new observation. We propose retrieval augmented training (RAT) as a way to speed up Exemplar VAE training by using approximate nearest neighbor search in the latent space to define a lower bound on log marginal likelihood. To enhance generalization, model parameters are learned using exemplar leave-oneout and subsampling. Experiments demonstrate the effectiveness of Exemplar VAEs on density estimation and representation learning. Importantly, generative data augmentation using Exemplar VAEs on permutation invariant MNIST and Fashion MNIST reduces classification error from 1.17\% to 0.69\% and from 8.56\% to 8.16\%. Code is available at https://github.com/sajadn/Exemplar-VAE.},
  langid = {english}
}

@article{olah_attention_2016,
  title = {Attention and {{Augmented Recurrent Neural Networks}}},
  author = {Olah, Chris and Carter, Shan},
  year = {2016},
  month = sep,
  journal = {Distill},
  issn = {2476-0757},
  abstract = {Recurrent neural networks are one of the staples of deep learning, allowing neural networks to work with sequences of data like text, audio and video. They can be used to boil a sequence down into a high-level understanding, to annotate sequences, and even to generate new sequences from scratch!}
}

@misc{olah_understanding_2015,
  title = {Understanding {{LSTM Networks}}},
  author = {Olah, Chris},
  year = {2015},
  journal = {colah.github.io},
  abstract = {Humans don't start their thinking from scratch every second. As you read this essay, you understand each word based on your understanding of previous words. You don't throw everything away and start thinking from scratch again. Your thoughts have persistence. Traditional neural networks can't do this, and it seems like a major shortcoming. For example, imagine you want to classify what kind of event is happening at every point in a movie. It's unclear how a traditional neural network could use its reasoning about previous events in the film to inform later ones. Recurrent neural networks address this issue. They are networks with loops in them, allowing information to persist.},
  howpublished = {http://colah.github.io/posts/2015-08-Understanding-LSTMs/}
}

@article{olshausen_emergence_1996,
  title = {Emergence of Simple-Cell Receptive Field Properties by Learning a Sparse Code for Natural Images},
  author = {Olshausen, Bruno A. and Field, David J.},
  year = {1996},
  journal = {Nature},
  volume = {381},
  number = {6583},
  pages = {607--609},
  issn = {00280836},
  abstract = {THE receptive fields of simple cells in mammalian primary visual cortex can be characterized as being spatially localized, oriented1\textendash 4 and bandpass (selective to structure at different spatial scales), comparable to the basis functions of wavelet transforms5,6. One approach to understanding such response properties of visual neurons has been to consider their relationship to the statistical structure of natural images in terms of efficient coding7\textendash 12. Along these lines, a number of studies have attempted to train unsupervised learning algorithms on natural images in the hope of developing receptive fields with similar properties13\textendash 18, but none has succeeded in producing a full set that spans the image space and contains all three of the above properties. Here we investigate the proposal8,12 that a coding strategy that maximizes sparseness is sufficient to account for these properties. We show that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex. The resulting sparse image code provides a more efficient representation for later stages of processing because it possesses a higher degree of statistical independence among its outputs.},
  isbn = {9781612849379},
  pmid = {8637596}
}

@article{ondel_bayesian_2019,
  title = {Bayesian {{Subspace Hidden Markov Model}} for {{Acoustic Unit Discovery}}},
  author = {Ondel, Lucas and Vydana, Hari Krishna and Burget, Luk{\'a}{\v s} and {\v C}ernock{\'y}, Jan},
  year = {2019},
  month = jul,
  journal = {arXiv:1904.03876 [cs, eess, stat]},
  eprint = {1904.03876},
  eprinttype = {arxiv},
  primaryclass = {cs, eess, stat},
  abstract = {This work tackles the problem of learning a set of language specific acoustic units from unlabeled speech recordings given a set of labeled recordings from other languages. Our approach may be described by the following two steps procedure: first the model learns the notion of acoustic units from the labelled data and then the model uses its knowledge to find new acoustic units on the target language. We implement this process with the Bayesian Subspace Hidden Markov Model (SHMM), a model akin to the Subspace Gaussian Mixture Model (SGMM) where each low dimensional embedding represents an acoustic unit rather than just a HMM's state. The subspace is trained on 3 languages from the GlobalPhone corpus (German, Polish and Spanish) and the AUs are discovered on the TIMIT corpus. Results, measured in equivalent Phone Error Rate, show that this approach significantly outperforms previous HMM based acoustic units discovery systems and compares favorably with the Variational Auto Encoder-HMM.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning}
}

@article{ondel_variational_2016,
  title = {Variational {{Inference}} for {{Acoustic Unit Discovery}}},
  author = {Ondel, Lucas and Burget, Luka{\v s} and {\v C}ernock{\'y}, Jan},
  year = {2016},
  month = jan,
  journal = {Procedia Computer Science},
  series = {{{SLTU-2016}} 5th {{Workshop}} on {{Spoken Language Technologies}} for {{Under-resourced}} Languages 09-12 {{May}} 2016 {{Yogyakarta}}, {{Indonesia}}},
  volume = {81},
  pages = {80--86},
  issn = {1877-0509},
  abstract = {Recently, several nonparametric Bayesian models have been proposed to automatically discover acoustic units in unlabeled data. Most of them are trained using various versions of the Gibbs Sampling (GS) method. In this work, we consider Variational Bayes (VB) as alternative inference process. Even though VB yields an approximate solution of the posterior distribution it can be easily parallelized which makes it more suitable for large database. Results show that, notwithstanding VB inference is an order of magnitude faster, it outperforms GS in terms of accuracy.},
  langid = {english},
  keywords = {acoustic unit discovery,Bayesian non-parametric,Variational Bayes}
}

@inproceedings{oord_conditional_2016,
  title = {Conditional Image Generation with {{PixelCNN}} Decoders},
  booktitle = {Proceedings of the 29th {{International Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {van den Oord, A{\"a}ron and Kalchbrenner, Nal and Espeholt, Lasse and Kavukcuoglu, Koray and Vinyals, Oriol and Graves, Alex},
  year = {2016},
  pages = {4790--4798},
  address = {{Barcelona, Spain}}
}

@inproceedings{oord_neural_2018,
  title = {Neural {{Discrete Representation Learning}}},
  booktitle = {Proceedings of the 30th {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {van den Oord, A{\"a}ron and Vinyals, Oriol and Kavukcuoglu, Koray},
  year = {2018},
  address = {{Long Beach, CA, USA}},
  abstract = {Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector QuantisedVariational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of ``posterior collapse'' -\textemdash{} where the latents are ignored when they are paired with a powerful autoregressive decoder -\textemdash{} typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning}
}

@inproceedings{oord_parallel_2018,
  title = {Parallel {{WaveNet}}: {{Fast}} High-Fidelity Speech Synthesis},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {van den Oord, A{\"a}ron and Li, Yazhe and Babuschkin, Igor and Simonyan, Karen and Vinyals, Oriol and Kavukcuoglu, Koray and {van den Driessche}, George and Lockhart, Edward and Cobo, Luis C. and Stimberg, Florian and Casagrande, Norman and Grewe, Dominik and Noury, Seb and Dieleman, Sander and Elsen, Erich and Kalchbrenner, Nal and Zen, Heiga and Graves, Alex and King, Helen and Walters, Tom and Belov, Dan and Hassabis, Demis},
  year = {2018},
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {80},
  eprint = {1711.10433},
  eprinttype = {arxiv},
  pages = {3915--3923},
  publisher = {{PMLR}},
  address = {{Stockholm, Sweden}},
  archiveprefix = {arXiv}
}

@inproceedings{oord_pixel_2016,
  title = {Pixel {{Recurrent Neural Networks}}},
  booktitle = {Proceedings of the 33rd {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {van den Oord, A{\"a}ron and Kalchbrenner, Nal and Kavukcuoglu, Koray},
  year = {2016},
  month = aug,
  publisher = {{Journal of Machine Learning}},
  address = {{New York, NY, USA}},
  abstract = {Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast twodimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@misc{oord_representation_2018,
  title = {Representation {{Learning}} with {{Contrastive Predictive Coding}}},
  author = {van den Oord, A{\"a}ron and Li, Yazhe and Vinyals, Oriol},
  year = {2018},
  month = jul,
  eprint = {1807.03748},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{oord_wavenet_2016,
  title = {{{WaveNet}}: {{A Generative Model}} for {{Raw Audio}}},
  shorttitle = {{{WaveNet}}},
  booktitle = {Proceedings of the 9th {{ISCA Speech Synthesis Workshop}}},
  author = {van den Oord, A{\"a}ron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  year = {2016},
  month = sep,
  address = {{Sunnyval, CA, USA}},
  abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
  keywords = {Generative models,Raw waveform}
}

@misc{opencv_development_team_opencv_2018,
  title = {{{OpenCV}} Documentation},
  author = {{OpenCV Development Team}},
  year = {2018},
  journal = {Documentation},
  howpublished = {https://docs.opencv.org/master/}
}

@inproceedings{oquab_learning_2014,
  title = {Learning and Transferring Mid-Level Image Representations Using Convolutional Neural Networks},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CCVPR}})},
  author = {Oquab, Maxime and Bottou, L{\'e}on and Laptev, Ivan and Sivic, Josef},
  year = {2014},
  pages = {1717--1724}
}

@article{ormerod_explaining_2010,
  title = {Explaining {{Variational Approximations}}},
  author = {Ormerod, J. T. and Wand, M. P.},
  year = {2010},
  month = may,
  journal = {The American Statistician},
  volume = {64},
  number = {2},
  pages = {140--153},
  issn = {0003-1305, 1537-2731},
  langid = {english}
}

@article{osterwalder_business_2012,
  title = {Business {{Model Generation20124Alexander Osterwalder}} and {{Yves Pigneur}}. {{Business Model Generation}} . {{New York}}, {{NY}}: {{Wiley}} 2011. 288 Pp. \textsterling 23.99, {{ISBN}}: 9780470876411},
  author = {Osterwalder, Alexander and Pigneur, Yves},
  year = {2012},
  journal = {Kybernetes},
  volume = {41},
  number = {5/6},
  pages = {823--824},
  publisher = {{Wiley,}},
  issn = {0368-492X},
  isbn = {978-84-234-2841}
}

@article{ouali_overview_2020,
  title = {An {{Overview}} of {{Deep Semi-Supervised Learning}}},
  author = {Ouali, Yassine and Hudelot, C{\'e}line and Tami, Myriam},
  year = {2020},
  month = jul,
  journal = {arXiv:2006.05278 [cs, stat]},
  eprint = {2006.05278},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Deep neural networks demonstrated their ability to provide remarkable performances on a wide range of supervised learning tasks (e.g., image classification) when trained on extensive collections of labeled data (e.g., ImageNet). However, creating such large datasets requires a considerable amount of resources, time, and effort. Such resources may not be available in many practical cases, limiting the adoption and the application of many deep learning methods. In a search for more data-efficient deep learning methods to overcome the need for large annotated datasets, there is a rising research interest in semi-supervised learning and its applications to deep neural networks to reduce the amount of labeled data required, by either developing novel methods or adopting existing semi-supervised learning frameworks for a deep learning setting. In this paper, we provide a comprehensive overview of deep semi-supervised learning, starting with an introduction to the field, followed by a summarization of the dominant semi-supervised approaches in deep learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{ovinnikov_poincare_nodate,
  title = {Poincar\'e {{Wasserstein Autoencoder}}},
  author = {Ovinnikov, Ivan},
  pages = {8},
  abstract = {This work presents a reformulation of the recently proposed Wasserstein autoencoder framework on a non-Euclidean manifold, the Poincar\'e ball model of the hyperbolic space Hn. By assuming the latent space to be hyperbolic, we can use its intrinsic hierarchy to impose structure on the learned latent space representations. We demonstrate the model in the visual domain to analyze some of its properties and show competitive results on a graph link prediction task.},
  langid = {english},
  keywords = {⛔ No DOI found,Hyperbolic space}
}

@article{paisley_variational_nodate,
  title = {Variational {{Bayesian Inference}} with {{Stochastic Search}}},
  author = {Paisley, John and Blei, David M and Jordan, Michael I},
  pages = {8},
  abstract = {Mean-field variational inference is a method for approximate Bayesian posterior inference. It approximates a full posterior distribution with a factorized set of distributions by maximizing a lower bound on the marginal likelihood. This requires the ability to integrate a sum of terms in the log joint likelihood using this factorized distribution. Often not all integrals are in closed form, which is typically handled by using a lower bound. We present an alternative algorithm based on stochastic optimization that allows for direct optimization of the variational lower bound. This method uses control variates to reduce the variance of the stochastic search gradient, in which existing lower bounds can play an important role. We demonstrate the approach on two non-conjugate models: logistic regression and an approximation to the HDP.},
  langid = {english},
  keywords = {⛔ No DOI found}
}

@inproceedings{panayotov_librispeech_2015,
  title = {Librispeech: {{An ASR}} Corpus Based on Public Domain Audio Books},
  booktitle = {Proceedings of the {{International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
  year = {2015},
  pages = {5206--5210},
  address = {{Brisbane, Australia}}
}

@inproceedings{papamakarios_masked_2017,
  title = {Masked {{Autoregressive Flow}} for {{Density Estimation}}},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Papamakarios, George and Pavlakou, Theo and Murray, Iain},
  year = {2017},
  eprint = {1705.07057},
  eprinttype = {arxiv},
  address = {{Long Beach, CA, USA}},
  abstract = {Autoregressive models are among the best performing neural density estimators. We describe an approach for increasing the flexibility of an autoregressive model, based on modelling the random numbers that the model uses internally when generating data. By constructing a stack of autoregressive models, each modelling the random numbers of the next model in the stack, we obtain a type of normalizing flow suitable for density estimation, which we call Masked Autoregressive Flow. This type of flow is closely related to Inverse Autoregressive Flow and is a generalization of Real NVP. Masked Autoregressive Flow achieves state-of-the-art performance in a range of general-purpose density estimation tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{papamakarios_normalizing_2019,
  title = {Normalizing {{Flows}} for {{Probabilistic Modeling}} and {{Inference}}},
  author = {Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
  year = {2019},
  month = dec,
  eprint = {1912.02762},
  eprinttype = {arxiv},
  abstract = {Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{papineni_bleu_2002,
  title = {{{BLEU}}: A Method for Automatic Evaluation of Machine Translation},
  author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  year = {2002},
  journal = {Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics},
  number = {July},
  eprint = {1702.00764},
  eprinttype = {arxiv},
  pages = {311--318},
  issn = {00134686},
  abstract = {Human evaluations of machine translation are extensive but expensive. Human eval- uations can take months to finish and in- volve human labor that can not be reused. We propose a method of automatic ma- chine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evalu- ation, and that has little marginal cost per run. We present this method as an auto- mated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.},
  archiveprefix = {arXiv},
  isbn = {1-55860-883-4}
}

@inproceedings{park_analysis_2017,
  title = {Analysis on the {{Dropout Effect}} in {{Convolutional Neural Networks}}},
  booktitle = {Lecture {{Notes}} in {{Computer Science}} (Including Subseries {{Lecture Notes}} in {{Artificial Intelligence}} and {{Lecture Notes}} in {{Bioinformatics}})},
  author = {Park, Sungheon and Kwak, Nojun},
  year = {2017},
  volume = {10112 LNCS},
  pages = {189--204},
  issn = {16113349},
  abstract = {Regularizing neural networks is an important task to reduce overfitting. Dropout [1] has been a widelyused regularization trick for neural networks. In convolutional neural networks (CNNs), dropout is usually applied to the fully connected layers. Meanwhile, the regularization effect of dropout in the convolutional layers has not been thoroughly analyzed in the literature. In this paper, we analyze the effect of dropout in the convolutional layers, which is indeed proved as a powerful generalization method. We observed that dropout in CNNs regularizes the networks by adding noise to the output feature maps of each layer, yielding robustness to variations of images. Based on this observation, we propose a stochastic dropout whose drop ratio varies for each iteration. Furthermore, we propose a new regularization method which is inspired by behaviors of image filters. Rather than randomly drop the activation, we selectively drop the activations which have high values across the feature map or across the channels. Experimental results validate the regularization performance of selective max-drop and stochastic dropout is competitive to the dropout or spatial dropout [2]. \textcopyright{} Springer International Publishing AG 2017.},
  isbn = {978-3-319-54183-9}
}

@article{park_efficient_2013,
  title = {An Efficient {{MapReduce}} Algorithm for Counting Triangles in a Very Large Graph},
  author = {Park, Ha-Myung and Chung, Chin-Wan},
  year = {2013},
  journal = {CIKM},
  abstract = {Triangle counting problem is one of the fundamental prob-lem in various domains. The problem can be utilized for computation of clustering coefficient, transitivity, trianglu-lar connectivity, trusses, etc. The problem have been exten-sively studied in internal memory but the algorithms are not scalable for enormous graphs. In recent years, the MapRe-duce has emerged as a de facto standard framework for pro-cessing large data through parallel computing. A MapRe-duce algorithm was proposed for the problem based on graph partitioning. However, the algorithm redundantly generates a large number of intermediate data that cause network over-load and prolong the processing time. In this paper, we pro-pose a new algorithm based on graph partitioning with a novel idea of triangle classification to count the number of triangles in a graph. The algorithm substantially reduces the duplication by classifying triangles into three types and processing each triangle differently according to its type. In the experiments, we compare the proposed algorithm with recent existing algorithms using both synthetic datasets and real-world datasets that are composed of millions of nodes and billions of edges. The proposed algorithm outperforms other algorithms in most cases. Especially, for a twitter dataset, the proposed algorithm is more than twice as fast as existing MapReduce algorithms. Moreover, the performance gap increases as the graph becomes larger and denser.},
  isbn = {9781450322638},
  keywords = {Graph,MapReduce}
}

@misc{park_hierarchical_2018,
  title = {A {{Hierarchical Latent Structure}} for {{Variational Conversation Modeling}}},
  author = {Park, Yookoon and Cho, Jaemin and Kim, Gunhee},
  year = {2018},
  month = apr,
  eprint = {1804.03424},
  eprinttype = {arxiv},
  abstract = {Variational autoencoders (VAE) combined with hierarchical RNNs have emerged as a powerful framework for conversation modeling. However, they suffer from the notorious degeneration problem, where the decoders learn to ignore latent variables and reduce to vanilla RNNs. We empirically show that this degeneracy occurs mostly due to two reasons. First, the expressive power of hierarchical RNN decoders is often high enough to model the data using only its decoding distributions without relying on the latent variables. Second, the conditional VAE structure whose generation process is conditioned on a context, makes the range of training targets very sparse; that is, the RNN decoders can easily overfit to the training data ignoring the latent variables. To solve the degeneration problem, we propose a novel model named Variational Hierarchical Conversation RNNs (VHCR), involving two key ideas of (1) using a hierarchical structure of latent variables, and (2) exploiting an utterance drop regularization. With evaluations on two datasets of Cornell Movie Dialog and Ubuntu Dialog Corpus, we show that our VHCR successfully utilizes latent variables and outperforms state-of-the-art models for conversation generation. Moreover, it can perform several new utterance control tasks, thanks to its hierarchical latent structure.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{park_specaugment:_2019,
  title = {{{SpecAugment}}: {{A Simple Data Augmentation Method}} for {{Automatic Speech Recognition}}},
  shorttitle = {{{SpecAugment}}},
  author = {Park, Daniel S. and Chan, William and Zhang, Yu and Chiu, Chung-Cheng and Zoph, Barret and Cubuk, Ekin D. and Le, Quoc V.},
  year = {2019},
  month = apr,
  eprint = {1904.08779},
  eprinttype = {arxiv},
  abstract = {We present SpecAugment, a simple data augmentation method for speech recognition. SpecAugment is applied directly to the feature inputs of a neural network (i.e., filter bank coefficients). The augmentation policy consists of warping the features, masking blocks of frequency channels, and masking blocks of time steps. We apply SpecAugment on Listen, Attend and Spell networks for end-to-end speech recognition tasks. We achieve state-of-the-art performance on the LibriSpeech 960h and Swichboard 300h tasks, outperforming all prior work. On LibriSpeech, we achieve 6.8\% WER on test-other without the use of a language model, and 5.8\% WER with shallow fusion with a language model. This compares to the previous state-of-the-art hybrid system of 7.5\% WER. For Switchboard, we achieve 7.2\%/14.6\% on the Switchboard/CallHome portion of the Hub5'00 test set without the use of a language model, and 6.8\%/14.1\% with shallow fusion, which compares to the previous state-of-the-art hybrid system at 8.3\%/17.3\% WER.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning}
}

@article{partaourides_asymmetric_2017,
  title = {Asymmetric Deep Generative Models},
  author = {Partaourides, Harris and Chatzis, Sotirios P.},
  year = {2017},
  month = jun,
  journal = {Neurocomputing},
  volume = {241},
  pages = {90--96},
  abstract = {Amortized variational inference, whereby the inferred latent variable posterior distributions are parameterized by means of neural network functions, has invigorated a new wave of innovation in the field of generative latent variable modeling, giving rise to the family of deep generative models (DGMs). Existing DGM formulations are based on the assumption of a symmetric Gaussian posterior over the model latent variables. This assumption, although mathematically convenient, can be well-expected to undermine the eventually obtained representation power, as it imposes apparent expressiveness limitations. Indeed, it has been recently shown that even some moderate increase in the latent variable posterior expressiveness, obtained by introducing an additional level of dependencies upon auxiliary (Gaussian) latent variables, can result in significant performance improvements in the context of semi-supervised learning tasks. Inspired from these advances, in this paper we examine whether a more potent increase in the expressiveness and representation power of modern DGMs can be achieved by completely relaxing their typical symmetric (Gaussian) latent variable posterior assumptions: Specifically, we consider DGMs with asymmetric posteriors, formulated as restricted multivariate skew-Normal (rMSN) distributions. We derive an efficient amortized variational inference algorithm for the proposed model, and exhibit its superiority over the current state-of-the-art in several semi-supervised learning benchmarks.},
  langid = {english},
  keywords = {Deep generative models,Restricted multivariate skew-Normal distribution,Semi-supervised learning,Variational inference}
}

@article{pascanu_diculty_nodate,
  title = {On the Difficulty of Training Recurrent Neural Networks},
  author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  pages = {9},
  abstract = {There are two widely known issues with properly training recurrent neural networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
  langid = {english},
  keywords = {⛔ No DOI found}
}

@misc{pascanu_how_2013,
  title = {How to {{Construct Deep Recurrent Neural Networks}}},
  author = {Pascanu, Razvan and G{\"u}l{\c c}ehre, {\c C}aglar and Cho, Kyunghyun and Bengio, Yoshua and Gulcehre, Caglar and Cho, Kyunghyun and Bengio, Yoshua},
  year = {2013},
  month = dec,
  eprint = {1312.6026},
  eprinttype = {arxiv},
  abstract = {In this paper, we propose a novel way to extend a recurrent neural network (RNN) to a deep RNN. We start by arguing that the concept of the depth in an RNN is not as clear as it is in feedforward neural networks. By carefully analyzing and understanding the architecture of an RNN, we define three points which may be made deeper; (1) input-to-hidden function, (2) hidden-to-hidden transition and (3) hidden-to-output function. This can be considered in addition to stacking multiple recurrent layers proposed earlier by Schmidhuber (1992). Based on this observation, we propose two novel architectures of a deep RNN and provide an alternative interpretation of these deep RNN's using a novel framework based on neural operators. The proposed deep RNN's are empirically evaluated on the tasks of polyphonic music prediction and language modeling. The experimental result supports our claim that the proposed deep RNN's benefit from the depth and outperform the conventional, shallow RNN.},
  archiveprefix = {arXiv}
}

@article{pascual_learning_2019,
  title = {Learning {{Problem-agnostic Speech Representations}} from {{Multiple Self-supervised Tasks}}},
  author = {Pascual, Santiago and Ravanelli, Mirco and Serr{\`a}, Joan and Bonafonte, Antonio and Bengio, Yoshua},
  year = {2019},
  month = apr,
  journal = {arXiv:1904.03416 [cs, eess, stat]},
  eprint = {1904.03416},
  eprinttype = {arxiv},
  primaryclass = {cs, eess, stat},
  abstract = {Learning good representations without supervision is still an open issue in machine learning, and is particularly challenging for speech signals, which are often characterized by long sequences with a complex hierarchical structure. Some recent works, however, have shown that it is possible to derive useful speech representations by employing a self-supervised encoder-discriminator approach. This paper proposes an improved self-supervised method, where a single neural encoder is followed by multiple workers that jointly solve different self-supervised tasks. The needed consensus across different tasks naturally imposes meaningful constraints to the encoder, contributing to discover general representations and to minimize the risk of learning superficial ones. Experiments show that the proposed approach can learn transferable, robust, and problem-agnostic features that carry on relevant information from the speech signal, such as speaker identity, phonemes, and even higher-level features such as emotional cues. In addition, a number of design choices make the encoder easily exportable, facilitating its direct usage or adaptation to different problems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning}
}

@inproceedings{paszke_automatic_2017,
  title = {Automatic Differentiation in {{PyTorch}}},
  booktitle = {Proceedings of the 31st {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Paszke, Adam and Chanan, Gregory and Lin, Zeming and Gross, Sam and Yang, Edward and Antiga, Luca and Devito, Zachary},
  year = {2017},
  abstract = {In this article, we describe an automatic differentiation module of PyTorch \textemdash{} a library designed to enable rapid research on machine learning models. It builds upon a few projects, most notably Lua Torch, Chainer, and HIPS Autograd, and provides a high performance environment with easy access to automatic differentiation of models executed on different devices (CPU and GPU). To make prototyping easier, PyTorch does not follow the symbolic approach used in many other deep learning frameworks, but focuses on differentiation of purely imperative programs, with a focus on extensibility and low overhead. Note that this preprint is a draft of certain sections from an upcoming paper covering all PyTorch features.}
}

@inproceedings{pathak_context_2016,
  title = {Context {{Encoders}}: {{Feature Learning}} by {{Inpainting}}},
  shorttitle = {Context {{Encoders}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Pathak, Deepak and Krahenbuhl, Philipp and Donahue, Jeff and Darrell, Trevor and Efros, Alexei A.},
  year = {2016},
  pages = {2536--2544}
}

@inproceedings{pathak_curiosity-driven_2017,
  title = {Curiosity-{{Driven Exploration}} by {{Self-Supervised Prediction}}},
  booktitle = {{{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}}},
  author = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A. and Darrell, Trevor},
  year = {2017},
  volume = {2017-July},
  eprint = {1705.05363},
  eprinttype = {arxiv},
  pages = {488--489},
  issn = {21607516},
  abstract = {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch. Demo video and code available at https://pathak22.github.io/noreward-rl/},
  archiveprefix = {arXiv},
  isbn = {978-1-5386-0733-6}
}

@misc{paulus_deep_2017,
  title = {A {{Deep Reinforced Model}} for {{Abstractive Summarization}}},
  author = {Paulus, Romain and Xiong, Caiming and Socher, Richard},
  year = {2017},
  eprint = {1705.04304},
  eprinttype = {arxiv},
  abstract = {Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents and summaries however these models often include repetitive and incoherent phrases. We introduce a neural network model with a novel intra-attention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL). Models trained only with supervised learning often exhibit "exposure bias" - they assume ground truth is provided at each step during training. However, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable. We evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, an improvement over previous state-of-the-art models. Human evaluation also shows that our model produces higher quality summaries.},
  archiveprefix = {arXiv},
  isbn = {2004012439},
  pmid = {23459267}
}

@misc{paulus_gradient_2021,
  title = {Gradient {{Estimation}} with {{Stochastic Softmax Tricks}}},
  author = {Paulus, Max B. and Choi, Dami and Tarlow, Daniel and Krause, Andreas and Maddison, Chris J.},
  year = {2021},
  month = feb,
  eprint = {2006.08063},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The Gumbel-Max trick is the basis of many relaxed gradient estimators. These estimators are easy to implement and low variance, but the goal of scaling them comprehensively to large combinatorial distributions is still outstanding. Working within the perturbation model framework, we introduce stochastic softmax tricks, which generalize the Gumbel-Softmax trick to combinatorial spaces. Our framework is a unified perspective on existing relaxed estimators for perturbation models, and it contains many novel relaxations. We design structured relaxations for subset selection, spanning trees, arborescences, and others. When compared to less structured baselines, we find that stochastic softmax tricks can be used to train latent variable models that perform better and discover more latent structure.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Gumbel softmax,Statistics - Machine Learning}
}

@article{pearl_causal_2009,
  title = {Causal Inference in Statistics: {{An}} Overview},
  shorttitle = {Causal Inference in Statistics},
  author = {Pearl, Judea},
  year = {2009},
  journal = {Statistics Surveys},
  volume = {3},
  number = {0},
  pages = {96--146},
  issn = {1935-7516},
  abstract = {This review presents empirical researchers with recent advances in causal inference, and stresses the paradigmatic shifts that must be undertaken in moving from traditional statistical analysis to causal analysis of multivariate data. Special emphasis is placed on the assumptions that underly all causal inferences, the languages used in formulating those assumptions, the conditional nature of all causal and counterfactual claims, and the methods that have been developed for the assessment of such claims. These advances are illustrated using a general theory of causation based on the Structural Causal Model (SCM) described in Pearl (2000a), which subsumes and unifies other approaches to causation, and provides a coherent mathematical foundation for the analysis of causes and counterfactuals. In particular, the paper surveys the development of mathematical tools for inferring (from a combination of data and assumptions) answers to three types of causal queries: (1) queries about the effects of potential interventions, (also called ``causal effects'' or ``policy evaluation'') (2) queries about probabilities of counterfactuals, (including assessment of ``regret,'' ``attribution'' or ``causes of effects'') and (3) queries about direct and indirect effects (also known as ``mediation''). Finally, the paper defines the formal and conceptual relationships between the structural and potential-outcome frameworks and presents tools for a symbiotic analysis that uses the strong features of both.},
  langid = {english},
  keywords = {★}
}

@inproceedings{pennington_glove:_2014,
  title = {Glove: {{Global Vectors}} for {{Word Representation}}},
  booktitle = {Proceedings of the {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D.},
  year = {2014},
  volume = {14},
  eprint = {1504.06654},
  eprinttype = {arxiv},
  pages = {1532--1543},
  issn = {10495258},
  abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arith- metic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global log- bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co- occurrence matrix, rather than on the en- tire sparse matrix or on individual context windows in a large corpus. On a recent word analogy task our model obtains 75\% accuracy, an improvement of 11\% over Mikolov et al. (2013). It also outperforms related word vector models on similarity tasks and named entity recognition.},
  archiveprefix = {arXiv},
  isbn = {978-1-937284-96-1},
  pmid = {1710995}
}

@article{pervez_spectral_nodate,
  title = {Spectral {{Smoothing Unveils Phase Transitions}} in {{Hierarchical Variational Autoencoders}}},
  author = {Pervez, Adeel and Gavves, Efstratios},
  pages = {10},
  abstract = {Variational autoencoders with deep stochastic hierarchies are known to suffer from the problem of posterior collapse, where the top layers fall back to the prior and become independent of input. We suggest that the hierarchical VAE objective explicitly includes the variance of the function parameterizing the mean and variance of the latent Gaussian distribution which itself is often a high variance function. Building on this we generalize VAE neural networks by incorporating a smoothing parameter motivated by Gaussian analysis to reduce higher frequency components and consequently the variance in parameterizing functions. We show this helps to solve the problem of posterior collapse. We further show that under such smoothing the VAE loss exhibits a phase transition, where the top layer KL divergence sharply drops to zero at a critical value of the smoothing parameter that is similar for the same model across datasets. We validate the phenomenon across model configurations and datasets.},
  langid = {english}
}

@misc{peters_deep_2018,
  title = {Deep Contextualized Word Representations},
  author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  year = {2018},
  eprint = {1802.05365},
  eprinttype = {arxiv},
  abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
  archiveprefix = {arXiv}
}

@article{peters_interpretable_nodate,
  title = {Interpretable {{Structure Induction}} via {{Sparse Attention}}},
  author = {Peters, Ben and Niculae, Vlad and Martins, Andr{\'e} F T},
  pages = {3},
  langid = {english}
}

@phdthesis{peters_machine_2007,
  title = {Machine {{Learning}} of Motor {{Skills}} for {{Robotics}}},
  author = {Peters, Jan},
  year = {2007},
  journal = {University of Southern California},
  number = {April},
  abstract = {Autonomous robots that can assist humans in situations of daily life have been a long standing vision of robotics, artificial intelligence, and cognitive sciences. A first step towards this goal is to create robots that can accomplish a multitude of different tasks, triggered by environmental context or higher level instruction. Early approaches to this goal during the heydays of artificial intelligence research in the late 1980ies, however, made it clear that an approach purely based on reasoning and human insights would not be able to model all the perceptuomotor tasks that a robot should fulfill. Instead, new hope was put in the growing wake of machine learning that promised fully adaptive control algorithms which learn both by observation and trial-and-error. However, to date, learning techniques have yet to fulfill this promise as only few methods manage to scale into the high-dimensional domains of manipulator robotics, or even the new upcoming trend of humanoid robotics, and usually scaling was only achieved in precisely pre-structured domains. In this thesis, we investigate the ingredients for a general approach to motor skill learning in order to get one step closer towards human-like performance. For doing so, we study two major components for such an approach, i.e., firstly, a theoretically well-founded general approach to representing the required control structures for task representation and execution and, secondly, appropriate learning algorithms which can be applied in this setting. As a theoretical foundation, we first study a general framework to generate control laws for real robots with a particular focus on skills represented as dynamical systems in differential constraint form. We present a point-wise optimal control framework resulting from a generalization of Gauss' principle and show how various well-known robot control laws can be derived by modifying the metric of the employed cost function. The framework has been successfully applied to task space tracking control for holonomic systems for several different metrics on the anthropomorphic SARCOS Master Arm. In order to overcome the limiting requirement of accurate robot models, we first employ learning methods to find learning controllers for task space control. However, when learning to execute a redundant control problem, we face the general problem of the non-convexity of the solution space which can force the robot to steer into physically impossible configurations if supervised learning methods are employed without further consideration. This problem can be resolved using two major insights, i.e., the learning problem can be treated as locally convex and the cost function of the analytical framework can be used to ensure global consistency. Thus, we derive an immediate reinforcement learning algorithm from the expectation-maximization point of view which results in a reward-weighted regression technique. This method can be used both for operational space control as well as general immediate reward reinforcement learning problems. We demonstrate the feasibility of the resulting framework on the problem of redundant end-effector tracking for both a simulated 3 degrees of freedom robot arm as well as for a simulated anthropomorphic SARCOS Master Arm. While learning to execute tasks in task space is an essential component to a general framework to motor skill learning, learning the actual task is of even higher importance, particularly as this issue is more frequently beyond the abilities of analytical approaches than execution. We focus on the learning of elemental tasks which can serve as the ``building blocks of movement generation'', called motor primitives. Motor primitives are parameterized task representations based on splines or nonlinear differential equations with desired attractor properties. While imitation learning of parameterized motor primitives is a relatively well-understood problem, the self-improvement by interaction of the system with the environment remains a challenging problem, tackled in the fourth chapter of this thesis. For pursuing this goal, we highlight the difficulties with current reinforcement learning methods, and outline both established and novel algorithms for the gradient-based improvement of parameterized policies. We compare these algorithms in the context of motor primitive learning, and show that our most modern algorithm, the Episodic Natural Actor-Critic outperforms previous algorithms by at least an order of magnitude. We demonstrate the efficiency of this reinforcement learning method in the application of learning to hit a baseball with an anthropomorphic robot arm. In conclusion, in this thesis, we have contributed a general framework for analytically computing robot control laws which can be used for deriving various previous control approaches and serves as foundation as well as inspiration for our learning algorithms. We have introduced two classes of novel reinforcement learning methods, i.e., the Natural Actor-Critic and the Reward-Weighted Regression algorithm. These algorithms have been used in order to replace the analytical components of the theoretical framework by learned representations. Evaluations have been performed on both simulated and real robot arms.},
  school = {University of Southern California}
}

@misc{petersen_matrix_2012,
  title = {The {{Matrix Cookbook}}},
  author = {Petersen, Kaare Breandt and Pedersen, Michael Syskind},
  year = {2012},
  publisher = {{Technical University of Denmark}},
  address = {{Kongens Lyngby, Denmark}},
  abstract = {These pages are a collection of facts (identities, approxima- tions, inequalities, relations, ...) about matrices and matters relating to them. It is collected in this form for the convenience of anyone who wants a quick desktop reference .},
  keywords = {★,Decompositions,Determinant,Linear algebra,Matrix algebra,Matrix calculus,Matrix identities,Matrix relations,Multivariate calculus,Probability,Reference work,Vector calculus}
}

@inproceedings{pham_dropout_2014,
  title = {Dropout {{Improves Recurrent Neural Networks}} for {{Handwriting Recognition}}},
  booktitle = {Proceedings of {{International Conference}} on {{Frontiers}} in {{Handwriting Recognition}} ({{ICFHR}})},
  author = {Pham, Vu and Bluche, Theodore and Kermorvant, Christopher and Louradour, Jerome},
  year = {2014},
  eprint = {1312.4569},
  eprinttype = {arxiv},
  pages = {285--290},
  issn = {21676453},
  abstract = {Recurrent neural networks (RNNs) with Long Short-Term memory cells currently hold the best known results in unconstrained handwriting recognition. We show that their performance can be greatly improved using dropout - a recently proposed regularization method for deep architectures. While previous works showed that dropout gave superior performance in the context of convolutional networks, it had never been applied to RNNs. In our approach, dropout is carefully used in the network so that it does not affect the recurrent connections, hence the power of RNNs in modeling sequence is preserved. Extensive experiments on a broad range of handwritten databases confirm the effectiveness of dropout on deep architectures even when the network mainly consists of recurrent and shared connections.},
  archiveprefix = {arXiv},
  isbn = {978-1-4799-4334-0},
  keywords = {Dropout,Handwriting recognition,Recurrent neural network}
}

@article{polyak_acceleration_1992,
  title = {Acceleration of {{Stochastic Approximation}} by {{Averaging}}},
  author = {Polyak, B. T. and Juditsky, A. B.},
  year = {1992},
  month = jul,
  journal = {SIAM Journal on Control and Optimization},
  volume = {30},
  number = {4},
  pages = {838--855},
  issn = {0363-0129, 1095-7138},
  abstract = {A new recursive algorithm of stochastic approximation type with the averaging of trajectories is investigated. Convergence with probability one is proved for a variety of classical optimization and identification problems. It is also demonstrated for these problems that the proposed algorithm achieves the highest possible rate of convergence.},
  langid = {english},
  keywords = {Stochastic optimisation,Weight averaging}
}

@inproceedings{poole_variational_2019,
  title = {On {{Variational Bounds}} of {{Mutual Information}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Poole, Ben and Ozair, Sherjil},
  year = {2019},
  month = jun,
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {97},
  pages = {5171--5180},
  publisher = {{PMLR}},
  address = {{Long Beach, CA, USA}},
  abstract = {Estimating and optimizing Mutual Information (MI) is core to many problems in machine learning; however, bounding MI in high dimensions is challenging. To establish tractable and scalable objectives, recent work has turned to variational bounds parameterized by neural networks, but the relationships and tradeoffs between these bounds remains unclear. In this work, we unify these recent developments in a single framework. We find that the existing variational lower bounds degrade when the MI is large, exhibiting either high bias or high variance. To address this problem, we introduce a continuum of lower bounds that encompasses previous bounds and flexibly trades off bias and variance. On high-dimensional, controlled problems, we empirically characterize the bias and variance of the bounds and their gradients and demonstrate the effectiveness of our new bounds for estimation and representation learning.},
  langid = {english}
}

@misc{pratap_wav2letter++:_2018,
  title = {Wav2letter++: {{The Fastest Open-source Speech Recognition System}}},
  shorttitle = {Wav2letter++},
  author = {Pratap, Vineel and Hannun, Awni and Xu, Qiantong and Cai, Jeff and Kahn, Jacob and Synnaeve, Gabriel and Liptchinsky, Vitaliy and Collobert, Ronan},
  year = {2018},
  month = dec,
  eprint = {1812.07625},
  eprinttype = {arxiv},
  abstract = {This paper introduces wav2letter++, the fastest open-source deep learning speech recognition framework. wav2letter++ is written entirely in C++, and uses the ArrayFire tensor library for maximum efficiency. Here we explain the architecture and design of the wav2letter++ system and compare it to other major open-source speech recognition systems. In some cases wav2letter++ is more than 2x faster than other optimized frameworks for training end-to-end neural networks for speech recognition. We also show that wav2letter++'s training times scale linearly to 64 GPUs, the highest we tested, for models with 100 million parameters. High-performance frameworks enable fast iteration, which is often a crucial factor in successful research and model tuning on new datasets and tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@inproceedings{prenger_waveglow_2019,
  title = {Waveglow: {{A}} Flow-Based Generative Network for Speech Synthesis},
  booktitle = {{{IEEE}} International Conference on Acoustics, Speech and Signal Processing, {{ICASSP}} 2019, Brighton, United Kingdom, May 12-17, 2019},
  author = {Prenger, Ryan and Valle, Rafael and Catanzaro, Bryan},
  year = {2019},
  pages = {3617--3621},
  publisher = {{IEEE}}
}

@book{press_numerical_2007,
  title = {Numerical Recipes},
  author = {Press, William H. and a. Teukolsky, Saul and Vetterling, William T. and Flannery, Brian P.},
  year = {2007},
  journal = {Cambridge University Press},
  volume = {29},
  abstract = {From Preface: ``I was just going to say, when I was interrupted:::'' begins Oliver Wendell Holmes in the second series of his famous essays, The Autocrat of the Breakfast Table. The interruption referred towas a gap of 25 years. In our case, as the autocrats of Numerical Recipes, the gap between our second and third editions has been ``only'' 15 years. Scientific computing has changed enormously in that time. The first edition of Numerical Recipes was roughly coincident with the first commercial success of the personal computer. The second edition came at about the time that the Internet, as we know it today, was created. Now, as we launch the third edition, the practice of science and engineering, and thus scientific computing, has been profoundly altered by the mature Internet and Web. It is no longer difficult to find somebody's algorithm, and usually free code, for almost any conceivable scien- tific application. The critical questions have instead become, ``How does it work?'' and ``Is it any good?'' Correspondingly, the second edition of Numerical Recipes has come to be valued more and more for its text explanations, concise mathematical derivations, critical judgments, and advice, and less for its code implementations per se. Recognizing the change, we have expanded and improved the text in many places in this edition and addedmany completely newsections. We seriously consid- ered leaving the code out entirely, or making it available only on theWeb. However, in the end, we decided that without code, it wouldn't be Numerical Recipes.That is, without code you, the reader, could never know whether our advice was in fact hon- est, implementable, and practical. Many discussions of algorithms in the literature and on the Web omit crucial details that can only be uncovered by actually coding (our job) or reading compilable code (your job). Also, we needed actual code to teach and illustrate the large number of lessons about object-oriented programming that are implicit and explicit in this edition.},
  isbn = {0-521-88068-8}
}

@inproceedings{qian_momentum_1999,
  title = {On the Momentum Term in Gradient Descent Learning Algorithms},
  booktitle = {Proceedings of the {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Qian, Ning},
  year = {1999},
  volume = {12},
  pages = {145--151},
  issn = {08936080},
  abstract = {A momentum term is usually included in the simulations of connectionist learning algorithms. Although it is well known that such a term greatly improves the speed of learning, there have been few rigorous studies of its mechanisms. In this paper, I show that in the limit of continuous time, the momentum parameter is analogous to the mass of Newtonian particles that move through a viscous medium in a conservative force field. The behavior of the system near a local minimum is equivalent to a set of coupled and damped harmonic oscillators. The momentum term improves the speed of convergence by bringing some eigen components of the system closer to critical damping. Similar results can be obtained for the discrete time case used in computer simulations. In particular, I derive the bounds for convergence on learning- rate and momentum parameters, and demonstrate that the momentum term can increase the range of learning rate over which the system converges. The optimal condition for convergence is also analyzed.},
  isbn = {1-212-54352-1},
  pmid = {12662723},
  keywords = {Critical damping,Damped harmonic oscillator,Gradient descent,Learning rate,Momentum,Speed of convergence}
}

@article{quitry_learning_2019,
  title = {Learning Audio Representations via Phase Prediction},
  author = {Quitry, F{\'e}lix de Chaumont and Tagliasacchi, Marco and Roblek, Dominik},
  year = {2019},
  month = oct,
  journal = {arXiv:1910.11910 [cs, eess]},
  eprint = {1910.11910},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {We learn audio representations by solving a novel self-supervised learning task, which consists of predicting the phase of the short-time Fourier transform from its magnitude. A convolutional encoder is used to map the magnitude spectrum of the input waveform to a lower dimensional embedding. A convolutional decoder is then used to predict the instantaneous frequency (i.e., the temporal rate of change of the phase) from such embedding. To evaluate the quality of the learned representations, we evaluate how they transfer to a wide variety of downstream audio tasks. Our experiments reveal that the phase prediction task leads to representations that generalize across different tasks, partially bridging the gap with fully-supervised models. In addition, we show that the predicted phase can be used as initialization of the Griffin-Lim algorithm, thus reducing the number of iterations needed to reconstruct the waveform in the time domain.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@misc{radford_learning_2021,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  year = {2021},
  month = feb,
  eprint = {2103.00020},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@misc{radford_unsupervised_2015,
  title = {Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks},
  author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
  year = {2015},
  eprint = {1511.06434},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@misc{raffel_feed-forward_2015,
  title = {Feed-{{Forward Networks}} with {{Attention Can Solve Some Long-Term Memory Problems}}},
  author = {Raffel, Colin and Ellis, Daniel P. W.},
  year = {2015},
  month = dec,
  eprint = {1512.08756},
  eprinttype = {arxiv},
  abstract = {We propose a simplified model of attention which is applicable to feed-forward neural networks and demonstrate that the resulting model can solve the synthetic ``addition'' and ``multiplication'' long-term memory problems for sequence lengths which are both longer and more widely varying than the best published results for these tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@misc{rainforth_tighter_2019,
  title = {Tighter {{Variational Bounds}} Are {{Not Necessarily Better}}},
  author = {Rainforth, Tom and Kosiorek, Adam R. and Le, Tuan Anh and Maddison, Chris J. and Igl, Maximilian and Wood, Frank and Teh, Yee Whye},
  year = {2019},
  month = mar,
  eprint = {1802.04537},
  eprinttype = {arxiv},
  abstract = {We provide theoretical and empirical evidence that using tighter evidence lower bounds (ELBOs) can be detrimental to the process of learning an inference network by reducing the signal-to-noise ratio of the gradient estimator. Our results call into question common implicit assumptions that tighter ELBOs are better variational objectives for simultaneous model learning and inference amortization schemes. Based on our insights, we introduce three new algorithms: the partially importance weighted auto-encoder (PIWAE), the multiply importance weighted auto-encoder (MIWAE), and the combination importance weighted autoencoder (CIWAE), each of which includes the standard importance weighted auto-encoder (IWAE) as a special case. We show that each can deliver improvements over IWAE, even when performance is measured by the IWAE target itself. Furthermore, our results suggest that PIWAE may be able to deliver simultaneous improvements in the training of both the inference and generative networks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{rajani_explain_2019,
  title = {Explain {{Yourself}}! {{Leveraging Language Models}} for {{Commonsense Reasoning}}},
  author = {Rajani, Nazneen Fatema and McCann, Bryan and Xiong, Caiming and Socher, Richard},
  year = {2019},
  month = jun,
  eprint = {1906.02361},
  eprinttype = {arxiv},
  abstract = {Deep learning models perform poorly on tasks that require commonsense reasoning, which often necessitates some form of worldknowledge or reasoning over information not immediately present in the input. We collect human explanations for commonsense reasoning in the form of natural language sequences and highlighted annotations in a new dataset called Common Sense Explanations (CoS-E). We use CoS-E to train language models to automatically generate explanations that can be used during training and inference in a novel Commonsense Auto-Generated Explanation (CAGE) framework. CAGE improves the state-of-the-art by 10\% on the challenging CommonsenseQA task. We further study commonsense reasoning in DNNs using both human and auto-generated explanations including transfer to out-of-domain tasks. Empirical results indicate that we can effectively leverage language models for commonsense reasoning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language}
}

@misc{rajpurkar_know_2018,
  title = {Know {{What You Don}}'t {{Know}}: {{Unanswerable Questions}} for {{SQuAD}}},
  shorttitle = {Know {{What You Don}}'t {{Know}}},
  author = {Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
  year = {2018},
  month = jun,
  eprint = {1806.03822},
  eprinttype = {arxiv},
  abstract = {Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuAD 2.0, the latest version of the Stanford Question Answering Dataset (SQuAD). SQuAD 2.0 combines existing SQuAD data with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD 2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuAD 2.0 is a challenging natural language understanding task for existing models: a strong neural system that gets 86\% F1 on SQuAD 1.1 achieves only 66\% F1 on SQuAD 2.0.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language}
}

@misc{rajpurkar_squad:_2016,
  title = {{{SQuAD}}: 100,000+ {{Questions}} for {{Machine Comprehension}} of {{Text}}},
  shorttitle = {{{SQuAD}}},
  author = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  year = {2016},
  month = jun,
  eprint = {1606.05250},
  eprinttype = {arxiv},
  abstract = {We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0\%, a significant improvement over a simple baseline (20\%). However, human performance (86.8\%) is much higher, indicating that the dataset presents a good challenge problem for future research. The dataset is freely available at https://stanford-qa.com.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language}
}

@misc{ramachandran_searching_2017,
  title = {Searching for {{Activation Functions}}},
  author = {Ramachandran, Prajit and Zoph, Barret and Le, Quoc V.},
  year = {2017},
  month = oct,
  eprint = {1710.05941},
  eprinttype = {arxiv},
  abstract = {The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance. Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU). Although various hand-designed alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains. In this work, we propose to leverage automatic search techniques to discover new activation functions. Using a combination of exhaustive and reinforcement learning-based search, we discover multiple novel activation functions. We verify the effectiveness of the searches by conducting an empirical evaluation with the best discovered activation function. Our experiments show that the best discovered activation function, f (x) = x {$\cdot$} sigmoid({$\beta$}x), which we name Swish, tends to work better than ReLU on deeper models across a number of challenging datasets. For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9\% for Mobile NASNet-A and 0.6\% for Inception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@misc{ramanujan_whats_2020,
  title = {What's {{Hidden}} in a {{Randomly Weighted Neural Network}}?},
  author = {Ramanujan, Vivek and Wortsman, Mitchell and Kembhavi, Aniruddha and Farhadi, Ali and Rastegari, Mohammad},
  year = {2020},
  month = mar,
  eprint = {1911.13299},
  eprinttype = {arxiv},
  abstract = {Training a neural network is synonymous with learning the values of the weights. In contrast, we demonstrate that randomly weighted neural networks contain subnetworks which achieve impressive performance without ever modifying the weight values. Hidden in a randomly weighted Wide ResNet-50 [32] we find a subnetwork (with random weights) that is smaller than, but matches the performance of a ResNet-34 [9] trained on ImageNet [4]. Not only do these ``untrained subnetworks'' exist, but we provide an algorithm to effectively find them. We empirically show that as randomly weighted neural networks with fixed weights grow wider and deeper, an ``untrained subnetwork'' approaches a network with learned weights in accuracy. Our code and pretrained models are available at: https://github.com/allenai/hidden-networks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@misc{ramesh_zero-shot_2021,
  title = {Zero-{{Shot Text-to-Image Generation}}},
  author = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  year = {2021},
  month = feb,
  eprint = {2102.12092},
  eprinttype = {arxiv},
  abstract = {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@misc{ramsauer_hopfield_2020,
  title = {Hopfield {{Networks}} Is {{All You Need}}},
  author = {Ramsauer, Hubert and Sch{\"a}fl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Gruber, Lukas and Holzleitner, Markus and Pavlovi{\'c}, Milena and Sandve, Geir Kjetil and Greiff, Victor and Kreil, David and Kopp, Michael and Klambauer, G{\"u}nter and Brandstetter, Johannes and Hochreiter, Sepp},
  year = {2020},
  month = jul,
  eprint = {2008.02217},
  eprinttype = {arxiv},
  abstract = {We show that the transformer attention mechanism is the update rule of a modern Hopfield network with continuous states. This new Hopfield network can store exponentially (with the dimension) many patterns, converges with one update, and has exponentially small retrieval errors. The number of stored patterns is traded off against convergence speed and retrieval error. The new Hopfield network has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. Transformer and BERT models operate in their first layers preferably in the global averaging regime, while they operate in higher layers in metastable states. The gradient in transformers is maximal for metastable states, is uniformly distributed for global averaging, and vanishes for a fixed point near a stored pattern. Using the Hopfield network interpretation, we analyzed learning of transformer and BERT models. Learning starts with attention heads that average and then most of them switch to metastable states. However, the majority of heads in the first layers still averages and can be replaced by averaging, e.g. our proposed Gaussian weighting. In contrast, heads in the last layers steadily learn and seem to use metastable states to collect information created in lower layers. These heads seem to be a promising target for improving transformers. Neural networks with Hopfield networks outperform other methods on immune repertoire classification, where the Hopfield net stores several hundreds of thousands of patterns. We provide a new PyTorch layer called "Hopfield", which allows to equip deep learning architectures with modern Hopfield networks as a new powerful concept comprising pooling, memory, and attention. GitHub: https://github.com/ml-jku/hopfield-layers},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
}

@misc{ranganath_hierarchical_2016,
  title = {Hierarchical {{Variational Models}}},
  author = {Ranganath, Rajesh and Tran, Dustin and Blei, David M.},
  year = {2016},
  month = may,
  eprint = {1511.02386},
  eprinttype = {arxiv},
  abstract = {Black box variational inference allows researchers to easily prototype and evaluate an array of models. Recent advances allow such algorithms to scale to high dimensions. However, a central question remains: How to specify an expressive variational distribution that maintains efficient computation? To address this, we develop hierarchical variational models (HVMs). HVMs augment a variational approximation with a prior on its parameters, which allows it to capture complex structure for both discrete and continuous latent variables. The algorithm we develop is black box, can be used for any HVM, and has the same computational efficiency as the original approximation. We study HVMs on a variety of deep discrete latent variable models. HVMs generalize other expressive variational distributions and maintains higher fidelity to the posterior.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology}
}

@inproceedings{ranzato_unsupervised_2007,
  title = {Unsupervised {{Learning}} of {{Invariant Feature Hierarchies}} with {{Applications}} to {{Object Recognition}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CCVPR}})},
  author = {Ranzato, Marc'Aurelio and Huang, Fu Jie and Boureau, Y-Lan and LeCun, Yann A.},
  year = {2007},
  month = jun,
  pages = {1--8},
  issn = {10636919},
  abstract = {We present an unsupervised method for learning a hi-erarchy of sparse feature detectors that are invariant to small shifts and distortions. The resulting feature extrac-tor consists of multiple convolution filters, followed by a feature-pooling layer that computes the max of each fil-ter output within adjacent windows, and a point-wise sig-moid non-linearity. A second level of larger and more in-variant features is obtained by training the same algorithm on patches of features from the first level. Training a su-pervised classifier on these features yields 0.64\% error on MNIST, and 54\% average recognition rate on Caltech 101 with 30 training samples per category. While the result-ing architecture is similar to convolutional networks, the layer-wise unsupervised training procedure alleviates the over-parameterization problems that plague purely super-vised learning procedures, and yields good performance with very few labeled training samples.},
  isbn = {1-4244-1180-7},
  pmid = {4270182}
}

@book{rasmussen_gaussian_2006,
  title = {Gaussian Processes for Machine Learning},
  author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  year = {2006},
  series = {Adaptive Computation and Machine Learning},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  isbn = {978-0-262-18253-9},
  langid = {english},
  lccn = {QA274.4 .R37 2006},
  keywords = {Data processing,Gaussian processes,Machine learning,Mathematical models},
  annotation = {OCLC: ocm61285753}
}

@inproceedings{ravanelli_multi-task_2020,
  title = {Multi-{{Task Self-Supervised Learning}} for {{Robust Speech Recognition}}},
  booktitle = {{{ICASSP}} 2020 - 2020 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Ravanelli, Mirco and Zhong, Jianyuan and Pascual, Santiago and Swietojanski, Pawel and Monteiro, Joao and Trmal, Jan and Bengio, Yoshua},
  year = {2020},
  month = may,
  pages = {6989--6993},
  issn = {2379-190X},
  abstract = {Despite the growing interest in unsupervised learning, extracting meaningful knowledge from unlabelled audio remains an open challenge. To take a step in this direction, we recently proposed a problem-agnostic speech encoder (PASE), that combines a convolutional encoder followed by multiple neural networks, called workers, tasked to solve self-supervised problems (i.e., ones that do not require manual annotations as ground truth). PASE was shown to capture relevant speech information, including speaker voice-print and phonemes. This paper proposes PASE+, an improved version of PASE for robust speech recognition in noisy and reverberant environments. To this end, we employ an online speech distortion module, that contaminates the input signals with a variety of random disturbances. We then propose a revised encoder that better learns short- and long-term speech dynamics with an efficient combination of recurrent and convolutional networks. Finally, we refine the set of workers used in self-supervision to encourage better cooperation. Results on TIMIT, DIRHA and CHiME-5 show that PASE+ significantly outperforms both the previous version of PASE as well as common acoustic features. Interestingly, PASE+ learns transferable representations suitable for highly mismatched acoustic conditions.},
  keywords = {Acoustic distortion,Acoustics,Convolution,Noise measurement,self-supervised learning,Speech processing,speech recognition,Speech recognition,Task analysis}
}

@misc{razavi_generating_2019,
  title = {Generating {{Diverse High-Fidelity Images}} with {{VQ-VAE-2}}},
  author = {Razavi, Ali and van den Oord, A{\"a}ron and Vinyals, Oriol},
  year = {2019},
  month = jun,
  eprint = {1906.00446},
  eprinttype = {arxiv},
  abstract = {We explore the use of Vector Quantized Variational AutoEncoder (VQ-VAE) models for large scale image generation. To this end, we scale and enhance the autoregressive priors used in VQ-VAE to generate synthetic samples of much higher coherence and fidelity than possible before. We use simple feed-forward encoder and decoder networks, making our model an attractive candidate for applications where the encoding and/or decoding speed is critical. Additionally, VQ-VAE requires sampling an autoregressive model only in the compressed latent space, which is an order of magnitude faster than sampling in the pixel space, especially for large images. We demonstrate that a multi-scale hierarchical organization of VQ-VAE, augmented with powerful priors over the latent codes, is able to generate samples with quality that rivals that of state of the art Generative Adversarial Networks on multifaceted datasets such as ImageNet, while not suffering from GAN's known shortcomings such as mode collapse and lack of diversity.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{razavi_preventing_2019,
  title = {Preventing {{Posterior Collapse}} with Delta-{{VAEs}}},
  author = {Razavi, Ali and van den Oord, A{\"a}ron and Poole, Ben and Vinyals, Oriol},
  year = {2019},
  month = jan,
  eprint = {1901.03416},
  eprinttype = {arxiv},
  abstract = {Due to the phenomenon of ``posterior collapse,'' current latent variable generative models pose a challenging design choice that either weakens the capacity of the decoder or requires augmenting the objective so it does not only maximize the likelihood of the data. In this paper, we propose an alternative that utilizes the most powerful generative models as decoders, whilst optimising the variational lower bound all while ensuring that the latent variables preserve and encode useful information. Our proposed {$\delta$}-VAEs achieve this by constraining the variational family for the posterior to have a minimum distance to the prior. For sequential latent variable models, our approach resembles the classic representation learning approach of slow feature analysis. We demonstrate the efficacy of our approach at modeling text on LM1B and modeling images: learning representations, improving sample quality, and achieving state of the art log-likelihood on CIFAR-10 and ImageNet 32 \texttimes{} 32.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{recht_beneath_2012,
  title = {Beneath the Valley of the Noncommutative Arithmetic-Geometric Mean Inequality: Conjectures, Case-Studies, and Consequences},
  shorttitle = {Beneath the Valley of the Noncommutative Arithmetic-Geometric Mean Inequality},
  author = {Recht, Benjamin and Re, Christopher},
  year = {2012},
  month = feb,
  abstract = {Randomized algorithms that base iteration-level decisions on samples from some pool are ubiquitous in machine learning and optimization. Examples include stochastic gradient descent and randomized coordinate descent. This paper makes progress at theoretically evaluating the difference in performance between sampling with- and without-replacement in such algorithms. Focusing on least means squares optimization, we formulate a noncommutative arithmetic-geometric mean inequality that would prove that the expected convergence rate of without-replacement sampling is faster than that of with-replacement sampling. We demonstrate that this inequality holds for many classes of random matrices and for some pathological examples as well. We provide a deterministic worst-case bound on the gap between the discrepancy between the two sampling models, and explore some of the impediments to proving this inequality in full generality. We detail the consequences of this inequality for stochastic gradient descent and the randomized Kaczmarz algorithm for solving linear systems.},
  langid = {english},
  keywords = {Sampling w/wo replacement}
}

@inproceedings{reddi_convergence_2018,
  title = {On the {{Convergence}} of {{Adam}} and {{Beyond}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Reddi, S. J. and Kale, S. and Kumar, S.},
  year = {2018},
  pages = {1--23},
  abstract = {Several recently proposed stochastic optimization methods that have been suc-cessfully used in training deep networks such as RMSPROP, ADAM, ADADELTA, NADAM, etc are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. It has been empirically observed that sometimes these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where ADAM does not converge to the optimal solution, and describe the precise problems with the previous analysis of ADAM algorithm. Our analysis suggests that the convergence issues may be fixed by endowing such algorithms with "long-term memory" of past gradients, and propose new variants of the ADAM algorithm which not only fix the convergence issues but often also lead to improved empirical performance.},
  keywords = {★}
}

@article{redmon_you_2015,
  title = {You {{Only Look Once}}: {{Unified}}, {{Real-Time Object Detection}}},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  year = {2015},
  journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CCVPR)},
  eprint = {1506.02640},
  eprinttype = {arxiv},
  pages = {779--788},
  issn = {01689002},
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
  archiveprefix = {arXiv},
  isbn = {978-1-4673-8851-1},
  pmid = {27295650}
}

@article{ren_faster_2017,
  title = {Faster {{R-CNN}}: {{Towards Real-Time Object Detection}} With},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  year = {2017},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {39},
  number = {6},
  pages = {1137--1149}
}

@inproceedings{ren_likelihood_2019,
  title = {Likelihood {{Ratios}} for {{Out-of-Distribution Detection}}},
  booktitle = {Proceedings of the 33rd {{International Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Ren, Jie and Liu, Peter J and Fertig, Emily and Snoek, Jasper and Poplin, Ryan and Depristo, Mark and Dillon, Joshua and Lakshminarayanan, Balaji},
  year = {2019},
  pages = {12},
  address = {{Vancouver, Canada}},
  abstract = {Discriminative neural networks offer little or no performance guarantees when deployed on data not generated by the same process as the training distribution. On such out-of-distribution (OOD) inputs, the prediction may not only be erroneous, but confidently so, limiting the safe deployment of classifiers in real-world applications. One such challenging application is bacteria identification based on genomic sequences, which holds the promise of early detection of diseases, but requires a model that can output low confidence predictions on OOD genomic sequences from new bacteria that were not present in the training data. We introduce a genomics dataset for OOD detection that allows other researchers to benchmark progress on this important problem. We investigate deep generative model based approaches for OOD detection and observe that the likelihood score is heavily affected by population level background statistics. We propose a likelihood ratio method for deep generative models which effectively corrects for these confounding background statistics. We benchmark the OOD detection performance of the proposed method against existing approaches on the genomics dataset and show that our method achieves state-of-the-art performance. We demonstrate the generality of the proposed method by showing that it significantly improves OOD detection when applied to deep generative models of images.},
  langid = {english},
  keywords = {⛔ No DOI found}
}

@misc{rezende_normalizing_2020,
  title = {Normalizing {{Flows}} on {{Tori}} and {{Spheres}}},
  author = {Rezende, Danilo Jimenez and Papamakarios, George and Racani{\`e}re, S{\'e}bastien and Albergo, Michael S. and Kanwar, Gurtej and Shanahan, Phiala E. and Cranmer, Kyle},
  year = {2020},
  month = feb,
  eprint = {2002.02428},
  eprinttype = {arxiv},
  abstract = {Normalizing flows are a powerful tool for building expressive distributions in high dimensions. So far, most of the literature has concentrated on learning flows on Euclidean spaces. Some problems however, such as those involving angles, are defined on spaces with more complex geometries, such as tori or spheres. In this paper, we propose and compare expressive and numerically stable flows on such spaces. Our flows are built recursively on the dimension of the space, starting from flows on circles, closed intervals or spheres.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{rezende_stochastic_2014,
  title = {Stochastic {{Backpropagation}} and {{Approximate Inference}} in {{Deep Generative Models}}},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
  year = {2014},
  month = jan,
  volume = {32},
  pages = {1278--1286},
  publisher = {{PMLR}},
  address = {{Beijing, China}},
  abstract = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic back-propagation -- rules for back-propagation through stochastic variables -- and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation.}
}

@inproceedings{rezende_variational_2015,
  title = {Variational {{Inference}} with {{Normalizing Flows}}},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
  year = {2015},
  address = {{Lille, France}},
  abstract = {The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology}
}

@misc{ribeiro_why_2016,
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  shorttitle = {"{{Why Should I Trust You}}?},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = {2016},
  month = feb,
  eprint = {1602.04938},
  eprinttype = {arxiv},
  abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{riedmiller_learning_2018,
  title = {Learning by {{Playing}} - {{Solving Sparse Reward Tasks}} from {{Scratch}}},
  author = {Riedmiller, Martin and Hafner, Roland and Lampe, Thomas and Neunert, Michael and Degrave, Jonas and {Van de Wiele}, Tom and Mnih, Volodymyr and Heess, Nicolas and Springenberg, Jost Tobias},
  year = {2018},
  eprint = {1802.10567},
  eprinttype = {arxiv},
  abstract = {We propose Scheduled Auxiliary Control (SAC-X), a new learning paradigm in the context of Reinforcement Learning (RL). SAC-X enables learning of complex behaviors - from scratch - in the presence of multiple sparse reward signals. To this end, the agent is equipped with a set of general auxiliary tasks, that it attempts to learn simultaneously via off-policy RL. The key idea behind our method is that active (learned) scheduling and execution of auxiliary policies allows the agent to efficiently explore its environment - enabling it to excel at sparse reward RL. Our experiments in several challenging robotic manipulation settings demonstrate the power of our approach.},
  archiveprefix = {arXiv}
}

@article{rieser_natural_2009,
  title = {Natural {{Language Generation}} as {{Planning Under Uncertainty}} for {{Spoken Dialogue Systems}}},
  author = {Rieser, Verena and Lemon, Oliver},
  year = {2009},
  pages = {9},
  abstract = {We present and evaluate a new model for Natural Language Generation (NLG) in Spoken Dialogue Systems, based on statistical planning, given noisy feedback from the current generation context (e.g. a user and a surface realiser). We study its use in a standard NLG problem: how to present information (in this case a set of search results) to users, given the complex tradeoffs between utterance length, amount of information conveyed, and cognitive load. We set these trade-offs by analysing existing MATCH data. We then train a NLG policy using Reinforcement Learning (RL), which adapts its behaviour to noisy feedback from the current generation context. This policy is compared to several baselines derived from previous work in this area. The learned policy significantly outperforms all the prior approaches.},
  langid = {english},
  keywords = {❓ Multiple DOI}
}

@inproceedings{rifai_contractive_2011,
  title = {Contractive {{Auto-Encoders}}: {{Explicit Invariance}} during {{Feature Extraction}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{International Conference}} on {{Machine Learning}}},
  author = {Rifai, Salah and Vincent, Pascal and Muller, Xavier and Glorot, Xavier and Bengio, Yoshua},
  year = {2011},
  series = {{{ICML}}'11},
  pages = {833--840},
  publisher = {{Omnipress}},
  address = {{Madison, WI, USA}},
  abstract = {We present in this paper a novel approach for training deterministic auto-encoders. We show that by adding a well chosen penalty term to the classical reconstruction cost function, we can achieve results that equal or surpass those attained by other regularized auto-encoders as well as denoising auto-encoders on a range of datasets. This penalty term corresponds to the Frobenius norm of the Jacobian matrix of the encoder activations with respect to the input. We show that this penalty term results in a localized space contraction which in turn yields robust features on the activation layer. Furthermore, we show how this penalty term is related to both regularized auto-encoders and denoising auto-encoders and how it can be seen as a link between deterministic and non-deterministic auto-encoders. We find empirically that this penalty helps to carve a representation that better captures the local directions of variation dictated by the data, corresponding to a lower-dimensional non-linear manifold, while being more invariant to the vast majority of directions orthogonal to the manifold. Finally, we show that by using the learned features to initialize a MLP, we achieve state of the art classification error on a range of datasets, surpassing other methods of pretraining.},
  isbn = {978-1-4503-0619-5}
}

@inproceedings{rifai_higher_2011,
  title = {Higher {{Order Contractive Auto-Encoder}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Rifai, Salah and Mesnil, Gr{\'e}goire and Vincent, Pascal and Muller, Xavier and Bengio, Yoshua and Dauphin, Yann and Glorot, Xavier},
  editor = {Gunopulos, Dimitrios and Hofmann, Thomas and Malerba, Donato and Vazirgiannis, Michalis},
  year = {2011},
  pages = {645--660},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  abstract = {We propose a novel regularizer when training an auto-encoder for unsupervised feature extraction. We explicitly encourage the latent representation to contract the input space by regularizing the norm of the Jacobian (analytically) and the Hessian (stochastically) of the encoder's output with respect to its input, at the training points. While the penalty on the Jacobian's norm ensures robustness to tiny corruption of samples in the input space, constraining the norm of the Hessian extends this robustness when moving further away from the sample. From a manifold learning perspective, balancing this regularization with the auto-encoder's reconstruction objective yields a representation that varies most when moving along the data manifold in input space, and is most insensitive in directions orthogonal to the manifold. The second order regularization, using the Hessian, penalizes curvature, and thus favors smooth manifold. We show that our proposed technique, while remaining computationally efficient, yields representations that are significantly better suited for initializing deep architectures than previously proposed approaches, beating state-of-the-art performance on a number of datasets.},
  isbn = {978-3-642-23783-6}
}

@article{risi_neuroevolution_2017,
  title = {Neuroevolution in {{Games}}: {{State}} of the {{Art}} and {{Open Challenges}}},
  author = {Risi, Sebastian and Togelius, Julian},
  year = {2017},
  journal = {IEEE Transactions on Computational Intelligence and AI in Games},
  volume = {9},
  number = {1},
  eprint = {1410.7326},
  eprinttype = {arxiv},
  issn = {1943068X},
  abstract = {This paper surveys research on applying neuroevolution (NE) to games. In neuroevolution, artificial neural networks are trained through evolutionary algorithms, taking inspiration from the way biological brains evolved. We analyse the application of NE in games along five different axes, which are the role NE is chosen to play in a game, the different types of neural networks used, the way these networks are evolved, how the fitness is determined and what type of input the network receives. The article also highlights important open research challenges in the field.},
  archiveprefix = {arXiv}
}

@article{riviere_unsupervised_2020,
  title = {Unsupervised Pretraining Transfers Well across Languages},
  author = {Rivi{\`e}re, Morgane and Joulin, Armand and Mazar{\'e}, Pierre-Emmanuel and Dupoux, Emmanuel},
  year = {2020},
  month = feb,
  journal = {arXiv:2002.02848 [cs, eess]},
  eprint = {2002.02848},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {Cross-lingual and multi-lingual training of Automatic Speech Recognition (ASR) has been extensively investigated in the supervised setting. This assumes the existence of a parallel corpus of speech and orthographic transcriptions. Recently, contrastive predictive coding (CPC) algorithms have been proposed to pretrain ASR systems with unlabelled data. In this work, we investigate whether unsupervised pretraining transfers well across languages. We show that a slight modification of the CPC pretraining extracts features that transfer well to other languages, being on par or even outperforming supervised pretraining. This shows the potential of unsupervised methods for languages with few linguistic resources.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@inproceedings{roberts_hierarchical_2018,
  title = {A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Roberts, Adam and Engel, Jesse H. and Raffel, Colin and Hawthorne, Curtis and Eck, Douglas},
  year = {2018},
  volume = {80},
  pages = {4361--4370},
  publisher = {{PMLR}},
  address = {{Stockholm, Sweden}},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/icml/RobertsERHE18.bib},
  timestamp = {Mon, 22 Jul 2019 13:51:23 +0200}
}

@article{roberts_weak_1997,
  title = {Weak Convergence and Optimal Scaling of Random Walk {{Metropolis}} Algorithms},
  author = {Roberts, G. O. and Gelman, A. and Gilks, W. R.},
  year = {1997},
  month = feb,
  journal = {The Annals of Applied Probability},
  volume = {7},
  number = {1},
  pages = {110--120},
  langid = {english}
}

@misc{roeder_sticking_2017,
  title = {Sticking the {{Landing}}: {{Simple}}, {{Lower-Variance Gradient Estimators}} for {{Variational Inference}}},
  shorttitle = {Sticking the {{Landing}}},
  author = {Roeder, Geoffrey and Wu, Yuhuai and Duvenaud, David},
  year = {2017},
  month = may,
  eprint = {1703.09194},
  eprinttype = {arxiv},
  abstract = {We propose a simple and general variant of the standard reparameterized gradient estimator for the variational evidence lower bound. Specifically, we remove a part of the total derivative with respect to the variational parameters that corresponds to the score function. Removing this term produces an unbiased gradient estimator whose variance approaches zero as the approximate posterior approaches the exact posterior. We analyze the behavior of this gradient estimator theoretically and empirically, and generalize it to more complex variational distributions such as mixtures and importance-weighted posteriors.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@incollection{rognvaldsson_brownian_1993,
  title = {Brownian {{Motion Updating}} of {{Multi-layered Perceptrons}}},
  booktitle = {{{ICANN}} '93},
  author = {R{\"o}gnvaldsson, Thorsteinn S.},
  year = {1993},
  pages = {527--532},
  publisher = {{Springer London}},
  address = {{London}}
}

@misc{rolfe_discrete_2016,
  title = {Discrete {{Variational Autoencoders}}},
  author = {Rolfe, Jason Tyler},
  year = {2016},
  month = sep,
  eprint = {1609.02200},
  eprinttype = {arxiv},
  abstract = {Probabilistic models with discrete latent variables naturally capture datasets composed of discrete classes. However, they are difficult to train efficiently, since backpropagation through discrete variables is generally not possible. We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables. The associated class of probabilistic models comprises an undirected discrete component and a directed hierarchical continuous component. The discrete component captures the distribution over the disconnected smooth manifolds induced by the continuous component. As a result, this class of models efficiently learns both the class of objects in an image, and their specific realization in pixels, from unsupervised data, and outperforms state-of-the-art methods on the permutation-invariant MNIST, Omniglot, and Caltech-101 Silhouettes datasets.},
  archiveprefix = {arXiv},
  isbn = {1511.02386}
}

@misc{roller_hash_2021,
  title = {Hash {{Layers For Large Sparse Models}}},
  author = {Roller, Stephen and Sukhbaatar, Sainbayar and Szlam, Arthur and Weston, Jason},
  year = {2021},
  month = jun,
  eprint = {2106.04426},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We investigate the training of sparse layers that use different parameters for different inputs based on hashing in large Transformer models. Specifically, we modify the feedforward layer to hash to different sets of weights depending on the current token, over all tokens in the sequence. We show that this procedure either outperforms or is competitive with learning-to-route mixture-of-expert methods such as Switch Transformers and BASE Layers, while requiring no routing parameters or extra terms in the objective function such as a load balancing loss, and no sophisticated assignment algorithm. We study the performance of different hashing techniques, hash sizes and input features, and show that balanced and random hashes focused on the most local features work best, compared to either learning clusters or using longer-range context. We show our approach works well both on large language modeling and dialogue tasks, and on downstream fine-tuning tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@inproceedings{ros_simple_2008,
  title = {A Simple Modification in {{CMA-ES}} Achieving Linear Time and Space Complexity},
  booktitle = {Lecture {{Notes}} in {{Computer Science}} (Including Subseries {{Lecture Notes}} in {{Artificial Intelligence}} and {{Lecture Notes}} in {{Bioinformatics}})},
  author = {Ros, Raymond and Hansen, Nikolaus},
  year = {2008},
  volume = {5199 LNCS},
  pages = {296--305},
  issn = {03029743},
  abstract = {This paper proposes a simple modification of the Covariance Matrix\textbackslash nAdaptation Evolution Strategy (CMA-ES) for high dimensional objective\textbackslash nfunctions, reducing the internal time and space complexity from quadratic\textbackslash nto linear. The covariance matrix is constrained to be diagonal and\textbackslash nthe resulting algorithm, sep-CMA-ES, samples each coordinate independently.\textbackslash nBecause the model complexity is reduced, the learning rate for the\textbackslash ncovariance matrix can be increased. Consequently, on essentially\textbackslash nseparable functions, sep-CMA-ES significantly outperforms CMA-ES\textbackslash n. For dimensions larger than a hundred, even on the non-separable\textbackslash nRosenbrock function, the sep-CMA-ES needs fewer function evaluations\textbackslash nthan CMA-ES .},
  isbn = {3-540-87699-5}
}

@misc{rosca_distribution_2018,
  title = {Distribution {{Matching}} in {{Variational Inference}}},
  author = {Rosca, Mihaela and Lakshminarayanan, Balaji and Mohamed, Shakir},
  year = {2018},
  month = feb,
  eprint = {1802.06847},
  eprinttype = {arxiv},
  abstract = {With the increasingly widespread deployment of generative models, there is a mounting need for a deeper understanding of their behaviors and limitations. In this paper, we expose the limitations of Variational Autoencoders (VAEs), which consistently fail to learn marginal distributions in both latent and visible spaces. We show this to be a consequence of learning by matching conditional distributions, and the limitations of explicit model and posterior distributions. It is popular to consider Generative Adversarial Networks (GANs) as a means of overcoming these limitations, leading to hybrids of VAEs and GANs. We perform a large-scale evaluation of several VAE-GAN hybrids and analyze the implications of class probability estimation for learning distributions. While promising, we conclude that at present, VAE-GAN hybrids have limited applicability: they are harder to scale, evaluate, and use for inference compared to VAEs; and they do not improve over the generation quality of GANs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{rosca_measure-valued_nodate,
  title = {Measure-{{Valued Derivatives}} for {{Approximate Bayesian Inference}}},
  author = {Rosca, Mihaela and Figurnov, Michael and Mohamed, Shakir and Mnih, Andriy},
  pages = {7},
  langid = {english},
  keywords = {⛔ No DOI found}
}

@article{rosenblatt_analytic_1962,
  title = {Analytic {{Techniques}} for the {{Study}} of {{Neural Nets}}},
  author = {Rosenblatt, Frank},
  year = {1962},
  journal = {Proceedings of AIEE Joint Automatic Control Conference},
  volume = {401},
  pages = {285--292}
}

@techreport{rosenblatt_perceptron_1957,
  title = {The {{Perceptron}} - {{A Perceiving}} and {{Recognizing Automaton}}},
  author = {Rosenblatt, Frank},
  year = {1957},
  pages = {Report 85-460-1},
  institution = {{Cornell Aeronautical Laboratory}}
}

@misc{roy_does_2021,
  title = {Does {{Your Dermatology Classifier Know What It Doesn}}'t {{Know}}? {{Detecting}} the {{Long-Tail}} of {{Unseen Conditions}}},
  shorttitle = {Does {{Your Dermatology Classifier Know What It Doesn}}'t {{Know}}?},
  author = {Roy, Abhijit Guha and Ren, Jie and Azizi, Shekoofeh and Loh, Aaron and Natarajan, Vivek and Mustafa, Basil and Pawlowski, Nick and Freyberg, Jan and Liu, Yuan and Beaver, Zach and Vo, Nam and Bui, Peggy and Winter, Samantha and MacWilliams, Patricia and Corrado, Greg S. and Telang, Umesh and Liu, Yun and Cemgil, Taylan and Karthikesalingam, Alan and Lakshminarayanan, Balaji and Winkens, Jim},
  year = {2021},
  month = apr,
  eprint = {2104.03829},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Supervised deep learning models have proven to be highly effective in classification of dermatological conditions. These models rely on the availability of abundant labeled training examples. However, in the real-world, many dermatological conditions are individually too infrequent for per-condition classification with supervised learning. Although individually infrequent, these conditions may collectively be common and therefore are clinically significant in aggregate. To prevent models from generating erroneous outputs on such examples, there remains a considerable unmet need for deep learning systems that can better detect such infrequent conditions. These infrequent `outlier' conditions are seen very rarely (or not at all) during training. In this paper, we frame this task as an out-of-distribution (OOD) detection problem. We set up a benchmark ensuring that outlier conditions are disjoint between the model training, validation, and test sets. Unlike traditional OOD detection benchmarks where the task is to detect dataset distribution shift, we aim at the more challenging task of detecting subtle semantic differences. We propose a novel hierarchical outlier detection (HOD) loss, which assigns multiple abstention classes corresponding to each training outlier class and jointly performs a coarse classification of inliers vs. outliers, along with fine-grained classification of the individual classes. We demonstrate that the proposed HOD loss based approach outperforms leading methods that leverage outlier data during training. Further, performance is significantly boosted by using recent representation learning methods (BiT, SimCLR, MICLe). Further, we explore ensembling strategies for OOD detection and propose a diverse ensemble selection process for the best result. We also perform a subgroup analysis over conditions of varying risk levels and different skin types to investigate how OOD performance changes over each subgroup and demonstrate the gains of our framework in comparison to baseline. Furthermore, we go beyond traditional performance metrics and introduce a cost matrix for model trust analysis to approximate downstream clinical impact. We use this cost matrix to compare the proposed method against the baseline, thereby making a stronger case for its effectiveness in real-world scenarios.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@misc{ruan_improving_2021,
  title = {Improving {{Lossless Compression Rates}} via {{Monte Carlo Bits-Back Coding}}},
  author = {Ruan, Yangjun and Ullrich, Karen and Severo, Daniel and Townsend, James and Khisti, Ashish and Doucet, Arnaud and Makhzani, Alireza and Maddison, Chris J.},
  year = {2021},
  month = feb,
  eprint = {2102.11086},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {Latent variable models have been successfully applied in lossless compression with the bits-back coding algorithm. However, bits-back suffers from an increase in the bitrate equal to the KL divergence between the approximate posterior and the true posterior. In this paper, we show how to remove this gap asymptotically by deriving bits-back coding algorithms from tighter variational bounds. The key idea is to exploit extended space representations of Monte Carlo estimators of the marginal likelihood. Naively applied, our schemes would require more initial bits than the standard bits-back coder, but we show how to drastically reduce this additional cost with couplings in the latent space. When parallel architectures can be exploited, our coders can achieve better rates than bits-back with little additional cost. We demonstrate improved lossless compression rates in a variety of settings, including entropy coding for lossy compression.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Theory,Computer Science - Machine Learning,Statistics - Computation}
}

@article{rumelhart_learning_1986,
  title = {Learning Representations by Back-Propagating Errors},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  year = {1986},
  journal = {Nature},
  volume = {323},
  pages = {533--536},
  abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal `hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure}
}

@incollection{rumelhart_learning_2013,
  title = {Learning {{Internal Representations}} by {{Error Propagation}}},
  booktitle = {Readings in {{Cognitive Science}}: {{A Perspective}} from {{Psychology}} and {{Artificial Intelligence}}},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  year = {2013},
  eprint = {1011.1669v3},
  eprinttype = {arxiv},
  pages = {399--421},
  issn = {1-55860-013-2},
  abstract = {This paper presents a generalization of the perception learning procedure for learning the correct sets of connections for arbitrary networks. The rule, falled the generalized delta rule, is a simple scheme for implementing a gradient descent method for finding weights that minimize the sum squared error of the sytem's performance. The major theoretical contribution of the work is the procedure called error propagation, whereby the gradient can be determined by individual units of the network based only on locally available information. The major empirical contribution of the work is to show that the problem of local minima not serious in this application of gradient descent.},
  archiveprefix = {arXiv},
  isbn = {1-55860-013-2},
  pmid = {25246403}
}

@inproceedings{sabour_dynamic_2017,
  title = {Dynamic {{Routing Between Capsules}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Neural Information Processing Systems}} ({{NIPS}})},
  author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E.},
  year = {2017},
  month = oct,
  eprint = {1710.09829},
  eprinttype = {arxiv},
  address = {{Long Beach, CA, USA}},
  abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
  archiveprefix = {arXiv},
  keywords = {★}
}

@misc{sadhu_wav2vec-c_2021,
  title = {Wav2vec-{{C}}: {{A Self-supervised Model}} for {{Speech Representation Learning}}},
  shorttitle = {Wav2vec-{{C}}},
  author = {Sadhu, Samik and He, Di and Huang, Che-Wei and Mallidi, Sri Harish and Wu, Minhua and Rastrow, Ariya and Stolcke, Andreas and Droppo, Jasha and Maas, Roland},
  year = {2021},
  month = jun,
  eprint = {2103.08393},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {Wav2vec-C introduces a novel representation learning technique combining elements from wav2vec 2.0 and VQ-VAE. Our model learns to reproduce quantized representations from partially masked speech encoding using a contrastive loss in a way similar to Wav2vec 2.0. However, the quantization process is regularized by an additional consistency network that learns to reconstruct the input features to the wav2vec 2.0 network from the quantized representations in a way similar to a VQ-VAE model. The proposed self-supervised model is trained on 10k hours of unlabeled data and subsequently used as the speech encoder in a RNN-T ASR model and fine-tuned with 1k hours of labeled data. This work is one of only a few studies of self-supervised learning on speech tasks with a large volume of real far-field labeled data. The Wav2vec-C encoded representations achieves, on average, twice the error reduction over baseline and a higher codebook utilization in comparison to wav2vec 2.0},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@inproceedings{sajjadi_enhancenet_2017,
  title = {{{EnhanceNet}}: {{Single Image Super-Resolution Through Automated Texture Synthesis}}},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Sajjadi, Mehdi S.M. and Scholkopf, Bernhard and Hirsch, Michael},
  year = {2017},
  eprint = {1612.07919},
  eprinttype = {arxiv},
  pages = {4501--4510},
  issn = {15505499},
  abstract = {Single image super-resolution is the task of inferring a high-resolution image from a single low-resolution input. Traditionally, the performance of algorithms for this task is measured using pixel-wise reconstruction measures such as peak signal-to-noise ratio (PSNR) which have been shown to correlate poorly with the human perception of image quality. As a result, algorithms minimizing these metrics tend to produce over-smoothed images that lack high-frequency textures and do not look natural despite yielding high PSNR values. We propose a novel application of automated texture synthesis in combination with a perceptual loss focusing on creating realistic textures rather than optimizing for a pixel-accurate reproduction of ground truth images during training. By using feed-forward fully convolutional neural networks in an adversarial training setting, we achieve a significant boost in image quality at high magnification ratios. Extensive experiments on a number of datasets show the effectiveness of our approach, yielding state-of-the-art results in both quantitative and qualitative benchmarks.},
  archiveprefix = {arXiv},
  isbn = {978-1-5386-1032-9}
}

@inproceedings{salakhutdinov_restricted_2007,
  title = {Restricted {{Boltzmann Machines}} for {{Collaborative Filtering}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Salakhutdinov, Ruslan and Mnih, Andriy and Hinton, Geoffrey E.},
  year = {2007},
  volume = {227},
  eprint = {1606.07129},
  eprinttype = {arxiv},
  pages = {791--798},
  issn = {1468-1218},
  abstract = {Most of the existing approaches to collaborative filtering cannot handle very large data sets. In this paper we show how a class of two-layer undirected graphical models, called Restricted Boltzmann Machines (RBM's), can be used to model tabular data, such as user's ratings of movies. We present efficient learning and inference procedures for this class of models and demonstrate that RBM's can be successfully applied to the Netflix data set, containing over 100 million user/movie ratings. We also show that RBM's slightly outperform carefully-tuned SVD models. When the predictions of multiple RBM models and multiple SVD models are linearly combined, we achieve an error rate that is well over 6\% better than the score of Netflix's own system.},
  archiveprefix = {arXiv},
  isbn = {978-1-59593-793-3},
  pmid = {19932002},
  keywords = {Stochastic network}
}

@misc{salimans_evolution_2017,
  title = {Evolution {{Strategies}} as a {{Scalable Alternative}} to {{Reinforcement Learning}}},
  author = {Salimans, Tim and Ho, Jonathan and Chen, Xi and Sidor, Szymon and Sutskever, Ilya},
  year = {2017},
  eprint = {1703.03864},
  eprinttype = {arxiv},
  abstract = {We explore the use of Evolution Strategies (ES), a class of black box optimization algorithms, as an alternative to popular MDP-based RL techniques such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using a novel communication strategy based on common random numbers, our ES implementation only needs to communicate scalars, making it possible to scale to over a thousand parallel workers. This allows us to solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.},
  archiveprefix = {arXiv}
}

@misc{salimans_improved_2016,
  title = {Improved {{Techniques}} for {{Training GANs}}},
  author = {Salimans, Tim and Goodfellow, Ian J. and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
  year = {2016},
  month = jun,
  eprint = {1606.03498},
  eprinttype = {arxiv},
  abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3\%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.},
  archiveprefix = {arXiv},
  isbn = {0924-6495},
  pmid = {23259955}
}

@inproceedings{salimans_pixelcnn_2017,
  title = {{{PixelCNN}}++: {{Improving}} the {{PixelCNN}} with {{Discretized Logistic Mixture Likelihood}} and {{Other Modifications}}},
  shorttitle = {{{PixelCNN}}++},
  booktitle = {Proceedings of the 5th {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Salimans, Tim and Karpathy, Andrej and Chen, Xi and Kingma, Diederik P.},
  year = {2017},
  month = apr,
  address = {{Toulon, France}},
  abstract = {PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available at https://github.com/openai/pixel-cnn. Our implementation contains a number of modifications to the original model that both simplify its structure and improve its performance. 1) We use a discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which we find to speed up training. 2) We condition on whole pixels, rather than R/G/B sub-pixels, simplifying the model structure. 3) We use downsampling to efficiently capture structure at multiple resolutions. 4) We introduce additional short-cut connections to further speed up optimization. 5) We regularize the model using dropout. Finally, we present state-of-the-art log likelihood results on CIFAR-10 to demonstrate the usefulness of these modifications.},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{salimans_weight_2016,
  title = {Weight {{Normalization}}: {{A Simple Reparameterization}} to {{Accelerate Training}} of {{Deep Neural Networks}}},
  booktitle = {Proceedings of the 30th {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Salimans, Tim and Kingma, Diederik P.},
  year = {2016},
  month = feb,
  address = {{Barcelona, Spain}},
  abstract = {We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.}
}

@article{samuel_studies_1959,
  title = {Some {{Studies}} in {{Machine Learning}} Using the {{Game}} of {{Checkers}}},
  author = {Samuel, Artur L},
  year = {1959},
  journal = {IBM Journal of research and development},
  volume = {3},
  number = {3},
  pages = {210--229},
  abstract = {Two machine-learning procedures have been investigated 1 in some detail using the game of checkers. Enough work has been done to verify the fact that a computer can be programmed so that it will learn to play a better game of checkers than can be played by the person who wrote the program. Further- more, it can learn to do this in a remarkably short period of time (8 or 10 hours of machine-playing time) when given only the rules of the game, a sense of direction, and a redundant and incomplete list of parameters which are thought to have something to do with the game, but whose correct signs and relative weights are unknown and unspecified. The principles of machine learning verified by these experiments are, of course, applicable to many other situations.}
}

@inproceedings{saul_exploiting_1996,
  title = {Exploiting Tractable Substructures in Intractable Networks},
  booktitle = {Advances in {{Neural Processing Systems}}},
  author = {Saul, Lawrence and Jordan, Michael},
  editor = {Touretzky, D. and Mozer, M. C. and Hasselmo, M.},
  year = {1996},
  volume = {8},
  publisher = {{MIT Press}}
}

@misc{savinov_step-unrolled_2021,
  title = {Step-Unrolled {{Denoising Autoencoders}} for {{Text Generation}}},
  author = {Savinov, Nikolay and Chung, Junyoung and Binkowski, Mikolaj and Elsen, Erich and van den Oord, Aaron},
  year = {2021},
  month = dec,
  eprint = {2112.06749},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In this paper we propose a new generative model of text, Step-unrolled Denoising Autoencoder (SUNDAE), that does not rely on autoregressive models. Similarly to denoising diffusion techniques, SUNDAE is repeatedly applied on a sequence of tokens, starting from random inputs and improving them each time until convergence. We present a simple new improvement operator that converges in fewer iterations than diffusion methods, while qualitatively producing better samples on natural language datasets. SUNDAE achieves state-of-the-art results (among non-autoregressive methods) on the WMT'14 English-to-German translation task and good qualitative results on unconditional language modeling on the Colossal Cleaned Common Crawl dataset and a dataset of Python code from GitHub. The non-autoregressive nature of SUNDAE opens up possibilities beyond left-to-right prompted generation, by filling in arbitrary blank patterns in a template.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@article{saxe_exact_2013,
  title = {Exact Solutions to the Nonlinear Dynamics of Learning in Deep Linear Neural Networks},
  author = {Saxe, Andrew M and McClelland, James L. and Ganguli, Surya},
  year = {2013},
  journal = {CoRR},
  volume = {abs/1312.6},
  eprint = {1312.6120},
  eprinttype = {arxiv},
  pages = {1--22},
  abstract = {Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos.},
  archiveprefix = {arXiv},
  isbn = {1312.6120}
}

@inproceedings{saxena_clockwork_2021,
  title = {Clockwork {{Variational Autoencoders}}},
  booktitle = {Proceedings of the 34th {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Saxena, Vaibhav and Ba, Jimmy and Hafner, Danijar},
  year = {2021},
  month = feb,
  eprint = {2102.09532},
  eprinttype = {arxiv},
  address = {{Virtual}},
  abstract = {Deep learning has enabled algorithms to generate realistic images. However, accurately predicting long video sequences requires understanding longterm dependencies and remains an open challenge. While existing video prediction models succeed at generating sharp images, they tend to fail at accurately predicting far into the future. We introduce the Clockwork VAE (CW-VAE), a video prediction model that leverages a hierarchy of latent sequences, where higher levels tick at slower intervals. We demonstrate the benefits of both hierarchical latents and temporal abstraction on 4 diverse video prediction datasets with sequences of up to 1000 frames, where CWVAE outperforms top video prediction models. Additionally, we propose a Minecraft benchmark for long-term video prediction. We conduct several experiments to gain insights into CW-VAE and confirm that slower levels learn to represent objects that change more slowly in the video, and faster levels learn to represent faster objects.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@article{schatz_evaluating_nodate,
  title = {Evaluating Speech Features with the {{Minimal-Pair ABX}} Task: {{Analysis}} of the Classical {{MFC}}/{{PLP}} Pipeline},
  author = {Schatz, Thomas and Peddinti, Vijayaditya and Bach, Francis and Jansen, Aren and Hermansky, Hynek and Dupoux, Emmanuel},
  pages = {5},
  abstract = {We present a new framework for the evaluation of speech representations in zero-resource settings, that extends and complements previous work by Carlin, Jansen and Hermansky [1]. In particular, we replace their Same/Different discrimination task by several Minimal-Pair ABX (MP-ABX) tasks. We explain the analytical advantages of this new framework and apply it to decompose the standard signal processing pipelines for computing PLP and MFC coefficients. This method enables us to confirm and quantify a variety of well-known and not-so-well-known results in a single framework.},
  langid = {english}
}

@inproceedings{schaul_high_2011,
  title = {High {{Dimensions}} and {{Heavy Tails}} for {{Natural Evolution Strategies}}},
  booktitle = {Proceedings of the {{Conference}} on {{Genetic}} and {{Evolutionary Computation}} ({{GECCO}})},
  author = {Schaul, Tom and Glasmachers, Tobias and Schmidhuber, J{\"u}rgen},
  year = {2011},
  abstract = {The family of natural evolution strategies (NES) offers a principled approach to real-valued evolutionary optimization. NES follows the natural gradient of the expected fitness on the parameters of its search distribution. While general in its formulation, previous research has focused on multivariate Gaussian search distributions. Here we exhibit problem classes for which other search distributions are more appropriate, and then derive corresponding NES-variants. First, for separable distributions we obtain SNES, whose complexity is only O(d) instead of O(d(3)). We apply SNES to problems of previously unattainable dimensionality, recovering lowest-energy structures on the Lennard-Jones atom clusters, and obtaining state-of-the-art results on neuro-evolution benchmarks. Second, we develop a new, equivalent formulation based on invariances. This allows for generalizing NES to heavy-tailed distributions, even those with undefined variance, which aids in overcoming deceptive local optima.},
  isbn = {978-1-4503-0557-0},
  keywords = {Black-box optimization,Global optimization,Natural gradient}
}

@article{schaul_investigating_2012,
  title = {Investigating the {{Impact}} of {{Adaptation Sampling}} in {{Natural Evolution Strategies}} on {{Black-box Optimization Testbeds}}},
  author = {Schaul, Tom},
  year = {2012},
  journal = {Proceedings of the Conference on Genetic and Evolutionary Computation (GECCO)},
  pages = {221},
  abstract = {Natural Evolution Strategies (NES) are a recent member of the class of real-valued optimization algorithms that are based on adapting search distributions. Exponential NES (xNES) are the most common instantiation of NES, and particularly appropriate for the BBOB 2012 benchmarks, given that many are non-separable, and their relatively small problem dimensions. The technique of adaptation sampling, which adapts learning rates online further improves the al-gorithm's performance. This report provides an extensive empirical comparison to study the impact of adaptation sampling in xNES, both on the noise-free and noisy BBOB testbeds.},
  isbn = {9781450311786},
  keywords = {Benchmarking,Evolution strategies,Natural gradient}
}

@phdthesis{schaul_studies_2011,
  title = {Studies in {{Continuous Black-box Optimization}}},
  author = {Schaul, Tom and Br{\"u}gge, B.},
  year = {2011},
  abstract = {Optimization is the research field that studies that studies the design of algorithms for finding the best solutions to problems we humans throw at them. While the whole domain is of important practical utility, the present thesis will focus on the subfield of continuous black-box optimization, presenting a collection of novel, state-of-the-art algorithms for solving problems in that class. First, we introduce a general-purpose algorithm called Natural Evolution Strategies (NES). In contrast to typical evolutionary algorithms which search in the vicinity of the fittest individuals in a population, evolution strategies aim at repeating the type of mutations that led to those individuals. We can characterize those mutations by a search distribution. The key idea of NES is to ascend the gradient on the parameters of that distribution towards higher expected fitness. We show how plain gradient ascent is destined to fail, and provide a viable alternative that instead descends along the natural gradient to adapt the search distribution, which appropriately normalizes the update step with respect to its uncertainty. Being derived from first principles, the NES approach can be extended to all types of search distributions that allow a parametric form, not just the classical multivariate Gaussian one. We derive a number of NES variants for different distributions, and show how they are useful on different problem classes. In addition, we rein in the computational cost, avoiding costly matrix inversions through an incremental change of coordinates. Two additional, novel techniques, importance mixing and adaptation sampling, allow us to automatically tune the learning rate and batch size to the problem, and thereby further reduce the average number of required fitness evaluations. A third technique, restart strategies, provides the algorithm with additional robustness in the presence of multiple local optima, or noise. Second, we introduce a new approach to costly black-box optimization, when fitness evaluations are very expensive. Here, we model the fitness function using state-of-the-art Gaussian process regression, and use the principle of artificial curiosity to direct exploration towards the most informative next evaluation candidate. Both the expected fitness improvement and the expected information gain can be derived explicitly from the Gaussian process model, and our method constructs a front of Pareto-optimal points according to these two criteria. This makes the exploration-exploitation trade-off explicit, and permits maximally informed candidate selection. In summary, this dissertation presents a collection of novel algorithms, for the general problem of continuous black-box optimization as well as a number of special cases, each validated empirically.},
  school = {Technical University of Munich}
}

@misc{schaul_unit_2013,
  title = {Unit {{Tests}} for {{Stochastic Optimization}}},
  author = {Schaul, Tom and Antonoglou, Ioannis and Silver, David},
  year = {2013},
  eprint = {1312.6055},
  eprinttype = {arxiv},
  abstract = {Optimization by stochastic gradient descent is an important component of many large-scale machine learning algorithms. A wide variety of such optimization algorithms have been devised; however, it is unclear whether these algorithms are robust and widely applicable across many different optimization landscapes. In this paper we develop a collection of unit tests for stochastic optimization. Each unit test rapidly evaluates an optimization algorithm on a small-scale, isolated, and well-understood difficulty, rather than in real-world scenarios where many such issues are entangled. Passing these unit tests is not sufficient, but absolutely necessary for any algorithms with claims to generality or robustness. We give initial quantitative and qualitative results on numerous established algorithms. The testing framework is open-source, extensible, and easy to apply to new algorithms.},
  archiveprefix = {arXiv},
  isbn = {9783642352881},
  pmid = {15003161}
}

@inproceedings{scherer_evaluation_2010,
  title = {Evaluation of {{Pooling Operations}} in {{Convolutional Architectures}} for {{Object Recognition}}},
  booktitle = {Lecture {{Notes}} in {{Computer Science}} (Including Subseries {{Lecture Notes}} in {{Artificial Intelligence}} and {{Lecture Notes}} in {{Bioinformatics}})},
  author = {Scherer, Dominik and M{\"u}ller, Andreas and Behnke, Sven},
  year = {2010},
  volume = {6354 LNCS},
  pages = {92--101},
  issn = {03029743},
  abstract = {A common practice to gain invariant features in object recog- nition models is to aggregate multiple low-level features over a small neighborhood. However, the differences between those models makes a comparison of the properties of different aggregation functions hard. Our aim is to gain insight into different functions by directly comparing them on a fixed architecture for several common object recognition tasks. Empirical results show that a maximum pooling operation significantly outperforms subsampling operations. Despite their shift-invariant proper- ties, overlapping pooling windows are no significant improvement over non-overlapping pooling windows. By applying this knowledge, we achieve state-of-the-art error rates of 4.57\% on the NORB normalized-uniform dataset and 5.6\% on the NORB jittered-cluttered dataset.},
  isbn = {3-642-15824-2}
}

@article{schiel_rhythm_2010,
  title = {Rhythm and {{Formant Features}} for {{Automatic Alcohol Detection}}},
  author = {Schiel, Florian and Heinrich, Christian and Neumeyer, Veronika},
  year = {2010},
  pages = {4},
  abstract = {Two speech feature sets, RMS rhythmicity and formant frequencies F1-F4, are analyzed for their ability to distinguish alcoholized from sober speech. We describe the statistical framework based on the Alcohol Language Corpus (ALC), including other factors such as gender, age and speaking style, and its application to our case. Rhythm features are calculated using a new method based solely on the short-time energy function; formant features are derived using the standard formant tracker SNACK. Our findings indicate that 3 rhythm and 3 formant features have a high potential to detect intoxication within the speech data of a subject. We also tested the hypothesis that vowels are more centralized in the F1/F2 space for alcoholized speech, but found that, on the contrary, subjects tend to hyperarticulate when being tested for intoxication.},
  langid = {english}
}

@inproceedings{schirrmeister_understanding_2020,
  title = {Understanding Anomaly Detection with Deep Invertible Networks through Hierarchies of Distributions and Features},
  booktitle = {Proceedings of the 33rd {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Schirrmeister, Robin and Zhou, Yuxuan and Ball, Tonio and Zhang, Dan},
  year = {2020},
  address = {{Virtual}}
}

@article{schmidhuber_deep_2015,
  title = {Deep {{Learning}} in {{Neural Networks}}: {{An Overview}}},
  author = {Schmidhuber, J{\"u}rgen},
  year = {2015},
  journal = {Proceedings of the International Joint Conference on Neural Networks (IJCNN)},
  volume = {61},
  eprint = {1404.7828},
  eprinttype = {arxiv},
  pages = {85--117},
  issn = {18792782},
  abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
  archiveprefix = {arXiv},
  pmid = {25462637},
  keywords = {Deep learning,Evolutionary computation,Reinforcement learning,Supervised learning,Unsupervised learning}
}

@techreport{schmidhuber_direct_1999,
  title = {Direct {{Policy Search}} and {{Uncertain Policy Evaluation}}},
  author = {Schmidhuber, J{\"u}rgen and Zhao, Jieyu},
  year = {1999},
  journal = {AAAI Technical Report SS-99-07},
  pages = {119--124},
  abstract = {Reinforcement learning based on direct search in policy space requires few assumptions about the environment. Hence it is applicable in certain situations where most traditional reinforcement learning algorithms based on dynamic programming are not, especially in partially observable, deterministic worlds. In realistic settings, however, reliable policy evaluations are complicated by numerous sources of uncertainty, such as stochasticity in policy and environment. Given a limited life-time, how much time should a direct policy searcher spend on policy evaluations to obtain reliable statistics? De-spite the fundamental nature of this question it has not received much attention yet. Our efficient approach based on the success-story algorithm (SSA) is radical in the sense that it never stops evaluating any pre-vious policy modification except those it undoes for lack of empirical evidence that they have contributed to lifelong reward accelerations. Here we identify SSA's fundamental advantages over traditional direct policy search (such as stochastic hill-climbing) on problems involving several sources of stochasticity and uncer-taint),. INTRODUCTION In this paper a learner's modifiable parameters that determine its behavior are called its policy. An al-gorithm that modifies the policy is called a learn-ing algorithm. In the context of reinforcement learn-ing (RL) there are two broad classes of learning algo-rithms: (1) methods based on value functions (VFs), and (2) direct search in policy space. VF-based algo-rithms learn a mapping from input-action pairs to ex-pected discounted future reward and use online vari-ants of dynamic programming (DP) (Bellman 1961) for constructing rewarding policies,}
}

@article{schmidhuber_making_nodate,
  title = {Making the {{World Differentiable}}: {{On Using Self-Supervised Fully Recurrent Neu}}\dbend al {{Networks}} for {{Dynamic Reinforcement Learning}} and {{Planning}} in {{Non-Stationary Environm}}\dbend nts},
  author = {Schmidhuber, Jiirgen},
  pages = {26},
  langid = {english}
}

@misc{schneider_wav2vec:_2019,
  title = {Wav2vec: {{Unsupervised Pre-training}} for {{Speech Recognition}}},
  shorttitle = {Wav2vec},
  author = {Schneider, Steffen and Baevski, Alexei and Collobert, Ronan and Auli, Michael},
  year = {2019},
  month = apr,
  eprint = {1904.05862},
  eprinttype = {arxiv},
  abstract = {We explore unsupervised pre-training for speech recognition by learning representations of raw audio. wav2vec is trained on large amounts of unlabeled audio data and the resulting representations are then used to improve acoustic model training. We pre-train a simple multi-layer convolutional neural network optimized via a noise contrastive binary classification task. Our experiments on WSJ reduce WER of a strong character-based log-mel filterbank baseline by up to 32\% when only a few hours of transcribed data is available. Our approach achieves 2.78\% WER on the nov92 test set. This outperforms Deep Speech 2, the best reported character-based system in the literature while using three orders of magnitude less labeled training data.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language}
}

@article{scholkopf_artificial_2015,
  title = {Artificial Intelligence: {{Learning}} to See and Act},
  author = {Sch{\"o}lkopf, Bernhard},
  year = {2015},
  journal = {Nature},
  volume = {518},
  number = {7540},
  pages = {486--487},
  issn = {14764687},
  abstract = {An artificial-intelligence system uses machine learning from massive training sets to teach itself to play 49 classic computer games, demonstrating that it can adapt to a variety of tasks. See Letter p.529},
  isbn = {doi:10.1038/518486a},
  pmid = {25719660}
}

@inproceedings{schroeder_code-excited_1985,
  title = {Code-Excited Linear Prediction({{CELP}}): {{High-quality}} Speech at Very Low Bit Rates},
  shorttitle = {Code-Excited Linear Prediction({{CELP}})},
  booktitle = {{{ICASSP}} '85. {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}},
  author = {Schroeder, M. and Atal, B.},
  year = {1985},
  month = apr,
  volume = {10},
  pages = {937--940},
  abstract = {We describe in this paper a code-excited linear predictive coder in which the optimum innovation sequence is selected from a code book of stored sequences to optimize a given fidelity criterion. Each sample of the innovation sequence is filtered sequentially through two time-varying linear recursive filters, one with a long-delay (related to pitch period) predictor in the feedback loop and the other with a short-delay predictor (related to spectral envelope) in the feedback loop. We code speech, sampled at 8 kHz, in blocks of 5-msec duration. Each block consisting of 40 samples is produced from one of 1024 possible innovation sequences. The bit rate for the innovation sequence is thus 1/4 bit per sample. We compare in this paper several different random and deterministic code books for their effectiveness in providing the optimum innovation sequence in each block. Our results indicate that a random code book has a slight speech quality advantage at low bit rates. Examples of speech produced by the above method will be played at the conference.},
  keywords = {Bit rate,Books,Feedback loop,Linear predictive coding,Nonlinear filters,Predictive maintenance,Predictive models,Speech coding,Speech synthesis,Technological innovation}
}

@misc{schulman_gradient_2015,
  title = {Gradient {{Estimation Using Stochastic Computation Graphs}}},
  author = {Schulman, John and Heess, Nicolas and Weber, Theophane and Abbeel, Pieter},
  year = {2015},
  eprint = {1506.05254},
  eprinttype = {arxiv},
  abstract = {In a variety of problems originating in supervised, unsupervised, and reinforcement learning, the loss function is defined by an expectation over a collection of random variables, which might be part of a probabilistic model or the external world. Estimating the gradient of this loss function, using samples, lies at the core of gradient-based learning algorithms for these problems. We introduce the formalism of stochastic computation graphs---directed acyclic graphs that include both deterministic functions and conditional probability distributions---and describe how to easily and automatically derive an unbiased estimator of the loss function's gradient. The resulting algorithm for computing the gradient estimator is a simple modification of the standard backpropagation algorithm. The generic scheme we propose unifies estimators derived in variety of prior work, along with variance-reduction techniques therein. It could assist researchers in developing intricate models involving a combination of stochastic and deterministic operations, enabling, for example, attention, memory, and control actions.},
  archiveprefix = {arXiv}
}

@misc{schulman_high-dimensional_2015,
  title = {High-{{Dimensional Continuous Control Using Generalized Advantage Estimation}}},
  author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael I. and Abbeel, Pieter},
  year = {2015},
  eprint = {1506.02438},
  eprinttype = {arxiv},
  abstract = {Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.},
  archiveprefix = {arXiv}
}

@misc{schulman_proximal_2017,
  title = {Proximal {{Policy Optimization Algorithms}}},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  year = {2017},
  eprint = {1707.06347},
  eprinttype = {arxiv},
  abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  archiveprefix = {arXiv}
}

@inproceedings{schulman_trust_2015,
  title = {Trust {{Region Policy Optimization}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
  year = {2015},
  eprint = {1502.05477},
  eprinttype = {arxiv},
  address = {{Lille, France}},
  abstract = {We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
  archiveprefix = {arXiv}
}

@incollection{schurer_adaptive_2004,
  title = {Adaptive {{Quasi-Monte Carlo Integration Based}} on {{MISER}} and {{VEGAS}}},
  booktitle = {Monte {{Carlo}} and {{Quasi-Monte Carlo Methods}} 2002},
  author = {Sch{\"u}rer, Rudolf},
  year = {2004},
  pages = {393-406 TS - CrossRef},
  abstract = {Quasi-Monte Carlo (QMC) routines are one of the most common tech-niques for solving integration problems in high dimensions. However, their efficiency degrades if the variation of the integrand is concentrated in small areas of the inte-gration domain. Adaptive algorithms cope with this situation by adjusting the flow of computation based on previous integrand evaluations. We explore ways to modify the Monte Carlo based adaptive algorithms MISER and VEGAS such that low-discrepancy point sets are used instead of random sam-ples. Experimental results show that the proposed algorithms outperform plain QMC as well as the original adaptive integration routine for certain classes of test cases.},
  isbn = {978-3-540-20466-4},
  keywords = {MISER,VEGAS}
}

@inproceedings{sedighizadeh_adaptive_2008,
  title = {Adaptive {{PID Controller}} Based on {{Reinforcement Learning}} for {{Wind Turbine Control}}},
  booktitle = {{{PROCEEDINGS OF WORLD ACADEMY OF SCIENCE}}, {{ENGINEERING AND TECHNOLOGY}}},
  author = {Sedighizadeh, M. and Rezazadeh, A.},
  year = {2008},
  pages = {257--262},
  abstract = {A self tuning PID control strategy using reinforcement learning is proposed in this paper to deal with the control of wind energy conversion systems (WECS). Actor-Critic learning is used to tune PID parameters in an adaptive way by taking advantage of the model-free and on-line learning properties of reinforcement learning effectively. In order to reduce the demand of storage space and to improve the learning efficiency, a single RBF neural network is used to approximate the policy function of Actor and the value function of Critic simultaneously. The inputs of RBF network are the system error, as well as the first and the second-order differences of error. The Actor can realize the mapping from the system state to PID parameters, while the Critic evaluates the outputs of the Actor and produces TD error. Based on TD error performance index and gradient descent method, the updating rules of RBF kernel function and network weights were given. Simulation results show that the proposed controller is efficient for WECS and it is perfectly adaptable and strongly robust, which is better than that of a conventional PID controller.}
}

@article{see_get_nodate,
  title = {Get {{To The Point}}: {{Summarization}} with {{Pointer-Generator Networks}}},
  author = {See, Abigail and Liu, Peter J. and Brain, Google and Manning, Christopher D.},
  abstract = {Neural sequence-to-sequence models have provided a viable new approach for ab-stractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the origi-nal text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we pro-pose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate repro-duction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail sum-marization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.}
}

@article{sehnke_parameter-exploring_2010,
  title = {Parameter-Exploring Policy Gradients},
  author = {Sehnke, Frank and Osendorfer, Christian and R{\"u}ckstie{\ss}, Thomas and Graves, Alex and Peters, Jan and Schmidhuber, J{\"u}rgen},
  year = {2010},
  month = may,
  journal = {Proceedings of the International Joint Conference on Neural Networks (IJCNN)},
  volume = {23},
  number = {4},
  pages = {551--559},
  issn = {08936080},
  abstract = {We present a model-free reinforcement learning method for partially observable Markov decision problems. Our method estimates a likelihood gradi-ent by sampling directly in parameter space, which leads to lower variance gradient estimates than obtained by regular policy gradient methods. We show that for several complex control tasks, including robust standing with a humanoid robot, this method outperforms well-known algorithms from the fields of standard policy gradients, finite difference methods and population based heuristics. We also show that the improvement is largest when the pa-rameter samples are drawn symmetrically. Lastly we analyse the importance of the individual components of our method by incrementally incorporating them into the other algorithms, and measuring the gain in performance after each step.},
  isbn = {9780769543000},
  pmid = {20061118}
}

@misc{semeniuta_hybrid_2017,
  title = {A {{Hybrid Convolutional Variational Autoencoder}} for {{Text Generation}}},
  author = {Semeniuta, Stanislau and Severyn, Aliaksei and Barth, Erhardt},
  year = {2017},
  month = feb,
  eprint = {1702.02390},
  eprinttype = {arxiv},
  abstract = {In this paper we explore the effect of architectural choices on learning a Variational Autoencoder (VAE) for text generation. In contrast to the previously introduced VAE model for text where both the encoder and decoder are RNNs, we propose a novel hybrid architecture that blends fully feed-forward convolutional and deconvolutional components with a recurrent language model. Our architecture exhibits several attractive properties such as faster run time and convergence, ability to better handle long sequences and, more importantly, it helps to avoid some of the major difficulties posed by training VAE models on textual data.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language}
}

@article{sener_multi-task_2018,
  title = {Multi-{{Task Learning}} as {{Multi-Objective Optimization}}},
  author = {Sener, Ozan and Koltun, Vladlen},
  year = {2018},
  month = oct,
  langid = {english}
}

@misc{sermanet_overfeat:_2013,
  title = {{{OverFeat}}: {{Integrated Recognition}}, {{Localization}} and {{Detection}} Using {{Convolutional Networks}}},
  author = {Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Michael and Fergus, Rob and LeCun, Yann A.},
  year = {2013},
  month = dec,
  eprint = {1312.6229},
  eprinttype = {arxiv},
  abstract = {We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.},
  archiveprefix = {arXiv}
}

@inproceedings{serra_input_2020,
  title = {Input Complexity and Out-of-Distribution Detection with Likelihood-Based Generative Models},
  booktitle = {Proceedings of the 8th {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Serr{\`a}, Joan and {\'A}lvarez, David and G{\'o}mez, Vicen{\c c} and Slizovskaia, Olga and N{\'u}{\~n}ez, Jos{\'e} F. and Luque, Jordi},
  year = {2020},
  address = {{Addis Ababa, Ethiopia}}
}

@article{settle_discriminative_2016,
  title = {Discriminative {{Acoustic Word Embeddings}}: {{Recurrent Neural Network-Based Approaches}}},
  shorttitle = {Discriminative {{Acoustic Word Embeddings}}},
  author = {Settle, Shane and Livescu, Karen},
  year = {2016},
  month = nov,
  journal = {arXiv:1611.02550 [cs]},
  eprint = {1611.02550},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Acoustic word embeddings --- fixed-dimensional vector representations of variable-length spoken word segments --- have begun to be considered for tasks such as speech recognition and query-by-example search. Such embeddings can be learned discriminatively so that they are similar for speech segments corresponding to the same word, while being dissimilar for segments corresponding to different words. Recent work has found that acoustic word embeddings can outperform dynamic time warping on query-by-example search and related word discrimination tasks. However, the space of embedding models and training approaches is still relatively unexplored. In this paper we present new discriminative embedding models based on recurrent neural networks (RNNs). We consider training losses that have been successful in prior work, in particular a cross entropy loss for word classification and a contrastive loss that explicitly aims to separate same-word and different-word pairs in a "Siamese network" training setting. We find that both classifier-based and Siamese RNN embeddings improve over previously reported results on a word discrimination task, with Siamese RNNs outperforming classification models. In addition, we present analyses of the learned embeddings and the effects of variables such as dimensionality and network structure.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@article{shahriari_taking_2016,
  title = {Taking the {{Human Out}} of the {{Loop}}: {{A Review}} of {{Bayesian Optimization}}},
  shorttitle = {Taking the {{Human Out}} of the {{Loop}}},
  author = {Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P. and {de Freitas}, Nando},
  year = {2016},
  month = jan,
  journal = {Proceedings of the IEEE},
  volume = {104},
  number = {1},
  pages = {148--175},
  issn = {0018-9219, 1558-2256},
  abstract = {Big data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures. The construction of such systems involves many distributed design choices. The end products (e.g., recommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involves many tunable configuration parameters. These parameters are often specified and hard-coded into the software by various developers or teams. If optimized jointly, these parameters can result in significant improvements. Bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years. It promises greater automation so as to increase both product quality and human productivity. This review paper introduces Bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.},
  langid = {english}
}

@article{shamir_without-replacement_2016,
  title = {Without-{{Replacement Sampling}} for {{Stochastic Gradient Methods}}: {{Convergence Results}} and {{Application}} to {{Distributed Optimization}}},
  shorttitle = {Without-{{Replacement Sampling}} for {{Stochastic Gradient Methods}}},
  author = {Shamir, Ohad},
  year = {2016},
  month = mar,
  abstract = {Stochastic gradient methods for machine learning and optimization problems are usually analyzed assuming data points are sampled with replacement. In practice, however, sampling without replacement is very common, easier to implement in many cases, and often performs better. In this paper, we provide competitive convergence guarantees for without-replacement sampling, under various scenarios, for three types of algorithms: Any algorithm with online regret guarantees, stochastic gradient descent, and SVRG. A useful application of our SVRG analysis is a nearly-optimal algorithm for regularized least squares in a distributed setting, in terms of both communication complexity and runtime complexity, when the data is randomly partitioned and the condition number can be as large as the data size per machine (up to logarithmic factors). Our proof techniques combine ideas from stochastic optimization, adversarial online learning, and transductive learning theory, and can potentially be applied to other stochastic optimization and learning problems.},
  langid = {english},
  keywords = {Sampling w/wo replacement}
}

@article{shannon_mathematical_1948,
  title = {A {{Mathematical Theory}} of {{Communication}}},
  author = {Shannon, Claude E},
  year = {1948},
  journal = {The Bell System Technical Journal},
  volume = {27},
  number = {July 1948},
  eprint = {chao-dyn/9411012},
  eprinttype = {arxiv},
  pages = {379--423},
  issn = {07246811},
  abstract = {The recent development of various methods of modulation such as PCM and PPM which exchange bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A basis for such a theory is contained in the important papers of Nyquist and Hartley on this subject. In the present paper we will extend the theory to include a number of new factors, in particular the effect of noise in the channel, and the savings possible due to the statistical structure of the original message and due to the nature of the final destination of the information. The fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point. Frequently the messages have meaning; that is they refer to or are correlated according to some system with certain physical or conceptual entities. These semantic aspects of communication are irrelevant to the engineering problem. The significant aspect is that the actual message is one selected from a set of possible messages. The system must be designed to operate for each possible selection, not just the one which will actually be chosen since this is unknown at the time of design. If the number of messages in the set is finite then this number or any monotonic function of this number can be regarded as a measure of the information produced when one message is chosen from the set, all choices being equally likely. As was pointed out by Hartley the most natural choice is the logarithmic function. Although this definition must be generalized considerably when we consider the influence of the statistics of the message and when we have a continuous range of messages, we will in all cases use an essentially logarithmic measure.},
  archiveprefix = {arXiv},
  isbn = {0252725484},
  pmid = {9230594}
}

@misc{shannon_optimizing_2017,
  title = {Optimizing Expected Word Error Rate via Sampling for Speech Recognition},
  author = {Shannon, Matt},
  year = {2017},
  month = jun,
  eprint = {1706.02776},
  eprinttype = {arxiv},
  abstract = {State-level minimum Bayes risk (sMBR) training has become the de facto standard for sequence-level training of speech recognition acoustic models. It has an elegant formulation using the expectation semiring, and gives large improvements in word error rate (WER) over models trained solely using cross-entropy (CE) or connectionist temporal classification (CTC). sMBR training optimizes the expected number of frames at which the reference and hypothesized acoustic states differ. It may be preferable to optimize the expected WER, but WER does not interact well with the expectation semiring, and previous approaches based on computing expected WER exactly involve expanding the lattices used during training. In this paper we show how to perform optimization of the expected WER by sampling paths from the lattices used during conventional sMBR training. The gradient of the expected WER is itself an expectation, and so may be approximated using Monte Carlo sampling. We show experimentally that optimizing WER during acoustic model training gives 5\% relative improvement in WER over a well-tuned sMBR baseline on a 2-channel query recognition task (Google Home).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
}

@inproceedings{sharif_razavian_cnn_2014,
  title = {{{CNN}} Features Off-the-Shelf: An Astounding Baseline for Recognition},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CCVPR}})},
  author = {Sharif Razavian, Ali and Azizpour, Hossein and Sullivan, Josephine and Carlsson, Stefan},
  year = {2014},
  pages = {806--813}
}

@article{shehab_generic_2013,
  title = {A {{Generic Feature Extraction Model}} Using {{Learnable Evolution Models}} ({{LEM}}+ {{ID3}}).},
  author = {Shehab, M. E. and Badran, K. and Salama, G. I.},
  year = {2013},
  journal = {International Journal of Computer Applications},
  volume = {64},
  number = {11},
  pages = {27--32},
  keywords = {Dynamic threshold classifier,Feature extraction,Pattern recognition}
}

@misc{shen_natural_2018,
  title = {Natural {{TTS Synthesis}} by {{Conditioning WaveNet}} on {{Mel Spectrogram Predictions}}},
  author = {Shen, Jonathan and Pang, Ruoming and Weiss, Ron J. and Schuster, Mike and Jaitly, Navdeep and Yang, Zongheng and Chen, Zhifeng and Zhang, Yu and Wang, Yuxuan and {Skerry-Ryan}, R. J. and Saurous, Rif A. and Agiomyrgiannakis, Yannis and Wu, Yonghui},
  year = {2018},
  month = feb,
  eprint = {1712.05884},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the conditioning input to WaveNet instead of linguistic, duration, and F0 features. We further show that using this compact acoustic intermediate representation allows for a significant reduction in the size of the WaveNet architecture.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language}
}

@techreport{shewchuk_introduction_1994,
  title = {An {{Introduction}} to the {{Conjugate Gradient Method Without}} the {{Agonizing Pain}}},
  author = {Shewchuk, Jonathan Richard},
  year = {1994},
  month = aug,
  institution = {{Carnegie Mellon University}},
  abstract = {The Conjugate Gradient Method is the most prominent iterative method for solving sparse systems of linear equations. Unfortunately, many textbook treatments of the topic are written with neither illustrations nor intuition, and their victims can be found to this day babbling senselessly in the corners of dusty libraries. For this reason, a deep, geometric understanding of the method has been reserved for the elite brilliant few who have painstakingly decoded the mumblings of their forebears. Nevertheless, the Conjugate Gradient Method is a composite of simple, elegant ideas that almost anyone can understand. Of course, a reader as intelligent as yourself will learn them almost effortlessly. The idea of quadratic forms is introduced and used to derive the methods of Steepest Descent, Conjugate Directions, and Conjugate Gradients. Eigenvectors are explained and used to examine the convergence of the Jacobi Method, Steepest Descent, and Conjugate Gradients. Other topics include preconditioning and the nonlinear Conjugate Gradient Method. I have taken pains to make this article easy to read. Sixty-six illustrations are provided. Dense prose is avoided. Concepts are explained in several different ways. Most equations are coupled with an intuitive interpretation.}
}

@misc{shi_convolutional_2015,
  title = {Convolutional {{LSTM Network}}: {{A Machine Learning Approach}} for {{Precipitation Nowcasting}}},
  shorttitle = {Convolutional {{LSTM Network}}},
  author = {Shi, Xingjian and Chen, Zhourong and Wang, Hao and Yeung, Dit-Yan and Wong, Wai-kin and Woo, Wang-chun},
  year = {2015},
  month = jun,
  eprint = {1506.04214},
  eprinttype = {arxiv},
  abstract = {The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-theart operational ROVER algorithm for precipitation nowcasting.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition}
}

@misc{shi_how_2020,
  title = {How {{I}} Learned to Stop Worrying and Write {{ELBO}} (and Its Gradients) in a Billion Ways},
  author = {Shi, Yuge},
  year = {2020},
  month = jun,
  howpublished = {https://yugeten.github.io/posts/2020/06/elbo/}
}

@misc{shi_learning_2022,
  title = {Learning {{Audio-Visual Speech Representation}} by {{Masked Multimodal Cluster Prediction}}},
  author = {Shi, Bowen and Hsu, Wei-Ning and Lakhotia, Kushal and Mohamed, Abdelrahman},
  year = {2022},
  month = jan,
  eprint = {2201.02184},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {Video recordings of speech contain correlated audio and visual information, providing a strong signal for speech representation learning from the speaker's lip movements and the produced sound. We introduce Audio-Visual Hidden Unit BERT (AV-HuBERT), a self-supervised representation learning framework for audio-visual speech, which masks multi-stream video input and predicts automatically discovered and iteratively refined multimodal hidden units. AV-HuBERT learns powerful audio-visual speech representation benefiting both lip-reading and automatic speech recognition. On the largest public lip-reading benchmark LRS3 (433 hours), AV-HuBERT achieves 32.5\% WER with only 30 hours of labeled data, outperforming the former state-of-the-art approach (33.6\%) trained with a thousand times more transcribed video data (31K hours). The lip-reading WER is further reduced to 26.9\% when using all 433 hours of labeled data from LRS3 and combined with self-training. Using our audio-visual representation on the same benchmark for audio-only speech recognition leads to a 40\% relative WER reduction over the state-of-the-art performance (1.3\% vs 2.3\%). Our code and models are available at https://github.com/facebookresearch/av\_hubert},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@misc{shi_robust_2022,
  title = {Robust {{Self-Supervised Audio-Visual Speech Recognition}}},
  author = {Shi, Bowen and Hsu, Wei-Ning and Mohamed, Abdelrahman},
  year = {2022},
  month = jan,
  eprint = {2201.01763},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {Audio-based automatic speech recognition (ASR) degrades significantly in noisy environments and is particularly vulnerable to interfering speech, as the model cannot determine which speaker to transcribe. Audio-visual speech recognition (AVSR) systems improve robustness by complementing the audio stream with the visual information that is invariant to noise and helps the model focus on the desired speaker. However, previous AVSR work focused solely on the supervised learning setup; hence the progress was hindered by the amount of labeled data available. In this work, we present a self-supervised AVSR framework built upon Audio-Visual HuBERT (AV-HuBERT), a state-of-the-art audio-visual speech representation learning model. On the largest available AVSR benchmark dataset LRS3, our approach outperforms prior state-of-the-art by \textasciitilde 50\% (28.0\% vs. 14.1\%) using less than 10\% of labeled data (433hr vs. 30hr) in the presence of babble noise, while reducing the WER of an audio-based model by over 75\% (25.8\% vs. 5.8\%) on average.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@inproceedings{shinohara_adversarial_2016,
  title = {Adversarial {{Multitask Learning}} of {{Deep Neural Networks}} for {{Robust Speech Recognition}}},
  author = {Shinohara, Yusuke},
  year = {2016},
  abstract = {A method of learning deep neural networks (DNNs) for noise robust speech recognition is proposed. It is widely known that representations (activations) of well-trained DNNs are highly invariant to noise, especially in higher layers, and such invariance leads to the noise robustness of DNNs. However, little is known about how to enhance such invariance of representations, which is a key for improving robustness. In this paper, we propose adversarial multi-task learning of DNNs for explicitly enhancing the invariance of representations. Specifically, a primary task of senone classification and a secondary task of domain (noise condition) classification are jointly solved. What is different from the standard multi-task learning is that the representation is learned adversarially to the secondary task, so that representation with low domain-classification accuracy is induced. As a result, senone-discriminative and domain-invariant representation is obtained, which leads to an improved robustness of DNNs. Experimental results on a noise-corrupted Wall Street Journal data set show the effectiveness of the proposed method.},
  keywords = {Adversary (cryptography),Artificial neural network,Computer multitasking,Deep learning,Multi-task learning,Neural network software,Robustness,Speech recognition,The Wall Street Journal}
}

@article{shon_slue_2021,
  title = {{{SLUE}}: {{New Benchmark Tasks}} for {{Spoken Language Understanding Evaluation}} on {{Natural Speech}}},
  shorttitle = {{{SLUE}}},
  author = {Shon, Suwon and Pasad, Ankita and Wu, Felix and Brusco, Pablo and Artzi, Yoav and Livescu, Karen and Han, Kyu J.},
  year = {2021},
  month = nov,
  journal = {arXiv:2111.10367 [cs, eess]},
  eprint = {2111.10367},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {Progress in speech processing has been facilitated by shared datasets and benchmarks. Historically these have focused on automatic speech recognition (ASR), speaker identification, or other lower-level tasks. Interest has been growing in higher-level spoken language understanding tasks, including using end-to-end models, but there are fewer annotated datasets for such tasks. At the same time, recent work shows the possibility of pre-training generic representations and then fine-tuning for several tasks using relatively little labeled data. We propose to create a suite of benchmark tasks for Spoken Language Understanding Evaluation (SLUE) consisting of limited-size labeled training sets and corresponding evaluation sets. This resource would allow the research community to track progress, evaluate pre-trained representations for higher-level tasks, and study open questions such as the utility of pipeline versus end-to-end approaches. We present the first phase of the SLUE benchmark suite, consisting of named entity recognition, sentiment analysis, and ASR on the corresponding datasets. We focus on naturally produced (not read or synthesized) speech, and freely available datasets. We provide new transcriptions and annotations on subsets of the VoxCeleb and VoxPopuli datasets, evaluation metrics and results for baseline models, and an open-source toolkit to reproduce the baselines and evaluate new models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@misc{shon_unsupervised_2018,
  title = {Unsupervised {{Representation Learning}} of {{Speech}} for {{Dialect Identification}}},
  author = {Shon, Suwon and Hsu, Wei-Ning and Glass, James},
  year = {2018},
  month = sep,
  eprint = {1809.04458},
  eprinttype = {arxiv},
  abstract = {In this paper, we explore the use of a factorized hierarchical variational autoencoder (FHVAE) model to learn an unsupervised latent representation for dialect identification (DID). An FHVAE can learn a latent space that separates the more static attributes within an utterance from the more dynamic attributes by encoding them into two different sets of latent variables. Useful factors for dialect identification, such as phonetic or linguistic content, are encoded by a segmental latent variable, while irrelevant factors that are relatively constant within a sequence, such as a channel or a speaker information, are encoded by a sequential latent variable. The disentanglement property makes the segmental latent variable less susceptible to channel and speaker variation, and thus reduces degradation from channel domain mismatch. We demonstrate that on fully-supervised DID tasks, an end-toend model trained on the features extracted from the FHVAE model achieves the best performance, compared to the same model trained on conventional acoustic features and an i-vector based system. Moreover, we also show that the proposed approach can leverage a large amount of unlabeled data for FHVAE training to learn domain-invariant features for DID, and significantly improve the performance in a lowresource condition, where the labels for the in-domain data are not available.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@misc{shrikumar_learning_2017,
  title = {Learning {{Important Features Through Propagating Activation Differences}}},
  author = {Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
  year = {2017},
  month = apr,
  eprint = {1704.02685},
  eprinttype = {arxiv},
  abstract = {The purported ``black box'' nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its `reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. A detailed video tutorial on the method is at http://goo.gl/qKb7pL and code is at http://goo.gl/RM8jvH.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@inproceedings{siddharth_learning_2017,
  title = {Learning Disentangled Representations with Semi-Supervised Deep Generative Models},
  booktitle = {Proceedings Og the 31st {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Siddharth, Narayanaswamy and Paige, Brooks and {van de Meent}, Jan-Willem and Desmaison, Alban and Goodman, Noah D. and Kohli, Pushmeet and Wood, Frank D. and Torr, Philip H. S.},
  editor = {Guyon, Isabelle and {von Luxburg}, Ulrike and Bengio, Samy and Wallach, Hanna M. and Fergus, Rob and Vishwanathan, S. V. N. and Garnett, Roman},
  year = {2017},
  pages = {5925--5935},
  address = {{Long Beach, CA, USA}}
}

@article{silver_deterministic_2014,
  title = {Deterministic {{Policy Gradient Algorithms}}},
  author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  year = {2014},
  journal = {Proceedings of the International Conference on Machine Learning (ICML)},
  pages = {387--395},
  issn = {1938-7228},
  abstract = {In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic pol- icy gradient has a particularly appealing form: it is the expected gradient of the action-value function. This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. We demonstrate that deterministic policy gradient algorithms can significantly outperform their stochastic counterparts in high-dimensional action spaces.},
  isbn = {9781634393973}
}

@article{silver_mastering_2016,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and {van den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc},
  year = {2016},
  journal = {Nature},
  volume = {529},
  number = {7587},
  pages = {484--489},
  publisher = {{Nature Publishing Group}},
  abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses `value networks' to evaluate board positions and `policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.}
}

@article{simard_best_2003,
  title = {Best Practices for Convolutional Neural Networks Applied to Visual Document Analysis},
  author = {Simard, Patrice Y. and Steinkraus, Dave and Platt, J. C.},
  year = {2003},
  journal = {Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings.},
  volume = {1},
  number = {Icdar},
  eprint = {1011.1669v3},
  eprinttype = {arxiv},
  pages = {958--963},
  issn = {15205363},
  abstract = {Neural Networks are a powerful technology for classification of visual inputs arising from documents. However, there is a confusing plethora of different neural network methods that are used in the literature and in industry. This paper describes a set of concrete best practices that document analysis researchers can use to get good results with neural networks. The most important practice is that convolutional neural networks are better suited for visual document tasks than fully connected networks. We propose that a simple "do-it-yourself" implementation of convolution neural networks does not require complex methods, such as momentum, weight decay, structure-dependent learning rates, averaging layers, tangent prop, or even finely-tuning the architecture. The end result is a very simple yet general architecture which can yield state-of-the-art performance for document analysis. We illustrate our claims on the MNIST set of English digit images.},
  archiveprefix = {arXiv},
  isbn = {0-7695-1960-1},
  pmid = {25246403},
  keywords = {Best practices,Concrete,Convolution,Handwriting recognition,Industrial training,Information processing,Neural network,Performance analysis,Support vector machines,Text analysis}
}

@article{simonyan_two-stream_2014,
  title = {Two-{{Stream Convolutional Networks}} for {{Action Recognition}} in {{Videos}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  year = {2014},
  journal = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
  eprint = {1406.2199},
  eprinttype = {arxiv},
  pages = {568--576},
  issn = {1098-6596},
  abstract = {We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to generalise the best performing hand-crafted features within a data-driven learning framework. Our contribution is three-fold. First, we propose a two-stream ConvNet architecture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multi-task learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both. Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it is competitive with the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification.},
  archiveprefix = {arXiv},
  isbn = {9788578110796},
  pmid = {25246403}
}

@misc{simonyan_very_2014,
  title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  author = {Simonyan, Karen and Zisserman, Andrew},
  year = {2014},
  eprint = {1409.1556},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@inproceedings{simsekli_tail-index_2019,
  title = {A {{Tail-Index Analysis}} of {{Stochastic Gradient Noise}} in {{Deep Neural Networks}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Simsekli, Umut and Sagun, Levent and G{\"u}rb{\"u}zbalaban, Mert},
  year = {2019},
  pages = {11},
  address = {{Long Beach, CA, USA}},
  abstract = {The gradient noise (GN) in the stochastic gradient descent (SGD) algorithm is often considered to be Gaussian in the large data regime by assuming that the classical central limit theorem (CLT) kicks in. This assumption is often made for mathematical convenience, since it enables SGD to be analyzed as a stochastic differential equation (SDE) driven by a Brownian motion. We argue that the Gaussianity assumption might fail to hold in deep learning settings and hence render the Brownian motion-based analyses inappropriate. Inspired by non-Gaussian natural phenomena, we consider the GN in a more general context and invoke the generalized CLT (GCLT), which suggests that the GN converges to a heavy-tailed {$\alpha$}-stable random variable. Accordingly, we propose to analyze SGD as an SDE driven by a Le\textasciiacute vy motion. Such SDEs can incur `jumps', which force the SDE transition from narrow minima to wider minima, as proven by existing metastability theory. To validate the {$\alpha$}stable assumption, we conduct experiments on common deep learning scenarios and show that in all settings, the GN is highly non-Gaussian and admits heavy-tails. We investigate the tail behavior in varying network architectures and sizes, loss functions, and datasets. Our results open up a different perspective and shed more light on the belief that SGD prefers wide minima.},
  langid = {english}
}

@article{singer_active_1982,
  title = {Active {{Comprehension}}: {{Problem-Solving Schema}} with {{Question Generation}} for {{Comprehension}} of {{Complex Short Stories}}},
  author = {Singer, Harry and Donlan, Dan},
  year = {1982},
  journal = {Reading Research Quarterly},
  volume = {17},
  number = {2},
  pages = {166},
  publisher = {{WileyInternational Literacy Association}},
  issn = {00340553},
  abstract = {A PROBLEM-SOLVING SCHEMA for comprehending short stories was augmented by construction of schema-general questions for each story element. Fifteen eleventh-grade students, randomly assigned to the experimental group, were taught to derive story-specific ques- tions from the schema-general questions as they read complex short stories. The control group read to answer questions posed before- hand by the teacher. Each group read six short stories over a three- week period. Criterion-referenced tests administered after each improve in reader-based processing of text and (2) that story gram- mar structures acquired prior to or during elementary school may be adequate for processing simple fables, but more adequate and more appropriate cognitive structures with strategies for making schema- general questions story-specific are necessary for processing, story resulted in statistically significant differences between the two groups. This evidence implies (1) that instruction can help students storing, and retrieving information derived from reading complex short stories.},
  isbn = {0034-0553}
}

@misc{singh_flava_2021,
  title = {{{FLAVA}}: {{A Foundational Language And Vision Alignment Model}}},
  shorttitle = {{{FLAVA}}},
  author = {Singh, Amanpreet and Hu, Ronghang and Goswami, Vedanuj and Couairon, Guillaume and Galuba, Wojciech and Rohrbach, Marcus and Kiela, Douwe},
  year = {2021},
  month = dec,
  eprint = {2112.04482},
  eprinttype = {arxiv},
  abstract = {State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety of downstream tasks. Generally, such models are often either cross-modal (contrastive) or multi-modal (with earlier fusion) but not both; and they often only target specific modalities or tasks. A promising direction would be to use a single holistic universal model, as a "foundation", that targets all modalities at once -- a true vision and language foundation model should be good at vision tasks, language tasks, and cross- and multi-modal vision and language tasks. We introduce FLAVA as such a model and demonstrate impressive performance on a wide range of 35 tasks spanning these target modalities.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition}
}

@misc{sinha_consistency_2021,
  title = {Consistency {{Regularization}} for {{Variational Auto-Encoders}}},
  author = {Sinha, Samarth and Dieng, Adji B.},
  year = {2021},
  month = may,
  eprint = {2105.14859},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Variational auto-encoders (VAEs) are a powerful approach to unsupervised learning. They enable scalable approximate posterior inference in latent-variable models using variational inference (VI). A VAE posits a variational family parameterized by a deep neural network called an encoder that takes data as input. This encoder is shared across all the observations, which amortizes the cost of inference. However the encoder of a VAE has the undesirable property that it maps a given observation and a semantics-preserving transformation of it to different latent representations. This "inconsistency" of the encoder lowers the quality of the learned representations, especially for downstream tasks, and also negatively affects generalization. In this paper, we propose a regularization method to enforce consistency in VAEs. The idea is to minimize the Kullback-Leibler (KL) divergence between the variational distribution when conditioning on the observation and the variational distribution when conditioning on a random semantic-preserving transformation of this observation. This regularization is applicable to any VAE. In our experiments we apply it to four different VAE variants on several benchmark datasets and found it always improves the quality of the learned representations but also leads to better generalization. In particular, when applied to the Nouveau Variational Auto-Encoder (NVAE), our regularization method yields state-of-the-art performance on MNIST and CIFAR-10. We also applied our method to 3D data and found it learns representations of superior quality as measured by accuracy on a downstream classification task.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@incollection{slaney_importance_1993,
  title = {On the Importance of Time- {{A}} Temporal Representatin of Sound},
  booktitle = {Visual {{Representations}} of {{Speech Signals}}},
  author = {Slaney, Malcolm and Lyon, Richard F.},
  year = {1993},
  eprint = {1011.1669v3},
  eprinttype = {arxiv},
  pages = {95--116},
  issn = {1098-6596},
  abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein-protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-{$\alpha$}-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD {$\leq$} 2.0 \AA{} for the interface backbone atoms) increased from 21\% with default Glide SP settings to 58\% with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63\% success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40\% of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
  archiveprefix = {arXiv},
  isbn = {978-85-7811-079-6},
  pmid = {25246403}
}

@article{slaney_learning_nodate,
  title = {Learning a {{Metric}} for {{Music Similarity}}},
  author = {Slaney, Malcolm and Weinberger, Kilian and White, William},
  pages = {6},
  abstract = {This paper describe five different principled ways to embed songs into a Euclidean metric space. In particular, we learn embeddings so that the pairwise Euclidean distance between two songs reflects semantic dissimilarity. This allows distance-based analysis, such as for example straightforward nearest-neighbor classification, to detect and potentially suggest similar songs within a collection. Each of the six approaches (baseline, whitening, LDA, NCA, LMNN and RCA) rotate and scale the raw feature space with a linear transform. We tune the parameters of these models using a song-classification task with content-based features.},
  langid = {english}
}

@inproceedings{snoek_can_2019,
  title = {Can You Trust Your Model's Uncertainty?  {{Evaluating}} Predictive Uncertainty under Dataset Shift},
  booktitle = {Proceedings of the 33rd {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Snoek, Jasper and Ovadia, Yaniv and Fertig, Emily and Lakshminarayanan, Balaji and Nowozin, Sebastian and Sculley, D and Dillon, Joshua and Ren, Jie and Nado, Zachary},
  year = {2019},
  pages = {12},
  address = {{Vancouver, Canada}},
  abstract = {Modern machine learning methods including deep learning have achieved great success in predictive accuracy for supervised learning tasks, but may still fall short in giving useful estimates of their predictive uncertainty. Quantifying uncertainty is especially critical in real-world settings, which often involve input distributions that are shifted from the training distribution due to a variety of factors including sample bias and non-stationarity. In such settings, well calibrated uncertainty estimates convey information about when a model's output should (or should not) be trusted. Many probabilistic deep learning methods, including Bayesian-and nonBayesian methods, have been proposed in the literature for quantifying predictive uncertainty, but to our knowledge there has not previously been a rigorous largescale empirical comparison of these methods under dataset shift. We present a largescale benchmark of existing state-of-the-art methods on classification problems and investigate the effect of dataset shift on accuracy and calibration. We find that traditional post-hoc calibration does indeed fall short, as do several other previous methods. However, some methods that marginalize over models give surprisingly strong results across a broad spectrum of tasks.},
  langid = {english}
}

@inproceedings{snyder_deep_2017,
  title = {Deep {{Neural Network Embeddings}} for {{Text-Independent Speaker Verification}}},
  booktitle = {Interspeech 2017},
  author = {Snyder, David and {Garcia-Romero}, Daniel and Povey, Daniel and Khudanpur, Sanjeev},
  year = {2017},
  month = aug,
  pages = {999--1003},
  publisher = {{ISCA}},
  abstract = {This paper investigates replacing i-vectors for text-independent speaker verification with embeddings extracted from a feedforward deep neural network. Long-term speaker characteristics are captured in the network by a temporal pooling layer that aggregates over the input speech. This enables the network to be trained to discriminate between speakers from variablelength speech segments. After training, utterances are mapped directly to fixed-dimensional speaker embeddings and pairs of embeddings are scored using a PLDA-based backend. We compare performance with a traditional i-vector baseline on NIST SRE 2010 and 2016. We find that the embeddings outperform i-vectors for short speech segments and are competitive on long duration test conditions. Moreover, the two representations are complementary, and their fusion improves on the baseline at all operating points. Similar systems have recently shown promising results when trained on very large proprietary datasets, but to the best of our knowledge, these are the best results reported for speaker-discriminative neural networks when trained and tested on publicly available corpora.},
  langid = {english}
}

@article{socher_recursive_2013,
  title = {Recursive Deep Models for Semantic Compositionality over a Sentiment Treebank},
  author = {Socher, Richard and Perelygin, Alex and Wu, Jy},
  year = {2013},
  journal = {Proceedings of the \ldots},
  eprint = {1512.03385},
  eprinttype = {arxiv},
  pages = {1631--1642},
  issn = {1932-6203},
  abstract = {Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80\% up to 85.4\%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7\%, an improvement of 9.7\% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.},
  archiveprefix = {arXiv},
  isbn = {9781937284978},
  pmid = {24086296}
}

@inproceedings{sohn_learning_2015,
  title = {Learning {{Structured Output Representation}} Using {{Deep Conditional Generative Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 28: {{Annual Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Sohn, Kihyuk and Lee, Honglak and Yan, Xinchen},
  year = {2015},
  pages = {9},
  address = {{Montreal, Quebec, Canada}},
  abstract = {Supervised deep learning has been successfully applied to many recognition problems. Although it can approximate a complex many-to-one function well when a large amount of training data is provided, it is still challenging to model complex structured output representations that effectively perform probabilistic inference and make diverse predictions. In this work, we develop a deep conditional generative model for structured output prediction using Gaussian latent variables. The model is trained efficiently in the framework of stochastic gradient variational Bayes, and allows for fast prediction using stochastic feed-forward inference. In addition, we provide novel strategies to build robust structured prediction algorithms, such as input noise-injection and multi-scale prediction objective at training. In experiments, we demonstrate the effectiveness of our proposed algorithm in comparison to the deterministic deep neural network counterparts in generating diverse but realistic structured output predictions using stochastic inference. Furthermore, the proposed training methods are complimentary, which leads to strong pixel-level object segmentation and semantic labeling performance on Caltech-UCSD Birds 200 and the subset of Labeled Faces in the Wild dataset.},
  langid = {english}
}

@inproceedings{sonderby_ladder_2016,
  title = {Ladder {{Variational Autoencoders}}},
  booktitle = {Proceedings of the 29th {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {S{\o}nderby, Casper Kaae and Raiko, Tapani and Maal{\o}e, Lars and S{\o}nderby, S{\o}ren Kaae and Winther, Ole},
  year = {2016},
  month = dec,
  address = {{Barcelona, Spain}},
  abstract = {Variational autoencoders are powerful models for unsupervised learning. However deep models with several layers of dependent stochastic variables are difficult to train which limits the improvements obtained using these highly expressive models. We propose a new inference model, the Ladder Variational Autoencoder, that recursively corrects the generative distribution by a data dependent approximate likelihood in a process resembling the recently proposed Ladder Network. We show that this model provides state of the art predictive log-likelihood and tighter log-likelihood lower bound compared to the purely bottom-up inference in layered Variational Autoencoders and other generative models. We provide a detailed analysis of the learned hierarchical latent representation and show that our new inference model is qualitatively different and utilizes a deeper more distributed hierarchy of latent variables. Finally, we observe that batch normalization and deterministic warm-up (gradually turning on the KL-term) are crucial for training variational models with many stochastic layers.},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{song_how_2021,
  title = {How to {{Train Your Energy-Based Models}}},
  author = {Song, Yang and Kingma, Diederik P.},
  year = {2021},
  month = feb,
  eprint = {2101.03288},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Energy-Based Models (EBMs), also known as non-normalized probabilistic models, specify probability density or mass functions up to an unknown normalizing constant. Unlike most other probabilistic models, EBMs do not place a restriction on the tractability of the normalizing constant, thus are more flexible to parameterize and can model a more expressive family of probability distributions. However, the unknown normalizing constant of EBMs makes training particularly difficult. Our goal is to provide a friendly introduction to modern approaches for EBM training. We start by explaining maximum likelihood training with Markov chain Monte Carlo (MCMC), and proceed to elaborate on MCMC-free approaches, including Score Matching (SM) and Noise Constrastive Estimation (NCE). We highlight theoretical connections among these three approaches, and end with a brief survey on alternative training methods, which are still under active research. Our tutorial is targeted at an audience with basic understanding of generative models who want to apply EBMs or start a research project in this direction.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{song_nonlinear_2020,
  title = {Nonlinear {{Equation Solving}}: {{A Faster Alternative}} to {{Feedforward Computation}}},
  shorttitle = {Nonlinear {{Equation Solving}}},
  author = {Song, Yang and Meng, Chenlin and Liao, Renjie and Ermon, Stefano},
  year = {2020},
  month = feb,
  eprint = {2002.03629},
  eprinttype = {arxiv},
  abstract = {Feedforward computations, such as evaluating a neural network or sampling from an autoregressive model, are ubiquitous in machine learning. The sequential nature of feedforward computation, however, requires a strict order of execution and cannot be easily accelerated with parallel computing. To enable parrallelization, we frame the task of feedforward computation as solving a system of nonlinear equations. We then propose to find the solution using a Jacobi or Gauss-Seidel fixed-point iteration method, as well as hybrid methods of both. Crucially, Jacobi updates operate independently on each equation and can be executed in parallel. Our method is guaranteed to give exactly the same values as the original feedforward computation with a reduced (or equal) number of parallel iterations. Experimentally, we demonstrate the effectiveness of our approach in accelerating 1) the evaluation of DenseNets on ImageNet and 2) autoregressive sampling of MADE and PixelCNN. We are able to achieve between 1.2 and 33 speedup factors under various conditions and computation models.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{song_speech-xlnet_2020,
  title = {Speech-{{XLNet}}: {{Unsupervised Acoustic Model Pretraining}} for {{Self-Attention Networks}}},
  shorttitle = {Speech-{{XLNet}}},
  booktitle = {Interspeech 2020},
  author = {Song, Xingchen and Wang, Guangsen and Huang, Yiheng and Wu, Zhiyong and Su, Dan and Meng, Helen},
  year = {2020},
  month = oct,
  pages = {3765--3769},
  publisher = {{ISCA}},
  langid = {english}
}

@inproceedings{sordoni_neural_2015,
  title = {A {{Neural Network Approach}} to {{Context-Sensitive Generation}} of {{Conversational Responses}}},
  booktitle = {Proceedings of the {{Annual Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}} ({{NAACL-HLT}})},
  author = {Sordoni, Alessandro and Galley, Michel and Auli, Michael and Brockett, Chris and Ji, Yangfeng and Mitchell, Margaret and Nie, Jian-Yun and Gao, Jianfeng and Dolan, Bill},
  year = {2015},
  month = jun,
  eprint = {1506.06714},
  eprinttype = {arxiv},
  abstract = {We present a novel response generation system that can be trained end to end on large quantities of unstructured Twitter conversations. A neural network architecture is used to address sparsity issues that arise when integrating contextual information into classic statistical models, allowing the system to take into account previous dialog utterances. Our dynamic-context generative models show consistent gains over both context-sensitive and non-context-sensitive Machine Translation and Information Retrieval baselines.},
  archiveprefix = {arXiv},
  isbn = {978-1-941643-49-5}
}

@misc{sorokin_deep_2015,
  title = {Deep {{Attention Recurrent Q-Network}}},
  author = {Sorokin, Ivan and Seleznev, Alexey and Pavlov, Mikhail and Fedorov, Aleksandr and Ignateva, Anastasiia},
  year = {2015},
  month = dec,
  eprint = {1512.01693},
  eprinttype = {arxiv},
  abstract = {A deep learning approach to reinforcement learning led to a general learner able to train on visual input to play a variety of arcade games at the human and superhuman levels. Its creators at the Google DeepMind's team called the approach: Deep Q-Network (DQN). We present an extension of DQN by "soft" and "hard" attention mechanisms. Tests of the proposed Deep Attention Recurrent Q-Network (DARQN) algorithm on multiple Atari 2600 games show level of performance superior to that of DQN. Moreover, built-in attention mechanisms allow a direct online monitoring of the training process by highlighting the regions of the game screen the agent is focusing on when making decisions.},
  archiveprefix = {arXiv}
}

@article{soudry_implicit_2018,
  title = {The {{Implicit Bias}} of {{Gradient Descent}} on {{Separable Data}}},
  author = {Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  year = {2018},
  month = mar,
  abstract = {We show that gradient descent on an unregularized logistic regression problem, for linearly separable datasets, converges to the direction of the max-margin (hard margin SVM) solution. The result generalizes also to other monotone decreasing loss functions with an infimum at infinity, to multi-class problems, and to training a weight layer in a deep network in a certain restricted setting. Furthermore, we show this convergence is very slow, and only logarithmic in the convergence of the loss itself. This can help explain the benefit of continuing to optimize the logistic or cross-entropy loss even after the training error is zero and the training loss is extremely small, and, as we show, even if the validation loss increases. Our methodology can also aid in understanding implicit regularization in more complex models and with other optimization methods.},
  langid = {english}
}

@article{spall_multivariate_1992,
  title = {Multivariate {{Stochastic Approximation Using}} a {{Simultaneous Perturbation Gradient Approximation}}},
  author = {Spall, James C.},
  year = {1992},
  journal = {IEEE Transactions on Automatic Control},
  volume = {37},
  number = {3},
  pages = {332--341},
  issn = {15582523},
  abstract = {The problem of finding a root of the multivariate gradient equation that arises in function minimization is considered. When only noisy measurements of the function are available, a stochastic approximation (SA) algorithm for the general Kiefer-Wolfowitz type is appropriate for estimating the root. The paper presents an SA algorithm that is based on a simultaneous perturbation gradient approximation instead of the standard finite-difference approximation of Keifer-Wolfowitz type procedures. Theory and numerical experience indicate that the algorithm can be significantly more efficient than the standard algorithms in large-dimensional problems.},
  keywords = {Acceleration,Adaptive control,Approximation algorithms,Convergence,Design for experiments,Differential equations,Finite difference methods,Function approximation,Kiefer-Wolfowitz type,Neural network,Noisy measurements,Q measurement,Simultaneous perturbation gradient approximation,Stochastic approximation,Stochastic processes}
}

@article{srivastava_dropout_2014,
  title = {Dropout: {{A Simple Way}} to {{Prevent Neural Networks}} from {{Overfitting}}},
  author = {Srivastava, Nitish and Hinton, Geoffrey E. and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
  year = {2014},
  journal = {Journal of Machine Learning Research},
  volume = {15},
  pages = {1929--1958},
  abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different " thinned " networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
  keywords = {Deep learning,Model combination,Neural network,Regularization}
}

@misc{staines_variational_2012,
  title = {Variational {{Optimization}}},
  author = {Staines, Joe and Barber, David},
  year = {2012},
  month = dec,
  eprint = {1212.4507},
  eprinttype = {arxiv},
  abstract = {We discuss a general technique that can be used to form a differentiable bound on the optima of non-differentiable or discrete objective functions. We form a unified description of these methods and consider under which circumstances the bound is concave. In particular we consider two concrete applications of the method, namely sparse learning and support vector classification.},
  archiveprefix = {arXiv},
  keywords = {★}
}

@article{stanley_hypercube-based_2009,
  title = {A {{Hypercube-Based Indirect Encoding}} for {{Evolving Large-Scale Neural Networks}}},
  author = {Stanley, Kenneth O. and D 'ambrosio, David and Gauci, Jason},
  year = {2009},
  journal = {Artificial Life Journal},
  volume = {15},
  number = {2},
  pages = {1--28},
  issn = {1064-5462},
  abstract = {Research in neuroevolution, i.e. evolving artificial neural networks (ANNs) through evolutionary algo-rithms, is inspired by the evolution of biological brains. Because natural evolution discovered intelligent brains with billions of neurons and trillions of connections, perhaps neuroevolution can do the same. Yet while neuroevolution has produced successful results in a variety of domains, the scale of natural brains remains far beyond reach. This paper presents a method called Hypercube-based NeuroEvolution of Aug-menting Topologies (HyperNEAT) that aims to narrow this gap. HyperNEAT employs an indirect encoding called connective Compositional Pattern Producing Networks (connective CPPNs) that can produce con-nectivity patterns with symmetries and repeating motifs by interpreting spatial patterns generated within a hypercube as connectivity patterns in a lower-dimensional space. The advantage of this approach is that it can exploit the geometry of the task by mapping its regularities onto the topology of the network, thereby shifting problem difficulty away from dimensionality to underlying problem structure. Furthermore, con-nective CPPNs can represent the same connectivity pattern at any resolution, allowing ANNs to scale to new numbers of inputs and outputs without further evolution. HyperNEAT is demonstrated through visual dis-crimination and food gathering tasks, including successful visual discrimination networks containing over eight million connections. The main conclusion is that the ability to explore the space of regular connectivity patterns opens up a new class of complex high-dimensional tasks to neuroevolution.},
  pmid = {19199382},
  keywords = {Compositional Pattern Producing Networks,CPPNs,HyperNEAT,Indirect encoding}
}

@misc{strauss_any_2022,
  title = {Any {{Variational Autoencoder Can Do Arbitrary Conditioning}}},
  author = {Strauss, Ryan R. and Oliva, Junier B.},
  year = {2022},
  month = jan,
  eprint = {2201.12414},
  eprinttype = {arxiv},
  abstract = {Arbitrary conditioning is an important problem in unsupervised learning, where we seek to model the conditional densities p(xu | xo) that underly some data, for all possible non-intersecting subsets o, u {$\subset$} \{1, . . . , d\}. However, the vast majority of density estimation only focuses on modeling the joint distribution p(x), in which important conditional dependencies between features are opaque. We propose a simple and general framework, coined Posterior Matching, that enables any Variational Autoencoder (VAE) to perform arbitrary conditioning, without modification to the VAE itself. Posterior Matching applies to the numerous existing VAE-based approaches to joint density estimation, thereby circumventing the specialized models required by previous approaches to arbitrary conditioning. We find that Posterior Matching achieves performance that is comparable or superior to current state-of-the-art methods for a variety of tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@techreport{su_differential_2015,
  title = {A {{Differential Equation}} for {{Modeling Nesterov}}'s {{Accelerated Gradient Method}}: {{Theory}} and {{Insights}}},
  author = {Su, Weijie and Boyd, Stephen and Cand{\`e}s, Emmanuel J},
  year = {2015},
  eprint = {1503.01243v2},
  eprinttype = {arxiv},
  abstract = {We derive a second-order ordinary differential equation (ODE) which is the limit of Nes-terov's accelerated gradient method. This ODE exhibits approximate equivalence to Nes-terov's scheme and thus can serve as a tool for analysis. We show that the continuous time ODE allows for a better understanding of Nesterov's scheme. As a byproduct, we obtain a family of schemes with similar convergence rates. The ODE interpretation also suggests restarting Nesterov's scheme leading to an algorithm, which can be rigorously proven to converge at a linear rate whenever the objective is strongly convex.},
  archiveprefix = {arXiv},
  keywords = {Convex optimization,Differential equations,First-order methods,Nesterov's accelerated momentum,Restarting}
}

@misc{subbaswamy_preventing_2019,
  title = {Preventing {{Failures Due}} to {{Dataset Shift}}: {{Learning Predictive Models That Transport}}},
  shorttitle = {Preventing {{Failures Due}} to {{Dataset Shift}}},
  author = {Subbaswamy, Adarsh and Schulam, Peter and Saria, Suchi},
  year = {2019},
  month = feb,
  eprint = {1812.04597},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Classical supervised learning produces unreliable models when training and target distributions differ, with most existing solutions requiring samples from the target domain. We propose a proactive approach which learns a relationship in the training domain that will generalize to the target domain by incorporating prior knowledge of aspects of the data generating process that are expected to differ as expressed in a causal selection diagram. Specifically, we remove variables generated by unstable mechanisms from the joint factorization to yield the Surgery Estimator\textemdash an interventional distribution that is invariant to the differences across environments. We prove that the surgery estimator finds stable relationships in strictly more scenarios than previous approaches which only consider conditional relationships, and demonstrate this in simulated experiments. We also evaluate on real world data for which the true causal diagram is unknown, performing competitively against entirely data-driven approaches.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{such_deep_2017,
  title = {Deep {{Neuroevolution}}: {{Genetic Algorithms Are}} a {{Competitive Alternative}} for {{Training Deep Neural Networks}} for {{Reinforcement Learning}}},
  author = {Such, Felipe Petroski and Madhavan, Vashisht and Conti, Edoardo and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
  year = {2017},
  month = dec,
  eprint = {1712.06567},
  eprinttype = {arxiv},
  abstract = {Deep artificial neural networks (DNNs) are typically trained via gradient-based learning algorithms, namely backpropagation. Evolution strategies (ES) can rival backprop-based algorithms such as Q-learning and policy gradients on challenging deep reinforcement learning (RL) problems. However, ES can be considered a gradient-based algorithm because it performs stochastic gradient descent via an operation similar to a finite-difference approximation of the gradient. That raises the question of whether non-gradient-based evolutionary algorithms can work at DNN scales. Here we demonstrate they can: we evolve the weights of a DNN with a simple, gradient-free, population-based genetic algorithm (GA) and it performs well on hard deep RL problems, including Atari and humanoid locomotion. The Deep GA successfully evolves networks with over four million free parameters, the largest neural networks ever evolved with a traditional evolutionary algorithm. These results (1) expand our sense of the scale at which GAs can operate, (2) suggest intriguingly that in some cases following the gradient is not the best choice for optimizing performance, and (3) make immediately available the multitude of neuroevolution techniques that improve performance. We demonstrate the latter by showing that combining DNNs with novelty search, which encourages exploration on tasks with deceptive or sparse reward functions, can solve a high-dimensional problem on which reward-maximizing algorithms (e.g.\textbackslash{} DQN, A3C, ES, and the GA) fail. Additionally, the Deep GA is faster than ES, A3C, and DQN (it can train Atari in \$\{\textbackslash raise.17ex\textbackslash hbox\{\$\textbackslash scriptstyle\textbackslash sim\$\}\}\$4 hours on one desktop or \$\{\textbackslash raise.17ex\textbackslash hbox\{\$\textbackslash scriptstyle\textbackslash sim\$\}\}\$1 hour distributed on 720 cores), and enables a state-of-the-art, up to 10,000-fold compact encoding technique.},
  archiveprefix = {arXiv}
}

@article{sugiyama_dimensionality_2010,
  title = {Dimensionality {{Reduction}} for {{Density Ratio Estimation}} in {{High-dimensional Spaces}}},
  author = {Sugiyama, Masashi and Kawanabe, Motoaki and Chui, Pui Ling},
  year = {2010},
  journal = {Proceedings of the International Joint Conference on Neural Networks (IJCNN)},
  volume = {23},
  number = {1},
  pages = {44--59},
  issn = {08936080},
  abstract = {The ratio of two probability density functions is becoming a quantity of interest these days in the machine learning and data mining communities since it can be used for various data processing tasks such as non-stationarity adaptation, outlier detection, and feature selection. Recently, several methods have been developed for directly estimating the density ratio without going through density estimation and were shown to work well in various practical problems. However, these methods still perform rather poorly when the dimensionality of the data domain is high. In this paper, we propose to incorporate a dimensionality reduction scheme into a density-ratio estimation procedure and experimentally show that the estimation accuracy in high-dimensional cases can be improved. \textcopyright{} 2009 Elsevier Ltd. All rights reserved.},
  pmid = {19631506},
  keywords = {Density ratio estimation,Dimensionality reduction,Local Fisher discriminant analysis,Unconstrained least-squares importance fitting}
}

@inproceedings{sukhbaatar_end--end_2015,
  title = {End-{{To-End Memory Networks}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Neural Information Processing Systems}} ({{NIPS}})},
  author = {Sukhbaatar, Sainbayar and Szlam, Arthur and Weston, Jason and Fergus, Rob},
  year = {2015},
  month = mar,
  eprint = {1503.08895},
  eprinttype = {arxiv},
  issn = {10495258},
  abstract = {We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network (Weston et al., 2015) but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.},
  archiveprefix = {arXiv},
  isbn = {1551-6709},
  pmid = {9377276},
  keywords = {Memory networks,Natural Language Processing,Question answering}
}

@inproceedings{sun_efficient_2009,
  title = {Efficient {{Natural Evolution Strategies}}},
  booktitle = {Proceedings of the {{Conference}} on {{Genetic}} and {{Evolutionary Computation}} ({{GECCO}})},
  author = {Sun, Yi and Wierstra, Daan and Schaul, Tom and Schmidhuber, J{\"u}rgen},
  year = {2009},
  eprint = {1209.5853v1},
  eprinttype = {arxiv},
  pages = {539},
  abstract = {Efficient Natural Evolution Strategies (eNES) is a novel alternative to conventional evolutionary algorithms, using the natural gradient to adapt the mutation distribution. Unlike previous methods based on natural gradients, eNES uses a fast algorithm to calculate the inverse of the exact Fisher information matrix, thus increasing both robustness and performance of its evolution gradient estimation, even in higher dimensions. Additional novel aspects of eNES include optimal fitness baselines and importance mixing (a procedure for updating the population with very few fitness evaluations). The algorithm yields competitive results on both unimodal and multimodal benchmarks.},
  archiveprefix = {arXiv},
  isbn = {978-1-60558-325-9},
  keywords = {Control Methods,Heuristic methods,Natural gradient,Optimization}
}

@misc{sun_training_2018,
  title = {Training {{Augmentation}} with {{Adversarial Examples}} for {{Robust Speech Recognition}}},
  author = {Sun, Sining and Yeh, Ching-Feng and Ostendorf, Mari and Hwang, Mei-Yuh and Xie, Lei},
  year = {2018},
  month = jun,
  eprint = {1806.02782},
  eprinttype = {arxiv},
  abstract = {This paper explores the use of adversarial examples in training speech recognition systems to increase robustness of deep neural network acoustic models. During training, the fast gradient sign method is used to generate adversarial examples augmenting the original training data. Different from conventional data augmentation based on data transformations, the examples are dynamically generated based on current acoustic model parameters. We assess the impact of adversarial data augmentation in experiments on the Aurora-4 and CHiME-4 single-channel tasks, showing improved robustness against noise and channel variation. Further improvement is obtained when combining adversarial examples with teacher/student training, leading to a 23\% relative word error rate reduction on Aurora-4.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning}
}

@inproceedings{suri_counting_2011,
  title = {Counting Triangles and the Curse of the Last Reducer},
  booktitle = {Proceedings of the 20th International Conference on {{World}} Wide Web - {{WWW}} '11},
  author = {Suri, Siddharth and Vassilvitskii, Sergei},
  year = {2011},
  pages = {607},
  publisher = {{ACM Press}},
  address = {{New York, New York, USA}},
  isbn = {978-1-4503-0632-4}
}

@inproceedings{sutskever_importance_2013,
  title = {On the {{Importance}} of {{Initialization}} and {{Momentum}} in {{Deep Learning}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Sutskever, Ilya and Martens, James and Dahl, George E. and Hinton, Geoffrey E.},
  year = {2013},
  volume = {28},
  pages = {8609--8613},
  abstract = {Recently, pre-trained deep neural networks (DNNs) have outperformed traditional acoustic models based on Gaussian mixture models (GMMs) on a variety of large vocabulary speech recognition benchmarks. Deep neural nets have also achieved excellent results on various computer vision tasks using a random ``dropout'' procedure that drastically improves generalization error by randomly omitting a fraction of the hidden units in all layers. Since dropout helps avoid overfitting, it has also been successful on a small-scale phone recognition task using larger neural nets. However, training deep neural net acoustic models for large vocabulary speech recognition takes a very long time and dropout is likely to only increase training time. Neural networks with rectified linear unit (ReLU) non-linearities have been highly successful for computer vision tasks and proved faster to train than standard sigmoid units, sometimes also improving discriminative performance. In this work, we show on a 50-hour English Broadcast News task that modified deep neural networks using ReLUs trained with dropout during frame level training provide an 4.2\% relative improvement over a DNN trained with sigmoid units, and a 14.4\% relative improvement over a strong GMM/HMM system. We were able to obtain our results with minimal human hyper-parameter tuning using publicly available Bayesian optimization code},
  keywords = {Acoustic modeling,Bayesian optimization,broadcast news,Deep learning,dropout,LVCSR,Neural network,Rectified linear unit}
}

@inproceedings{sutskever_sequence_2014,
  title = {Sequence to Sequence Learning with Neural Networks},
  booktitle = {Proceedings of the {{Conference}} on {{Neural Information Processing Systems}} ({{NIPS}})},
  author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
  year = {2014},
  eprint = {1409.3215v3},
  eprinttype = {arxiv},
  pages = {3104--3112},
  abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excel-lent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the pas-sive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
  archiveprefix = {arXiv}
}

@misc{sutskever_towards_2015,
  title = {Towards {{Principled Unsupervised Learning}}},
  author = {Sutskever, Ilya and Jozefowicz, Rafal and Gregor, Karol and Rezende, Danilo and Lillicrap, Tim and Vinyals, Oriol},
  year = {2015},
  month = dec,
  eprint = {1511.06440},
  eprinttype = {arxiv},
  abstract = {General unsupervised learning is a long-standing conceptual problem in machine learning. Supervised learning is successful because it can be solved by the minimization of the training error cost function. Unsupervised learning is not as successful, because the unsupervised objective may be unrelated to the supervised task of interest. For an example, density modelling and reconstruction have often been used for unsupervised learning, but they did not produced the sought-after performance gains, because they have no knowledge of the supervised tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning}
}

@misc{sutter_generalized_2021,
  title = {Generalized {{Multimodal ELBO}}},
  author = {Sutter, Thomas M. and Daunhawer, Imant and Vogt, Julia E.},
  year = {2021},
  month = may,
  eprint = {2105.02470},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Multiple data types naturally co-occur when describing real-world phenomena and learning from them is a long-standing goal in machine learning research. However, existing self-supervised generative models approximating an ELBO are not able to fulfill all desired requirements of multimodal models: their posterior approximation functions lead to a trade-off between the semantic coherence and the ability to learn the joint data distribution. We propose a new, generalized ELBO formulation for multimodal data that overcomes these limitations. The new objective encompasses two previous methods as special cases and combines their benefits without compromises. In extensive experiments, we demonstrate the advantage of the proposed method compared to state-of-the-art models in selfsupervised, generative learning tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{sutton_learning_1988,
  title = {Learning to {{Predict}} by the {{Method}} of {{Temporal Differences}}},
  author = {Sutton, Richard S.},
  year = {1988},
  journal = {Journal of Machine Learning},
  volume = {3},
  number = {1},
  pages = {9--44},
  issn = {08856125},
  abstract = {This article introduces a class of incremental learning procedures specialized for prediction--that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the differnce between \{\textbackslash em temporally succesive predictions\}. Although such \{\textbackslash em temporal-difference methods\} have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods; \{\textbackslash em and\} they produce more accurate predictions. We argue that most problems to which uspervised learning is currently applied are really prediction problems of the sort to which temporal-difference methods can be applied to advantage.},
  isbn = {0885-6125},
  pmid = {22182453},
  keywords = {Connectionism,Credit assignment,Incremental learning,Prediction}
}

@book{sutton_reinforcement_1998,
  title = {Reinforcement {{Learning}}: {{An Introduction}}},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {1998},
  issn = {1600-0579},
  abstract = {Reinforcement learning, one of the most active research areas in artificialintelligence, is a computational approach to learning whereby an agent tries to maximize the totalamount of reward it receives when interacting with a complex, uncertain environment. InReinforcement Learning, Richard Sutton and Andrew Barto provide a clear andsimple account of the key ideas and algorithms of reinforcement learning. Their discussion rangesfrom the history of the field's intellectual foundations to the most recent developments andapplications. The only necessary mathematical background is familiarity with elementary concepts ofprobability.},
  isbn = {978-0-262-19398-6}
}

@book{sutton_reinforcement_2018,
  title = {Reinforcement {{Learning}} - {{An Introduction}}},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {2018},
  series = {Adaptive {{Computation}} and {{Machine Learning}}},
  edition = {Second},
  publisher = {{MIT Press}}
}

@misc{suzuki_information_2014,
  title = {Information {{Geometry}} and {{Statistical Manifold}}},
  author = {Suzuki, Mashbat},
  year = {2014},
  month = oct,
  eprint = {1410.3369},
  eprinttype = {arxiv},
  abstract = {We review basic notions in the field of information geometry such as Fisher metric on statistical manifold, \$\textbackslash alpha\$-connection and corresponding curvature following Amari's work . We show application of information geometry to asymptotic statistical inference.},
  archiveprefix = {arXiv},
  keywords = {Adaboost,Exponential family,Logistic model,Maximum likelihood}
}

@inproceedings{szegedy_going_2015,
  title = {Going Deeper with Convolutions},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CCVPR}})},
  author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  year = {2015},
  pages = {1--9}
}

@misc{szegedy_inception-v4_2016,
  title = {Inception-v4, Inception-Resnet and the Impact of Residual Connections on Learning},
  author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alex},
  year = {2016},
  eprint = {1602.07261},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@misc{szegedy_intriguing_2013,
  title = {Intriguing Properties of Neural Networks},
  author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian J. and Fergus, Rob},
  year = {2013},
  month = dec,
  eprint = {1312.6199},
  eprinttype = {arxiv},
  abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
  archiveprefix = {arXiv},
  isbn = {1549-9618},
  pmid = {22545027}
}

@article{szegedy_rethinking_2015,
  title = {Rethinking the {{Inception Architecture}} for {{Computer Vision}}},
  author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
  year = {2015},
  journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CCVPR)},
  eprint = {1512.00567},
  eprinttype = {arxiv},
  pages = {2818--2826},
  issn = {08866236},
  abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2\% top-1 and 5.6\% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5\% top-5 error on the validation set (3.6\% error on the test set) and 17.3\% top-1 error on the validation set.},
  archiveprefix = {arXiv},
  isbn = {978-1-4673-8851-1},
  pmid = {8190083}
}

@article{tagliasacchi_pre-training_2020,
  title = {Pre-{{Training Audio Representations With Self-Supervision}}},
  author = {Tagliasacchi, Marco and Gfeller, Beat and Quitry, F{\'e}lix de Chaumont and Roblek, Dominik},
  year = {2020},
  journal = {IEEE Signal Processing Letters},
  volume = {27},
  pages = {600--604},
  issn = {1558-2361},
  abstract = {We explore self-supervision as a way to learn general purpose audio representations. Specifically, we propose two self-supervised tasks: Audio2Vec, which aims at reconstructing a spectrogram slice from past and future slices and TemporalGap, which estimates the distance between two short audio segments extracted at random from the same audio clip. We evaluate how the representations learned via self-supervision transfer to different downstream tasks, either training a task-specific linear classifier on top of the pretrained embeddings, or fine-tuning a model end-to-end for each downstream task. Our results show that the representations learned with Audio2Vec transfer better than those learned by fully-supervised training on Audioset. In addition, by fine-tuning Audio2Vec representations it is possible to outperform fully-supervised models trained from scratch on each task, when limited data is available, thus improving label efficiency.},
  keywords = {audio processing,Computer architecture,Decoding,Predictive models,Self-supervised learning,Spectrogram,Task analysis,Time-frequency analysis,Training}
}

@inproceedings{taigman_deepface_2014,
  title = {Deepface: {{Closing}} the Gap to Human-Level Performance in Face Verification},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CCVPR}})},
  author = {Taigman, Yaniv and Yang, Ming and Ranzato, Marc'Aurelio and Wolf, Lior},
  year = {2014},
  pages = {1701--1708}
}

@misc{tan_survey_2021,
  title = {A {{Survey}} on {{Neural Speech Synthesis}}},
  author = {Tan, Xu and Qin, Tao and Soong, Frank and Liu, Tie-Yan},
  year = {2021},
  month = jun,
  eprint = {2106.15561},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {Text to speech (TTS), or speech synthesis, which aims to synthesize intelligible and natural speech given text, is a hot research topic in speech, language, and machine learning communities and has broad applications in the industry. As the development of deep learning and artificial intelligence, neural network-based TTS has significantly improved the quality of synthesized speech in recent years. In this paper, we conduct a comprehensive survey on neural TTS, aiming to provide a good understanding of current research and future trends. We focus on the key components in neural TTS, including text analysis, acoustic models, and vocoders, and several advanced topics, including fast TTS, low-resource TTS, robust TTS, expressive TTS, and adaptive TTS, etc. We further summarize resources related to TTS (e.g., datasets, opensource implementations) and discuss future research directions. This survey can serve both academic researchers and industry practitioners working on TTS.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Multimedia,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@article{tarvainen_mean_2017,
  title = {Mean Teachers Are Better Role Models: {{Weight-averaged}} Consistency Targets Improve Semi-Supervised Deep Learning Results},
  shorttitle = {Mean Teachers Are Better Role Models},
  author = {Tarvainen, Antti and Valpola, Harri},
  year = {2017},
  month = mar,
  langid = {english},
  keywords = {Ensemble methods,Polyak averaging}
}

@article{tascikaraoglu_review_2014,
  title = {A Review of Combined Approaches for Prediction of Short-Term Wind Speed and Power},
  author = {Tascikaraoglu, A. and Uzunoglu, M.},
  year = {2014},
  journal = {Renewable and Sustainable Energy Reviews},
  volume = {34},
  pages = {243--254},
  abstract = {With the continuous increase of wind power penetration in power systems, the problems caused by the volatile nature of wind speed and its occurrence in the system operations such as scheduling and dispatching have drawn attention of system operators, utilities and researchers towards the state-of-the-art wind speed and power forecasting methods. These methods have the required capability of reducing the influence of the intermittent wind power on system operations as well as of harvesting the wind energy effectively. In this context, combining different methodologies in order to circumvent the challenging model selection and take advantage of the unique strength of plausible models have recently emerged as a promising research area. Therefore, a comprehensive research about the combined models is called on for how these models are constructed and affect the forecasting performance. Aiming to fill the mentioned research gap, this paper outlines the combined forecasting approaches and presents an up-to date annotated bibliography of the wind forecasting literature. Furthermore, the paper also points out the possible further research directions of combined techniques so as to help the researchers in the field develop more effective wind speed and power forecasting methods.}
}

@inproceedings{theis_note_2016,
  title = {A Note on the Evaluation of Generative Models},
  booktitle = {Proceedings of the 4th {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Theis, Lucas and van den Oord, A{\"a}ron and Bethge, Matthias},
  year = {2016},
  month = may,
  address = {{San Juan, Puerto Rico}},
  abstract = {Probabilistic generative models can be used for compression, denoising, inpainting, texture synthesis, semi-supervised learning, unsupervised feature learning, and other tasks. Given this wide range of applications, it is not surprising that a lot of heterogeneity exists in the way these models are formulated, trained, and evaluated. As a consequence, direct comparison between models is often difficult. This article reviews mostly known but often underappreciated properties relating to the evaluation and interpretation of generative models with a focus on image models. In particular, we show that three of the currently most commonly used criteria\textemdash average log-likelihood, Parzen window estimates, and visual fidelity of samples\textemdash are largely independent of each other when the data is high-dimensional. Good performance with respect to one criterion therefore need not imply good performance with respect to the other criteria. Our results show that extrapolation from one criterion to another is not warranted and generative models need to be evaluated directly with respect to the application(s) they were intended for. In addition, we provide examples demonstrating that Parzen window estimates should generally be avoided.},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{thorpe_introduction_nodate,
  title = {Introduction to {{Optimal Transport}}},
  author = {Thorpe, Matthew},
  pages = {56},
  langid = {english}
}

@article{tishby_consistent_1988,
  title = {Consistent {{Inference}} of {{Probabilities}} in {{Layered Networks}}: {{Predictions}} and {{Generalization}}},
  author = {Tishby, Naftali and Levin, Esther and Solla, Sara A.},
  year = {1988},
  abstract = {The problem of learning a general input-output relation using a " layered neural network " is discussed in a statistical framework. By imposing the consistency condition that the error minimization be equivalent to a likelihood maximization for training the network, we anive at a Gibbs distribution on a canonical ensemble of networks with the same architecture. This statistical description enables us to evaluate the probability of a correct prediction of an independent example, after training the network on a given training set. The prediction probability is highly correlated with the generalization ability of the network, as measured outside the mining set. This suggests a general and practical criterion for training layered networks by minimizing prediction errors. We demonstrate the utility of this criterion for selecting the optimal architecture in the contiguity problem. As a theoretical application of the statistical formalism, we discuss the question of learning curves and estimate the sufficient training size needed for correct generalization, in a simple example.}
}

@article{tishby_deep_2015,
  title = {Deep {{Learning}} and the {{Information Bottleneck Principle}}},
  author = {Tishby, Naftali and Zaslavsky, Noga},
  year = {2015},
  month = mar,
  langid = {english}
}

@misc{tolstikhin_mlp-mixer_2021,
  title = {{{MLP-Mixer}}: {{An}} All-{{MLP Architecture}} for {{Vision}}},
  shorttitle = {{{MLP-Mixer}}},
  author = {Tolstikhin, Ilya and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Keysers, Daniel and Uszkoreit, Jakob and Lucic, Mario and Dosovitskiy, Alexey},
  year = {2021},
  month = may,
  eprint = {2105.01601},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Convolutional Neural Networks (CNNs) are the go-to model for computer vision. Recently, attention-based networks, such as the Vision Transformer, have also become popular. In this paper we show that while convolutions and attention are both sufficient for good performance, neither of them are necessary. We present MLP-Mixer, an architecture based exclusively on multi-layer perceptrons (MLPs). MLP-Mixer contains two types of layers: one with MLPs applied independently to image patches (i.e. "mixing" the per-location features), and one with MLPs applied across patches (i.e. "mixing" spatial information). When trained on large datasets, or with modern regularization schemes, MLP-Mixer attains competitive scores on image classification benchmarks, with pre-training and inference cost comparable to state-of-the-art models. We hope that these results spark further research beyond the realms of well established CNNs and Transformers.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@misc{tolstikhin_wasserstein_2019,
  title = {Wasserstein {{Auto-Encoders}}},
  author = {Tolstikhin, Ilya and Bousquet, Olivier and Gelly, Sylvain and Schoelkopf, Bernhard},
  year = {2019},
  month = dec,
  eprint = {1711.01558},
  eprinttype = {arxiv},
  abstract = {We propose the Wasserstein Auto-Encoder (WAE)\textemdash a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE) [1]. This regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE) [2]. Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality, as measured by the FID score.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@incollection{toolenaar_features_2004,
  title = {Features for {{Audio Classification}}},
  booktitle = {Algorithms in {{Ambient Intelligence}}},
  author = {Breebaart, Jeroen and McKinney, Martin F.},
  editor = {Toolenaar, Frank and Verhaegh, Wim F. J. and Aarts, Emile and Korst, Jan},
  year = {2004},
  volume = {2},
  pages = {113--129},
  publisher = {{Springer Netherlands}},
  address = {{Dordrecht}},
  abstract = {Four audio feature sets are evaluated in their ability to differentiate five audio classes: popular music, classical music, speech, noise and crowd noise. The feature sets include low-level signal properties, mel-frequency spectral coefficients, and two new sets based on perceptual models of hearing. The temporal behavior of the features is analyzed and parameterized and these parameters are included as additional features. Using a standard Gaussian framework for classification, results show that the temporal behavior of features is important for automatic audio classification. In addition, classification is better, on average, if based on features from models of auditory perception rather than on standard features.},
  isbn = {978-90-481-6490-5 978-94-017-0703-9},
  langid = {english}
}

@inproceedings{toshev_deeppose_2014,
  title = {Deeppose: {{Human}} Pose Estimation via Deep Neural Networks},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CCVPR}})},
  author = {Toshev, Alexander and Szegedy, Christian},
  year = {2014},
  pages = {1653--1660}
}

@misc{townsend_lossless_2021,
  title = {Lossless {{Compression}} with {{Latent Variable Models}}},
  author = {Townsend, James},
  year = {2021},
  month = apr,
  eprint = {2104.10544},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {We develop a simple and elegant method for lossless compression using latent variable models, which we call 'bits back with asymmetric numeral systems' (BB-ANS). The method involves interleaving encode and decode steps, and achieves an optimal rate when compressing batches of data. We demonstrate it firstly on the MNIST test set, showing that state-of-the-art lossless compression is possible using a small variational autoencoder (VAE) model. We then make use of a novel empirical insight, that fully convolutional generative models, trained on small images, are able to generalize to images of arbitrary size, and extend BB-ANS to hierarchical latent variable models, enabling state-of-the-art lossless compression of full-size colour images from the ImageNet dataset. We describe 'Craystack', a modular software framework which we have developed for rapid prototyping of compression using deep generative models.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Theory,Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning}
}

@inproceedings{townsend_practical_2019,
  title = {Practical {{Lossless Compression With Latent Variables Using Bits Back Coding}}},
  booktitle = {7th {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Townsend, James and Bird, Thomas and Barber, David},
  year = {2019},
  pages = {13},
  address = {{New Orleans, LA, USA}},
  abstract = {Deep latent variable models have seen recent success in many data domains. Lossless compression is an application of these models which, despite having the potential to be highly useful, has yet to be implemented in a practical manner. We present `Bits Back with ANS' (BB-ANS), a scheme to perform lossless compression with latent variable models at a near optimal rate. We demonstrate this scheme by using it to compress the MNIST dataset with a variational autoencoder model (VAE), achieving compression rates superior to standard methods with only a simple VAE. Given that the scheme is highly amenable to parallelization, we conclude that with a sufficiently high quality generative model this scheme could be used to achieve substantial improvements in compression rate with acceptable running time. We make our implementation available open source at https://github.com/bits-back/bits-back.},
  langid = {english},
  keywords = {⛔ No DOI found}
}

@misc{trischler_newsqa:_2016,
  title = {{{NewsQA}}: {{A Machine Comprehension Dataset}}},
  shorttitle = {{{NewsQA}}},
  author = {Trischler, Adam and Wang, Tong and Yuan, Xingdi and Harris, Justin and Sordoni, Alessandro and Bachman, Philip and Suleman, Kaheer},
  year = {2016},
  month = nov,
  eprint = {1611.09830},
  eprinttype = {arxiv},
  abstract = {We present NewsQA, a challenging machine comprehension dataset of over 100,000 human-generated question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting of spans of text from the corresponding articles. We collect this dataset through a four-stage process designed to solicit exploratory questions that require reasoning. A thorough analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing textual entailment. We measure human performance on the dataset and compare it to several strong neural models. The performance gap between humans and machines (0.198 in F1) indicates that significant progress can be made on NewsQA through future research. The dataset is freely available at https://datasets.maluuba.com/NewsQA.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@misc{trowitzsch_nigens_2020,
  title = {The {{NIGENS General Sound Events Database}}},
  author = {Trowitzsch, Ivo and Taghia, Jalil and Kashef, Youssef and Obermayer, Klaus},
  year = {2020},
  month = jan,
  eprint = {1902.08314},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {Computational auditory scene analysis is gaining interest in the last years. Trailing behind the more mature field of speech recognition, it is particularly general sound event detection that is attracting increasing attention. Crucial for training and testing reasonable models is having available enough suitable data -- until recently, general sound event databases were hardly found. We release and present a database with 714 wav files containing isolated high quality sound events of 14 different types, plus 303 `general' wav files of anything else but these 14 types. All sound events are strongly labeled with perceptual on- and offset times, paying attention to omitting in-between silences. The amount of isolated sound events, the quality of annotations, and the particular general sound class distinguish NIGENS from other databases.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Multimedia,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@misc{tsai_note_2021,
  title = {A {{Note}} on {{Connecting Barlow Twins}} with {{Negative-Sample-Free Contrastive Learning}}},
  author = {Tsai, Yao-Hung Hubert and Bai, Shaojie and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  year = {2021},
  month = apr,
  eprint = {2104.13712},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In this report, we relate the algorithmic design of Barlow Twins' method to the Hilbert-Schmidt Independence Criterion (HSIC), thus establishing it as a contrastive learning approach that is free of negative samples. Through this perspective, we argue that Barlow Twins (and thus the class of negative-sample-free contrastive learning methods) suggests a possibility to bridge the two major families of self-supervised learning philosophies: non-contrastive and contrastive approaches. In particular, Barlow twins exemplified how we could combine the best practices of both worlds: avoiding the need of large training batch size and negative sample pairing (like non-contrastive methods) and avoiding symmetry-breaking network designs (like contrastive methods).},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning}
}

@article{tsai_self-supervised_2021,
  title = {Self-Supervised {{Learning}} from a {{Multi-view Perspective}}},
  author = {Tsai, Yao-Hung Hubert and Wu, Yue and Salakhutdinov, Ruslan and Morency, Louis-Philippe},
  year = {2021},
  month = mar,
  journal = {arXiv:2006.05576 [cs, stat]},
  eprint = {2006.05576},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {As a subset of unsupervised representation learning, self-supervised representation learning adopts self-defined signals as supervision and uses the learned representation for downstream tasks, such as object detection and image captioning. Many proposed approaches for self-supervised learning follow naturally a multi-view perspective, where the input (e.g., original images) and the self-supervised signals (e.g., augmented images) can be seen as two redundant views of the data. Building from this multi-view perspective, this paper provides an information-theoretical framework to better understand the properties that encourage successful self-supervised learning. Specifically, we demonstrate that self-supervised learned representations can extract task-relevant information and discard task-irrelevant information. Our theoretical framework paves the way to a larger space of self-supervised learning objective design. In particular, we propose a composite objective that bridges the gap between prior contrastive and predictive learning objectives, and introduce an additional objective term to discard task-irrelevant information. To verify our analysis, we conduct controlled experiments to evaluate the impact of the composite objectives. We also explore our framework's empirical generalization beyond the multi-view perspective, where the cross-view redundancy may not be clearly observed.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{tschannen_recent_2018,
  title = {Recent {{Advances}} in {{Autoencoder-Based Representation Learning}}},
  author = {Tschannen, Michael and Bachem, Olivier and Lucic, Mario},
  year = {2018},
  month = dec,
  eprint = {1812.05069},
  eprinttype = {arxiv},
  abstract = {Learning useful representations with little or no supervision is a key challenge in artificial intelligence. We provide an in-depth review of recent advances in representation learning with a focus on autoencoder-based models. To organize these results we make use of meta-priors believed useful for downstream tasks, such as disentanglement and hierarchical organization of features. In particular, we uncover three main mechanisms to enforce such properties, namely (i) regularizing the (approximate or aggregate) posterior distribution, (ii) factorizing the encoding and decoding distribution, or (iii) introducing a structured prior distribution. While there are some promising results, implicit or explicit supervision remains a key enabler and all current methods use strong inductive biases and modeling assumptions. Finally, we provide an analysis of autoencoder-based representation learning through the lens of rate-distortion theory and identify a clear tradeoff between the amount of prior knowledge available about the downstream tasks, and how useful the representation is for this task.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{tucci_introduction_2013,
  title = {Introduction to {{Judea Pearl}}'s {{Do-Calculus}}},
  author = {Tucci, Robert R.},
  year = {2013},
  month = apr,
  eprint = {1305.5506},
  eprinttype = {arxiv},
  abstract = {This is a purely pedagogical paper with no new results. The goal of the paper is to give a fairly self-contained introduction to Judea Pearl's do-calculus, including proofs of his 3 rules.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence}
}

@inproceedings{tucker_doubly_2019,
  title = {Doubly {{Reparameterized Gradient Estimators}} for {{Monte Carlo Objectives}}},
  booktitle = {7th {{International Conference}} on {{Learning Representations}} ({{ICLR}} 2019)},
  author = {Tucker, George and Lawson, Dieterich and Gu, Shixiang and Maddison, Chris J.},
  year = {2019},
  publisher = {{OpenReview.net}},
  address = {{New Orleans, LA, USA}}
}

@misc{tucker_rebar_2017,
  title = {{{REBAR}}: {{Low-variance}}, Unbiased Gradient Estimates for Discrete Latent Variable Models},
  author = {Tucker, George and Mnih, Andriy and Maddison, Chris J. and Lawson, Dieterich and {Sohl-Dickstein}, Jascha},
  year = {2017},
  month = mar,
  eprint = {1703.07370},
  eprinttype = {arxiv},
  abstract = {Learning in models with discrete latent variables is challenging due to high variance gradient estimators. Generally, approaches have relied on control variates to reduce the variance of the REINFORCE estimator. Recent work (Jang et al. 2016, Maddison et al. 2016) has taken a different approach, introducing a continuous relaxation of discrete variables to produce low-variance, but biased, gradient estimates. In this work, we combine the two approaches through a novel control variate that produces low-variance, \textbackslash emph\{unbiased\} gradient estimates. Then, we introduce a modification to the continuous relaxation and show that the tightness of the relaxation can be adapted online, removing it as a hyperparameter. We show state-of-the-art variance reduction on several benchmark generative modeling tasks, generally leading to faster convergence to a better final log-likelihood.},
  archiveprefix = {arXiv}
}

@misc{turek_efficient_2017,
  title = {Efficient, Sparse Representation of Manifold Distance Matrices for Classical Scaling},
  author = {Turek, Javier S. and Huth, Alexander},
  year = {2017},
  eprint = {1705.10887},
  eprinttype = {arxiv},
  abstract = {Symmetric matrices are widely used in machine learning problems such as kernel machines and manifold learning. Using large datasets often requires computing low-rank approximations of these symmetric matrices so that they fit in memory. In this paper, we present a novel method based on biharmonic interpolation for low-rank matrix approximation. The method exploits knowledge of the data manifold to learn an interpolation operator that approximates values using a subset of randomly selected landmark points. This operator is readily sparsified, reducing memory requirements by at least two orders of magnitude without significant loss in accuracy. We show that our method can approximate very large datasets using twenty times more landmarks than other methods. Further, numerical results suggest that our method is stable even when numerical difficulties arise for other methods.},
  archiveprefix = {arXiv}
}

@article{turing_computing_1950,
  title = {Computing Machinery and Intelligence},
  author = {Turing, Alan M.},
  year = {1950},
  journal = {Mind},
  volume = {59},
  number = {236},
  pages = {433--460},
  abstract = {Will computers and robots ever think and communicate\textbackslash nthe way humans do? When a computer crosses the\textbackslash nthreshold into self-consciousness, will it immediately\textbackslash njump into the Internet and create a World Mind? This is\textbackslash nan exploration of both the philosophical and\textbackslash nmethodological issues surrounding the search for true\textbackslash nartificial intelligence.},
  keywords = {★}
}

@inproceedings{ulyanov_texture_2016,
  title = {Texture Networks: {{Feed-forward}} Synthesis of Textures and Stylized Images},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Ulyanov, Dmitry and Lebedev, Vadim and Vedaldi, Andrea and Lempitsky, Victor},
  year = {2016}
}

@inproceedings{uria_rnade_2013,
  title = {{{RNADE}}: {{The}} Real-Valued Neural Autoregressive Density-Estimator},
  booktitle = {Proceedings of the 27th {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Uria, Benigno and Murray, Iain and Larochelle, Hugo},
  year = {2013},
  pages = {9},
  address = {{Lake Tahoe, Nevada, USA}},
  abstract = {We introduce RNADE, a new model for joint density estimation of real-valued vectors. Our model calculates the density of a datapoint as the product of onedimensional conditionals modeled using mixture density networks with shared parameters. RNADE learns a distributed representation of the data, while having a tractable expression for the calculation of densities. A tractable likelihood allows direct comparison with other methods and training by standard gradientbased optimizers. We compare the performance of RNADE on several datasets of heterogeneous and perceptual data, finding it outperforms mixture models in all but one case.},
  langid = {english},
  keywords = {⛔ No DOI found}
}

@inproceedings{vahdat_nvae_2020,
  title = {{{NVAE}}: {{A Deep Hierarchical Variational Autoencoder}}},
  shorttitle = {{{NVAE}}},
  booktitle = {34th {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Vahdat, Arash and Kautz, Jan},
  year = {2020},
  month = jul,
  address = {{Virtual}},
  abstract = {Normalizing flows, autoregressive models, variational autoencoders (VAEs), and deep energy-based models are among competing likelihood-based frameworks for deep generative learning. Among them, VAEs have the advantage of fast and tractable sampling and easy-to-access encoding networks. However, they are currently outperformed by other models such as normalizing flows and autoregressive models. While the majority of the research in VAEs is focused on the statistical challenges, we explore the orthogonal direction of carefully designing neural architectures for hierarchical VAEs. We propose Nouveau VAE (NVAE), a deep hierarchical VAE built for image generation using depth-wise separable convolutions and batch normalization. NVAE is equipped with a residual parameterization of Normal distributions and its training is stabilized by spectral regularization. We show that NVAE achieves state-of-the-art results among non-autoregressive likelihood-based models on the MNIST, CIFAR-10, and CelebA HQ datasets and it provides a strong baseline on FFHQ. For example, on CIFAR-10, NVAE pushes the state-of-the-art from 2.98 to 2.91 bits per dimension, and it produces high-quality images on CelebA HQ as shown in Fig. 1. To the best of our knowledge, NVAE is the first successful VAE applied to natural images as large as 256\texttimes 256 pixels.},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{van_baalen_bayesian_2020,
  title = {Bayesian {{Bits}}: {{Unifying Quantization}} and {{Pruning}}},
  shorttitle = {Bayesian {{Bits}}},
  author = {{van Baalen}, Mart and Louizos, Christos and Nagel, Markus and Amjad, Rana Ali and Wang, Ying and Blankevoort, Tijmen and Welling, Max},
  year = {2020},
  month = may,
  eprint = {2005.07093},
  eprinttype = {arxiv},
  abstract = {We introduce Bayesian Bits, a practical method for joint mixed precision quantization and pruning through gradient based optimization. Bayesian Bits employs a novel decomposition of the quantization operation, which sequentially considers doubling the bit width. At each new bit width, the residual error between the full precision value and the previously rounded value is quantized. We then decide whether or not to add this quantized residual error for a higher effective bit width and lower quantization noise. By starting with a power-of-two bit width, this decomposition will always produce hardware-friendly configurations, and through an additional 0-bit option, serves as a unified view of pruning and quantization. Bayesian Bits then introduces learnable stochastic gates, which collectively control the bit width of the given tensor. As a result, we can obtain low bit solutions by performing approximate inference over the gates, with prior distributions that encourage most of them to be switched off. We further show that, under some assumptions, L0 regularization of the network parameters corresponds to a specific instance of the aforementioned framework. We experimentally validate our proposed method on several benchmark datasets and show that we can learn pruned, mixed precision networks that provide a better trade-off between accuracy and efficiency than their static bit width equivalents.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{van_de_schoot_bayesian_2021,
  title = {Bayesian Statistics and Modelling},
  author = {{van de Schoot}, Rens and Depaoli, Sarah and King, Ruth and Kramer, Bianca and M{\"a}rtens, Kaspar and Tadesse, Mahlet G. and Vannucci, Marina and Gelman, Andrew and Veen, Duco and Willemsen, Joukje and Yau, Christopher},
  year = {2021},
  month = dec,
  journal = {Nature Reviews Methods Primers},
  volume = {1},
  number = {1},
  pages = {1},
  issn = {2662-8449},
  abstract = {Bayesian statistics is an approach to data analysis based on Bayes' theorem, where available knowledge about parameters in a statistical model is updated with the information in observed data. The background knowledge is expressed as a prior distribution and combined with observational data in the form of a likelihood function to determine the posterior distribution. The posterior can also be used for making predictions about future events. This Primer describes the stages involved in Bayesian analysis, from specifying the prior and data models to deriving inference, model checking and refinement. We discuss the importance of prior and posterior predictive checking, selecting a proper technique for sampling from a posterior distribution, variational inference and variable selection. Examples of successful applications of Bayesian analysis across various research fields are provided, including in social sciences, ecology, genetics, medicine and more. We propose strategies for reproducibility and reporting standards, outlining an updated WAMBS (when to Worry and how to Avoid the Misuse of Bayesian Statistics) checklist. Finally, we outline the impact of Bayesian analysis on artificial intelligence, a major goal in the next decade.},
  langid = {english}
}

@article{van_der_maaten_visualizing_2008,
  title = {Visualizing High-Dimensional Data Using t-Sne},
  author = {Van Der Maaten, L. J. P. and Hinton, Geoffrey E.},
  year = {2008},
  journal = {Journal of Machine Learning Research},
  volume = {9},
  eprint = {1307.1662},
  eprinttype = {arxiv},
  pages = {2579--2605},
  issn = {1532-4435},
  abstract = {We present a new technique called ``t-SNE'' that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence theway in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualiza- tions produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.},
  archiveprefix = {arXiv},
  isbn = {1532-4435},
  pmid = {20652508},
  keywords = {Dimensionality reduction,Embedding algorithms,Manifold learning,Multidimensional scaling,Visualization}
}

@misc{van_hasselt_deep_2015,
  title = {Deep {{Reinforcement Learning}} with {{Double Q-learning}}},
  author = {{van Hasselt}, Hado and Guez, Arthur and Silver, David},
  year = {2015},
  month = sep,
  eprint = {1509.06461},
  eprinttype = {arxiv},
  abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
  archiveprefix = {arXiv},
  isbn = {0004-3702},
  pmid = {26150344}
}

@article{van_hasselt_double_2010,
  title = {Double {{Q-learning}}},
  author = {{van Hasselt}, Hado},
  year = {2010},
  journal = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
  pages = {1--9},
  abstract = {In some stochastic environments the well-known reinforcement learning algo-rithm Q-learning performs very poorly. This poor performance is caused by large overestimations of action values. These overestimations result from a positive bias that is introduced because Q-learning uses the maximum action value as an approximation for the maximum expected action value. We introduce an alter-native way to approximate the maximum expected value for any set of random variables. The obtained double estimator method is shown to sometimes under-estimate rather than overestimate the maximum expected value. We apply the double estimator to Q-learning to construct Double Q-learning, a new off-policy reinforcement learning algorithm. We show the new algorithm converges to the optimal policy and that it performs well in some settings in which Q-learning per-forms poorly due to its overestimation.},
  isbn = {9781617823800}
}

@inproceedings{van_kuyk_information_2017,
  title = {On the Information Rate of Speech Communication},
  booktitle = {2017 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Van Kuyk, Steven and Kleijn, W. Bastiaan and Hendriks, Richard C.},
  year = {2017},
  month = mar,
  pages = {5625--5629},
  publisher = {{IEEE}},
  address = {{New Orleans, LA}},
  abstract = {The key to the success of speech-based technology is an understanding of human speech communication. While significant advances have been made, a unified theory of speech communication that is both comprehensive and quantitative is yet to emerge. In this paper we approach speech communication from an information theoretical perspective. Without relying on prior knowledge of speech production, language, or auditory processing, we develop a new methodology for measuring the information rate of speech. Instead we rely on having recordings of multiple talkers saying the same utterance. In general, our results are consistent with a linguistic understanding of speech communication.},
  isbn = {978-1-5090-4117-6},
  langid = {english}
}

@article{varga_assessment_1993,
  title = {Assessment for Automatic Speech Recognition: {{II}}. {{NOISEX-92}}: {{A}} Database and an Experiment to Study the Effect of Additive Noise on Speech Recognition Systems},
  shorttitle = {Assessment for Automatic Speech Recognition},
  author = {Varga, Andrew and Steeneken, Herman J.M.},
  year = {1993},
  month = jul,
  journal = {Speech Communication},
  volume = {12},
  number = {3},
  pages = {247--251},
  issn = {01676393},
  abstract = {The NOISEX-92 experiment and database is described and discussed. NOISEX-92 specifies a carefully controlled experiment on artificially noisy speech data, examining performance for a limited digit recognition task but with a relatively wide range of noises and signal-to-noise ratios. Example recognition results are given.},
  langid = {english},
  keywords = {Data augmentation}
}

@inproceedings{vaswani_attention_2017,
  title = {Attention {{Is All You Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = jun,
  eprint = {1706.03762},
  eprinttype = {arxiv},
  pages = {5998--6008},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  isbn = {9781577357384},
  pmid = {1000303116},
  keywords = {★}
}

@inproceedings{villa_learning_2016,
  title = {Learning {{Multiple Timescales}} in {{Recurrent Neural Networks}}},
  booktitle = {Artificial {{Neural Networks}} and {{Machine Learning}} \textendash{} {{ICANN}} 2016},
  author = {Alpay, Tayfun and Heinrich, Stefan and Wermter, Stefan},
  editor = {Villa, Alessandro E.P. and Masulli, Paolo and Pons Rivero, Antonio Javier},
  year = {2016},
  volume = {9886},
  pages = {132--139},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  abstract = {Recurrent Neural Networks (RNNs) are powerful architectures for sequence learning. Recent advances on the vanishing gradient problem have led to improved results and an increased research interest. Among recent proposals are architectural innovations that allow the emergence of multiple timescales during training. This paper explores a number of architectures for sequence generation and prediction tasks with long-term relationships. We compare the Simple Recurrent Network (SRN) and Long Short-Term Memory (LSTM) with the recently proposed Clockwork RNN (CWRNN), Structurally Constrained Recurrent Network (SCRN), and Recurrent Plausibility Network (RPN) with regard to their capabilities of learning multiple timescales. Our results show that partitioning hidden layers under distinct temporal constraints enables the learning of multiple timescales, which contributes to the understanding of the fundamental conditions that allow RNNs to self-organize to accurate temporal abstractions.},
  isbn = {978-3-319-44777-3 978-3-319-44778-0},
  langid = {english}
}

@inproceedings{vincent_extracting_2008,
  title = {Extracting and Composing Robust Features with Denoising Autoencoders},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
  year = {2008},
  eprint = {1412.6550v4},
  eprinttype = {arxiv},
  pages = {1096--1103},
  issn = {1605582050},
  abstract = {Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.},
  archiveprefix = {arXiv},
  isbn = {978-1-60558-205-4},
  pmid = {15540460}
}

@article{vinyals_show_2015,
  title = {Show and {{Tell}}: {{A Neural Image Caption Generator}}},
  author = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
  year = {2015},
  journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CCVPR)},
  eprint = {1411.4555},
  eprinttype = {arxiv},
  pages = {3156--3164},
  abstract = {Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.},
  archiveprefix = {arXiv},
  isbn = {9781467369640}
}

@inproceedings{virmaux_lipschitz_2018,
  title = {Lipschitz Regularity of Deep Neural Networks: Analysis and Efficient Estimation},
  booktitle = {32nd {{Conference}} on {{Neural Information Processing Systems}}},
  author = {Virmaux, Aladin and Scaman, Kevin},
  year = {2018},
  pages = {10},
  address = {{Montr\'eal, Canada}},
  abstract = {Deep neural networks are notorious for being sensitive to small well-chosen perturbations, and estimating the regularity of such architectures is of utmost importance for safe and robust practical applications. In this paper, we investigate one of the key characteristics to assess the regularity of such methods: the Lipschitz constant of deep learning architectures. First, we show that, even for two layer neural networks, the exact computation of this quantity is NP-hard and state-ofart methods may significantly overestimate it. Then, we both extend and improve previous estimation methods by providing AutoLip, the first generic algorithm for upper bounding the Lipschitz constant of any automatically differentiable function. We provide a power method algorithm working with automatic differentiation, allowing efficient computations even on large convolutions. Second, for sequential neural networks, we propose an improved algorithm named SeqLip that takes advantage of the linear computation graph to split the computation per pair of consecutive layers. Third we propose heuristics on SeqLip in order to tackle very large networks. Our experiments show that SeqLip can significantly improve on the existing upper bounds. Finally, we provide an implementation of AutoLip in the PyTorch environment that may be used to better estimate the robustness of a given neural network to small perturbations or regularize it using more precise Lipschitz estimations.},
  langid = {english},
  keywords = {⛔ No DOI found}
}

@misc{voita_information-theoretic_2020,
  title = {Information-{{Theoretic Probing}} with {{Minimum Description Length}}},
  author = {Voita, Elena and Titov, Ivan},
  year = {2020},
  month = mar,
  eprint = {2003.12298},
  eprinttype = {arxiv},
  abstract = {To measure how well pretrained representations encode some linguistic property, it is common to use accuracy of a probe, i.e. a classifier trained to predict the property from the representations. Despite widespread adoption of probes, differences in their accuracy fail to adequately reflect differences in representations. For example, they do not substantially favour pretrained representations over randomly initialized ones. Analogously, their accuracy can be similar when probing for genuine linguistic labels and probing for random synthetic tasks. To see reasonable differences in accuracy with respect to these random baselines, previous work had to constrain either the amount of probe training data or its model size. Instead, we propose an alternative to the standard probes, information-theoretic probing with minimum description length (MDL). With MDL probing, training a probe to predict labels is recast as teaching it to effectively transmit the data. Therefore, the measure of interest changes from probe accuracy to the description length of labels given representations. In addition to probe quality, the description length evaluates "the amount of effort" needed to achieve the quality. This amount of effort characterizes either (i) size of a probing model, or (ii) the amount of data needed to achieve the high quality. We consider two methods for estimating MDL which can be easily implemented on top of the standard probing pipelines: variational coding and online coding. We show that these methods agree in results and are more informative and stable than the standard probes.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language}
}

@misc{wan_generalized_2017,
  title = {Generalized {{End-to-End Loss}} for {{Speaker Verification}}},
  author = {Wan, Li and Wang, Quan and Papir, Alan and Moreno, Ignacio Lopez},
  year = {2017},
  month = oct,
  eprint = {1710.10467},
  eprinttype = {arxiv},
  abstract = {In this paper, we propose a new loss function called generalized end-to-end (GE2E) loss, which makes the training of speaker verification models more efficient than our previous tuple-based end-to-end (TE2E) loss function. Unlike TE2E, the GE2E loss function updates the network in a way that emphasizes examples that are difficult to verify at each step of the training process. Additionally, the GE2E loss does not require an initial stage of example selection. With these properties, our model with the new loss function decreases speaker verification EER by more than 10\%, while reducing the training time by 60\% at the same time. We also introduce the MultiReader technique, which allows us to do domain adaptation - training a more accurate model that supports multiple keywords (i.e. "OK Google" and "Hey Google") as well as multiple dialects.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning}
}

@inproceedings{wan_regularization_2013,
  title = {Regularization of {{Neural Networks}} Using {{Dropconnect}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Wan, Li and Zeiler, Matthew and Zhang, Sixin and LeCun, Yann and Fergus, Rob},
  year = {2013},
  number = {1},
  pages = {109--111},
  issn = {0162-8828},
  abstract = {We introduce DropConnect, a generalization of Dropout (Hinton et al., 2012), for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recognition benchmarks by aggregating multiple DropConnect-trained models.},
  pmid = {797520}
}

@article{wang_800_1998,
  title = {An 800 Bps {{VQ}}-based {{LPC}} Voice Coder},
  author = {Wang, Xianglin and Kuo, C.-C. Jay},
  year = {1998},
  month = may,
  journal = {The Journal of the Acoustical Society of America},
  volume = {103},
  number = {5},
  pages = {2778--2778},
  issn = {0001-4966},
  langid = {english}
}

@misc{wang_computational_2018,
  title = {Computational {{Protein Design}} with {{Deep Learning Neural Networks}}},
  author = {Wang, Jingxue and Cao, Huali and Zhang, John Z. H. and Qi, Yifei},
  year = {2018},
  month = jan,
  eprint = {1801.07130},
  eprinttype = {arxiv},
  abstract = {Computational protein design has a wide variety of applications. Despite its remarkable success, designing a protein for a given structure and function is still a challenging task. On the other hand, the number of solved protein structures is rapidly increasing while the number of unique protein folds has reached a steady number, suggesting more structural information is being accumulated on each fold. Deep learning neural network is a powerful method to learn such big data set and has shown superior performance in many machine learning fields. In this study, we applied the deep learning neural network approach to computational protein design for predicting the probability of 20 natural amino acids on each residue in a protein. A large set of protein structures was collected and a multi-layer neural network was constructed. A number of structural properties were extracted as input features and the best network achieved an accuracy of 38.3\%. Using the network output as residue type restraints was able to improve the average sequence identity in designing three natural proteins using Rosetta. Moreover, the predictions from our network show \textasciitilde 3\% higher sequence identity than a previous method. Results from this study may benefit further development of computational protein design methods.},
  archiveprefix = {arXiv}
}

@article{wang_fast_2013,
  title = {Fast Dropout Training},
  author = {Wang, Sida I. and Manning, Christopher D.},
  year = {2013},
  journal = {Proceedings of the International Conference on Machine Learning (ICML)},
  volume = {28},
  pages = {118--126},
  abstract = {Preventing feature co-adaptation by encouraging independent contributions from different features often improves classification and regression performance. Dropout training (Hinton et al., 2012) does this by randomly dropping out (zeroing) hidden units and input features during training of neural networks. However, repeatedly sampling a random subset of input features makes training much slower. Based on an examination of the implied objective function of dropout training, we show how to do fast dropout training by sampling from or integrating a Gaussian approximation, instead of doing Monte Carlo optimization of this objective. This approximation, justified by the central limit theorem and empirical evidence, gives an order of magnitude speedup and more stability. We show how to do fast dropout training for classification, regression, and multilayer neural networks. Beyond dropout, our technique is extended to integrate out other types of noise and small image transformations.}
}

@misc{wang_further_2020,
  title = {Further {{Analysis}} of {{Outlier Detection}} with {{Deep Generative Models}}},
  author = {Wang, Ziyu and Dai, Bin and Wipf, David and Zhu, Jun},
  year = {2020},
  month = oct,
  eprint = {2010.13064},
  eprinttype = {arxiv},
  abstract = {The recent, counter-intuitive discovery that deep generative models (DGMs) can frequently assign a higher likelihood to outliers has implications for both outlier detection applications as well as our overall understanding of generative modeling. In this work, we present a possible explanation for this phenomenon, starting from the observation that a model's typical set and high-density region may not coincide. From this vantage point we propose a novel outlier test, the empirical success of which suggests that the failure of existing likelihood-based outlier tests does not necessarily imply that the corresponding generative model is uncalibrated. We also conduct additional experiments to help disentangle the impact of low-level texture versus high-level semantics in differentiating outliers. In aggregate, these results suggest that modifications to the standard evaluation practices and benchmarks commonly applied in the literature are needed.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{wang_glue:_2018,
  title = {{{GLUE}}: {{A Multi-Task Benchmark}} and {{Analysis Platform}} for {{Natural Language Understanding}}},
  shorttitle = {{{GLUE}}},
  author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
  year = {2018},
  month = apr,
  eprint = {1804.07461},
  eprinttype = {arxiv},
  abstract = {For natural language understanding (NLU) technology to be maximally useful, it must be able to process language in a way that is not exclusive to a single task, genre, or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation (GLUE) benchmark, a collection of tools for evaluating the performance of models across a diverse set of existing NLU tasks. By including tasks with limited training data, GLUE is designed to favor and encourage models that share general linguistic knowledge across tasks. GLUE also includes a hand-crafted diagnostic test suite that enables detailed linguistic analysis of models. We evaluate baselines based on current methods for transfer and representation learning and find that multi-task training on all tasks performs better than training a separate model per task. However, the low absolute performance of our best model indicates the need for improved general NLU systems.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language}
}

@article{wang_identifying_2018,
  title = {Identifying {{Generalization Properties}} in {{Neural Networks}}},
  author = {Wang, Huan and Keskar, Nitish Shirish and Xiong, Caiming and Socher, Richard},
  year = {2018},
  month = sep,
  abstract = {While it has not yet been proven, empirical evidence suggests that model generalization is related to local properties of the optima which can be described via the Hessian. We connect model generalization with the local property of a solution under the PAC-Bayes paradigm. In particular, we prove that model generalization ability is related to the Hessian, the higher-order ``smoothness'' terms characterized by the Lipschitz constant of the Hessian, and the scales of the parameters. Guided by the proof, we propose a metric to score the generalization capability of the model, as well as an algorithm that optimizes the perturbed model accordingly.},
  langid = {english},
  keywords = {Generalization,PAC-Bayes,Perturbation,Second-order methods}
}

@article{wang_industrial-strength_nodate,
  title = {An {{Industrial-Strength Audio Search Algorithm}}},
  author = {Wang, Avery Li-Chun},
  pages = {7},
  langid = {english},
  keywords = {⛔ No DOI found}
}

@article{wang_proposal_2007,
  title = {A {{Proposal}} of {{Adaptive PID Controller Based}} on {{Reinforcement Learning}}},
  author = {Wang, Xue-Song and Cheng, Yu-Hu and Sun, Wei},
  year = {2007},
  journal = {Journal of China University of Mining and Technology},
  volume = {17},
  number = {1},
  pages = {40--44},
  issn = {10061266},
  abstract = {Aimed at the lack of self-tuning PID parameters in conventional PID controllers, the structure and learning algorithm of an adaptive PID controller based on reinforcement learning were proposed. Actor-Critic learning was used to tune PID parameters in an adaptive way by taking advantage of the model-free and on-line learning properties of re-inforcement learning effectively. In order to reduce the demand of storage space and to improve the learning efficiency, a single RBF neural network was used to approximate the policy function of Actor and the value function of Critic si-multaneously. The inputs of RBF network are the system error, as well as the first and the second-order differences of error. The Actor can realize the mapping from the system state to PID parameters, while the Critic evaluates the outputs of the Actor and produces TD error. Based on TD error performance index and gradient descent method, the updating rules of RBF kernel function and network weights were given. Simulation results show that the proposed controller is efficient for complex nonlinear systems and it is perfectly adaptable and strongly robust, which is better than that of a conventional PID controller.},
  isbn = {8651683995},
  keywords = {Actor-critic learning,Adaptive PID control,Reinforcement learning,RNF network}
}

@article{wang_robust_2007,
  title = {Robust {{Speech Rate Estimation}} for {{Spontaneous Speech}}},
  author = {Wang, D. and Narayanan, S. S.},
  year = {2007},
  month = nov,
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  volume = {15},
  number = {8},
  pages = {2190--2201},
  issn = {1558-7916},
  abstract = {In this paper, we propose a direct method for speech rate estimation from acoustic features without requiring any automatic speech transcription. We compare various spectral and temporal signal analysis and smoothing strategies to better characterize the underlying syllable structure to derive speech rate. The proposed algorithm extends the methods of spectral sub- band correlation by including temporal correlation and the use of prominent spectral subbands for improving the signal correlation essential for syllable detection. Furthermore, to address some of the practical robustness issues in previously proposed methods, we introduce some novel components into the algorithm such as the use of pitch confidence for filtering spurious syllable envelope peaks, magnifying window for tackling neighboring syllable smearing, and relative peak measure thresholds for pseudo peak rejection. We also describe an automated approach for learning algorithm parameters from data, and find the optimal settings through Monte Carlo simulations and parameter sensitivity analysis. Final experimental evaluations are conducted based on a portion of the Switchboard corpus for which manual phonetic segmentation information, and published results for direct comparison are available. The results show a correlation coefficient of 0.745 with respect to the ground truth based on manual segmentation. This result is about a 17\% improvement compared to the current best single estimator and a 11\% improvement over the multiestimator evaluated on the same Switchboard database.},
  keywords = {acoustic feature,Automatic speech recognition,automatic speech transcription,correlation methods,estimation theory,Hidden Markov models,Humans,learning algorithm parameter,Monte Carlo methods,Monte Carlo simulation,Natural languages,parameter sensitivity analysis,Rich speech transcription,Robustness,sensitivity analysis,Signal analysis,smoothing methods,Smoothing methods,smoothing strategy,spectral analysis,spectral sub-band correlation,spectral-temporal signal analysis,speech processing,Speech processing,speech prosody,speech rate estimation,Speech recognition,spontaneous speech processing,spontaneous speech rate estimation,syllable detection,Viterbi algorithm}
}

@misc{wang_speaker_2017,
  title = {Speaker {{Diarization}} with {{LSTM}}},
  author = {Wang, Quan and Downey, Carlton and Wan, Li and Mansfield, Philip Andrew and Moreno, Ignacio Lopez},
  year = {2017},
  month = oct,
  eprint = {1710.10468},
  eprinttype = {arxiv},
  abstract = {For many years, i-vector based audio embedding techniques were the dominant approach for speaker verification and speaker diarization applications. However, mirroring the rise of deep learning in various domains, neural network based audio embeddings, also known as d-vectors, have consistently demonstrated superior speaker verification performance. In this paper, we build on the success of d-vector based speaker verification systems to develop a new d-vector based approach to speaker diarization. Specifically, we combine LSTM-based d-vector audio embeddings with recent work in non-parametric clustering to obtain a state-of-the-art speaker diarization system. Our system is evaluated on three standard public datasets, suggesting that d-vector based diarization systems offer significant advantages over traditional i-vector based systems. We achieved a 12.0\% diarization error rate on NIST SRE 2000 CALLHOME, while our model is trained with out-of-domain data from voice search logs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning}
}

@inproceedings{wang_t-cvae_2019,
  title = {T-{{CVAE}}: {{Transformer-Based Conditioned Variational Autoencoder}} for {{Story Completion}}},
  shorttitle = {T-{{CVAE}}},
  booktitle = {Proceedings of the 28th {{International Joint Conference}} on {{Artificial Intelligence}} ({{IJCAI}})},
  author = {Wang, Tianming and Wan, Xiaojun},
  year = {2019},
  month = aug,
  pages = {5233--5239},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  address = {{Macao, China}},
  abstract = {Story completion is a very challenging task of generating the missing plot for an incomplete story, which requires not only understanding but also inference of the given contextual clues. In this paper, we present a novel conditional variational autoencoder based on Transformer for missing plot generation. Our model uses shared attention layers for encoder and decoder, which make the most of the contextual clues, and a latent variable for learning the distribution of coherent story plots. Through drawing samples from the learned distribution, diverse reasonable plots can be generated. Both automatic and manual evaluations show that our model generates better story plots than stateof-the-art models in terms of readability, diversity and coherence.},
  isbn = {978-0-9992411-4-1},
  langid = {english}
}

@misc{wang_towards_2021,
  title = {Towards {{Learning Universal Audio Representations}}},
  author = {Wang, Luyu and Luc, Pauline and Wu, Yan and Recasens, Adria and Smaira, Lucas and Brock, Andrew and Jaegle, Andrew and Alayrac, Jean-Baptiste and Dieleman, Sander and Carreira, Joao and {van den Oord}, Aaron},
  year = {2021},
  eprint = {2111.12124},
  eprinttype = {arxiv},
  primaryclass = {cs.SD},
  archiveprefix = {arXiv}
}

@article{wang_unispeech_2021,
  title = {{{UniSpeech}}: {{Unified Speech Representation Learning}} with {{Labeled}} and {{Unlabeled Data}}},
  shorttitle = {{{UniSpeech}}},
  author = {Wang, Chengyi and Wu, Yu and Qian, Yao and Kumatani, Kenichi and Liu, Shujie and Wei, Furu and Zeng, Michael and Huang, Xuedong},
  year = {2021},
  month = jun,
  journal = {arXiv:2101.07597 [cs, eess]},
  eprint = {2101.07597},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {In this paper, we propose a unified pre-training approach called UniSpeech to learn speech representations with both unlabeled and labeled data, in which supervised phonetic CTC learning and phonetically-aware contrastive self-supervised learning are conducted in a multi-task learning manner. The resultant representations can capture information more correlated with phonetic structures and improve the generalization across languages and domains. We evaluate the effectiveness of UniSpeech for cross-lingual representation learning on public CommonVoice corpus. The results show that UniSpeech outperforms self-supervised pretraining and supervised transfer learning for speech recognition by a maximum of 13.4\% and 17.8\% relative phone error rate reductions respectively (averaged over all testing languages). The transferability of UniSpeech is also demonstrated on a domain-shift speech recognition task, i.e., a relative word error rate reduction of 6\% against the previous approach.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@article{wang_unsupervised_2020,
  title = {Unsupervised {{Pre-training}} of {{Bidirectional Speech Encoders}} via {{Masked Reconstruction}}},
  author = {Wang, Weiran and Tang, Qingming and Livescu, Karen},
  year = {2020},
  month = may,
  journal = {arXiv:2001.10603 [cs, eess]},
  eprint = {2001.10603},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {We propose an approach for pre-training speech representations via a masked reconstruction loss. Our pre-trained encoder networks are bidirectional and can therefore be used directly in typical bidirectional speech recognition models. The pre-trained networks can then be fine-tuned on a smaller amount of supervised data for speech recognition. Experiments with this approach on the LibriSpeech and Wall Street Journal corpora show promising results. We find that the main factors that lead to speech recognition improvements are: masking segments of sufficient width in both time and frequency, pre-training on a much larger amount of unlabeled data than the labeled data, and domain adaptation when the unlabeled and labeled data come from different domains. The gain from pre-training is additive to that of supervised data augmentation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@misc{wang_vlmo_2021,
  title = {{{VLMo}}: {{Unified Vision-Language Pre-Training}} with {{Mixture-of-Modality-Experts}}},
  shorttitle = {{{VLMo}}},
  author = {Wang, Wenhui and Bao, Hangbo and Dong, Li and Wei, Furu},
  year = {2021},
  month = nov,
  eprint = {2111.02358},
  eprinttype = {arxiv},
  abstract = {We present a unified Vision-Language pretrained Model (VLMo) that jointly learns a dual encoder and a fusion encoder with a modular Transformer network. Specifically, we introduce Mixture-of-Modality-Experts (MoME) Transformer, where each block contains a pool of modality-specific experts and a shared self-attention layer. Because of the modeling flexibility of MoME, pretrained VLMo can be fine-tuned as a fusion encoder for vision-language classification tasks, or used as a dual encoder for efficient image-text retrieval. Moreover, we propose a stagewise pre-training strategy, which effectively leverages large-scale image-only and text-only data besides image-text pairs. Experimental results show that VLMo achieves state-of-the-art results on various vision-language tasks, including VQA and NLVR2. The code and pretrained models are available at https://aka.ms/vlmo.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@article{wang_wav2vec-switch_2021,
  title = {Wav2vec-{{Switch}}: {{Contrastive Learning}} from {{Original-noisy Speech Pairs}} for {{Robust Speech Recognition}}},
  shorttitle = {Wav2vec-{{Switch}}},
  author = {Wang, Yiming and Li, Jinyu and Wang, Heming and Qian, Yao and Wang, Chengyi and Wu, Yu},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.04934 [cs, eess]},
  eprint = {2110.04934},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {The goal of self-supervised learning (SSL) for automatic speech recognition (ASR) is to learn good speech representations from a large amount of unlabeled speech for the downstream ASR task. However, most SSL frameworks do not consider noise robustness which is crucial for real-world applications. In this paper we propose wav2vec-Switch, a method to encode noise robustness into contextualized representations of speech via contrastive learning. Specifically, we feed original-noisy speech pairs simultaneously into the wav2vec 2.0 network. In addition to the existing contrastive learning task, we switch the quantized representations of the original and noisy speech as additional prediction targets of each other. By doing this, it enforces the network to have consistent predictions for the original and noisy speech, thus allows to learn contextualized representation with noise robustness. Our experiments on synthesized and real noisy data show the effectiveness of our method: it achieves 2.9--4.9\% relative word error rate (WER) reduction on the synthesized noisy LibriSpeech data without deterioration on the original data, and 5.7\% on CHiME-4 real 1-channel noisy data compared to a data augmentation baseline even with a strong language model for decoding. Our results on CHiME-4 can match or even surpass those with well-designed speech enhancement components.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@article{weber_reinforced_2015,
  title = {Reinforced {{Variational Inference}}},
  author = {Weber, Theophane and Heess, Nicolas and Eslami, S. M. Ali and Schulman, John and Wingate, David and Silver, David},
  year = {2015},
  journal = {Proceedings of the Conference on Neural Information Processing Systems (NIPS)},
  number = {2},
  pages = {1--9}
}

@article{weiler_coordinate_nodate,
  title = {Coordinate {{Independent Convolutional Networks}}},
  author = {Weiler, Maurice and Verlinde, Erik and Forr{\'e}, Patrick and Welling, Max},
  pages = {271},
  abstract = {Motivated by the vast success of deep convolutional networks, there is a great interest in generalizing convolutions to non-Euclidean manifolds. A major complication in comparison to flat spaces is that it is unclear in which alignment a convolution kernel should be applied on a manifold. The underlying reason for this ambiguity is that general manifolds do not come with a canonical choice of reference frames (gauge). Kernels and features therefore have to be expressed relative to arbitrary coordinates. We argue that the particular choice of coordinatization should not affect a network's inference \textendash{} it should be coordinate independent. A simultaneous demand for coordinate independence and weight sharing is shown to result in a requirement on the network to be equivariant under local gauge transformations (changes of local reference frames). The ambiguity of reference frames depends thereby on the G-structure of the manifold, such that the necessary level of gauge equivariance is prescribed by the corresponding structure group G. Coordinate independent convolutions are proven to be equivariant w.r.t. those isometries that are symmetries of the G-structure. The resulting theory is formulated in a coordinate free fashion in terms of fiber bundles. To exemplify the design of coordinate independent convolutions, we implement a convolutional network on the M\"obius strip. The generality of our differential geometric formulation of convolutional networks is demonstrated by an extensive literature review which explains a large number of Euclidean CNNs, spherical CNNs and CNNs on general surfaces as specific instances of coordinate independent convolutions.},
  langid = {english}
}

@techreport{weinstein_structured_2020,
  type = {Preprint},
  title = {A Structured Observation Distribution for Generative Biological Sequence Prediction and Forecasting},
  author = {Weinstein, Eli N. and Marks, Debora S.},
  year = {2020},
  month = aug,
  institution = {{Genomics}},
  abstract = {Generative probabilistic modeling of biological sequences has widespread existing and potential application across biology and biomedicine, from evolutionary biology to epidemiology to protein design. Many standard sequence analysis methods preprocess data using a multiple sequence alignment (MSA) algorithm, one of the most widely used computational methods in all of science(Van Noorden et al., 2014). However, as we show in this article, training generative probabilistic models with MSA preprocessing leads to statistical pathologies in the context of sequence prediction and forecasting. To address these problems, we propose a principled drop-in alternative to MSA preprocessing in the form of a structured observation distribution (the ``MuE'' distribution). The MuE is a latent alignment model in which not only the alignment variable but also the regressor sequence can be latent. We prove theoretically that the MuE distribution comprehensively generalizes popular methods for inferring biological sequence alignments, and provide a precise characterization of how such biological models have differed from natural language latent alignment models. We show empirically that models that use the MuE as an observation distribution outperform comparable methods across a variety of datasets, and apply MuE models to a novel problem for generative probabilistic sequence models: forecasting pathogen evolution.},
  langid = {english}
}

@misc{weiss_wave-tacotron_2021,
  title = {Wave-{{Tacotron}}: {{Spectrogram-free}} End-to-End Text-to-Speech Synthesis},
  shorttitle = {Wave-{{Tacotron}}},
  author = {Weiss, Ron J. and {Skerry-Ryan}, R. J. and Battenberg, Eric and Mariooryad, Soroosh and Kingma, Diederik P.},
  year = {2021},
  month = feb,
  eprint = {2011.03568},
  eprinttype = {arxiv},
  abstract = {We describe a sequence-to-sequence neural network which directly generates speech waveforms from text inputs. The architecture extends the Tacotron model by incorporating a normalizing flow into the autoregressive decoder loop. Output waveforms are modeled as a sequence of non-overlapping fixed-length blocks, each one containing hundreds of samples. The interdependencies of waveform samples within each block are modeled using the normalizing flow, enabling parallel training and synthesis. Longer-term dependencies are handled autoregressively by conditioning each flow on preceding blocks. This model can be optimized directly with maximum likelihood, without using intermediate, hand-designed features nor additional loss terms. Contemporary state-of-the-art text-to-speech (TTS) systems use a cascade of separately learned models: one (such as Tacotron) which generates intermediate features (such as spectrograms) from text, followed by a vocoder (such as WaveRNN) which generates waveform samples from the intermediate features. The proposed system, in contrast, does not use a fixed intermediate representation, and learns all parameters end-to-end. Experiments show that the proposed model generates speech with quality approaching a state-of-the-art neural TTS system, with significantly improved generation speed.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@article{welford_note_1962,
  title = {Note on a {{Method}} for {{Calculating Correct Sums}} of {{Squares}} and {{Products}}},
  author = {Welford, B. P.},
  year = {1962},
  journal = {Technometrics},
  volume = {4},
  number = {3},
  pages = {419--420},
  issn = {00401706}
}

@article{welling_bayesian_2011,
  title = {Bayesian Posterior Contraction Rates for Linear Severely Ill-Posed Inverse Problems},
  author = {Welling, Max and Teh, Yee Whye},
  year = {2011},
  journal = {Proceedings of the International Conference on Machine Learning (ICML)},
  volume = {22},
  number = {3},
  eprint = {1203.5753v5},
  eprinttype = {arxiv},
  pages = {681--688},
  issn = {10495258},
  abstract = {In this paper we propose a new framework for learning from large scale datasets based on iterative learning from small mini-batches. By adding the right amount of noise to a standard stochastic gradient optimization algorithm we show that the iterates will converge to samples from the true posterior distribution as we anneal the stepsize. This seamless transition between optimization and Bayesian posterior sampling provides an in-built protection against overfitting. We also propose a practical method for Monte Carlo estimates of posterior statistics which monitors a ``sampling threshold'' and collects samples after it has been surpassed. We apply the method to three models: a mixture of Gaussians, logistic regression and ICA with natural gradients.},
  archiveprefix = {arXiv},
  isbn = {978-1-4503-0619-5},
  keywords = {Gaussian prior,Posterior consistency,Rate of contraction,Severely ill-posed problems}
}

@inproceedings{wenzel_how_2020,
  title = {How {{Good}} Is the {{Bayes Posterior}} in {{Deep Neural Networks Really}}?},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Wenzel, Florian and Roth, Kevin and Veeling, Bastiaan S and Swiatkowski, Jakub and Tran, Linh and Mandt, Stephan and Snoek, Jasper and Salimans, Tim and Jenatton, Rodolphe and Nowozin, Sebastian},
  year = {2020},
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {119},
  pages = {10248--10259},
  publisher = {{PMLR}},
  address = {{Virtual}},
  abstract = {During the past five years the Bayesian deep learning community has developed increasingly accurate and efficient approximate inference procedures that allow for Bayesian inference in deep neural networks. However, despite this algorithmic progress and the promise of improved uncertainty quantification and sample efficiency there are\textemdash as of early 2020\textemdash no publicized deployments of Bayesian neural networks in industrial practice. In this work we cast doubt on the current understanding of Bayes posteriors in popular deep neural networks: we demonstrate through careful MCMC sampling that the posterior predictive induced by the Bayes posterior yields systematically worse predictions compared to simpler methods including point estimates obtained from SGD. Furthermore, we demonstrate that predictive performance is improved significantly through the use of a ``cold posterior'' that overcounts evidence. Such cold posteriors sharply deviate from the Bayesian paradigm but are commonly used as heuristic in Bayesian deep learning papers. We put forward several hypotheses that could explain cold posteriors and evaluate the hypotheses through experiments. Our work questions the goal of accurate posterior approximations in Bayesian deep learning: If the true Bayes posterior is poor, what is the use of more accurate approximations? Instead, we argue that it is timely to focus on understanding the origin of the improved performance of cold posteriors.},
  langid = {english}
}

@inproceedings{weston_memory_2014,
  title = {Memory {{Networks}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Weston, Jason and Chopra, Sumit and Bordes, Antoine},
  year = {2014},
  eprint = {1410.3916},
  eprinttype = {arxiv},
  issn = {1098-7576},
  abstract = {We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.},
  archiveprefix = {arXiv},
  isbn = {978-1-4244-6917-8},
  pmid = {9377276},
  keywords = {Memory networks,Natural Language Processing,Question answering}
}

@misc{weston_towards_2015,
  title = {Towards {{AI-Complete Question Answering}}: {{A Set}} of {{Prerequisite Toy Tasks}}},
  shorttitle = {Towards {{AI-Complete Question Answering}}},
  author = {Weston, Jason and Bordes, Antoine and Chopra, Sumit and Rush, Alexander M. and {van Merri{\"e}nboer}, Bart and Joulin, Armand and Mikolov, Tomas},
  year = {2015},
  month = feb,
  eprint = {1502.05698},
  eprinttype = {arxiv},
  abstract = {One long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. To measure progress towards that goal, we argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. We believe many existing learning systems can currently not solve them, and hence our aim is to classify these tasks into skill sets, so that researchers can identify (and then rectify) the failings of their systems. We also extend and improve the recently introduced Memory Networks model, and show it is able to solve some, but not all, of the tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Statistics - Machine Learning}
}

@misc{wiegreffe_attention_2019,
  title = {Attention Is Not Not {{Explanation}}},
  author = {Wiegreffe, Sarah and Pinter, Yuval},
  year = {2019},
  month = sep,
  eprint = {1908.04626},
  eprinttype = {arxiv},
  abstract = {Attention mechanisms play a central role in NLP systems, especially within recurrent neural network (RNN) models. Recently, there has been increasing interest in whether or not the intermediate representations offered by these modules may be used to explain the reasoning for a model's prediction, and consequently reach insights regarding the model's decision-making process. A recent paper claims that `Attention is not Explanation' (Jain and Wallace, 2019). We challenge many of the assumptions underlying this work, arguing that such a claim depends on one's definition of explanation, and that testing it needs to take into account all elements of the model. We propose four alternative tests to determine when/whether attention can be used as explanation: a simple uniform-weights baseline; a variance calibration based on multiple random seed runs; a diagnostic framework using frozen weights from pretrained models; and an end-to-end adversarial attention training protocol. Each allows for meaningful interpretation of attention mechanisms in RNN models. We show that even when reliable adversarial distributions can be found, they don't perform well on the simple diagnostic, indicating that prior work does not disprove the usefulness of attention mechanisms for explainability.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language}
}

@article{wierstra_natural_2008,
  title = {Natural {{Evolution Strategies}}},
  author = {Wierstra, Daan and Schaul, Tom and Peters, Jan and Schmidhuber, J{\"u}rgen},
  year = {2008},
  journal = {Journal of Machine Learning Research},
  volume = {15},
  eprint = {1106.4487},
  eprinttype = {arxiv},
  pages = {3381--3387},
  issn = {15337928},
  abstract = {This paper presents Natural Evolution Strategies (NES), a novel algorithm for performing real-valued 'black box' function optimization: optimizing an unknown objective function where algorithm-selected function measurements constitute the only information accessible to the method. Natural Evolution Strategies search the fitness landscape using a multivariate normal distribution with a self-adapting mutation matrix to generate correlated mutations in promising regions. NES shares this property with Covariance Matrix Adaption (CMA), an Evolution Strategy (ES) which has been shown to perform well on a variety of high-precision optimization tasks. The Natural Evolution Strategies algorithm, however, is simpler, less ad-hoc and more principled. Self-adaptation of the mutation matrix is derived using a Monte Carlo estimate of the natural gradient towards better expected fitness. By following the natural gradient instead of the 'vanilla' gradient, we can ensure efficient update steps while preventing early convergence due to overly greedy updates, resulting in reduced sensitivity to local suboptima. We show NES has competitive performance with CMA on unimodal tasks, while outperforming it on several multimodal tasks that are rich in deceptive local optima. \textcopyright{} 2008 IEEE.},
  archiveprefix = {arXiv},
  isbn = {978-1-4244-1822-0},
  keywords = {★,Black-box optimization,Evolution strategies,Natural gradient,Sampling,Stochastic search}
}

@article{wilks_large-sample_1938,
  title = {The Large-Sample Distribution of the Likelihood Ratio for Testing Composite Hypotheses},
  author = {Wilks, S. S.},
  year = {1938},
  month = mar,
  journal = {Annals of Mathematical Statistics},
  volume = {9},
  number = {1},
  pages = {60--62}
}

@article{williams_simple_1992,
  title = {Simple {{Statistical Gradient-Following Algorithms}} for {{Connectionist Reinforcement Learning}}},
  author = {Williams, Ronald J.},
  year = {1992},
  journal = {Journal of Machine Learning},
  volume = {8},
  number = {3},
  pages = {229--256},
  abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
  isbn = {978-1-4615-3618-5},
  keywords = {Connectionist networks,Gradient descent,Reinforcement learning}
}

@misc{wilson_bayesian_2020,
  title = {Bayesian {{Deep Learning}} and a {{Probabilistic Perspective}} of {{Generalization}}},
  author = {Wilson, Andrew Gordon and Izmailov, Pavel},
  year = {2020},
  month = apr,
  eprint = {2002.08791},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The key distinguishing property of a Bayesian approach is marginalization, rather than using a single setting of weights. Bayesian marginalization can particularly improve the accuracy and calibration of modern deep neural networks, which are typically underspecified by the data, and can represent many compelling but different solutions. We show that deep ensembles provide an effective mechanism for approximate Bayesian marginalization, and propose a related approach that further improves the predictive distribution by marginalizing within basins of attraction, without significant overhead. We also investigate the prior over functions implied by a vague distribution over neural network weights, explaining the generalization properties of such models from a probabilistic perspective. From this perspective, we explain results that have been presented as mysterious and distinct to neural network generalization, such as the ability to fit images with random labels, and show that these results can be reproduced with Gaussian processes. We also show that Bayesian model averaging alleviates double descent, resulting in monotonic performance improvements with increased flexibility. Finally, we provide a Bayesian perspective on tempering for calibrating predictive distributions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{wilson_evolving_2018,
  title = {Evolving Simple Programs for Playing {{Atari}} Games},
  author = {Wilson, Dennis G. and {Cussat-Blanc}, Sylvain and Luga, Herv{\'e} and Miller, Julian F.},
  year = {2018},
  eprint = {1806.05695v1},
  eprinttype = {arxiv},
  abstract = {Cartesian Genetic Programming (CGP) has previously shown capabilities in image processing tasks by evolving programs with a function set specialized for computer vision. A similar approach can be applied to Atari playing. Programs are evolved using mixed type CGP with a function set suited for matrix operations, including image processing, but allowing for controller behavior to emerge. While the programs are relatively small, many controllers are competitive with state of the art methods for the Atari benchmark set and require less training time. By evaluating the programs of the best evolved individuals, simple but eeective strategies can be found. CCS CONCEPTS \textbullet Computing methodologies \textrightarrow{} Artiicial intelligence; Model development and analysis;},
  archiveprefix = {arXiv},
  keywords = {Artiicial intelligence,Games,Genetic programming,Image analysis}
}

@techreport{wilson_marginal_2017,
  title = {The {{Marginal Value}} of {{Adaptive Gradient Methods}} in {{Machine Learning}}},
  author = {Wilson, Ashia C and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nathan and Recht, Benjamin},
  year = {2017},
  abstract = {Adaptive optimization methods, which perform local optimization with a metric constructed from the history of iterates, are becoming increasingly popular for training deep neural networks. Examples include AdaGrad, RMSProp, and Adam. We show that for simple overparameterized problems, adaptive methods often find drastically different solutions than gradient descent (GD) or stochastic gradient descent (SGD). We construct an illustrative binary classification problem where the data is linearly separable, GD and SGD achieve zero test error, and AdaGrad, Adam, and RMSProp attain test errors arbitrarily close to half. We additionally study the empirical generalization capability of adaptive methods on several state-of-the-art deep learning models. We observe that the solutions found by adaptive methods generalize worse (often significantly worse) than SGD, even when these solutions have better training performance. These results suggest that practitioners should reconsider the use of adaptive methods to train neural networks.}
}

@misc{wingate_automated_2013,
  title = {Automated {{Variational Inference}} in {{Probabilistic Programming}}},
  author = {Wingate, David and Weber, Theophane},
  year = {2013},
  eprint = {1301.1299},
  eprinttype = {arxiv},
  abstract = {We present a new algorithm for approximate inference in probabilistic programs, based on a stochastic gradient for variational programs. This method is efficient without restrictions on the probabilistic program; it is particularly practical for distributions which are not analytically tractable, including highly structured distributions that arise in probabilistic programs. We show how to automatically derive mean-field probabilistic programs and optimize them, and demonstrate that our perspective improves inference efficiency over other algorithms.},
  archiveprefix = {arXiv}
}

@article{wiseman_learning_nodate,
  title = {Learning {{Anaphoricity}} and {{Antecedent Ranking Features}} for {{Coreference Resolution}}},
  author = {Wiseman, Sam and Rush, Alexander M. and Shieber, Stuart M. and Weston, Jason},
  abstract = {We introduce a simple, non-linear mention-ranking model for coreference resolution that attempts to learn distinct feature representations for anaphoricity detection and antecedent ranking, which we encourage by pre-training on a pair of corresponding subtasks. Although we use only simple, unconjoined features, the model is able to learn useful representa-tions, and we report the best overall score on the CoNLL 2012 English test set to date.}
}

@article{wolpert_no_1997,
  title = {No Free Lunch Theorems for Optimization},
  author = {Wolpert, D.H. and Macready, W.G.},
  year = {1997},
  month = apr,
  journal = {IEEE Transactions on Evolutionary Computation},
  volume = {1},
  number = {1},
  pages = {67--82},
  issn = {1089778X},
  abstract = {A framework is developed to explore the connection between effective optimization algorithms and the problems they are solving. A number of ``no free lunch'' (NFL) theorems are presented which establish that for any algorithm, any elevated performance over one class of problems is offset by performance over another class. These theorems result in a geometric interpretation of what it means for an algorithm to be well suited to an optimization problem. Applications of the NFL theorems to information-theoretic aspects of optimization and benchmark measures of performance are also presented. Other issues addressed include time-varying optimization problems and a priori ``head-to-head'' minimax distinctions between optimization algorithms, distinctions that result despite the NFL theorems' enforcing of a type of uniformity over all algorithms.},
  langid = {english},
  keywords = {Evolutionary algorithms,Information theory,No free lunch}
}

@inproceedings{wu_differentiable_2019,
  title = {Differentiable Antithetic Sampling for Variance Reduction in Stochastic Variational Inference},
  booktitle = {The 22nd {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}} ({{AISTATS}})},
  author = {Wu, Mike and Goodman, Noah D. and Ermon, Stefano},
  year = {2019},
  pages = {2877--2886},
  address = {{Naha, Okinawa, Japan}}
}

@misc{wu_googles_2016,
  title = {Google's {{Neural Machine Translation System}}: {{Bridging}} the {{Gap}} between {{Human}} and {{Machine Translation}}},
  author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus},
  year = {2016},
  eprint = {1609.08144},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@inproceedings{wu_group_2018,
  title = {Group {{Normalization}}},
  booktitle = {Proceedings of the {{European Conference}} on {{Computer Vision}} ({{ECCV}})},
  author = {Wu, Yuxin and He, Kaiming},
  year = {2018},
  month = jun,
  eprint = {1803.08494},
  eprinttype = {arxiv},
  pages = {pp. 3-19},
  abstract = {Batch Normalization (BN) is a milestone technique in the development of deep learning, enabling various networks to train. However, normalizing along the batch dimension introduces problems \textemdash{} BN's error increases rapidly when the batch size becomes smaller, caused by inaccurate batch statistics estimation. This limits BN's usage for training larger models and transferring features to computer vision tasks including detection, segmentation, and video, which require small batches constrained by memory consumption. In this paper, we present Group Normalization (GN) as a simple alternative to BN. GN divides the channels into groups and computes within each group the mean and variance for normalization. GN's computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GN has 10.6\% lower error than its BN counterpart when using a batch size of 2; when using typical batch sizes, GN is comparably good with BN and outperforms other normalization variants. Moreover, GN can be naturally transferred from pre-training to fine-tuning. GN can outperform its BNbased counterparts for object detection and segmentation in COCO,1 and for video classification in Kinetics, showing that GN can effectively replace the powerful BN in a variety of tasks. GN can be easily implemented by a few lines of code in modern libraries.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@article{wu_towards_2015,
  title = {Towards Dropout Training for Convolutional Neural Networks},
  author = {Wu, Haibing and Gu, Xiaodong},
  year = {2015},
  journal = {Neural Networks},
  volume = {71},
  eprint = {1512.00242},
  eprinttype = {arxiv},
  pages = {1--10},
  issn = {18792782},
  abstract = {Recently, dropout has seen increasing use in deep learning. For deep convolutional neural networks, dropout is known to work well in fully-connected layers. However, its effect in convolutional and pooling layers is still not clear. This paper demonstrates that max-pooling dropout is equivalent to randomly picking activation based on a multinomial distribution at training time. In light of this insight, we advocate employing our proposed probabilistic weighted pooling, instead of commonly used max-pooling, to act as model averaging at test time. Empirical evidence validates the superiority of probabilistic weighted pooling. We also empirically show that the effect of convolutional dropout is not trivial, despite the dramatically reduced possibility of over-fitting due to the convolutional architecture. Elaborately designing dropout training simultaneously in max-pooling and fully-connected layers, we achieve state-of-the-art performance on MNIST, and very competitive results on CIFAR-10 and CIFAR-100, relative to other approaches without data augmentation. Finally, we compare max-pooling dropout and stochastic pooling, both of which introduce stochasticity based on multinomial distributions at pooling stage.},
  archiveprefix = {arXiv},
  isbn = {0893-6080},
  pmid = {26277608},
  keywords = {Convolutional neural networks,Deep learning,Max-pooling dropout}
}

@misc{wu_transferable_2019,
  title = {Transferable {{Multi-Domain State Generator}} for {{Task-Oriented Dialogue Systems}}},
  author = {Wu, Chien-Sheng and Madotto, Andrea and {Hosseini-Asl}, Ehsan and Xiong, Caiming and Socher, Richard and Fung, Pascale},
  year = {2019},
  month = may,
  eprint = {1905.08743},
  eprinttype = {arxiv},
  abstract = {Over-dependence on domain ontology and lack of knowledge sharing across domains are two practical and yet less studied problems of dialogue state tracking. Existing approaches generally fall short in tracking unknown slot values during inference and often have difficulties in adapting to new domains. In this paper, we propose a TRAnsferable Dialogue statE generator (TRADE) that generates dialogue states from utterances using a copy mechanism, facilitating knowledge transfer when predicting (domain, slot, value) triplets not encountered during training. Our model is composed of an utterance encoder, a slot gate, and a state generator, which are shared across domains. Empirical results demonstrate that TRADE achieves state-of-the-art joint goal accuracy of 48.62\% for the five domains of MultiWOZ, a human-human dialogue dataset. In addition, we show its transferring ability by simulating zero-shot and few-shot dialogue state tracking for unseen domains. TRADE achieves 60.58\% joint goal accuracy in one of the zero-shot domains, and is able to adapt to few-shot cases without forgetting already trained domains.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@misc{xiao_fashionmnist_2017,
  title = {Fashion-{{MNIST}}: A {{Novel Image Dataset}} for {{Benchmarking Machine Learning Algorithms}}},
  author = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  year = {2017},
  eprint = {2102.06171},
  eprinttype = {arxiv},
  primaryclass = {cs.CV},
  abstract = {We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at https://github.com/zalandoresearch/fashion-mnist},
  archiveprefix = {arXiv},
  keywords = {Dataset}
}

@inproceedings{xiao_likelihood_2020,
  title = {Likelihood {{Regret}}: {{An Out-of-Distribution Detection Score}} for {{Variational Auto-Encoder}}},
  booktitle = {Proceedings of the 33rd {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Xiao, Zhisheng and Yan, Qing and Amit, Yali},
  year = {2020},
  address = {{Virtual}}
}

@misc{xiong_dcn+:_2017,
  title = {{{DCN}}+: {{Mixed Objective}} and {{Deep Residual Coattention}} for {{Question Answering}}},
  author = {Xiong, Caiming and Zhong, Victor and Socher, Richard},
  year = {2017},
  month = oct,
  eprint = {1711.00106},
  eprinttype = {arxiv},
  abstract = {Traditional models for question answering optimize using cross entropy loss, which encourages exact answers at the cost of penalizing nearby or overlapping answers that are sometimes equally accurate. We propose a mixed objective that combines cross entropy loss with self-critical policy learning. The objective uses rewards derived from word overlap to solve the misalignment between evaluation metric and optimization objective. In addition to the mixed objective, we improve dynamic coattention networks (DCN) with a deep residual coattention encoder that is inspired by recent work in deep self-attention and residual networks. Our proposals improve model performance across question types and input lengths, especially for long questions that requires the ability to capture long-term dependencies. On the Stanford Question Answering Dataset, our model achieves state-of-the-art results with 75.1\% exact match accuracy and 83.1\% F1, while the ensemble obtains 78.9\% exact match accuracy and 86.0\% F1.},
  archiveprefix = {arXiv}
}

@misc{xiong_dynamic_2016,
  title = {Dynamic Memory Networks for Visual and Textual Question Answering},
  author = {Xiong, Caiming and Merity, Stephen and Socher, Richard},
  year = {2016},
  eprint = {1603.01417},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@article{xu_towards_2011,
  title = {Towards {{Optimal One Pass Large Scale Learning}} with {{Averaged Stochastic Gradient Descent}}},
  author = {Xu, Wei},
  year = {2011},
  month = jul,
  langid = {english},
  keywords = {Polyak averaging}
}

@article{xu_unsupervised_2017,
  title = {Unsupervised {{Feature Learning Based}} on {{Deep Models}} for {{Environmental Audio Tagging}}},
  author = {Xu, Yong and Huang, Qiang and Wang, Wenwu and Foster, Peter and Sigtia, Siddharth and Jackson, Philip J. B. and Plumbley, Mark D.},
  year = {2017},
  month = jun,
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {25},
  number = {6},
  pages = {1230--1241},
  issn = {2329-9304},
  abstract = {Environmental audio tagging aims to predict only the presence or absence of certain acoustic events in the interested acoustic scene. In this paper, we make contributions to audio tagging in two parts, respectively, acoustic modeling and feature learning. We propose to use a shrinking deep neural network (DNN) framework incorporating unsupervised feature learning to handle the multilabel classification task. For the acoustic modeling, a large set of contextual frames of the chunk are fed into the DNN to perform a multilabel classification for the expected tags, considering that only chunk (or utterance) level rather than frame-level labels are available. Dropout and background noise aware training are also adopted to improve the generalization capability of the DNNs. For the unsupervised feature learning, we propose to use a symmetric or asymmetric deep denoising auto-encoder (syDAE or asyDAE) to generate new data-driven features from the logarithmic Mel-filter banks features. The new features, which are smoothed against background noise and more compact with contextual information, can further improve the performance of the DNN baseline. Compared with the standard Gaussian mixture model baseline of the DCASE 2016 audio tagging challenge, our proposed method obtains a significant equal error rate (EER) reduction from 0.21 to 0.13 on the development set. The proposed asyDAE system can get a relative 6.7\% EER reduction compared with the strong DNN baseline on the development set. Finally, the results also show that our approach obtains the state-of-the-art performance with 0.15 EER on the evaluation set of the DCASE 2016 audio tagging task while EER of the first prize of this challenge is 0.17.},
  keywords = {Acoustics,Audio recording,DCASE 2016,deep de-noising auto-encoder,deep neural networks,environmental audio tagging,Machine learning,Noise measurement,Speech,Tagging,Training,unsupervised feature learning}
}

@misc{xue_byt5_2021,
  title = {{{ByT5}}: {{Towards}} a Token-Free Future with Pre-Trained Byte-to-Byte Models},
  shorttitle = {{{ByT5}}},
  author = {Xue, Linting and Barua, Aditya and Constant, Noah and {Al-Rfou}, Rami and Narang, Sharan and Kale, Mihir and Roberts, Adam and Raffel, Colin},
  year = {2021},
  month = may,
  eprint = {2105.13626},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Most widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units. Encoding text as a sequence of tokens requires a tokenizer, which is typically created as an independent artifact from the model. Token-free models that instead operate directly on raw text (bytes or characters) have many benefits: they can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by removing complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences. We carefully characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language}
}

@misc{yang_improved_2017,
  title = {Improved {{Variational Autoencoders}} for {{Text Modeling}} Using {{Dilated Convolutions}}},
  author = {Yang, Zichao and Hu, Zhiting and Salakhutdinov, Ruslan and {Berg-Kirkpatrick}, Taylor},
  year = {2017},
  month = jun,
  eprint = {1702.08139},
  eprinttype = {arxiv},
  abstract = {Recent work on generative text modeling has found that variational autoencoders (VAE) with LSTM decoders perform worse than simpler LSTM language models (Bowman et al., 2015). This negative result is so far poorly understood, but has been attributed to the propensity of LSTM decoders to ignore conditioning information from the encoder. In this paper, we experiment with a new type of decoder for VAE: a dilated CNN. By changing the decoder's dilation architecture, we control the size of context from previously generated words. In experiments, we find that there is a trade-off between contextual capacity of the decoder and effective use of encoding information. We show that when carefully managed, VAEs can outperform LSTM language models. We demonstrate perplexity gains on two datasets, representing the first positive language modeling result with VAE. Further, we conduct an in-depth investigation of the use of VAE (with our new decoding architecture) for semi-supervised and unsupervised labeling tasks, demonstrating gains over several strong baselines.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@misc{yang_semi-supervised_2017,
  title = {Semi-{{Supervised QA}} with {{Generative Domain-Adaptive Nets}}},
  author = {Yang, Zhilin and Hu, Junjie and Salakhutdinov, Ruslan and Cohen, William W.},
  year = {2017},
  eprint = {1702.02206},
  eprinttype = {arxiv},
  abstract = {We study the problem of semi-supervised question answering----utilizing unlabeled text to boost the performance of question answering models. We propose a novel training framework, the Generative Domain-Adaptive Nets. In this framework, we train a generative model to generate questions based on the unlabeled text, and combine model-generated questions with human-generated questions for training question answering models. We develop novel domain adaptation algorithms, based on reinforcement learning, to alleviate the discrepancy between the model-generated data distribution and the human-generated data distribution. Experiments show that our proposed framework obtains substantial improvement from unlabeled text.},
  archiveprefix = {arXiv},
  isbn = {9781945626753},
  keywords = {★}
}

@inproceedings{yang_stacked_2016,
  title = {Stacked Attention Networks for Image Question Answering},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CCVPR}})},
  author = {Yang, Zichao and He, Xiaodong and Gao, Jianfeng and Deng, Li and Smola, Alex},
  year = {2016},
  pages = {21--29}
}

@misc{yang_xlnet:_2019,
  title = {{{XLNet}}: {{Generalized Autoregressive Pretraining}} for {{Language Understanding}}},
  shorttitle = {{{XLNet}}},
  author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
  year = {2019},
  month = jun,
  eprint = {1906.08237},
  eprinttype = {arxiv},
  abstract = {With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, XLNet outperforms BERT on 20 tasks, often by a large margin, and achieves state-of-the-art results on 18 tasks including question answering, natural language inference, sentiment analysis, and document ranking.1.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{yao_adahessian_2020,
  title = {{{ADAHESSIAN}}: {{An Adaptive Second Order Optimizer}} for {{Machine Learning}}},
  shorttitle = {{{ADAHESSIAN}}},
  author = {Yao, Zhewei and Gholami, Amir and Shen, Sheng and Keutzer, Kurt and Mahoney, Michael W.},
  year = {2020},
  month = jun,
  eprint = {2006.00719},
  eprinttype = {arxiv},
  abstract = {We introduce ADAHESSIAN, a second order stochastic optimization algorithm which dynamically incorporates the curvature of the loss function via ADAptive estimates of the HESSIAN. Second order algorithms are among the most powerful optimization algorithms with superior convergence properties as compared to first order methods such as SGD and Adam. The main disadvantage of traditional second order methods is their heavier periteration computation and poor accuracy as compared to first order methods. To address these, we incorporate several novel approaches in ADAHESSIAN, including: (i) a new variance reduction estimate of the Hessian diagonal with low computational overhead; (ii) a root-mean-square exponential moving average to smooth out variations of the Hessian diagonal across different iterations; and (iii) a block diagonal averaging to reduce the variance of Hessian diagonal elements. We show that ADAHESSIAN achieves new state-of-the-art results by a large margin as compared to other adaptive optimization methods, including variants of Adam. In particular, we perform extensive tests on CV, NLP, and recommendation system tasks and find that ADAHESSIAN: (i) achieves 1.80\%/1.45\% higher accuracy on ResNets20/32 on Cifar10, and 5.55\% higher accuracy on ImageNet as compared to Adam; (ii) outperforms AdamW for transformers by 0.27/0.33 BLEU score on IWSLT14/WMT14 and 1.8/1.0 PPL on PTB/Wikitext-103; and (iii) achieves 0.032\% better score than Adagrad for DLRM on the Criteo Ad Kaggle dataset. Importantly, we show that the cost per iteration of ADAHESSIAN is comparable to first-order methods, and that it exhibits robustness towards its hyperparameters. The code for ADAHESSIAN is open-sourced and publicly-available [1].},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Numerical Analysis,Statistics - Machine Learning}
}

@misc{yao_depth-gated_2015,
  title = {Depth-{{Gated Recurrent Neural Networks}}},
  author = {Yao, Kaisheng and Cohn, Trevor and Vylomova, Katerina and Duh, Kevin and Dyer, Chris},
  year = {2015},
  eprint = {1508.03790},
  eprinttype = {arxiv},
  abstract = {In this short note, we present an extension of long short-term memory (LSTM) neural networks to using a depth gate to connect memory cells of adjacent layers. Doing so introduces a linear dependence between lower and upper layer recurrent units. Importantly, the linear dependence is gated through a gating function, which we call depth gate. This gate is a function of the lower layer memory cell, the input to and the past memory cell of this layer. We conducted experiments and verified that this new architecture of LSTMs was able to improve machine translation and language modeling performances.},
  archiveprefix = {arXiv}
}

@article{yarra_mode-shape_2016,
  title = {A Mode-Shape Classification Technique for Robust Speech Rate Estimation and Syllable Nuclei Detection},
  author = {Yarra, Chiranjeevi and Deshmukh, Om D. and Ghosh, Prasanta Kumar},
  year = {2016},
  month = apr,
  journal = {Speech Communication},
  volume = {78},
  pages = {62--71},
  issn = {01676393},
  abstract = {Acoustic feature based speech (syllable) rate estimation and syllable nuclei detection are important problems in automatic speech recognition (ASR), computer assisted language learning (CALL) and fluency analysis. A typical solution for both the problems consists of two stages. The first stage involves computing a short-time feature contour such that most of the peaks of the contour correspond to the syllabic nuclei. In the second stage, the peaks corresponding to the syllable nuclei are detected. In this work, instead of the peak detection, we perform a mode-shape classification, which is formulated as a supervised binary classification problem \textendash{} mode-shapes representing the syllabic nuclei as one class and remaining as the other. We use the temporal correlation and selected sub-band correlation (TCSSBC) feature contour and the mode-shapes in the TCSSBC feature contour are converted into a set of feature vectors using an interpolation technique. A support vector machine classifier is used for the classification. Experiments are performed separately using Switchboard, TIMIT and CTIMIT corpora in a five-fold cross validation setup. The average correlation coefficients for the syllable rate estimation turn out to be 0.6761, 0.6928 and 0.3604 for three corpora respectively, which outperform those obtained by the best of the existing peak detection techniques. Similarly, the average F-scores (syllable level) for the syllable nuclei detection are 0.8917, 0.8200 and 0.7637 for three corpora respectively.},
  langid = {english}
}

@article{ye_deep_2021,
  title = {Deep {{Mixture Generative Autoencoders}}},
  author = {Ye, Fei and Bors, Adrian G.},
  year = {2021},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  pages = {1--15},
  issn = {2162-237X, 2162-2388},
  abstract = {Variational autoencoders (VAEs) are one of the most popular unsupervised generative models which rely on learning latent representations of data. In this paper, we extend the classical concept of Gaussian mixtures into the deep variational framework by proposing a mixture of VAEs (MVAE). Each component in the MVAE model is implemented by a variational encoder and has an associated sub-decoder. The separation between the latent spaces modelled by different encoders is enforced using the d-variable Hilbert-Schmidt Independence Criterion (dHSIC) criterion. Each component would capture different data variational features. We also propose a mechanism for finding the appropriate number of VAE components for a given task, leading to an optimal architecture. The differentiable categorical Gumbel-Softmax distribution is used in order to generate dropout masking parameters within the end-toend backpropagation training framework. Extensive experiments show that the proposed MAVE model learns a rich latent data representation and is able to discover additional underlying data factors.},
  langid = {english}
}

@inproceedings{yeh_unsupervised_2019,
  title = {Unsupervised Speech Recognition via Segmental Empirical Output Distribution Matching},
  booktitle = {7th {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Yeh, Chih-Kuan and Chen, Jianshu and Yu, Chengzhu and Yu, Dong},
  year = {2019},
  address = {{New Orleans, LA, USA}}
}

@inproceedings{yi_stochastic_2009,
  title = {Stochastic {{Search}} Using the {{Natural Gradient}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Yi, Sun and Wierstra, Daan and Schaul, Tom and Schmidhuber, J{\"u}rgen},
  year = {2009},
  pages = {1161--1168},
  address = {{Montreal, Quebec, Canada}},
  abstract = {To optimize unknown 'fitness' functions, we present Natural Evolution Strategies, a novel algorithm that constitutes a principled alter-native to standard stochastic search meth-ods. It maintains a multinormal distribution on the set of solution candidates. The Nat-ural Gradient is used to update the distribu-tion's parameters in the direction of higher expected fitness, by efficiently calculating the inverse of the exact Fisher information ma-trix whereas previous methods had to use approximations. Other novel aspects of our method include optimal fitness baselines and importance mixing, a procedure adjusting batches with minimal numbers of fitness eval-uations. The algorithm yields competitive re-sults on a number of benchmarks.},
  isbn = {978-1-60558-516-1},
  keywords = {Evolution strategies,Natural gradient,Stochastic search}
}

@article{yin_expanded_2011,
  title = {The {{Expanded State Space Kalman}} Filter for {{GPS}} Navigation},
  author = {Yin, Jianjun and Gu, Ming and Zhang, Jianqiu},
  year = {2011},
  month = nov,
  journal = {Information Technology Journal},
  volume = {10},
  number = {11},
  pages = {2091--2097},
  issn = {18125638},
  abstract = {A novel dynamic state space model was established for Global Positioning System (GPS) navigation by adopting the polynomial predictive filtering idea and state dimension expansion. We called the new model expanded state space model which was established without the exact knowledge of the original state dynamics, i.e., we way use the proposed model to describe the state dynamics no matter we know the original state propagation well or not. A correspondent Expanded State Space Kalman filter (ESSKF) was then presented based on the proposed model. The results of the GPS navigation examples demonstrated that the proposed method did work better than the existed Extended Kalman Filter (EKF), especially in the situations that the state dynamics were not known well. \textcopyright{} 2011 Asian Network for Scientific Information.},
  keywords = {Expanded state space Kalman filter,Kalman filter}
}

@inproceedings{yin_neural_2016,
  title = {Neural {{Generative Question Answering}}},
  booktitle = {Proceedings of the {{International Joint Conference}} on {{Artificial Intelligence}} ({{IJCAI}})},
  author = {Yin, Jun and Jiang, Xin and Lu, Zhengdong and Shang, Lifeng and Li, Hang and Li, Xiaoming},
  year = {2016},
  abstract = {This paper presents an end-to-end neural net-work model, named Neural Generative Question Answering (GENQA), that can generate answers to simple factoid questions, based on the facts in a knowledge-base. More specifically, the model is built on the encoder-decoder framework for sequence-to-sequence learning, while equipped with the ability to enquire the knowledge-base, and is trained on a corpus of question-answer pairs, with their associated triples in the knowledge-base. Empirical study shows the proposed model can ef-fectively deal with the variations of questions and answers, and generate right and natural answers by referring to the facts in the knowledge-base. The experiment on question answering demon-strates that the proposed model can outperform an embedding-based QA model as well as a neural di-alogue model trained on the same data.},
  keywords = {Natural Language Processing,Question answering}
}

@misc{yoshida_spectral_2017,
  title = {Spectral {{Norm Regularization}} for {{Improving}} the {{Generalizability}} of {{Deep Learning}}},
  author = {Yoshida, Yuichi and Miyato, Takeru},
  year = {2017},
  month = may,
  eprint = {1705.10941},
  eprinttype = {arxiv},
  abstract = {We investigate the generalizability of deep learning based on the sensitivity to input perturbation. We hypothesize that the high sensitivity to the perturbation of data degrades the performance on it. To reduce the sensitivity to perturbation, we propose a simple and effective regularization method, referred to as spectral norm regularization, which penalizes the high spectral norm of weight matrices in neural networks. We provide supportive evidence for the abovementioned hypothesis by experimentally confirming that the models trained using spectral norm regularization exhibit better generalizability than other baseline methods.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{yosinski_how_2014,
  title = {How Transferable Are Features in Deep Neural Networks?},
  author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
  year = {2014},
  journal = {Proceedings of the Conference on Neural Information Processing Systems (NeurIPS)},
  eprint = {1411.1792},
  eprinttype = {arxiv},
  pages = {3320--3328},
  issn = {10495258},
  abstract = {Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.},
  archiveprefix = {arXiv},
  isbn = {978-1-5090-0620-5}
}

@misc{you_large_2017,
  title = {Large {{Batch Training}} of {{Convolutional Networks}}},
  author = {You, Yang and Gitman, Igor and Ginsburg, Boris},
  year = {2017},
  month = sep,
  eprint = {1708.03888},
  eprinttype = {arxiv},
  abstract = {A common way to speed up training of large convolutional networks is to add computational units. Training is then performed using data-parallel synchronous Stochastic Gradient Descent (SGD) with mini-batch divided between computational units. With an increase in the number of nodes, the batch size grows. But training with large batch size often results in the lower model accuracy. We argue that the current recipe for large batch training (linear learning rate scaling with warm-up) is not general enough and training may diverge. To overcome this optimization difficulties we propose a new training algorithm based on Layer-wise Adaptive Rate Scaling (LARS). Using LARS, we scaled Alexnet up to a batch size of 8K, and Resnet-50 to a batch size of 32K without loss in accuracy.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition}
}

@misc{you_large_2019,
  title = {Large {{Batch Optimization}} for {{Deep Learning}}: {{Training BERT}} in 76 Minutes},
  shorttitle = {Large {{Batch Optimization}} for {{Deep Learning}}},
  author = {You, Yang and Li, Jing and Reddi, Sashank and Hseu, Jonathan and Kumar, Sanjiv and Bhojanapalli, Srinadh and Song, Xiaodan and Demmel, James and Hsieh, Cho-Jui},
  year = {2019},
  month = apr,
  eprint = {1904.00962},
  eprinttype = {arxiv},
  abstract = {Training large deep neural networks on massive datasets is very challenging. One promising approach to tackle this issue is through the use of large batch stochastic optimization. However, our understanding of this approach in the context of deep learning is still very limited. Furthermore, the current approaches in this direction are heavily hand-tuned. To this end, we first study a general adaptation strategy to accelerate training of deep neural networks using large minibatches. Using this strategy, we develop a new layer-wise adaptive large batch optimization technique called LAMB. We also provide a formal convergence analysis of LAMB as well as the previous published layerwise optimizer LARS, showing convergence to a stationary point in general nonconvex settings. Our empirical results demonstrate the superior performance of LAMB for BERT and ResNet-50 training. In particular, for BERT training, our optimization technique enables use of very large batches sizes of 32868; thereby, requiring just 8599 iterations to train (as opposed to 1 million iterations in the original paper). By increasing the batch size to the memory limit of a TPUv3 pod, BERT training time can be reduced from 3 days to 76 minutes (Table 1). Finally, we also demonstrate that LAMB outperforms previous largebatch training algorithms for ResNet-50 on ImageNet; obtaining state-of-the-art performance in just a few minutes.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{yu_improved_nodate,
  title = {Improved {{Bottleneck Features Using Pretrained Deep Neural Networks}}},
  author = {Yu, Dong and Seltzer, Michael L},
  pages = {4},
  abstract = {Bottleneck features have been shown to be effective in improving the accuracy of automatic speech recognition (ASR) systems. Conventionally, bottleneck features are extracted from a multi-layer perceptron (MLP) trained to predict context-independent monophone states. The MLP typically has three hidden layers and is trained using the backpropagation algorithm. In this paper, we propose two improvements to the training of bottleneck features motivated by recent advances in the use of deep neural networks (DNNs) for speech recognition. First, we show how the use of unsupervised pretraining of a DNN enhances the network's discriminative power and improves the bottleneck features it generates. Second, we show that a neural network trained to predict context-dependent senone targets produces better bottleneck features than one trained to predict monophone states. Bottleneck features trained using the proposed methods produced a 16\% relative reduction in sentence error rate over conventional bottleneck features on a large vocabulary business search task.},
  langid = {english}
}

@article{yu_incorporating_2007,
  title = {Incorporating {{Prior Domain Knowledge Into Inductive Machine Learning}}},
  author = {Yu, Ting and Jan, Tony and Simoff, Simeon and Debenham, John},
  year = {2007},
  month = jan,
  abstract = {The paper reviews the recent developments of incorporating prior domain knowledge into inductive machine learning, and proposes a guideline that incorporates prior domain knowledge in three key issues of inductive machine learning algorithms: consistency, gen-eralization and convergence. With respect to each issue, this paper gives some approaches to improve the performance of the inductive machine learning algorithms and discusses the risks of incorporating domain knowledge. As a case study, a hierarchical modelling method, VQSVM, is proposed and tested over some imbalanced data sets with various imbalance ratios and various numbers of subclasses.}
}

@misc{yuan_machine_2017,
  title = {Machine {{Comprehension}} by {{Text-to-Text Neural Question Generation}}},
  author = {Yuan, Xingdi and Wang, Tong and Gulcehre, Caglar and Sordoni, Alessandro and Bachman, Philip and Subramanian, Sandeep and Zhang, Saizheng and Trischler, Adam},
  year = {2017},
  eprint = {1705.02012},
  eprinttype = {arxiv},
  abstract = {We propose a recurrent neural model that generates natural-language questions from documents, conditioned on answers. We show how to train the model using a combination of supervised and reinforcement learning. After teacher forcing for standard maximum likelihood training, we fine-tune the model using policy gradient techniques to maximize several rewards that measure question quality. Most notably, one of these rewards is the performance of a question-answering system. We motivate question generation as a means to improve the performance of question answering systems. Our model is trained and evaluated on the recent question-answering dataset SQuAD.},
  archiveprefix = {arXiv}
}

@inproceedings{yusuf_hierarchical_2021,
  title = {A {{Hierarchical Subspace Model}} for {{Language-Attuned Acoustic Unit Discovery}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Yusuf, Bolaji and Ondel, Lucas and Burget, Luk{\'a}{\v s} and {\v C}ernock{\'y}, Jan and Sara{\c c}lar, Murat},
  year = {2021},
  month = jun,
  pages = {3710--3714},
  issn = {2379-190X},
  abstract = {In this work, we propose a hierarchical subspace model for acoustic unit discovery. In this approach, we frame the task as one of learning embeddings on a low-dimensional phonetic subspace, and simultaneously specify the subspace itself as an embedding on a hyper-subspace. We train the hyper-subspace on a set of transcribed languages and transfer it to the target language. In the target language, we infer both the language and unit embeddings in an unsupervised manner, and in so doing, we simultaneously learn a subspace of units specific to that language and the units that dwell on it. We conduct experiments on TIMIT and two low-resource languages: Mboshi and Yoruba. Results show that our model outperforms major acoustic unit discovery techniques, both in terms of clustering quality and segmentation accuracy.},
  keywords = {acoustic unit discovery,Acoustics,Conferences,Hidden Markov models,hierarchical subspace model,Phonetics,Signal processing,Speech processing,Task analysis,unsupervised learning}
}

@inproceedings{zadeh_tensor_2017,
  title = {Tensor {{Fusion Network}} for {{Multimodal Sentiment Analysis}}},
  booktitle = {Proceedings of the 2017 {{Conference}} on {{Empirical Methods}} in {{Natural}}           {{Language Processing}}},
  author = {Zadeh, Amir and Chen, Minghai and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  year = {2017},
  pages = {1103--1114},
  publisher = {{Association for Computational Linguistics}},
  address = {{Copenhagen, Denmark}},
  abstract = {Multimodal sentiment analysis is an increasingly popular research area, which extends the conventional language-based definition of sentiment analysis to a multimodal setup where other relevant modalities accompany language. In this paper, we pose the problem of multimodal sentiment analysis as modeling intra-modality and inter-modality dynamics. We introduce a novel model, termed Tensor Fusion Network, which learns both such dynamics end-to-end. The proposed approach is tailored for the volatile nature of spoken language in online videos as well as accompanying gestures and voice. In the experiments, our model outperforms state-ofthe-art approaches for both multimodal and unimodal sentiment analysis.},
  langid = {english}
}

@misc{zaremba_reinforcement_2015,
  title = {Reinforcement {{Learning Neural Turing Machines}}},
  author = {Zaremba, Wojciech and Sutskever, Ilya},
  year = {2015},
  eprint = {1505.00521v1},
  eprinttype = {arxiv},
  abstract = {The expressive power of a machine learning model is closely related to the num-ber of sequential computational steps it can learn. For example, Deep Neural Networks have been more successful than shallow networks because they can per-form a greater number of sequential computational steps (each highly parallel). The Neural Turing Machine (NTM) [8] is a model that can compactly express an even greater number of sequential computational steps, so it is even more powerful than a DNN. Its memory addressing operations are designed to be differentiable; thus the NTM can be trained with backpropagation. While differentiable memory is relatively easy to implement and train, it necessi-tates accessing the entire memory content at each computational step. This makes it difficult to implement a fast NTM. In this work, we use the Reinforce algorithm to learn where to access the memory, while using backpropagation to learn what to write to the memory. We call this model the RL-NTM. Reinforce allows our model to access a constant number of memory cells at each computational step, so its implementation can be faster. The RL-NTM is the first model that can, in principle, learn programs of unbounded running time. We successfully trained the RL-NTM to solve a number of algorithmic tasks that are simpler than the ones solvable by the fully differentiable NTM. As the RL-NTM is a fairly intricate model, we needed a method for verifying the correctness of our implementation. To do so, we developed a simple technique for numerically checking arbitrary implementations of models that use Reinforce, which may be of independent interest.},
  archiveprefix = {arXiv},
  isbn = {9781424438617}
}

@misc{zbontar_barlow_2021,
  title = {Barlow {{Twins}}: {{Self-Supervised Learning}} via {{Redundancy Reduction}}},
  shorttitle = {Barlow {{Twins}}},
  author = {Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, St{\'e}phane},
  year = {2021},
  month = may,
  eprint = {2103.03230},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio},
  abstract = {Self-supervised learning (SSL) is rapidly closing the gap with supervised methods on large computer vision benchmarks. A successful approach to SSL is to learn representations which are invariant to distortions of the input sample. However, a recurring issue with this approach is the existence of trivial constant solutions. Most current methods avoid such solutions by careful implementation details. We propose an objective function that naturally avoids such collapse by measuring the cross-correlation matrix between the outputs of two identical networks fed with distorted versions of a sample, and making it as close to the identity matrix as possible. This causes the representation vectors of distorted versions of a sample to be similar, while minimizing the redundancy between the components of these vectors. The method is called Barlow Twins, owing to neuroscientist H. Barlow's redundancy-reduction principle applied to a pair of identical networks. Barlow Twins does not require large batches nor asymmetry between the network twins such as a predictor network, gradient stopping, or a moving average on the weight updates. It allows the use of very high-dimensional output vectors. Barlow Twins outperforms previous methods on ImageNet for semi-supervised classification in the low-data regime, and is on par with current state of the art for ImageNet classification with a linear classifier head, and for transfer tasks of classification and object detection.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition}
}

@misc{zeiler_adadelta:_2012,
  title = {{{ADADELTA}}: {{An Adaptive Learning Rate Method}}},
  author = {Zeiler, Matthew D.},
  year = {2012},
  month = dec,
  eprint = {1212.5701},
  eprinttype = {arxiv},
  abstract = {We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.},
  archiveprefix = {arXiv}
}

@inproceedings{zeiler_visualizing_2014,
  title = {Visualizing and Understanding Convolutional Networks},
  booktitle = {Lecture {{Notes}} in {{Computer Science}} (Including Subseries {{Lecture Notes}} in {{Artificial Intelligence}} and {{Lecture Notes}} in {{Bioinformatics}})},
  author = {Zeiler, Matthew D. and Fergus, Rob},
  year = {2014},
  volume = {8689 LNCS},
  eprint = {1311.2901},
  eprinttype = {arxiv},
  pages = {818--833},
  issn = {16113349},
  abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky \textbackslash etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
  archiveprefix = {arXiv},
  isbn = {978-3-319-10589-5},
  pmid = {26353135}
}

@inproceedings{zeng_robust_2006,
  title = {Robust {{GMM Based Gender Classification}} Using {{Pitch}} and {{RASTA-PLP Parameters}} of {{Speech}}},
  booktitle = {2006 {{International Conference}} on {{Machine Learning}} and {{Cybernetics}}},
  author = {Zeng, Yu-min and Wu, Zhen-yang and Falk, Tiago and Chan, Wai-yip},
  year = {2006},
  pages = {3376--3379},
  publisher = {{IEEE}},
  address = {{Dalian, China}},
  abstract = {A novel gender classification system has been proposed based on Gaussian Mixture Models, which apply the combined parameters of pitch and 10th order relative spectral perceptual linear predictive coefficients to model the characteristics of male and female speech. The performances of gender classification system have been evaluated on the conditions of clean speech, noisy speech and multi-language. The simulations show that the performance of the proposed gender classifier is excellent; it is very robust for noise and completely independent of languages; the classification accuracy is as high as above 98\% for all clean speech and remains 95\% for most noisy speech, even the SNR of speech is degraded to 0dB.},
  isbn = {978-1-4244-0061-4},
  langid = {english}
}

@inproceedings{zhai_s4l_2019,
  title = {{{S4L}}: {{Self-Supervised Semi-Supervised Learning}}},
  shorttitle = {{{S4L}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Zhai, Xiaohua and Oliver, Avital and Kolesnikov, Alexander and Beyer, Lucas},
  year = {2019},
  month = oct,
  pages = {1476--1485},
  issn = {2380-7504},
  abstract = {This work tackles the problem of semi-supervised learning of image classifiers. Our main insight is that the field of semi-supervised learning can benefit from the quickly advancing field of self-supervised visual representation learning. Unifying these two approaches, we propose the framework of self-supervised semi-supervised learning (S4L) and use it to derive two novel semi-supervised image classification methods. We demonstrate the effectiveness of these methods in comparison to both carefully tuned baselines, and existing semi-supervised learning methods. We then show that S4L and existing semi-supervised methods can be jointly trained, yielding a new state-of-the-art result on semi-supervised ILSVRC-2012 with 10\% of labels.},
  keywords = {Benchmark testing,Computer vision,Semisupervised learning,Standards,Task analysis,Training,Visualization}
}

@misc{zhang_advances_2018,
  title = {Advances in {{Variational Inference}}},
  author = {Zhang, Cheng and Butepage, Judith and Kjellstrom, Hedvig and Mandt, Stephan},
  year = {2018},
  month = oct,
  eprint = {1711.05597},
  eprinttype = {arxiv},
  abstract = {Many modern unsupervised or semi-supervised machine learning algorithms rely on Bayesian probabilistic models. These models are usually intractable and thus require approximate inference. Variational inference (VI) lets us approximate a high-dimensional Bayesian posterior with a simpler variational distribution by solving an optimization problem. This approach has been successfully applied to various models and large-scale applications. In this review, we give an overview of recent trends in variational inference. We first introduce standard mean field variational inference, then review recent advances focusing on the following aspects: (a) scalable VI, which includes stochastic approximations, (b) generic VI, which extends the applicability of VI to a large class of otherwise intractable models, such as non-conjugate models, (c) accurate VI, which includes variational models beyond the mean field approximation or with atypical divergences, and (d) amortized VI, which implements the inference over local latent variables with inference networks. Finally, we provide a summary of promising future research directions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{zhang_colorful_2016,
  title = {Colorful Image Colorization},
  booktitle = {European {{Conference}} on {{Computer Vision}}},
  author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A.},
  year = {2016},
  pages = {649--666}
}

@misc{zhang_fully_2018,
  title = {Fully {{Supervised Speaker Diarization}}},
  author = {Zhang, Aonan and Wang, Quan and Zhu, Zhenyao and Paisley, John and Wang, Chong},
  year = {2018},
  month = oct,
  eprint = {1810.04719},
  eprinttype = {arxiv},
  abstract = {In this paper, we propose a fully supervised speaker diarization approach, named unbounded interleaved-state recurrent neural networks (UIS-RNN). Given extracted speaker-discriminative embeddings (a.k.a. d-vectors) from input utterances, each individual speaker is modeled by a parameter-sharing RNN, while the RNN states for different speakers interleave in the time domain. This RNN is naturally integrated with a distance-dependent Chinese restaurant process (ddCRP) to accommodate an unknown number of speakers. Our system is fully supervised and is able to learn from examples where time-stamped speaker labels are annotated. We achieved a 7.6\% diarization error rate on NIST SRE 2000 CALLHOME, which is better than the state-of-the-art method using spectral clustering. Moreover, our method decodes in an online fashion while most state-of-the-art systems rely on offline clustering.},
  archiveprefix = {arXiv},
  keywords = {★}
}

@inproceedings{zhang_learning_2019,
  title = {Learning {{Latent Representations}} for {{Style Control}} and {{Transfer}} in {{End-to-end Speech Synthesis}}},
  booktitle = {{{ICASSP}} 2019 - 2019 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Zhang, Ya-Jie and Pan, Shifeng and He, Lei and Ling, Zhen-Hua},
  year = {2019},
  month = may,
  pages = {6945--6949},
  issn = {2379-190X},
  abstract = {In this paper, we introduce the Variational Autoencoder (VAE) to an end-to-end speech synthesis model, to learn the latent representation of speaking styles in an unsupervised manner. The style representation learned through VAE shows good properties such as disentangling, scaling, and combination, which makes it easy for style control. Style transfer can be achieved in this framework by first inferring style representation through the recognition network of VAE, then feeding it into TTS network to guide the style in synthesizing speech. To avoid Kullback-Leibler (KL) divergence collapse in training, several techniques are adopted. Finally, the proposed model shows good performance of style control and outperforms Global Style Token (GST) model in ABX preference tests on style transfer.},
  keywords = {Decoding,Interpolation,Mathematical model,Spectrogram,Speech,speech synthesis,Speech synthesis,style transfer,Training,unsupervised learning,variational autoencoder}
}

@misc{zhang_neglected_2021,
  title = {The Neglected Sibling: {{Isotropic}} Gaussian Posterior for {{VAE}}},
  author = {Zhang, Lan and Buntine, Wray and Shareghi, Ehsan},
  year = {2021},
  eprint = {2110.07383},
  eprinttype = {arxiv},
  primaryclass = {cs.LG},
  archiveprefix = {arXiv}
}

@misc{zhang_noisy_2017,
  title = {Noisy {{Natural Gradient}} as {{Variational Inference}}},
  author = {Zhang, Guodong and Sun, Shengyang and Duvenaud, David and Grosse, Roger},
  year = {2017},
  eprint = {1712.02390},
  eprinttype = {arxiv},
  abstract = {Variational Bayesian neural nets combine the flexibility of deep learning with Bayesian uncertainty estimation. Unfortunately, there is a tradeoff between cheap but simple variational families (e.g.\textasciitilde fully factorized) or expensive and complicated inference procedures. We show that natural gradient ascent with adaptive weight noise implicitly fits a variational posterior to maximize the evidence lower bound (ELBO). This insight allows us to train full-covariance, fully factorized, or matrix-variate Gaussian variational posteriors using noisy versions of natural gradient, Adam, and K-FAC, respectively, making it possible to scale up to modern-size ConvNets. On standard regression benchmarks, our noisy K-FAC algorithm makes better predictions and matches Hamiltonian Monte Carlo's predictive variances better than existing methods. Its improved uncertainty estimates lead to more efficient exploration in active learning, and intrinsic motivation for reinforcement learning.},
  archiveprefix = {arXiv}
}

@article{zhang_pushing_2020,
  title = {Pushing the {{Limits}} of {{Semi-Supervised Learning}} for {{Automatic Speech Recognition}}},
  author = {Zhang, Yu and Qin, James and Park, Daniel S. and Han, Wei and Chiu, Chung-Cheng and Pang, Ruoming and Le, Quoc V. and Wu, Yonghui},
  year = {2020},
  month = oct,
  journal = {arXiv:2010.10504 [cs, eess]},
  eprint = {2010.10504},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {We employ a combination of recent developments in semi-supervised learning for automatic speech recognition to obtain state-of-the-art results on LibriSpeech utilizing the unlabeled audio of the Libri-Light dataset. More precisely, we carry out noisy student training with SpecAugment using giant Conformer models pre-trained using wav2vec 2.0 pre-training. By doing so, we are able to achieve word-error-rates (WERs) 1.4\%/2.6\% on the LibriSpeech test/test-other sets against the current state-of-the-art WERs 1.7\%/3.3\%.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@misc{zhang_relationship_2017,
  title = {On the {{Relationship Between}} the {{OpenAI Evolution Strategy}} and {{Stochastic Gradient Descent}}},
  author = {Zhang, Xingwen and Clune, Jeff and Stanley, Kenneth O.},
  year = {2017},
  eprint = {1712.06564},
  eprinttype = {arxiv},
  abstract = {Because stochastic gradient descent (SGD) has shown promise optimizing neural networks with millions of parameters and few if any alternatives are known to exist, it has moved to the heart of leading approaches to reinforcement learning (RL). For that reason, the recent result from OpenAI showing that a particular kind of evolution strategy (ES) can rival the performance of SGD-based deep RL methods with large neural networks provoked surprise. This result is difficult to interpret in part because of the lingering ambiguity on how ES actually relates to SGD. The aim of this paper is to significantly reduce this ambiguity through a series of MNIST-based experiments designed to uncover their relationship. As a simple supervised problem without domain noise (unlike in most RL), MNIST makes it possible (1) to measure the correlation between gradients computed by ES and SGD and (2) then to develop an SGD-based proxy that accurately predicts the performance of different ES population sizes. These innovations give a new level of insight into the real capabilities of ES, and lead also to some unconventional means for applying ES to supervised problems that shed further light on its differences from SGD. Incorporating these lessons, the paper concludes by demonstrating that ES can achieve 99\% accuracy on MNIST, a number higher than any previously published result for any evolutionary method. While not by any means suggesting that ES should substitute for SGD in supervised learning, the suite of experiments herein enables more informed decisions on the application of ES within RL and other paradigms.},
  archiveprefix = {arXiv}
}

@inproceedings{zhang_speech_2009,
  title = {Speech Rhythm Guided Syllable Nuclei Detection},
  booktitle = {2009 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  author = {Zhang, Yaodong and Glass, James R.},
  year = {2009},
  month = apr,
  pages = {3797--3800},
  publisher = {{IEEE}},
  address = {{Taipei, Taiwan}},
  abstract = {In this paper, we present a novel speech-rhythm-guided syllablenuclei location detection algorithm. As a departure from conventional methods, we introduce an instantaneous speech rhythm estimator to predict possible regions where syllable nuclei can appear. Within a possible region, a simple slope based peak counting algorithm is used to get the exact location of each syllable nucleus. We verify the correctness of our method by investigating the syllable nuclei interval distribution in TIMIT dataset, and evaluate the performance by comparing with a state-of-the-art syllable nuclei based speech rate detection approach.},
  isbn = {978-1-4244-2353-8},
  langid = {english}
}

@article{zhang_understanding_2017,
  title = {Understanding {{Deep Learning Requires Rethinking Generalization}}},
  author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  year = {2017},
  journal = {International Conference on Learning Representations (ICLR)},
  abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training.  Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice.  We interpret our experimental findings by comparison with traditional models.},
  langid = {english}
}

@inproceedings{zhang_understanding_2021,
  title = {Understanding {{Failures}} in {{Out-of-Distribution Detection}} with {{Deep Generative Models}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}} ({{ICML}} 2021)},
  author = {Zhang, Lily H and Goldstein, Mark and Ranganath, Rajesh},
  year = {2021},
  volume = {139},
  pages = {10},
  publisher = {{PMLR}},
  abstract = {Deep generative models (DGMs) seem a natural fit for detecting out-of-distribution (OOD) inputs, but such models have been shown to assign higher probabilities or densities to OOD images than images from the training distribution. In this work, we explain why this behavior should be attributed to model misestimation. We first prove that no method can guarantee performance beyond random chance without assumptions on which out-distributions are relevant. We then interrogate the typical set hypothesis, the claim that relevant out-distributions can lie in high likelihood regions of the data distribution, and that OOD detection should be defined based on the data distribution's typical set. We highlight the consequences implied by assuming support overlap between in- and out-distributions, as well as the arbitrariness of the typical set for OOD detection. Our results suggest that estimation error is a more plausible explanation than the misalignment between likelihood-based OOD detection and outdistributions of interest, and we illustrate how even minimal estimation error can lead to OOD detection failures, yielding implications for future work in deep generative modeling and OOD detection.},
  langid = {english}
}

@article{zhang_very_2016,
  title = {Very {{Deep Convolutional Networks}} for {{End-to-End Speech Recognition}}},
  author = {Zhang, Yu and Chan, William and Jaitly, Navdeep},
  year = {2016},
  month = oct,
  abstract = {Sequence-to-sequence models have shown success in end-to-end speech recognition. However these models have only used shallow acoustic encoder networks. In our work, we successively train very deep convolutional networks to add more expressive power and better generalization for end-to-end ASR models. We apply network-in-network principles, batch normalization, residual connections and convolutional LSTMs to build very deep recurrent and convolutional structures. Our models exploit the spectral structure in the feature space and add computational depth without overfitting issues. We experiment with the WSJ ASR task and achieve 10.5\textbackslash\% word error rate without any dictionary or language using a 15 layer deep network.},
  langid = {english},
  keywords = {★}
}

@misc{zhao_infovae_2018,
  title = {{{InfoVAE}}: {{Information Maximizing Variational Autoencoders}}},
  shorttitle = {{{InfoVAE}}},
  author = {Zhao, Shengjia and Song, Jiaming and Ermon, Stefano},
  year = {2018},
  month = may,
  eprint = {1706.02262},
  eprinttype = {arxiv},
  abstract = {A key advance in learning generative models is the use of amortized inference distributions that are jointly trained with the models. We find that existing training objectives for variational autoencoders can lead to inaccurate amortized inference distributions and, in some cases, improving the objective provably degrades the inference quality. In addition, it has been observed that variational autoencoders tend to ignore the latent variables when combined with a decoding distribution that is too flexible. We again identify the cause in existing training criteria and propose a new class of objectives (InfoVAE) that mitigate these problems. We show that our model can significantly improve the quality of the variational posterior and can make effective use of the latent features regardless of the flexibility of the decoding distribution. Through extensive qualitative and quantitative analyses, we demonstrate that our models outperform competing approaches on multiple performance metrics.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{zhao_learning_2017,
  title = {Learning Hierarchical Features from Deep Generative Models},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Zhao, Shengjia and Song, Jiaming and Ermon, Stefano},
  year = {2017},
  pages = {4091--4099},
  address = {{Sydney, Australia}}
}

@misc{zhao_learning_2017a,
  title = {Learning {{Hierarchical Features}} from {{Generative Models}}},
  author = {Zhao, Shengjia and Song, Jiaming and Ermon, Stefano},
  year = {2017},
  month = jun,
  eprint = {1702.08396},
  eprinttype = {arxiv},
  abstract = {Deep neural networks have been shown to be very successful at learning feature hierarchies in supervised learning tasks. Generative models, on the other hand, have benefited less from hierarchical models with multiple layers of latent variables. In this paper, we prove that hierarchical latent variable models do not take advantage of the hierarchical structure when trained with existing variational methods, and provide some limitations on the kind of features existing models can learn. Finally we propose an alternative architecture that do not suffer from these limitations. Our model is able to learn highly interpretable and disentangled hierarchical features on several natural image datasets with no task specific regularization or prior knowledge.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{zhao_stochastic_2014,
  title = {Stochastic {{Optimization}} with {{Importance Sampling}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Zhao, Peilin and Zhang, Tong},
  year = {2014},
  eprint = {1401.2753},
  eprinttype = {arxiv},
  address = {{Lille, France}},
  abstract = {Uniform sampling of training data has been commonly used in traditional stochastic optimization algorithms such as Proximal Stochastic Gradient Descent (prox-SGD) and Proximal Stochastic Dual Coordinate Ascent (prox-SDCA). Although uniform sampling can guarantee that the sampled stochastic quantity is an unbiased estimate of the corresponding true quantity, the resulting estimator may have a rather high variance, which negatively affects the convergence of the underlying optimization procedure. In this paper we study stochastic optimization with importance sampling, which improves the convergence rate by reducing the stochastic variance. Specifically, we study prox-SGD (actually, stochastic mirror descent) with importance sampling and prox-SDCA with importance sampling. For prox-SGD, instead of adopting uniform sampling throughout the training process, the proposed algorithm employs importance sampling to minimize the variance of the stochastic gradient. For prox-SDCA, the proposed importance sampling scheme aims to achieve higher expected dual value at each dual coordinate ascent step. We provide extensive theoretical analysis to show that the convergence rates with the proposed importance sampling methods can be significantly improved under suitable conditions both for prox-SGD and for prox-SDCA. Experiments are provided to verify the theoretical analysis.},
  archiveprefix = {arXiv}
}

@misc{zhao_towards_2016,
  title = {Towards {{End-to-End Learning}} for {{Dialog State Tracking}} and {{Management}} Using {{Deep Reinforcement Learning}}},
  author = {Zhao, Tiancheng and Eskenazi, Maxine},
  year = {2016},
  month = jun,
  eprint = {1606.02560},
  eprinttype = {arxiv},
  abstract = {This paper presents an end-to-end framework for task-oriented dialog systems using a variant of Deep Recurrent QNetworks (DRQN). The model is able to interface with a relational database and jointly learn policies for both language understanding and dialog strategy. Moreover, we propose a hybrid algorithm that combines the strength of reinforcement learning and supervised learning to achieve faster learning speed. We evaluated the proposed model on a 20 Question Game conversational game simulator. Results show that the proposed method outperforms the modular-based baseline and learns a distributed representation of the latent dialog state.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@article{zheng_conditional_2015,
  title = {Conditional {{Random Fields}} as {{Recurrent Neural Networks}}},
  author = {Zheng, Shuai and Jayasumana, Sadeep and Torr, Philip H. S.},
  year = {2015},
  journal = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  pages = {1529--1537}
}

@inproceedings{zhou_computation_1988,
  title = {Computation of {{Optical Flow}} Using a {{Neural Network}}},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Neural Networks}}},
  author = {Zhou, Y. T. and Chellappa, R.},
  year = {1988},
  number = {86},
  pages = {71-78 vol.2},
  publisher = {{IEEE}},
  isbn = {0-7803-0999-5}
}

@inproceedings{zhou_deep_2017,
  title = {Deep Forest: {{Towards}} an {{Alternative}} to {{Deep Neural Networks}}},
  booktitle = {{{IJCAI International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Zhou, Zhi Hua and Feng, Ji},
  year = {2017},
  eprint = {1702.08835},
  eprinttype = {arxiv},
  pages = {3553--3559},
  issn = {10450823},
  abstract = {In this paper, we propose gcForest, a decision tree ensemble approach with performance highly competitive to deep neural networks. In contrast to deep neural networks which require great effort in hyper-parameter tuning, gcForest is much easier to train. Actually, even when gcForest is applied to different data from different domains, excellent performance can be achieved by almost same settings of hyper-parameters. The training process of gcForest is efficient and scalable. In our experiments its training time running on a PC is comparable to that of deep neural networks running with GPU facilities, and the efficiency advantage may be more apparent because gcForest is naturally apt to parallel implementation. Furthermore, in contrast to deep neural networks which require large-scale training data, gcForest can work well even when there are only small-scale training data. Moreover, as a tree-based approach, gcForest should be easier for theoretical analysis than deep neural networks.},
  archiveprefix = {arXiv},
  isbn = {978-0-9992411-0-3},
  keywords = {Deep learning,Ensemble methods}
}

@article{zhou_improved_nodate,
  title = {Improved {{Regularization Techniques}} for {{End-To-End Speech Recognition}}},
  author = {Zhou, Yingbo and Xiong, Caiming and Socher, Richard},
  pages = {5},
  abstract = {Regularization is important for end-to-end speech models, since the models are highly flexible and easy to overfit. Data augmentation and dropout has been important for improving end-to-end models in other domains. However, they are relatively under explored for end-to-end speech models. Therefore, we investigate the effectiveness of both methods for end-to-end trainable, deep speech recognition models. We augment audio data through random perturbations of tempo, pitch, volume, temporal alignment, and adding random noise. We further investigate the effect of dropout when applied to the inputs of all layers of the network. We show that the combination of data augmentation and dropout give a relative performance improvement on both Wall Street Journal (WSJ) and LibriSpeech dataset of over 20\%. Our model performance is also competitive with other end-to-end speech models on both datasets.},
  langid = {english},
  keywords = {⛔ No DOI found}
}

@article{zhou_minimal_2016,
  title = {Minimal Gated Unit for Recurrent Neural Networks},
  author = {Zhou, Guo Bing and Wu, Jianxin and Zhang, Chen Lin and Zhou, Zhi-Hua},
  year = {2016},
  month = mar,
  journal = {International Journal of Automation and Computing},
  volume = {13},
  number = {3},
  eprint = {1603.09420},
  eprinttype = {arxiv},
  pages = {226--234},
  issn = {17518520},
  abstract = {Recently recurrent neural networks (RNN) has been very successful in handling sequence data. However, understanding RNN and finding the best practices for RNN is a difficult task, partly because there are many competing and complex hidden units (such as LSTM and GRU). We propose a gated unit for RNN, named as Minimal Gated Unit (MGU), since it only contains one gate, which is a minimal design among all gated hidden units. The design of MGU benefits from evaluation results on LSTM and GRU in the literature. Experiments on various sequence data show that MGU has comparable accuracy with GRU, but has a simpler structure, fewer parameters, and faster training. Hence, MGU is suitable in RNN's applications. Its simple architecture also means that it is easier to evaluate and tune, and in principle it is easier to study MGU's properties theoretically and empirically.},
  archiveprefix = {arXiv},
  keywords = {★,Deep learning,Gated recurrent unit (GRU),Gated unit,Long short-term memory (LSTM),Minimal gated unit (MGU),Recurrent neural network}
}

@inproceedings{zhu_generative_2016,
  title = {Generative Visual Manipulation on the Natural Image Manifold},
  booktitle = {European {{Conference}} on {{Computer Vision}}},
  author = {Zhu, Jun-Yan and Kr{\"a}henb{\"u}hl, Philipp and Shechtman, Eli and Efros, Alexei A.},
  year = {2016},
  pages = {597--613}
}

@article{zhu_recent_2016,
  title = {Recent Progress in Machine Learning-Based Methods for Protein Fold Recognition},
  author = {Zhu, Jianwei and Zhang, Haicang and Li, Shuai Cheng and Wang, Chao and Kong, Lupeng and Sun, Shiwei and Zheng, Wei-mou and Bu, Dongbo and Wei, Leyi and Zou, Quan},
  year = {2016},
  journal = {International Journal of Molecular Sciences},
  volume = {17},
  number = {12},
  pages = {1--13},
  issn = {14220067},
  abstract = {Knowledge on protein folding has a profound impact on understanding the heterogeneity and molecular function of proteins, further facilitating drug design. Predicting the 3D structure (fold) of a protein is a key problem in molecular biology. Determination of the fold of a protein mainly relies on molecular experimental methods. With the development of next-generation sequencing techniques, the discovery of new protein sequences has been rapidly increasing. With such a great number of proteins, the use of experimental techniques to determine protein folding is extremely difficult because these techniques are time consuming and expensive. Thus, developing computational prediction methods that can automatically, rapidly, and accurately classify unknown protein sequences into specific fold categories is urgently needed. Computational recognition of protein folds has been a recent research hotspot in bioinformatics and computational biology. Many computational efforts have been made, generating a variety of computational prediction methods. In this review, we conduct a comprehensive survey of recent computational methods, especially machine learning-based methods, for protein fold recognition. This review is anticipated to assist researchers in their pursuit to systematically understand the computational recognition of protein folds.},
  isbn = {8617092261008},
  pmid = {27999256},
  keywords = {Computational method,Machine learning,Protein fold recognition}
}

@inproceedings{zhu_s3vae_2020,
  title = {{{S3VAE}}: {{Self-supervised}} Sequential {{VAE}} for Representation Disentanglement and Data Generation},
  booktitle = {Proceedings of the {{Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zhu, Yizhe and Min, Martin Renqiang and Kadav, Asim and Graf, Hans Peter},
  year = {2020},
  pages = {6537--6546},
  address = {{Seattle, WA, USA}}
}

@article{zhu_wav2vec-s_2021,
  title = {Wav2vec-{{S}}: {{Semi-Supervised Pre-Training}} for {{Speech Recognition}}},
  shorttitle = {Wav2vec-{{S}}},
  author = {Zhu, Han and Wang, Li and Hou, Ying and Wang, Jindong and Cheng, Gaofeng and Zhang, Pengyuan and Yan, Yonghong},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.04484 [cs, eess]},
  eprint = {2110.04484},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {Self-supervised pre-training has dramatically improved the performance of automatic speech recognition (ASR). However, most existing self-supervised pre-training approaches are task-agnostic, i.e., could be applied to various downstream tasks. And there is a gap between the task-agnostic pre-training and the task-specific downstream fine-tuning, which may degrade the downstream performance. In this work, we propose a novel pre-training paradigm called wav2vec-S, where we use task-specific semi-supervised pre-training to bridge this gap. Specifically, the semi-supervised pre-training is conducted on the basis of self-supervised pre-training such as wav2vec 2.0. Experiments on ASR show that compared to wav2vec 2.0, wav2vec-S only requires marginal increment of pre-training time but could significantly improve ASR performance on in-domain, cross-domain and cross-lingual datasets. The average relative WER reductions are 26.3\% and 6.3\% for 1h and 10h fine-tuning, respectively.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@misc{zilly_recurrent_2016,
  title = {Recurrent {{Highway Networks}}},
  author = {Zilly, Julian Georg and Srivastava, Rupesh Kumar and Koutn{\'i}k, Jan and Schmidhuber, J{\"u}rgen},
  year = {2016},
  month = jul,
  eprint = {1607.03474},
  eprinttype = {arxiv},
  abstract = {Many sequential processing tasks require complex nonlinear transition functions from one step to the next. However, recurrent neural networks with "deep" transition functions remain difficult to train, even when using Long Short-Term Memory (LSTM) networks. We introduce a novel theoretical analysis of recurrent networks based on Ger\v{s}gorin's circle theorem that illuminates several modeling and optimization issues and improves our understanding of the LSTM cell. Based on this analysis we propose Recurrent Highway Networks, which extend the LSTM architecture to allow step-to-step transition depths larger than one. Several language modeling experiments demonstrate that the proposed architecture results in powerful and efficient models. On the Penn Treebank corpus, solely increasing the transition depth from 1 to 10 improves word-level perplexity from 90.6 to 65.4 using the same number of parameters. On the larger Wikipedia datasets for character prediction (text8 and enwik8), RHNs outperform all previous results and achieve an entropy of 1.27 bits per character.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}


