%!TEX root = ../thesis.tex

\chapter[introduction]{Introduction}\label{chp:introduction}
%
% SIZE: ~9 pages
%
% OUTLINE: Broad introduction to the topic.
% - Motivate the reader.
% - Why should they care about this topic? 
% - What is the historical context of decision-support, what are the practical implications of it, and what happens when it fails.

% \todo[inline]{Lead-in with conversations and the patient interview and how it affects the patient journey. How healthcare has seen rapid advances in medicine and medical devices for diagnostics and treatment but the patient interview and triaging has seen little development.}

In an era where technology increasingly intertwines with healthcare, artificial intelligence is on the verge of becoming part of standard medical practice. 
Growing administrative burdens, aging populations, and rapid scientific progress in medicine and medical devices have created a need to rethink how medical professionals are best enabled to successfully do their job. 
The application of artificial intelligence in medical decision-making and particularly for diagnosing and recommending treatments is likely to hold significant potential to reduce the effect of these issues and improve patient outcomes. 
However, as such decisions impact critical aspects of human health and well-being, this prospect also raises concerns related to the reliability of artificial intelligence systems \parencite{chen_ethical_2021, shailaja_machine_2018, ahmad_interpretable_2018}.

The evolution of technology to support and automate decision-making can be traced back to the development of algorithms and mathematical techniques by ancient civilizations. 
The Egyptian Rhind Mathematical Papyrus, dated to around 1550 BCE, describes various mathematical techniques for doing calculations involving multiplication, division, and fractions, and how to use them for practical purposes like measuring land, constructing buildings, and sharing resources \parencite{georges_universal_2001}. 
Around the same time, the Babylonians had developed a sophisticated system of mathematics that included algorithms for solving quadratic equations, calculating areas of shapes, and numerical methods for approximating square roots \parencite{fowler_square_1998}. 

These computing tools brought with them novel challenges. The Babylonian mathematics, for instance, did not formalize the difference between rational and irrational numbers often leading to approximation errors such as the common practice of approximating $\pi$ with 3 \parencite{georges_universal_2001}. Apart from such inherent limitations of the tools, inadequate human proficiency in their use was also a source of error. For example, the Plimpton 322 clay tablet (\cref{fig:plimpton_332}) contains several errors, presumably made by a novice in Babylonian mathematics \parencite{neugebauer_mathematical_1945,britton_plimpton_2011}. 
% Easy to overlook, the challenges we face in adopting new technologies today are not new but have been faced by humans for thousands of years. Today, as in ancient times, it was important to understand the limitations of technology and the need for human expertise in its use.

\begin{figure}
    \centering
    % \includegraphics[width=0.90\textwidth]{plimpton_322.png}
    \includegraphics[height=0.4\textheight]{plimpton_322.png}
    \caption[Technology is difficult. The scribe of the Plimpton 322 clay tablet ($\sim$1800 BCE) made several errors when listing Pythagorean triplets.]{ Technology is difficult. The scribe of the Plimpton 322 clay tablet ($\sim$1800 BCE) made several errors when listing Pythagorean triplets, presumably struggling to grapple with Babylonian mathematics. \parencite[photo credit][]{neugebauer_mathematical_1945}}
    \label{fig:plimpton_332}
\end{figure}

While efforts to ease human decision-making have been ongoing for thousands of years, it was not until 1642 that the first mechanical calculator was successfully designed by Blaise Pascal. Refined by Gottfried Leibniz during the latter half of the 17th century, the mechanical calculator marked the beginning of a new era where rudimentary calculations could be automated and performed with greater speed and accuracy than humans. 
In the following centuries, the development of computing machines continued, and in the 1820s, Charles Babbage proposed the first programmable mechanical computer, the Difference Engine. Although never completed, it laid the foundation for his later design of the Analytical Engine in 1837 which is considered to be the first general-purpose computer design. 
Working with Babbage, Ada Lovelace is widely regarded as the first to have recognized that programmable computers could have applications beyond pure arithmetic. However, only with the advent of electronics were the first successful general-purpose computers built, starting with the ENIAC in 1945 (\cref{fig:eniac_programmers}).
% While the potential of a general-purpose programmable computer with applications beyond pure arithmetic was recognized by Ada Lovelace,  
% it was not until the 1940s and 50s that % / 
% it took one-hundred years before / 
% the first programmable, electronic, general-purpose digital computers were developed, starting with the ENIAC in 1945 (\cref{fig:eniac_programmers}) \parencite{georges_universal_2001}. 
As manufacturing processes were developed and scaled, electronic computers were adopted widely across society for various applications in science and industry, including healthcare \parencite{georges_universal_2001, harari_sapiens_2011}.
% The societal changes brought about by the widespread adoption of computer technology through the latter half of the 20th century were so profound that some scholars refer to the era as the information age \parencite{georges_universal_2001, harari_sapiens_2011}.

\begin{figure}
    \centering
    % \includegraphics[width=0.95\textwidth]{eniac_programmers.jpeg}
    \includegraphics[height=0.4\textheight]{eniac_programmers.jpeg}
    \caption[Ruth Lichterman (left) and Marlyn Wescoff (middle) were two of the several female programmers of the ENIAC.]{ Ruth Lichterman (left) and Marlyn Wescoff (middle) were two of the several female programmers of the ENIAC. \parencite[photo credit][]{usarmyresearchlaboratoryarltechnicallibrary_female_1940}}
    \label{fig:eniac_programmers}
\end{figure}

With the digital revolution continuing into the 21st century and the worldwide spread of the internet, the amount of data available to decision-makers has grown exponentially. 
This has led to a surge of interest in the scientific field of machine learning, a subfield of artificial intelligence that focuses on developing algorithms that can learn from data to recognize patterns and make predictions to guide decision-making. 
Enabled by a massive increase in the performance and availability of parallel computing resources, such methods have set new standards for the abilities of computer systems in various domains, such as speech recognition, computer vision, and natural language processing \parencite{lecun_deep_2015}. 
In recent years, machine learning has made its way into the healthcare sector, where it has shown promising results in tasks such as medical imaging \parencite{lundervold_overview_2019}, drug discovery \parencite{chen_rise_2018}, and clinical decision support \parencite{cite15, cite14}. 

\begin{figure}
    \centering
    \includegraphics[width=0.90\textwidth]{uber_nhsa_accident.png}
    \caption[A pedestrian was killed by an Uber self-driving car in Tempe, Arizona in 2018.]{ A pedestrian was killed by an Uber self-driving car in Tempe, Arizona in 2018. The car's sensors detected the pedestrian $5.6$ seconds before the crash, but the self-driving system failed to recognize its own uncertainty in multiple object misclassifications and so did not correctly predict her path or reduce the SUV's speed \parencite[photo credit][]{nationaltransportationsafetyboardnhsa_collision_2019}.}
    % \caption{Aerial view of crash location showing path of pedestrian as she attempted to cross N. Mill Avenue and movement and speed of SUV at three points before impact. Pedestrian's path shows her position from initial detection ($5.6$ seconds before impact) until impact; SUV's position is shown at corresponding times beginning $4.2$ seconds before impact \parencite{nationaltransportationsafetyboardnhsa_collision_2019}.}
    \label{fig:uber_nhsa_accident}
\end{figure}

However, these technologies come with the same risks as previous generations did, and in some cases, may amplify them. 
Limitations of the technology itself, human error, and the complexity of the real world are all factors that can lead to failures. A pertinent example is the use of machine learning in autonomous driving systems: 
In 2018, the first incident of a self-driving car killing a pedestrian took place in Tempe, Arizona. 
The car, an SUV operated by Uber, was driving in autonomous mode when it struck a pedestrian crossing the street with her bicycle. 
According to the investigation by the National Transportation Safety Board, the car's sensors detected the pedestrian more than five seconds before the crash, but the system did not manage to correctly predict her path or reduce the SUV's speed. 
Specifically, during those seconds, the system incorrectly classified the pedestrian more than ten times, first as a vehicle, then as an unknown object and ultimately as a bicyclist, each time changing, or resetting, the predicted path. 
About one second before the crash, the system determined that a collision was imminent, but the situation exceeded the constraints within which the autonomous driving system was allowed to operate, and the car's safety driver failed to intervene (\cref{fig:uber_nhsa_accident}) \parencite{nationaltransportationsafetyboardnhsa_collision_2019}. 
%
And Uber is not the only actor facing the challenges of applying machine learning in real-world scenarios. A recent report by the Washington Post found that Tesla's Autopilot system has been activated during at least 736 crashes in the last four years with 16 fatalities \parencite{siddiqui_17_2023}. 
% And though recent statistics published by Waymo, 
% %the self-driving car company owned by Google's parent company Alphabet, 
% revealed far fewer crashes, with only 2 of 20 incidents over 1 million miles meeting reporting criteria, it is interesting to note that the cars are all operated in the sunny states of California and Arizona, where driving conditions vary relatively little \parencite{hawkins_waymo_2023}. 
While driver-assistance systems undoubtedly help reduce accidents in many cases, and has great future potential, the issues of current systems highlight the need for machine learning systems that are robust in rare, adverse or difficult situations. 


Recently, European policymakers proposed a set of requirements to ensure that machine learning applications are safe, reliable, and trustworthy \parencite{europeancommission_briefing_2021}. 
This AI Act groups applications into three categories based on the potential risk they pose to society: limited, high, and unacceptable. Limited risk applications, such as in computer games, must meet transparency requirements, and technologies such as public facial recognition are banned due to unacceptable risk. 
High risk applications include autonomous vehicles and systems within healthcare. To avoid critical errors, these must meet a set of strict requirements focusing on anti-discrimination and robustness.

One of the key ways to improve the safety, reliability and trustworthiness of machine learning systems is to equip them with the ability accurately estimate the uncertainty of their predictions. 
While any model will inevitably make a wrong prediction, by accurately estimating the uncertainty it can communicate to other systems or humans when it is not to be trusted, and so, reduce the risk that wrong predictions lead to catastrophic failures. 
Fittingly, a recent study by the European Parliamentary Research Services concluded: 

\begin{center}

\textit{"Future AI solutions for healthcare should be implemented by integrating uncertainty estimation, a relatively new field of research that aims to provide clinicians with clinically useful indications on the degree of confidence in AI predictions"} \parencite{europeanparliament_artificial_2022}. 

\end{center}

\vspace{0.5em}
\begin{center}
\noindent\rule{0.2\textwidth}{0.5pt}
\end{center}
\vspace{1em}

% OUTLINE: Primer overview of the thesis. What are the topics and what is the context of the research.
% \noindent The research presented in this thesis will focus on the use of machine learning in relation to uncertainty estimation, speech processing, and medical conversations. 
\noindent The research presented in this thesis deals with the use of machine learning for medical conversations with a focus on uncertainty estimation and speech processing. 
The research project was partially funded by a grant from the national Danish Innovation Fund (grant no.\@ 0153-00167B) and defined and carried out in collaboration with the Danish company Corti, which develops computer software for medical decision support. 
In the rest of this chapter, we will provide a high-level introduction to these topics and discuss the motivation behind the research project.


% \todo[inline]{Can we find a better example than the Rhind Papyrus, or an additional one? It could be more on-point with decision-making and more well-known. Ideally, weave in an example of a failure of such technology.}

\section{Uncertainty by example: Corti use-cases}
%
% OUTLINE: Motivate the research project. 
% - What is the context of the research and why is it important? 
% - What are the current applications of machine learning in the domain (decision-support in healthcare)?
% - What are the practical implications of the research? 
% - What are the challenges and limitations of the current state-of-the-art?
%
At the start of this project in 2020, Corti's main product was a software system providing decision-support for call-takers that operate the 1-1-2 emergency line in Copenhagen, Denmark, and the 9-1-1 emergency line in Seattle, Washington, in the United States.
The system is in continued use in 2023, and provides an interface for navigating a graph-based protocol, defined by the local emergency medical services, which dispatchers use to triage patients during emergency calls. 
Calls are transcribed in real-time using automatic speech recognition built for the customer-domain and the system makes suggestions for the dispatcher to use in the conversation with the caller, including notifications about forgotten protocol questions \parencite{havtorn_multiqt_2020} and potential cases of cardiac arrests \parencite{cite15, cite14}. The system logs the details about the call and the actions taken by the dispatcher for later use in review, training and quality assurance. 


\subsection{Stroke recognition in emergency calls} \label{subsec:motivation-stroke-recognition}
%
\paragraph{Case background} In 2021, Corti entered a research collaboration with the Capital Region of Denmark and the Copenhagen Emergency Services to develop a system for recognizing stroke cases in calls to the 1-1-2 emergency line and the 1813 medical helpline. 
Stroke is one of the biggest causes of disability and death worldwide \parencite{cite1,cite2,cite3} and effective treatment is highly time-sensitive \parencite{cite4,cite5}. The most common gateway to specialized treatment and hospital admittance is through prehospital telehealth services like emergency medical call centers, nurse advice call lines, and out-of-hours health services \parencite{cite6,cite7}, however, studies have found that approximately half of all patients with stroke do not receive the correct triage for their condition from call-takers \parencite{cite10,cite11,cite12}. 
This is likely due to the complexity of stroke cases which exhibit a wide range of symptoms that can be difficult to recognize over the phone. Additionally, stroke cases are relatively rare, occurring in only $0.25\%$ of all calls made to the Copenhagen 1-1-2 emergency line in 2021. This makes it difficult for call-takers to gain practical experience with stroke cases which compounds to the difficulty of recognizing them. Although several efforts have been made to improve recognition rates, there is still a need for better tools to support call-takers in recognizing stroke cases \parencite{cite13,cite14,cite15}.

\paragraph{Uncertainty in stroke recognition} A reasonable machine learning pipeline to assist in recognizing stroke cases on calls to emergency services might consist of an automatic speech recognition model that transcribes the conversation and a binary classifier that estimates the probability that the transcript describes a case of stroke. 
Such a system might be trained on a dataset calls with verified stroke and non-stroke cases and evaluated against a held-out test set of calls where the call-taker has indicated whether they suspect a stroke. Due to the low prevalence of stroke cases, the dataset would likely be imbalanced, and the number of stroke cases limited. 
% The performance of such a system would depend on the quality of the automatic speech recognition model, the quality of the classifier, and the quality of the data used to train them. 
In such a system, many factors can lead to classification errors, and when that happens, we would like the model to express high degree of uncertainty. 
For instance, the automatic speech recognition model might make an error in the transcription of particular word or phrase due to a noisy environment, overlapping, slurred or mumbling speech, or words not in the model's training data. 
The classifier might misclassify a conversation due to medically ambiguous symptoms, a general lack of information given in the conversation, or transcription errors made by the speech recognizer. 
How to best make such models accurately represent and estimate the uncertainty of given predictions, and how to use it to improve the value of such a system, especially in cooperation with the call taker, is an open question.
What is clear, however, is that the ability of such a system to accurately estimate the uncertainty of given predictions is crucial for its safe and reliable deployment in the real world. Overconfident predictions could lead to unnecessary delays in treatment, misdiagnoses, increased costs for the healthcare system, and potentially fatal consequences for patients.


\subsection{Medical coding of clinical notes} \label{subsec:motivation-medical-coding}
%
\paragraph{Case background} During the course of this project, Corti's portfolio has expanded to include a software system for medical coding of clinical notes. 
When a patient is admitted to a hospital, the medical staff will write a clinical note describing the patient's diagnosis and the procedures they underwent, including drug prescriptions. 
These notes are then used for billing and reimbursement purposes, as well as for research and quality assurance. 
The clinical notes are written, usually by a doctor, in natural language adhering to a certain structure. Later, a medical coder will assign a set of codes to the note based on its content. 
The process of medical coding is time-consuming and error-prone due to the vast number and high complexity of medical diagnoses and procedures. For instance, the widely used International Classification of Diseases (ICD) standard consist of 55,000 medical codes in version 10 and 85,000 in version 11 \parencite{worldhealthorganisationwho_international_2023}. Additionally, a single medical note will usually contain several diagnoses and procedures which must all be inferred from the clinical note \parencite{johnsonMIMICIIIFreelyAccessible2016,johnsonMIMICIVFreelyAccessible2023}. 
Furthermore, any single code can have several criteria defined by official guidelines that determine under which conditions the code is mutually exclusive with other codes, and when other codes must be coded along with it \parencite{centersformedicaremedicaidservicesus_icd10cm_2023}. 
% Official guidelines describe how to code specific diagnoses and procedures and combinations thereof . 
% For instance, a code may have several exclusion or inclusion criteria which define which other codes must be coded along with it, or when they must not. 

\paragraph{Uncertainty in automated medical coding} A reasonable machine learning pipeline to assist in medical coding might consist of a natural language processing model that extracts relevant information from the clinical note and outputs a number of probability distributions that each estimate the probability that the note should be assigned one of the medical codes. 
However, training and using such as system comes with several challenges. 
For instance, the prevalence of different medical codes in the training data is highly imbalanced; it is common to have several orders of magnitude difference between the frequency of the most frequent code and the least frequent code. Since each clinical note is associated with multiple codes that have complex co-occurrence patterns, it is often impossible to exactly correct for this class imbalance by stratified sampling, which is an otherwise common approach to deal with class imbalance in machine learning. 
% This also gives rise to 
Furthermore, some codes are highly similar, for instance the ICD-10 codes Z87.891 ``Personal history of nicotine dependence'' and F17.210 ``Nicotine dependence, cigarettes, uncomplicated'', but in practice, only one of them should be assigned to a given note. 
As with the stroke recognition system, we would like the model to express high degree of uncertainty when it makes a mistake. For example, if the model is uncertain about two similar codes, it might be appropriate to ask a human expert to review the note and make the final decision. Additionally, the lack of training data for the rarest codes makes them difficult to learn to robustly predict, and so, we would often want the model to be uncertain for rare codes. 


\section{Machine learning reliability} \label{sec:machine-learning-reliability}
%
Several factors define the reliability of a machine learning model. 
Besides model performance and accuracy, important factors include the interpretability of how the model functions, the explainability of its predictions, fairness in its treatment of different groups, and robustness to noise, outliers and adversarial attacks. 
Since many modern machine learning models are deep neural networks with millions or billions of parameters, their size and complexity make them inherently difficult to interpret, and their predictions hard to explain. 
Due to high cost and practical infeasibility, the vast amounts of data needed to train such models are often not manually curated for quality, and so, may contain biases and errors which models are well-equipped to learn to mirror, risking fairness \parencite{burkart_survey_2021}. 
Finally, a number of factors, including the ability of deep neural networks to overfit, or even memorize, their training data \parencite{arpit_closer_2017, burg_memorization_2021}, can lead to models that are sensitive to adverse noise conditions and outliers. 

The ability of a model to accurately estimate the uncertainty of its predictions plays an important role in its reliability. 
While an accurate uncertainty estimate provides a certain degree of interpretability, it especially improves robustness by indicating that uncertain predictions are not to be trusted. 
However, overfitting, memorization and the use of training objectives that are proxies for the evaluation metrics of interest often lead to models that are miscalibrated; that is, the predicted probability of a class does not reflect the true probability of the model being correct \parencite{guo_calibration_2017, kull_temperature_2019}. 
% This means, for instance, that if for a certain input the model assigns 70\% probability to a class, it would not generally be right 70\% of the time - if we could repeat the prediction multiple times under similar circumstances \parencite{guo_calibration_2017, kull_temperature_2019}.
Usually, the predicted probabilities are too extreme which leads to overconfident predictions. This means a model will often assign a high probability to the predicted class, even when it is wrong. 

Miscalibrated models are unreliable, especially in high risk applications. Although no model is perfect, the inability of a human, or another model, to assess the certainty with which a prediction was made, makes it impossible to know how to weigh it in a decision-making process. 
% A connection can be made back to the incident with the Uber self-driving car. 
The multitude of rapid misclassifications made by the Uber self-driving car in Tempe is likely an indication that the model responsible for object-classification was overconfident in its predictions and unable to represent or communicate uncertainty to other systems in the car, or the safety driver. Instead, the model's predictions were taken at face value and subsystems in the car acted on them without appropriate consideration to their reliability.


\subsection{Model calibration} \label{subsec:model-calibration}
% 
The problem of miscalibration in machine learning models is well-known and has been studied for decades \parencite{lewis_sequential_1995, platt_probabilistic_1999, garczarek_classification_2002, zadrozny_transforming_2002, bennett_using_2003, niculescu-mizil_predicting_2005}. One of the best known methods for calibrating a binary classifier is Platt scaling which fits a logistic regression model to the model outputs, assuming that the miscalibration can be corrected by a logarithmic function \parencite{platt_probabilistic_1999}. Another method is isotonic regression which instead fits a non-parametric, monotonic function \parencite{zadrozny_transforming_2002}. 
More recently, \textcite{guo_calibration_2017} proposed a single-parameter variant of Platt-scaling that fits only a temperature parameter on the logits of a neural network classifier. 
These methods are generally simple and effective, but not without limitations. 
To perform the calibration, most of the methods require a held-out validation set on which the model was not trained, Platt scaling and isotonic regression are not naively applicable to multi-class classification problems, and how to calibrate models for structured prediction tasks, such as speech recognition and machine translation, remains an open problem \parencite{astudillo_uncertainty_2010, astudillo_integration_2013, jayashankar_detecting_2020}.

More importantly, correct calibration of a machine learning model does not guarantee that the predicted probability is accurate. 
Specifically, even a well-calibrated model can be overconfident for data that were not presented to it during training such as rare events, outliers, and adverse examples, sometimes collectively referred to as out-of-distribution examples. 
For instance, take a perfectly calibrated model trained to classify images of cats and dogs. If presented with an image of a horse, it has no option but to distribute 100\% total probability across the cat and dog categories, even though that is clearly wrong. 
Worse yet, to indicate the uncertainty, we might expect the model to assign 50\% probability to each of the cat and dog categories, but that behavior is not guaranteed. 
Specifically, since no horses were in the training data, the model will not have learned specialized features for horses, nor learned to associate relevant known features with horses. So, if a particular horse has features that resemble those learned for a dog more than for a cat, the model might assign arbitrarily high probability to the dog class. Therefore, even perfectly calibrated models risks being confidently wrong \parencite{zhou_survey_2022}. 


\subsection{Understanding uncertainty} \label{subsec:understanding-uncertainty}
% 
% OUTLINE: Introduce aleatoric and epistemic uncertainty. 
% - Discuss the distinction between them and how they relate to the problem of uncertainty estimation in machine learning.
% - 
% 
As hinted at in the previous section, there are different sources of uncertainty in machine learning models. 
At a fundamental level, we can decompose the \textit{predictive uncertainty} into uncertainty that is present in the knowledge we have, and uncertainty that originates from the knowledge that we do not have.
These sources are sometimes called \emph{known unknowns} and \emph{unknown unknowns}, or referred to in terms of \textit{aleatoric uncertainty} and \textit{epistemic uncertainty} \parencite{kendall_what_2017}. 
Aleatoric comes from the Latin word 'aleatorius' for 'dice player' or 'gambler' and refers to uncertainty present in the data itself due to randomness in the process that generated it. This kind of uncertainty is commonly seen as irreducible but, since it is represented in the collected data, it can usually be modelled directly. 
Epistemic comes from the Greek word 'epistḗmē' which means 'knowledge' and refers to uncertainty due to things one could in principle know, but does not in practice. Such lack of knowledge could for instance be due to a lack of data, or an improper model specification. 
Aleatoric uncertainty is sometimes referred to as \textit{stochastic uncertainty} and epistemic uncertainty as \textit{systematic uncertainty} \parencite{kendall_what_2017}. 
% Although the distinction between aleatoric and epistemic uncertainty is important, it is not always useful. For instance, a model may be uncertain about a particular prediction due to any of these two kinds of uncertainty. For that reason, the quantity of interest is usually the combined aleatoric and epistemic uncertainties, often referred to as \textit{predictive uncertainty}. 

An example of aleatoric uncertainty is the uncertainty in the transcription of a word due to a noisy environment or overlapping, slurred or mumbling speech. 
Unknown words are arguably sources of epistemic uncertainty although a good speech recognition model may generalize well if the word's spelling and pronunciation follow the same patterns as the words in the training data. Another example of epistemic uncertainty is the occurrence of truly out-of-distribution examples, such as the horse in the image classifier example from earlier: inputs unlike anything the model has seen during training. %, especially if they require outputs unavailable to the model. 
In high-dimensional data with a practically unlimited diversity in out-of-distribution examples, it is impossible to collect enough data to eliminate all potential sources of epistemic uncertainty: By including horses in the training data, the model would likely learn to recognize them, but would still be unable to recognize giraffes, or zebras, or unicorns. 
Ultimately, we must accept that any practical machine learning model will have some sources of epistemic uncertainty. 

Since aleatoric uncertainty is present in the data, it can usually be modelled directly. For instance, in the case of speech recognition, we can model the uncertainty in the transcription of a word by a distribution over the words in the vocabulary. But how do we model epistemic uncertainty? 
Since sources of epistemic uncertainty are by definition those not represented in the data, it is not generally possible to model them directly. This makes epistemic uncertainty more difficult to quantify than aleatoric uncertainty. 
Methods for estimating epistemic uncertainty include Bayesian neural networks \parencite{mackay_practical_1992, neal_bayesian_1995} which represent it by an explicit distribution over learned model parameters. Ensemble methods \parencite{gal_dropout_2016,lakshminarayanan_simple_2017} take a similar approach but use an implicit distribution. 
Other approaches include anomaly detection and the recent field of out-of-distribution detection which, for example, represent epistemic uncertainty by special output classes for out-of-distribution data, distributions over data representations, or distances between them (see \cref{sec:out-of-distribution-detection}). In this thesis, we will focus on the latter approach. 

% In practice, we are interested in the predictive uncertainty, so we need to include both aleatoric and epistemic parts in the uncertainty estimate. 
% Since sources of epistemic uncertainty are by definition those not represented in the data, it is not generally possible to model them directly, as it is for aleatoric uncertainty. This makes epistemic uncertainty more difficult to model and quantify than aleatoric uncertainty. 
% % Methods for estimating epistemic uncertainty include Bayesian neural networks \parencite{mackay_practical_1992, neal_bayesian_1995}, ensembling \parencite{gal_dropout_2016,lakshminarayanan_simple_2017} and a suite of recent research into out-of-distribution detection (see \cref{sec:out-of-distribution-detection}). 
% In contrast to aleatoric uncertainty, which is usually represented by a distribution over the data, epistemic uncertainty is often represented by an implicit or explicit distribution over the model parameters, such as in Bayesian neural networks \parencite{mackay_practical_1992, neal_bayesian_1995} and ensemble models \parencite{gal_dropout_2016,lakshminarayanan_simple_2017}. In most cases, such methods require multiple forward passes through the model to estimate the predictive uncertainty making them computationally expensive and impractical for real-time applications. Within the field of out-of-distribution detection, the epistemic uncertainty is generally not expressed in model parameters, but e.g. by a distribution over data representations, output probabilities with special classes for out-of-distribution data, or by distances in feature spaces. In such cases, the uncertainty can often be estimated in a single forward pass through the model.


\section{Thesis scope and outline}

This introduction provided a high-level overview of the motivation behind the research project by focusing on the use-cases of stroke recognition and automated medical coding as well as speech processing and the importance of uncertainty quantification. 
The remainder of \cref{part:background} consists of two chapters. 
\Cref{chp:main-contributions} presents the research hypotheses and contributions of the thesis via the included papers. 
\Cref{chp:technical-background} provides relevant technical background to the included papers, including uncertainty, out-of-distribution detection, and variational autoencoders. 

\Cref{part:unsupervised-uncertainty-estimation,part:self"=supervised-speech-representation-learning,part:medical-applications} contain a number of chapters that each correspond to a primary paper. 
\Cref{part:unsupervised-uncertainty-estimation} is made up of three papers on variational autoencoders and their applications to out-of-distribution detection and speech modelling \parencite{havtorn_hierarchical_2021,havtorn_benchmarking_2022,bergamin_modelagnostic_2022}. 
\Cref{part:self"=supervised-speech-representation-learning} is a single, comprehensive review paper on self"=supervised speech representation learning \parencite{mohamed_selfsupervised_2022}. 
\Cref{part:medical-applications} consists of two papers on applications of machine learning within the medical domain, specifically recognition of stroke cases in calls to medical helplines \parencite{wenstrup_retrospective_2023} and medical coding of clinical notes \parencite{edin_automated_2023}. 

Finally, \cref{part:discussion-and-conclusion} concludes the thesis by summarizing the main contributions and discussing the presented work and future directions for research.
