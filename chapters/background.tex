%!TEX root = ../thesis.tex

% ~8 pages

\chapter[technical background]{Technical background}\label{chp:technical-background}

While the papers in this thesis were written to be self-contained, page limits and the need to maintain a specific focus forced us to limit the scope of the technical background that each paper provides. In this chapter, we present a more in-depth overview of the technical background relevant to the papers in this thesis. 
We start by quantifying the concept of uncertainty relating it to information and probability. We then introduce the task of out-of-distribution detection and review the existing literature on the topic. 
Finally, we describe variational autoencoders, how they can be used to learn representations of speech, and some of their challenges. %Finally, we introduce the concept of self"=supervised learning and highlight important differences to variational autoencoders. 


\section{Uncertainty and information} \label{sec:uncertainty-information-theory}

In society, the concept of uncertainty goes by many names, and its meaning can vary depending on the specific context. However, across most quantitative scientific fields, a consensus definition appears to align with the following \parencite{hubbard_how_2014}:
%
\begin{quote}
    \centering\itshape
    Uncertainty is the lack of certainty; a state of limited knowledge where it is impossible to exactly describe the existing state, a future outcome, or more than one possible outcome.
\end{quote}
%
Although such a purely lexical definition of uncertainty might prompt a philosophical inquiry, it highlights an important connection between uncertainty, information and probability. When information is limited, we can describe the state of the world only with uncertainty; we are simply not certain. In that sense, uncertainty is a word we use to refer to missing information, whether we know what is missing or not. 
A natural way to describe an uncertain world state is to use probabilities. To our luck, information theory provides exactly that; a mathematically rigorous method for quantifying information through the language of probability theory. In the following, we shall see that, in this context, uncertainty can be understood as the information entropy of a random variable \parencite{mackay_information_2003}. 


\subsection{Entropy for discrete random variables}

First characterized by Claude Shannon in 1948 \parencite{shannon_mathematical_1948}, information entropy, $H(X)$, is a measure of the average amount of information contained in a discrete random variable $X$. Shannon's definition follows from three fundamental axioms of information theory. Let $I_X(x)$ be the information carried by a specific outcome $x$ of sampling the discrete random variable $X$ with probability mass function $p_X(x)$. Then, these axioms are as follows:
\begin{enumerate}[label=(\Roman*)]
    \item The more likely an outcome is, the less information it carries; $I_X(x)$ is a monotonically decreasing function in $p_X(x)$.
    \item Outcomes that are certain to happen carry no information; if $p_X(x) = 1$ then $I_X(x) = 0$.
    \item The joint information carried by independent outcomes is the sum of the information carried by each outcome; if $x_i$ and $x_j$ are independent then $I(x_i, x_j) = I(x_i) + I(x_j)$. 
\end{enumerate}

From these axioms, Shannon found that the \emph{information content} $I_X(x)$ of an outcome $x$ with probability $p_X(x)$ can suitably be defined as the negative logarithm of the probability of the outcome,
%
\begin{equation} \label{eq:information-content}
    I_X(x) = -\log_b p_X(x) \enspace .
\end{equation}
%

The information entropy of a discrete random variable $X$ is defined as the \emph{expected information content} of an outcome of $X$,
%
\begin{equation} \label{eq:information-entropy}
    H(X) = \mathbb{E}_X\left[I_X(x)\right] = - \sum_{x\in X} p_X(x) \log p_X(x) \enspace .
\end{equation}
%
This means that $H(X)$ measures the amount of information we can expect to gain from observing an outcome of $X$, when we know only its distribution $p_X(x)$. If all possible outcomes are equally likely, then $H(X)$ is maximized, and if only one outcome is possible, then $H(X)$ is minimized.

\todo[inline]{Come up with a suitable example explaining the intuition behind information entropy.}


\subsection{Entropy for continuous random variables}

To generalize the concept of information entropy to continuous random variables, Shannon originally replaced the sum over the probability mass function in \eqref{eq:information-entropy} with an integral over the probability density function, as suggested by the definition $H(X) = \mathbb{E}_X\left[I_X(x)\right]$. This approach leads to the definition of the information entropy of a continuous random variable $X$ as, 
%
\begin{equation} \label{eq:differential-entropy}
    H(X) = - \int_{\mathcal{X}} p_X(x) \log p_X(x) \, dx \enspace ,
\end{equation}
%
which is called the \emph{differential entropy} \parencite{shannon_mathematical_1948}. Here, $\mathcal{X}$ is the support of $X$. 

However, the differential entropy does not have several of the desirable properties of the discrete version; it can be negative, it is not invariant under a change of variables, and it is not dimensionally correct%
\footnote{\label{fn:dimensional-analysis-of-differential-entropy}
    Information entropy has the same dimensionality as information content which is typically measured in units of bits, but depends on the base of the logarithm. 
    However, the bit is not a base unit of the International System of Units (SI), nor is it an official derived unit.
    A bit is quite simply a number, specifically 0 or 1. If something is said to have a size of 4 bits, it means that it can be described with 4 binary digits, i.e. 4 numbers, each either 0 or 1.

    As such, it might be useful to think of information as a dimensionless quantity rather than a quantity with units.
    Following this interpretation, the discrete information entropy in \cref{eq:information-entropy} is also dimensionless, because a probability mass function is dimensionless. 
    Since probability density functions have dimensionality of the inverse of some quantity, e.g. inverse length, the dimensionality of the continuous differential entropy in \cref{eq:differential-entropy} becomes,
    \begin{equation*}
        \text{dim} \left( H(X) \right) = \text{dim} \left( \int_{\mathcal{X}} p_X(x) \log p_X(x) \, dx \right) = \frac{1}{\text{length}} \log\left(\text{length}\right) \enspace ,
    \end{equation*}
    which is clearly not dimensionless and therefore cannot correspond directly to the discrete information entropy.
    % As a final twist on this story, note that the SI system does actually have a unit for an amount of substance: the mole \parencite{internationalbureauofweightsandmeasures_systeme_2019}. 
    % The mole is defined as the amount of substance that contains as many elementary entities as there are atoms in $\SI{12}{g}$ of carbon-12, that is $\SI{1}{mol} = 6.02214076\times10^{23}$.
    % Taking the position that a mole is quite simply a number (not a number of particles) \parencite{debievre_atomic_1992,baranski_atomic_2012}, and a bit is a number of elementary (information) entities, we could define a bit as a derived unit from the mole as $\SI{1}{bit} = \SI{1}{mol} / 6.02214076\times10^{23}$.
}. 
For these reasons, the differential entropy may not be a suitable measure of information, or uncertainty, for continuous random variables. 

Instead, \textcite{jaynes_information_1957,jaynes_prior_1968} argued that the information entropy of a continuous distribution should be defined as the limiting density of increasingly dense discrete distributions, $H_{N\rightarrow\infty}(X)$. This argument leads to,
%
\begin{equation} \label{eq:corrected-differential-entropy-jaynes}
    H_{N\rightarrow\infty}(X) \equiv \lim_{N\rightarrow\infty} [H_{N}(X) - \log N] = - \int_{\mathcal{X}} p(x) \log \left(\frac{p(x)}{q(x)}\right) \, dx \enspace ,
\end{equation}
%
where $H_N(X)$ is the limiting density for discrete points, $q(x)$ is a uniform density over the quantization of the continuous space, and we have subtracted a $\log N$ term that would go to infinity in the limit of infinite points.
By doing this, the information entropy becomes positive, dimensionally correct, and invariant under a change of variables%
\footnote{\label{fn:change-of-variables-invariance-of-corrected-differential-entropy}
    Under a change of independent variable from $x$ to $y(x)$, we have that
    \begin{align*}
        \widetilde{w}(y)\,dy = w(x)\,dx \enspace , \quad
        \widetilde{q}(y)\,dy = q(x)\,dx \enspace .
    \end{align*}
    Plugging this into the differential entropy \cref{eq:corrected-differential-entropy-jaynes} we arrive at the original expression but now with $y$ as the independent variable,
    \begin{equation*}
        H_{N\rightarrow\infty}(y(x)) =  = - \int_{\mathcal{Y}} \widetilde{w}(y)\,\frac{dy}{dx} \log \frac{\widetilde{w}(y)\frac{dy}{dx}}{\widetilde{q}(y)\frac{dy}{dx}} \, dx = - \int_{\mathcal{Y}} w(y) \log \frac{w(y)}{q(y)} \, dy \enspace .
    \end{equation*}
}.

The form on the right-hand side of \cref{eq:corrected-differential-entropy-jaynes} can be recognized as the negative Kullback-Leibler divergence of $p(x)$ to $q(x)$ \parencite{kullback_information_1959},
%
\begin{equation} \label{eq:kullback-leibler-divergence}
    D_{KL}(p(x)\parallel q(x)) = \int_{\mathcal{X}} p(x) \log \left(\frac{p(x)}{q(x)}\right) \enspace .
\end{equation}
%
The Kullback-Leibler divergence $D_{KL}(p(x)\parallel q(x))$, also known as the \emph{information gain}, is often interpreted as the amount of additional information required to represent events from a distribution with density $p(x)$ using a code optimized for a distribution with density $q(x)$. In other words, it measures how surprised one would be if they used distribution $q$ to represent events from distribution $p$, i.e. the \emph{relative entropy} of $p$ with respect to $q$.

Returning to Jaynes' argument and \cref{eq:corrected-differential-entropy-jaynes}, we can interpret this to say that the information entropy of a continuous random variable $X$ should be defined as the expected difference in information entropy between its density and a uniform density. 
Identifying the Kullback-Leibler divergence as a measure of this relative information entropy provides a natural point of convergence for this section since it is equivalently defined for both discrete and continuous random variables, contrary to the differential entropy. 
In this view, information entropy is a quantity that distills the distribution of a random variable into a single number that describes the diversity of the potential outcomes of the random variable.

The Kullback-Leibler divergence plays a central role in the training of variational autoencoders, which we introduce in \cref{sec:variational-autoencoders}. In \cref{chp:paper-hierarchical} we shall see how the Kullback-Leibler divergence emerges in a likelihood-ratio test statistic for out-of-distribution detection using hierarchical variational autoencoders. 
% In the following, we will see that this interpretation of information entropy as a measure of diversity is useful for understanding the concept of uncertainty in machine learning.

% In fact, Kullback initially motivated the divergence as an expected likelihood ratio \cref{kullback_information_1959}.


\section{Out-of-distribution detection} \label{sec:out-of-distribution-detection}
% 
% OUTLINE: C.f. https://openaccess.thecvf.com/content/CVPR2023W/VAND/papers/Graham_Denoising_Diffusion_Models_for_Out-of-Distribution_Detection_CVPRW_2023_paper.pdf
% - Formalize the problem. https://arxiv.org/pdf/2302.10326.pdf
%   - Assume we have a dataset $\mathcal{D}_{\text{train}}$ of in-distribution data and we would like to build a model that can be used to assess whether a certain data point $\xb$ is sampled from $\mathcal{D}_{\text{train}}$ or not. Given a test point $\xb$, the model outputs a score $s(\xb)$ that indicates whether $\xb$ is in-distribution or out-of-distribution. 
%   - In the context of neural networks, this problem dates back to at least 1994 \parencite{bishop_novelty_1994}. 
%
% - Categorisation into supervised and unsupervised. 
%   - Supervised: Whether p(y|x) should be trusted for a given x. 
%   - Unsupervised: Whether x should be trusted and used at all. 
%
In this section we introduce out-of-distribution (OOD) detection and connect it to the concept of uncertainty. 
Since the related work sections of the papers in \cref{chp:paper-hierarchical,chp:paper-modelagnostic} are relatively short, we also provide a concise review of the literature on out-of-distribution detection. 

\subsection{An overview of the task}
%
OOD detection is the task of identifying test data that is unlikely to originate from the distribution of the training data and, in the context of neural networks, dates back several decades \parencite{bishop_novelty_1994, chang_figure_1993}.
In the general case, we assume that we have a domain of in-distribution data, $\mathcal{D}_{\text{in}}$, and we would like to build a model that can be used to assess whether a test data point $\xb$ originates from that domain or not. 
Since such OOD data is unknown to the model, it is, by definition, a source of epistemic uncertainty. This makes OOD detection closely related to uncertainty quantification. 
% Only in cases where strong prior knowledge of the data is incorporated into a model, can we expect it to be able to make reliable predictions on OOD data.

% In the literature, the task is also referred to as open set recognition \parencite{bendale_open_2016, cardoso_weightless_2017, panaredabusto_open_2017}, outlier detection \parencite{xu_deep_2014, mor_confidence_2018}, or selective prediction \parencite{geifman_selective_2017} . 
OOD detection bears many similarities with anomaly detection, novelty detection, open set recognition, and outlier detection \parencite{yang_generalized_2022}. 
Some of these differences are subtle, and the terminology is not always used consistently in the literature. To provide some clarity, we will briefly outline the taxonomy of \textcite{yang_generalized_2022}: 
Outlier detection most clearly differs from the other tasks by directly processing all observations and aiming to select outliers from a single contaminated dataset. 
The remaining tasks differ in whether they detect both covariate and semantic shift and require the simultaneous classification of in-distribution classes. %, and whether the in-distribution data contains more than one class. 
Anomaly detection deals with multiclass data and detects both covariate and semantic shift without requiring simultaneous classification of in-distribution classes whereas novelty detection, open set recognition and OOD detection are usually concerned with semantic shifts. 
Although a vague difference, novelty detection usually defined in terms of a class of normal data, while OOD detection centers around the distribution of the training data. Different from novelty detection, OOD detection methods sometimes draw on examples of OOD data or require the simultaneous classification of in-distribution classes, although this is not always the case. 
OOD detection benchmarks almost always take OOD data to be from external datasets different from the training dataset which distinguishes it from open set recognition that usually uses a single dataset split into ID and OOD data. 
% Another closely related field is that of network-based uncertainty estimation \parencite{neal_bayesian_1995, hernandez-lobato_probabilistic_2015, gawlikowski_survey_2023}. %[14, 10, 36, 20]. 
Although we share the sentiment of \textcite{yang_generalized_2022}, who propose to unify the tasks as ``generalized out-of-distribution detection'', this thesis will follow the nomenclature used in the works most related to ours and refer to the task as out-of-distribution detection.

Recently, several approaches for deep neural networks have been developed that address the rejection of samples $\xb\notin\mathcal{D}_{\text{in}}$. 
One way to categorize the different approaches is based on whether the underlying model is a classifier that estimates a conditional probability distribution $p(y|\xb)$ over some target variable $y$, or a model that learns a probability density $p(\xb)$ over the input itself. We might refer to the former as \emph{supervised} OOD detection, and the latter as \emph{unsupervised} OOD detection \parencite{graham_denoising_2023,liu_unsupervised_2023a}. It is important to note that this distinction relates only to the availability of some target value, $y$ - not whether OOD data is available for supervision\footnote{
    In our overview, we distinguish between supervised and unsupervised out-of-distribution by letting methods be classified as supervised that use any kind of target value $y$, whether it relates to an original (in-distribution) task, available OOD data, or both. This is the same distinction made by \textcite{graham_denoising_2023} and \textcite{liu_unsupervised_2023a}. 
    It is important to note, however, that in other works, the distinction is made based on whether the model is trained on OOD data or not \parencite{hendrycks_baseline_2017,liu_energybased_2020}. This difference in nomenclature is currently unresolved in the literature. 
}. 
Supervised OOD detection tries to assess whether the model's prediction $\hat{y}$ via $p(y|\xb)$ should be trusted for a given input $\xb$, whereas unsupervised OOD detection judges whether the input $\xb$ should be trusted and used at all. 

Central to any OOD detection method is the ability to assign a score $s(\xb) \in \mathbb{R}$ to a given input $\xb$ that indicates the degree to which the input is likely to be OOD. After defining a score, we typically use a validation set to tune a threshold $\tau$ such that $\xb$ is considered OOD if $s(\xb) > \tau$. The threshold is typically chosen such that performance on the validation set is above some level, for instance by imposing constraints on the recall and precision. Many works also evaluate the performance of OOD detection methods using the area under the receiver operating characteristic (AUROC) curve which does not require a threshold to be chosen.

There are several ways to define a score $s(\xb)$, and they be used to further categorize OOD detection methods. 
Unsupervised methods often derive the score 
from the \emph{likelihood} assigned to the input \parencite{bishop_novelty_1994,choi_waic_2019,kirichenko_why_2020,ren_likelihood_2019,serra_input_2020,xiao_likelihood_2020,morningstar_density_2021,nalisnick_detecting_2019,bergamin_modelagnostic_2022,maaloe_biva_2019,havtorn_hierarchical_2021}, 
from a \emph{reconstruction} of the input \parencite{sakurada_anomaly_2014,xia_learning_2015,lyudchik_outlier_2016,zhou_anomaly_2017,chen_outlier_2017,schlegl_unsupervised_2017,zong_deep_2018,li_madgan_2019,graham_denoising_2023,liu_unsupervised_2023a}, or 
from a hidden \emph{representation} of the input \parencite{denouden_improving_2018,hendrycks_using_2019,ahmadian_likelihoodfree_2021,bergman_classificationbased_2020,tack_csi_2020,sehwag_ssd_2021,xiao_we_2021}.  % https://arxiv.org/pdf/2302.10326.pdf
Supervised methods often derive the score 
from the \emph{probabilities} given by the predictive distribution $p(y|\xb)$ \parencite{hendrycks_baseline_2017,hendrycks_scaling_2022}, 
from the \emph{logits} of $p(y|\xb)$ \parencite{hendrycks_scaling_2022,liu_energybased_2020}, or 
use a latent \emph{representation} of the input \parencite{lee_simple_2018,li_anomaly_2019,ndiour_outofdistribution_2020,cook_outlier_2020,zaeemzadeh_outofdistribution_2021}, similar to unsupervised methods. 
In the following sections, we provide a more in-depth overview of the different approaches to OOD detection following the above categorization into supervised and unsupervised methods. For the supervised methods we further distinguish between whether methods use real OOD data, synthetic OOD data, or no OOD data at all. 

\subsection{Supervised out-of-distribution detection}
% OUTLINE:
% - Supervised OODD:
%   - Requires some examples of (synthetic) OODD data, which in some sense makes it inherently flawed. We can never really be sure that we have good enough coverage of OODD data.
%   - One limitation of model-dependent OoD techniques is that they may discard information about p(x) in learning the task-specific model p(y|x). That information may be useful for OoD detection.
%   - Likelihood Ratios for Out-of-Distribution Detection \parencite{ren_likelihood_2019} provides a number of baselines for supervised OODD:
%       - 1 Maximum class probability of the predictive distribution p(y|x) \parencite{hendrycks_baseline_2017}
%       - 2 Entropy of predictive distribution p(y|x) \parencite{ren_likelihood_2019}
%       - 3 ODIN \parencite{liang_enhancing_2018} (Uses synthetic OOD data by adding noise to in-distribution)
%       - 4 Mahalanobis distance to class conditional Gaussian distribution \parencite{lee_simple_2018}
%       - 5 DeepEnsemble \parencite{lakshminarayanan_simple_2017} The classifier-based ensemble method that uses the average of the predictions from multiple independently trained models with random initialization of network parameters and random shuffling of training inputs
%       - 6 The log-odds of a binary classifier trained to distinguish between in-distribution inputs from all classes as one class and randomly perturbed in-distribution inputs as the other. \parencite{ren_likelihood_2019}
%       - 7 The maximum class probability over K in-distribution classes of a (K + 1)-class classifier where the additional class is perturbed in-distribution. \parencite{ren_likelihood_2019}
%       - 8 The maximum class probability of a K-class classifier for in-distribution classes but the predicted class distribution is explicitly trained to output uniform distribution on perturbed in-distribution inputs. \parencite{ren_likelihood_2019}. This is similar to using simulated OOD inputs from generative adversarial networks \parencite{lee_training_2018} or using auxiliary datasets of outliers \parencite{hendrycks_deep_2019} for calibration purpose.
%   - X Deep semi"=supervised anomaly detection \parencite{ruff_deep_2020}.
%   - Variational information bottleneck (VIB) (Alemi et al., 2018b) performs divergence estimation in latent space to detect OoD, but is technically a model-dependent technique because the latent code is trained jointly with the downstream classification task. \parencite{alemi_deep_2017}
%
% - ViM: Out-Of-Distribution with Virtual-logit Matchinghttps://arxiv.org/pdf/2203.10807.pdf in cludes a short overview of many modern methods
%
In the supervised setting the supervision can come from different sources depending on how much we know, or assume to know, about the OOD data. Taking supervision from the target variable $y$ requires few assumptions while to representative OOD data. 

% Actual OOD data
\paragraph{Methods using real OOD data}
Methods that use representative OOD data have achieved high performance since they can directly learn to distinguish between in-distribution and OOD data. 
\textcite{hendrycks_deep_2019} augment the original training objective with a task-dependent outlier exposure loss that aims to make the output logits discriminative of outlier data. 
In a similar vein, \textcite{dhamija_reducing_2018} propose losses designed to maximize the entropy of $p(y|\xb)$ and decrease feature magnitudes for OOD data sampled from other datasets. 
\textcite{ruff_deep_2020} use semi"=supervised learning and learn representations of in-distribution data that concentrate close to a centroid in latent space while labeled outliers are pushed away from the centroid. 
Other methods including MCD \parencite{yu_unsupervised_2019}, NGC \parencite{wu_ngc_2021}, and UDG \parencite{yang_semantically_2021} use external, unlabeled, noisy data to improve OOD detection performance without requiring cleanly labeled OOD examples. 
As we discussed earlier, for many modern applications of machine learning, input data is often high dimensional and complex which makes it difficult to obtain enough representative OOD data to ensure robust OOD detection capabilities. 

% Synthetic OOD data
\paragraph{Methods using synthetic OOD data}
A number of methods do not require access to actual OOD data but synthesize it instead. 
Several methods do so by adding noise to in-distribution data \parencite{liang_enhancing_2018, lee_simple_2018, ren_likelihood_2019}. 
For instance, \textcite{ren_likelihood_2019} propose a number of baselines including training a binary classifier to distinguish between original and perturbed in-distribution data. They also propose adding an OOD class to softmax classifiers and training it to predict perturbed in-distribution data, or alternatively, training the predicted class distribution to output uniform distribution for perturbed in-distribution inputs. 
While these methods are appealing, their weaknesses have been pointed out by later work which generally improve on their performance. An example is ODIN \parencite{liang_enhancing_2018}, in which the authors propose to calibrate $p(y|\xb)$ with temperature scaling \parencite{guo_calibration_2017} and add gradient-based, input-dependent perturbations to the inputs and use the calibrated maximum class probability as the OOD score. 
\textcite{vyas_outofdistribution_2018} train an ensemble of classifiers on different subsets of the training data, with the left out data taken as OOD, and propose novel loss over $p(y|\xb)$ that seeks to maintain a predefined margin between its average entropy for the OOD and in-distribution examples. 
Another approach generates OOD inputs using a generative adversarial network \parencite{lee_training_2018}. 
Similar to actual OOD data, the usefulness of synthetic OOD data is also fundamentally limited by the intractability of sampling the complete space of OOD data.

% No OOD data, probs
\paragraph{Methods not using OOD data}
The least informed supervised OOD detection methods do not require instantiations of any kind of OOD data, real or synthetic. 
A baseline approach uses the maximum class probability of $p(y|\xb)$ directly by noting that it tends to be larger for correctly classified examples \parencite{hendrycks_baseline_2017}. Another baseline method proposes that a high entropy of $p(y|\xb)$ indicates an OOD input \parencite{ren_likelihood_2019} 
Other methods that derive the score from the classifier probabilities include \textcite{lakshminarayanan_simple_2017} who propose to use an ensemble of independently trained classifiers to discriminate between in-distribution and OOD data by evaluating the agreement between the classifiers, \textcite{devries_learning_2018} who augment the network with a confidence estimation branch that learns to estimate the confidence of the classifier separately from the probability, and \textcite{huang_importance_2021} who compute the gradient of the KL-divergence of the predictive distribution $p(y|\xb)$ to a uniform distribution noting that the magnitude of gradients is higher for in-distribution data than for OOD data. 
The Variational Information Bottleneck \parencite{alemi_deep_2017} jointly learns a probabilistic latent representation, $\zb$ and $p(y|\xb)$, using a generalized variational autoencoder \parencite{kingma_autoencoding_2014} that tries to maximize the mutual information between $\zb$ and $y$.

\textcite{hsu_generalized_2020} proposes a generalized version of ODIN that removes the need for simulating OOD data.
The authors note that most current methods make a closed world assumption and implicitly condition on the in-domain $\mathcal{D}_{\text{in}}$ in the form of the predictive distribution $p(y|\xb, \mathcal{D}_{\text{in}})$. With this observation, the authors decompose the $p(y|\xb, \mathcal{D}_{\text{in}})$ into a joint class-domain probability and a domain probability,
%
\begin{equation} \label{eq: joint class-domain and domain probability}
    p(y|\xb,\mathcal{D}_{\text{in}}) = \frac{p(y, \mathcal{D}_{\text{in}}|\xb)}{p(\mathcal{D}_{\text{in}}|\xb)} \enspace .
\end{equation}
%
Without data from the out-domain, it is not possible to directly learn either $p(y, \mathcal{D}_{\text{in}}|\xb)$ or $p(\mathcal{D}_{\text{in}}|\xb)$. Instead, the authors use this observation to impose the inductive bias of predicting logits as a fraction between two carefully designed network branches, imitating the form of \cref{eq: joint class-domain and domain probability}. 

% No OOD data, logits
Although some works use the maximum softmax probability as a score for OOD detection \parencite{hendrycks_deep_2019,ren_likelihood_2019}, several works have noted that it is not generally reliable \parencite{hendrycks_scaling_2022,liu_energybased_2020}. 
\textcite{liu_energybased_2020} make an interesting argument as to why based on the energy $E(\xb; f)$ of a softmax classifier $f(\xb)$ \parencite{lecun_tutorial_2006},
%
\begin{equation} \label{eq:softmax-classifier-energy}
    E(\xb; f) = - \log \sum_{i=1}^K \exp \left( f_i(\xb) \right) \enspace .
\end{equation}
%
Specifically, the authors write the maximum softmax probability as,
%
\begin{align} \label{eq:softmax-classifier-maximum-probability}
    \max_y p(y|\xb;f) 
    &= \max_y \frac{\exp f_y(\xb)}{\sum_{i=1}^K \exp f_i(\xb)} \nonumber \\
    &= \frac{\exp f^\text{max}(\xb)}{\sum_{i=1}^K \exp f_i(\xb)} \nonumber \\
    &= \frac{1}{\sum_{i=1}^K \exp \left(f_i(\xb) - f^\text{max}(\xb)\right)} \enspace ,
\end{align}
where $f^\text{max}(\xb) = \max_i f_i(\xb)$. 
The authors then relate the maximum softmax probability to the energy by noting that,
%
\begin{align}
    \log \max_y p(y|\xb;f) = E\left(\xb; f(\xb) - f^\text{max}(\xb)\right) = E(\xb; f) - f^\text{max}(\xb) \enspace .
\end{align}
%
This shows that log of the softmax confidence score is equivalent to the special case of the energy score, where all the logits are shifted by their maximum logit value. 
The authors then note that the energy $E(\xb; f)$ empirically tends to be larger for OOD data than for in-distribution data, while $f^\text{max}(\xb)$ tends to be smaller. 
They conclude that this shift results in the maximum softmax probability being a biased score for OOD detection and propose to instead use the energy directly. This energy-score was improved in ReAct by feature clipping \parencite{sun_react_2021}. 

% No OOD, feature
The final category of supervised methods derive the score from a latent representation of the input. 
Some methods define OOD classes. \textcite{huang_mos_2021} groups the classes of the target variable $y$ and defines an OOD class for each group. Each training example is then the correct target for one group and an OOD example for all other groups; a kind of hierarchical version of the OOD class of simpler baselines based on noise augmentation \parencite{ren_likelihood_2019}. 
Similarly, to represent a virtual OOD class, \textcite{wang_vim_2022} generates an additional logit by first computing the residual of the input's latent space representation against a principal feature space and then converting it to a valid logit by matching its mean over training samples to the average maximum logits. 

Other methods note that the difficulty of detecting OOD data might be attributed to the curse of dimensionality in the learned feature spaces and propose to use dimensionality reduction techniques. In \textcite{ndiour_outofdistribution_2020}, apply dimensionality reduction on learned, high-dimensional features to capture the true feature subspace and compute the norm of the difference between the original feature and the pre-image of its low-dimensional manifold embedding. 
\textcite{zaeemzadeh_outofdistribution_2021} forces the ID samples to embed into a union of 1-dimensional subspaces during training and computes the minimum angular distance from the feature to the class-wise subspaces. 
NuSA \parencite{cook_outlier_2020} uses projects features onto the column space of the classification weight matrix and computes the ratio of the norm the projected and original features.
\textcite{lee_simple_2018} fit a multivariate Gaussian distribution to the activations of the penultimate layer of a pre"=trained classifier and use the Mahalanobis distance to this distribution to evaluate whether inputs are OOD. This method can also be seen as ameliorating the curse of dimensionality by clustering the high-dimensional feature space. 
Finally, Bayesian neural networks have also been proposed for OOD detection, but their performance is not yet competitive with other methods \parencite{henning_are_2021,dangelo_outofdistribution_2022,nguyen_out_2022}. 

% Summary and weaknesses of supervised OOD detection approaches
A general weakness of all supervised out-of-distribution detection is that in learning the task-specific model $p(y|\xb)$ a model may discard information about $p(\xb)$ which could be useful for out-of-distribution detection.
Methods that use real OOD data achieve high performance but are limited by the availability of OOD data and how well it covers the large variation in OOD data.

% This makes them inherently flawed since we can never really be sure that we have good enough coverage of OOD data.
% Furthermore, the methods that use synthetic OOD data are limited by the intractability of sampling the complete space of OOD data. 


%       - 2 Entropy of predictive distribution p(y|x) \parencite{ren_likelihood_2019}
%       - 6 The log-odds of a binary classifier trained to distinguish between in-distribution inputs from all classes as one class and randomly perturbed in-distribution inputs as the other. \parencite{ren_likelihood_2019}
%       - 7 The maximum class probability over K in-distribution classes of a (K + 1)-class classifier where the additional class is perturbed in-distribution. \parencite{ren_likelihood_2019}
%       - 8 The maximum class probability of a K-class classifier for in-distribution classes but the predicted class distribution is explicitly trained to output uniform distribution on perturbed in-distribution inputs. \parencite{ren_likelihood_2019}. This is similar to using simulated OOD inputs from generative adversarial networks \parencite{lee_training_2018} or using auxiliary datasets of outliers \parencite{hendrycks_deep_2019} for calibration purpose.

%       - 3 ODIN \parencite{liang_enhancing_2018} (Uses synthetic OOD data by adding noise to in-distribution)
%       - 4 Mahalanobis distance to class conditional Gaussian distribution \parencite{lee_training_2018}
%       - 5 DeepEnsemble \parencite{lakshminarayanan_simple_2017} The classifier-based ensemble method that uses the average of the predictions from multiple independently trained models with random initialization of network parameters and random shuffling of training inputs

%   - Variational information bottleneck (VIB) (Alemi et al., 2018b) performs divergence estimation in latent space to detect OoD, but is technically a model-dependent technique because the latent code is trained jointly with the downstream classification task. \parencite{alemi_deep_2017}

% either synthetic out-of-distribution data or from real out-of-distribution data. Synthetic out-of-distribution data is data that is generated by some process that is known to be out-of-distribution. For example, if we are training a model to classify images of cats and dogs, we could generate synthetic out-of-distribution data by adding noise to the images. Real out-of-distribution data is data that is known to be out-of-distribution, but is not generated synthetically. For example, if we are training a model to classify images of cats and dogs, we could use images of cars as real out-of-distribution data.


\subsection{Unsupervised out-of-distribution detection}
% OUTLINE:
% - Unsupervised ODDD
%   - No OODD data required. Isolates uncertainty related to the input data.
%   - Unsupervised can be categorised into generative and reconstructive.
%   - Generative
%       - 9 Compute WAIC (\parencite{watanabe_algebraic_2009, watanabe_asymptotic_2010}) using ensemble of generative models \parencite{choi_waic_2019}
%       - Likelihood Ratios for Out-of-Distribution Detection \parencite{ren_likelihood_2019}
%       - Input complexity and out-of-distribution detection with likelihood-based generative models \parencite{serra_input_2020}
%       - Likelihood regret for variational autoencoders \parencite{xiao_likelihood_2020}
%       - Typicality test for generative OODD (Detecting out-of-distribution inputs to deep generative models using typicality) \parencite{nalisnick_detecting_2019}
%       - Density of states estimation for out of distribution detection \parencite{morningstar_density_2021}
%   - Reconstructive
%       - A review of novelty detection \parencite{pimentel_review_2014}
%       - Outlier detection using autoencoders \parencite{lyudchik_outlier_2016} and ensembles of autoencoders \parencite{chen_outlier_2017}
%       - Improving reconstruction autoencoder out-of-distribution detection with mahalanobis distance \parencite{denouden_improving_2018}
%       - Denosing diffusion models for out-of-distribution detection \parencite{graham_denoising_2023}
%       - Diffusion inpainting \parencite{liu_unsupervised_2023a}

% , MC Dropout (Gal & Ghahramani, 2016) and  model a calibrated predictive distribution for a classification task.

For high-dimensional data, the most successful unsupervised models for OOD detection are deep autoencoders \parencite{hinton_reducing_2006}, self"=supervised methods \parencite{mikolov_efficient_2013,devlin_bert_2018,schneider_wav2vec_2019,chen_simple_2020}, and deep generative models \parencite{kingma_autoencoding_2014,rezende_stochastic_2014,dinh_nice_2015,rezende_variational_2015,ho_denoising_2020,hinton_fast_2006,oord_conditional_2016,goodfellow_generative_2014}. 
Other important density estimation methods that have been applied to OOD detection include kernel density estimation \parencite{parzen_estimation_1962}, nearest neighbor methods \parencite{cover_nearest_1967}, support vector machines \parencite{cortes_supportvector_1995,scholkopf_estimating_2001}, and Gaussian mixture models \parencite{dempster_maximum_1977}. 
However, these methods are not well suited for high-dimensional data such as images, text or audio, and have had little direct impact on the recent work on OOD detection. 

% In the following we will review the literature on unsupervised OOD detection using deep generative models and categorize the different approaches based on whether they derive their OOD score from the likelihood, a reconstruction, or a latent representation of the input.

\paragraph{Likelihood-based}
% \parencite{
%     X bishop_novelty_1994,
%     X choi_waic_2019,
%     X kirichenko_why_2020,
%     X ren_likelihood_2019,
%     X serra_input_2020,
%     X xiao_likelihood_2020,
%     X morningstar_density_2021,
%     X nalisnick_detecting_2019,
%     X bergamin_modelagnostic_2022,
%     X maaloe_biva_2019,
%     X havtorn_hierarchical_2021
% },
%
% Among the deep generative models that compute exact likelihoods are flow-based models, autoregressive models, and diffusion models, whereas variational autoencoders compute a lower bound on the likelihood, energy-based models usually require approximate methods, and generative adversarial networks do not compute an explicit likelihood at all.
\textcite{bishop_novelty_1994} first proposed to use the likelihood assigned to data by a generative model as a measure for detecting anomalous data. Simply put, since the likelihood measures ``how probable the data is'', OOD data is expected to give lower likelihoods than in-distribution data. Although, this method originally gave encouraging results, the advent of deep generative models and their application to high-dimensional data has lead many to observe likelihoods for OOD data that are higher than for in-distribution data \parencite{choi_waic_2019,nalisnick_detecting_2019,hendrycks_deep_2019,kirichenko_why_2020}. Such results have sparked interest in trying to explain this phenomenon and many works have proposed new scores for OOD detection derived from the likelihood. 

In \textcite{ren_likelihood_2019}, the authors propose to use the likelihood ratio between a model trained on in-distribution data and a background model trained on perturbed in-distribution data as the OOD score. 
\textcite{serra_input_2020} argue that the failure of deep generative models is due to the high-influence that the input complexity has on the likelihood. Therefore, they propose to use a general lossless image compression algorithm as a background model. 
\textcite{choi_waic_2019} propose to use the Watanabe information criterion (WAIC) computed from the likelihood \parencite{watanabe_algebraic_2009, watanabe_asymptotic_2010}. 
For variational autoencoders, other work proposes to refit the encoder on a test data example, hypothesizing that the likelihood of OOD data will improve more than for in-distribution data, and use this ``likelihood regret'' as the OOD score \parencite{xiao_likelihood_2020}. 
\textcite{maaloe_biva_2019} provide initial results that a loosened variational bound on the likelihood, using only encoded representations from the top-most latent variables in a hierarchical variational autoencoder, can improve OOD detection performance. In \textcite{havtorn_hierarchical_2021}, we show that variational autoencoders are surprisingly good at reconstructing OOD data and propose an improved score based on the likelihood ratio of such loosened bounds.

A different approach is to use the typicality set hypothesis \parencite{nalisnick_detecting_2019}. The typicality set is the subset of the model full support in data space, where the model samples from, that does not overlap with regions of maximal likelihood. \textcite{nalisnick_detecting_2019} propose to use the typicality set as a test statistic for OOD detection while \textcite{morningstar_density_2021} propose to use the related idea of density of states of the model. 
In \textcite{bergamin_modelagnostic_2022} we use Fisher's method \parencite{fisher_statistical_1925} to combine Rao's score test statistic \parencite{rao_large_1948} with the typicality set test statistic hence including information from both the gradient and the likelihood.


\paragraph{Reconstruction-based}
% \parencite{
%     X sakurada_anomaly_2014                 encode and decode data with autoencoders. Dimensionality reduction
%     X lyudchik_outlier_2016                 encode and decode data with autoencoders.
%     X xia_learning_2015                     encode and decode data with autoencoders. We observe that when data are reconstructed from low-dimensional representations, the inliers and the outliers can be well separated according to their reconstruction errors. Based on this basic observation, we gradually inject discriminative information in the learning process of an autoencoder to make the inliers and the outliers more separable.
%     X zhou_anomaly_2017                     encode and decode data with autoencoders. inspired by Robust Principal Component Analysis, and we split the input data X into two parts, , where  can be effectively reconstructed by a deep autoencoder and  contains the outliers and noise in the original data X
%     X zong_deep_2018                        encode and decode data with autoencoders. jointly learn a deep autoencoder and a Gaussian Mixture Model on the learned hidden representations and link their method to neural variational inference \parencite{mnih_neural_2014}.
%     X chen_outlier_2017                     encode and decode data with an ensemble of autoencoders.
%     X schlegl_unsupervised_2017             perform generative adversarial networks inversion for a data point, and evaluate its reconstruction error and discriminator confidence under the inverted latent variable.
%     X li_madgan_2019                        perform generative adversarial networks inversion for a data point, and evaluate its reconstruction error and discriminator confidence under the inverted latent variable.
%     X graham_denoising_2023                 leverages diffusion models to reconstruct images at varied diffusion steps, while we mask and inpaint an image repeatedly with fixed steps.
%     X liu_unsupervised_2023a                LMD lifts an image off its original manifold by corrupting it, and maps it towards the in-domain manifold with a diffusion model. For an out-of-domain image, the mapped image would have a large distance away from its original manifold, and LMD would identify it as OOD accordingly.
% }
%
A number of methods derive the OOD score from a reconstruction error. 
Among the first methods are \textcite{sakurada_anomaly_2014, lyudchik_outlier_2016} who note that dimensionality reduction helps separate inliers and outliers and propose to use deep autoencoders to reconstruct the input and evaluate the reconstruction error. \textcite{xia_learning_2015} take a similar approach but also propose to inject discriminative information in the learning process. 
Drawing inspiration from Robust Principal Component Analysis, \textcite{zhou_anomaly_2017} propose to first split the input data into a dense low-rank factor and a sparse factor, assuming that outliers are caught in the sparse factor, and then use a deep autoencoder to reconstruct the dense factor. 
\textcite{zong_deep_2018} jointly learn a deep autoencoder and a Gaussian Mixture Model on the learned hidden representations and draw parallels of their method to neural variational inference \parencite{mnih_neural_2014}. 
Similarly to the ensemble-based methods for supervised OOD detection, \textcite{chen_outlier_2017} propose to use an ensemble of autoencoders to reconstruct the input and use the median reconstruction error as the OOD score. 

Although generative adversarial networks do not have the ability to encode a given data point, methods have been proposed to invert the generator to find a latent representation that can be used to reconstruct the input and use the reconstruction error as well as a discriminator score for OOD detection \parencite{schlegl_unsupervised_2017,li_madgan_2019}. 

Diffusion models have also been used for OOD detection via a reconstruction-based score. \textcite{graham_denoising_2023} add varying amounts of diffusion noise to an input image and show that reconstructions of OOD inputs from appropriate noise levels fall back onto the in-domain manifold resulting in high reconstruction error. 
\textcite{liu_unsupervised_2023a} lift an image off its original manifold by sampling a number of masks, and then maps it towards the in-domain manifold with a diffusion model, using the median reconstruction error as the OOD score.


% In \textcite{zhou_anomaly_2017} 

\paragraph{Representation-based}
% https://arxiv.org/pdf/2302.10326.pdf
% \parencite{
%     X denouden_improving_2018,              leverages the latent variables of an autoencoder, and evaluates the Mahalanobis distance in the latent space along with the data reconstruction error
%     X ahmadian_likelihoodfree_2021,         extracts low-level features from the encoder of an invertible generative model.
%     X hendrycks_using_2019,                 learn a representation over the in-domain data through self"=supervised training
%     X bergman_classificationbased_2020,     learn a representation over the in-domain data through self"=supervised training
%     X tack_csi_2020,                        learn a representation over the in-domain data through self"=supervised training
%     X sehwag_ssd_2021,                      learn a representation over the in-domain data through self"=supervised training
%     X xiao_we_2021                          further shows that one can instead use a strong pre"=trained feature extractor (self"=supervised)  while maintaining comparable performance.
% }
%
\textcite{denouden_improving_2018} suggest that reconstruction-based approaches fail to capture particular anomalies that lie far from known inlier samples in latent space but near the latent dimension manifold defined by the parameters of the model. They propose to measure the Mahalanobis distance between the global Gaussian distribution of training set in latent space and an encoded test input. 
\textcite{xiao_we_2021} instead propose to use an existing, strong foundation model, pre"=trained with a self"=supervised objective, to extract features from the input, and then fit a Gaussian Mixture Model to the features using the minimal Mahalanobis distance to the mixture components as the OOD score.

Several other works also use self"=supervised representations of the in-domain data for OOD detection \parencite{hendrycks_using_2019,bergman_classificationbased_2020}. 
\textcite{tack_csi_2020} propose to use a contrastive objective to learn representations of the in-domain data contrasted with data augmented in-samples and use a softmax classifier trained on the representations to compute the OOD score. \textcite{sehwag_ssd_2021} present a similar approach but use the feature space Mahalanobis distance, similar to \textcite{denouden_improving_2018}. 
\textcite{ahmadian_likelihoodfree_2021} propose to use the latent representation of an invertible generative model to compute the OOD score.


\section{Variational autoencoders}
%
% \paragraph{Deep generative models} 
VAEs belong to the class of deep generative models which are models that learn a probabilistic representation of the input data and can be used to generate new samples from the same distribution. 
There exist at least six main classes of deep generative models: variational autoencoders \parencite{kingma_autoencoding_2014,rezende_stochastic_2014,ranganath_hierarchical_2016,vahdat_nvae_2020,child_very_2021}, normalizing flow models \parencite{dinh_nice_2015,rezende_variational_2015,dinh_density_2017,kingma_glow_2018,grathwohl_ffjord_2018}, diffusion models \parencite{sohl-dickstein_deep_2015,song_generative_2019, ho_denoising_2020, vahdat_scorebased_2021}, energy-based models \parencite{lecun_tutorial_2006,hinton_fast_2006,salakhutdinov_efficient_2010,du_implicit_2019}, autoregressive models \parencite{oord_conditional_2016,oord_wavenet_2016,radford_improving_2018}, and generative adversarial networks \parencite{goodfellow_generative_2014,arjovsky_wasserstein_2017,brock_large_2019,karras_stylebased_2019}. 
% Besides data generation, common properties of deep generative models include the ability to compute an exact or approximate likelihood of input $p(\xb)$, reconstruct an input $\xb$ from its latent representation $\zb$, and represent an input $\xb$ in a latent space. These properties enable various methods for their use in OOD detection. 

% A variational autoencoder (VAE) \parencite{kingma_autoencoding_2014,rezende_stochastic_2014} is a type of deep generative model and an instance of a latent variable model. 
In the following, we will introduce VAEs starting from the definition of the group of latent variable models to which they belong. We then derive the marginal likelihood objective used to train VAEs, discuss its different components, and highlight some central challenges facing VAEs.


\subsection{Latent variable models}
The fundamental assumption underlying the definition of latent variable models is that a data point $\xb$ is created via a generative process that involves one or more unobserved, stochastic latent variables  $\zb$. 
Latent variable model design centers around capturing this generative process and learning to generate new data points $\xb$. The generative process is usually defined by a joint distribution $p(\xb, \zb)$ over the data and latent variables. 
We can write the marginal distribution of the data as,
%
\begin{equation} \label{eq:latent-variable-model-marginal-distribution}
    p(\xb) = \int p(\xb,\zb) \, d\zb \enspace .
\end{equation}
%
The ability to infer the latent variables $\zb$ from a given data point $\xb$ is of interest in applications that focus on representation learning. Inference is performed by computing the posterior distribution $p(\zb|\xb)$ via Bayes' theorem,
%
\begin{equation} \label{eq:latent-variable-model-posterior-distribution}
    p(\zb|\xb) = \frac{p(\xb,\zb)}{p(\xb)} \enspace .
\end{equation}
%
In many latent variable models, the joint distribution is factorized as $p(\xb,\zb) = p(\xb|\zb)p(\zb)$ including Gaussian mixture models \parencite{dempster_maximum_1977}, hidden Markov models \parencite{rabiner_tutorial_1989}, and probabilistic principal component analysis \parencite{tipping_probabilistic_1999}. 

The marginal distribution $p(\xb)$ involves an integral over all possible values of the latent variable $\zb$ that is often analytically intractable. This is the case when the latent variable space is high-dimensional or the conditional distribution $p(\xb|\zb)$ is complex, as with non-conjugate models or neural networks. 
In these cases, approximate methods such as variational inference \parencite{jordan_introduction_1999} and Markov Chain Monte Carlo (MCMC) \parencite{mohamed_monte_2019} are used to approximately compute the marginal and posterior distributions. 


\subsection{Variational autoencoders} \label{sec:variational-autoencoders}
To model high-dimensional data, \textcite{kingma_autoencoding_2014,rezende_stochastic_2014} propose to parameterize the generative model $p_\thetab(\xb,\zb)$ using a neural network with parameters $\thetab$. Specifically, they let $p(\xb,\zb) = p_\thetab(\xb|\zb)p(\zb)$ and use a neural network for the generative model $p_\thetab(\xb|\zb)$ and any distribution that can be efficiently sampled and evaluated for the prior $p(\zb)$. 
This choice makes both the marginal and posterior distributions intractable and so, training and inference require approximate methods. 
While MCMC methods are computationally expensive for large neural networks, variational inference provides a more scalable alternative by estimating the posterior distribution $p(\zb|\xb)$ with a variational approximation. 


\paragraph{Evidence lower bound}
VAEs amortize the cost of variational inference by parameterizing $q_\phib(\zb|\xb)$ as a single neural network which makes it possible to efficiently compute approximate posterior distributions for any input $\xb$. The approximate posterior can also be used to compute the so-called evidence lower bound (ELBO) on the marginal distribution $p(\xb)$, 
%
 \begin{align} \label{eq:variational-autoencoder-elbo}
    \log p(\xb)
    &= \log \int p_\thetab(\xb|\zb)p(\zb),\textrm{d}\zb \nonumber \\
    &= \log \int q_\phib(\zb|\xb) \frac{p_\thetab(\xb|\zb)p(\zb)}{q_\phib(\zb|\xb)}\,\textrm{d}\zb \nonumber \\
    &\geq \int q_\phib(\zb|\xb) \log \left( \frac{p_\thetab(\xb|\zb)p(\zb)}{q_\phib(\zb|\xb)} \right) \,\textrm{d}\zb \nonumber \\
    &\geq \mathbb{E}_{q_\phib(\zb|\xb)} \left[ \log \frac{p_\thetab(\xb|\zb)p(\zb)}{q_\phib(\zb|\xb)} \right] \equiv \mathcal{L}_{\thetab,\phib}(\xb) \enspace .
\end{align}
%
Although analytically evaluating the expectation is intractable, it can be approximated by sampling from the variational approximation $q_\phib(\zb|\xb)$. 
Since both the generative model $p_\thetab(\xb|\zb)$ and the variational approximation $q_\phib(\zb|\xb)$ are parameterized by neural networks that can be efficiently evaluated, and the prior $p(\zb)$ is simple, this makes it possible to compute the ELBO for any input $\xb$.

Since the joint factorizes in VAEs, we can also write the ELBO in \cref{eq:variational-autoencoder-elbo} as,
%
\begin{align} \label{eq:variational-autoencoder-elbo-factorized}
    \mathcal{L}_{\thetab,\phib}(\xb)
    % &\equiv \mathbb{E}_{q_\phib(\zb|\xb)} \left[ \log \frac{p_\thetab(\xb|\zb)p(\zb)}{q_\phib(\zb|\xb)}\right] \nonumber \\
    &= \mathbb{E}_{q_\phib(\zb|\xb)}\left[\log p_\thetab(\xb|\zb)\right] + \mathbb{E}_{q_\phib(\zb|\xb)} \left[ \log \frac{p(\zb)}{q_\phib(\zb|\xb)}\right] \nonumber \\
    &= \underset{\text{reconstruction loss}}{\underbrace{\mathbb{E}_{q_\phib(\zb|\xb)}\left[\log p_\thetab(\xb|\zb)\right]}} - \underset{\text{KL-divergence to prior}}{\underbrace{D_\text{KL}\left( q_\phib(\zb|\xb) \,||\, p(\zb) \right)}} \enspace .
\end{align}
%
This reveals a commonly used interpretation of the ELBO: 
The expected log-likelihood of the data under the generative model $p_\thetab(\xb|\zb)$ acts as a reconstruction loss encouraging the model to make $q_\phib(\zb|\xb)$ as peaky as possible to make reconstruction easy. 
The negative KL-divergence between $q_\phib(\zb|\xb)$ and the prior $p(\zb)$ acts as a regularizer that forces the approximate posterior to be within the support of the prior (reverse KL-divergence). 
A well-fitted VAE will have a large expected log-likelihood of the data under the generative model enabled by an informative latent variable. However, an optimal KL-divergence of zero entails that $q_\phib(\zb|\xb) = p(\zb)$ which means that $\zb$ is independent of $\xb$. This local optimum is referred to as posterior collapse and is particularly prone to happen for strong generative models, e.g. with autoregressive dependencies $p(\xb_{t}|\xb_{1:t},\zb)$. Some works have proposed to mitigate posterior collapse by tempering the KL-divergence term \parencite{alemi_fixing_2018,higgins_vvae_2017} or by adding additional terms to the ELBO \parencite{zhao_infovae_2018}. 


\paragraph{Sampling}
For some choices of prior and approximate posterior, the KL-divergence term can be computed analytically, but we generally must estimate the reconstruction loss by sampling from the approximate posterior. 
This Monte Carlo estimator can be written as,
%
\begin{equation} \label{eq:variational-autoencoder-elbo-sampling}
    \mathcal{L}_{\thetab,\phib}(\xb)
    = \frac{1}{K} \sum_{k=1}^K \log p_\thetab(\xb|\zb_k) - D_\text{KL}\left( q_\phib(\zb|\xb) \,||\, p(\zb) \right) \enspace ,
\end{equation}
%
where $\zb_k \sim q_\phib(\zb|\xb)$ are samples from the approximate posterior. 
To reduce the variance of the estimate, \textcite{burda_importance_2016} propose to use importance sampling to reweigh the samples.


\paragraph{Reparameterization} 
Via the ELBO, we want to use maximum likelihood estimation to jointly optimize the generative model $p_\thetab(\xb|\zb)$ and the variational approximation $q_\phib(\zb|\xb)$ with respect to $\thetab$ and $\phib$. 
Gradient-based optimization works well for neural networks, but the large amounts of data force the use of mini-batch, or stochastic, gradient descent. 
Computing the gradient of the ELBO with respect to $\thetab$ is straightforward,
%
\begin{equation}
    % \nabla_\thetab \mathbb{E}_{q_\phib(\zb|\xb)} \left[ \log \frac{p_\thetab(\xb|\zb)p(\zb)}{q_\phib(\zb|\xb)} \right] 
    \nabla_\thetab \mathcal{L}_{\thetab,\phib}(\xb)
    = \mathbb{E}_{q_\phib(\zb|\xb)} \left[ \nabla_\thetab \log p_\thetab(\xb|\zb) \right] \enspace .
\end{equation}
%
The gradient with respect to the $\phib$ is more challenging since $\phib$ occurs in the expectation itself making it impossible to move the gradient inside the expectation. 
However, by reparameterizing the latent variable $\zb$ as a deterministic function of the input $\xb$ and a random variable $\epsilonb\sim p(\epsilonb)$, $\zb = g_\phib(\xb,\epsilonb)$, we can write the gradient as a path-wise derivative,
%
\begin{equation}
    % \nabla_\phib \mathbb{E}_{q_\phib(\zb|\xb)} \left[ \log \frac{p_\thetab(\xb|\zb)p(\zb)}{q_\phib(\zb|\xb)} \right] 
    \nabla_\phib \mathcal{L}_{\thetab,\phib}(\xb)
    = \mathbb{E}_{p(\epsilonb)} \left[ \nabla_\phib \log \frac{p_\thetab\left(\xb|g_\phib(\xb,\epsilonb)\right)p\left(g_\phib(\xb,\epsilonb)\right)}{q_\phib\left(g_\phib(\xb,\epsilonb)|\xb\right)} \right] \enspace . \\
\end{equation}
%
The most important instantiation of this reparameterization is the diagonal Gaussian case, where $\zb = \mu_\phib(\xb) + \sigma_\phib(\xb) \odot \epsilonb$ with $\epsilonb\sim\mathcal{N}(0,\mathbf{I})$. 
By assuming the covariance matrix is diagonal, this is a mean-field approximation which imposes independence between the latent variables in $\zb$. 

Although the variance of the path-wise derivative is lower than most alternatives, such as score function estimators, it is not negligible, and a number of works have attempted to reduce it.
\textcite{roeder_sticking_2017} note that a score function estimator can be factored out of the path-wise derivative and propose to ignore it to reduce gradient variance. 
\textcite{rainforth_tighter_2019} show that this problem is exacerbated for the importance weighted ELBO \parencite{burda_importance_2016} and that increasing the number of samples in the importance weighted bound also increases gradient variance for the inference network, hurting its ability to learn useful representations. 
After showing that removing the score function factor introduces bias, \textcite{tucker_doubly_2019} propose to reparameterize it too, giving rise to an unbiased, low variance gradient estimator that improves with more samples. Later work has generalized this estimator to hierarchical models \parencite{bauer_generalized_2021}. 

\paragraph{Hierarchical models}\label{sec:hierarchical-vae-background}
The mean-field assumption imposed by letting $\zb \sim\mathcal{N}(\mub,\mathbf{I}\sigmab^2)$ can often be more restrictive for model expressiveness than we would like. 
While learning a full covariance is a simple solution, it is not always sufficiently computationally efficient and, in any case, enables learning only linear covariance between elements of $\zb$. 
The drawbacks of mean-field approximation and linear covariance has lead to research into learning hierarchies of several non-linearly dependent latent variables; an idea well in line with the usual motivations behind deep neural networks such as efficient, compositional representation \cite{lecun_deep_2015}. 
Such hierarchical models are usually formalized by introducing a set of $L$ latent variables  $\zb=\zb_1, \dots, \zb_L$ and letting the generative model be defined as,
%
\begin{equation}
    p_\thetab(\xb|\zb)=p(\xb|\zb_1) p_\thetab(\zb_1|\zb_2) \cdots p_\thetab(\zb_{L-1}|\zb_L) p(\zb_L) \enspace .
\end{equation}
%
This top-down generative model can be efficiently sampled via ancestral sampling; first $\zb_L$ is drawn from the prior $p(\zb_L)$, and then each $\zb_l$ is drawn from the corresponding $p(\zb_l|\zb_{l+1})$ until we can draw $p(\xb|\zb_1)$. 

The inference model can then be defined in two ways respectively referred to as \emph{bottom-up} \parencite{burda_importance_2016}
\begin{equation}
    %q_\phib(\zb|\xb)=q_\phib(\zb_1|\xb)q_\phib(\zb_2|\zb_1)\cdots q_\phib(\zb_L|\zb_{L-1})
    q_\phib(\zb|\xb) = q_\phib(\zb_1|\xb)\textstyle\prod_{i=2}^{L} q_\phib(\zb_i|\zb_{i-1})
\end{equation}
and \emph{top-down} \parencite{sonderby_ladder_2016}
\begin{equation}
    % q_\phib(\zb|\xb)=q_\phib(\zb_{L}|\xb)q_\phib(\zb_{L-1}|\zb_L)\cdots q_\phib(\zb_2|\zb_1),
    q_\phib(\zb|\xb) = q_\phib(\zb_L|\xb)\textstyle\prod_{i=L-1}^{1} q_\phib(\zb_{i}|\zb_{i+1}) \ .
\end{equation}
A variant of the variational autoencoder that employs a bidirectional inference network has also been proposed \parencite{maaloe_biva_2019}.
Regardless of the choice of inference model, the resulting hierarchical VAE is trained using the ELBO \cref{eq:variational-autoencoder-elbo} and the reparameterization trick.
Recent architectural advances have alleviated the posterior collapse problem and made it possible to train VAEs with many latent variables \parencite{maaloe_biva_2019,vahdat_nvae_2020,child_very_2021}. 

% Until recently, hierarchical VAEs gave inferior likelihoods compared to state-of-the-art autoregressive \parencite{ho_flow_2019} and flow-based models \parencite{salimans_pixelcnn_2017}.
% This was changed by \textcite{maaloe_biva_2019}, \textcite{vahdat_nvae_2020}, and \textcite{child_very_2021}, which introduced complementary methods to extend the number of latent variables to a very deep hierarchy resulting in state-of-the-art likelihood performance.



\paragraph{Mutual information interpretation}
Training by stochastic gradient descent training involves sampling mini-batches of data points $\xb$ from the empirical training data distribution $\hat{p}(\xb)$ and evaluating the gradient of the ELBO with respect to the parameters $\thetab$ and $\phib$ for each mini-batch. 
To gain some additional insight into the objective that we are optimizing, we will exploit this expectation over the data distribution $\hat{p}(\xb)$ to form a factorization similar to \cref{eq:variational-autoencoder-elbo-factorized} but with the aggregated posterior $q_\phib(\zb) = \int q_\phib(\zb|\xb)\hat{p}(\xb)\,\mathrm{d}\xb$ in the KL-divergence \parencite{tomczak_trouble_2022}. 
The marginal likelihood under this expectation is lower bounded by the same expectation over the ELBO,
%
\begin{align} \label{eq:variational-autoencoder-elbo-expected}
    \mathbb{E}_{\hat{p}(\xb)}\left[ \log p(\xb) \right] 
    &\geq \mathbb{E}_{q_{}(\xb,\zb)}\left[ \log \frac{p_{}(\xb|\zb)p(\zb)}{q_{}(\zb|\xb)} \right] \enspace ,
\end{align}
%
where we have defined $q(\xb) = \hat{p}(\xb)$ and $q_{}(\xb,\zb) = q_{}(\zb|\xb)\hat{p}(\xb)$ and dropped the parameter subscripts for ease of notation. 
This allows us to focus on the KL-divergence in \cref{eq:variational-autoencoder-elbo-factorized} which we can rewrite as follows,
%
\begin{align} \label{eq:variational-autoencoder-kl-factor-expected}
    \mathbb{E}_{\hat{p}(\xb)} \left[ D_\text{KL}\left( q_{}(\zb|\xb) \,||\, p(\zb) \right) \right]
    &= \mathbb{E}_{q_{}(\zb,\xb)} \left[ \log \frac{q_{}(\zb|\xb)}{p(\zb)}\right] \nonumber \\
    % &= - \mathbb{E}_{q_{}(\zb)}\left[ \log p(\zb) \right] + \mathbb{E}_{q_{}(\xb,\zb)}\left[ \log q_{}(\zb | \xb) \right] \nonumber \\
    &= \mathbb{E}_{q_{}(\zb)}\left[ \log \frac{1}{p(\zb)} \right] + \mathbb{E}_{q_{}(\xb,\zb)}\left[ \log \frac{q_{}(\xb,\zb)}{q(\xb)} \right] \nonumber \\
    % &= - \mathbb{E}_{q_{}(\zb)}\left[ \log p(\zb) \right] + \mathbb{E}_{q_{}(\xb,\zb)}\left[ \log \frac{q_{}(\xb,\zb)}{q(\xb)} \right] \nonumber \\ 
    % &\qquad + \mathbb{E}_{q_{}(\zb)}\left[ \log q_{}(\zb) \right] - \mathbb{E}_{q_{}(\zb)}\left[ \log q_{}(\zb) \right] \nonumber \\
    &= \mathbb{E}_{q_{}(\zb)}\left[ \log \frac{q_{}(\zb)}{p(\zb)} \right] - \mathbb{E}_{q_{}(\xb,\zb)}\left[ \log \frac{q_{}(\xb,\zb)}{q(\xb)q_{}(\zb)} \right] \nonumber \\
    &= D_\mathrm{KL}\left( q_{}(\zb) \parallel p(\zb) \right) - I_{q_{}(\xb,\zb)}[\xb;\zb] \enspace .
\end{align}
%
Inserting this back into the ELBO in \cref{eq:variational-autoencoder-elbo-factorized}, we get,
%
\begin{equation} \label{eq:variational-autoencoder-elbo-factorized-expected}
    \mathbb{E}_{q_{}(\xb,\zb)} \left[ \mathcal{L}_{\thetab,\phib}(\xb) \right] = 
    \underset{\text{average reconstruction}}{\underbrace{
        \mathbb{E}_{q_{}(\xb,\zb)}\left[ \log p_{}(\xb | \zb) \right]
    }}
    - 
    \underset{\text{marginal KL to prior}}{\underbrace{
    \vphantom{\mathbb{E}_{q_{}(\xb,\zb)}\left[ \log p_{}(\xb | \zb) \right]}{
        D_{\mathrm{KL}}\left[ q_{}(\zb) \parallel p(\zb) \right]
    }
    }} 
    -
    \underset{\text{mutual information}}{\underbrace{
    I_{q_{}(\xb,\zb)}\left[\xb ; \zb \right]
    }} \enspace . \nonumber
\end{equation}
%
In this form, the KL-divergence term of \cref{eq:variational-autoencoder-elbo-factorized} has been factored into the marginal KL-divergence of the aggregated posterior to the prior and the mutual information between the data and the latent variables. 
The marginal KL-divergence is minimized by making the aggregated posterior $q_{}(\zb)$ match the prior $p(\zb)$. Contrary to the KL-divergence term of \cref{eq:variational-autoencoder-elbo-factorized}, driving the marginal KL-divergence to zero does not enforce independence between $\xb$ and $\zb$. 
However, this alternative form reveals that the ELBO is maximized by minimizing the mutual information between $\xb$ and $\zb$, $I_{q_{}(\xb,\zb)}\left[\xb ; \zb \right]$. 
This result indicates that the usual interpretation of $\zb$ as a representation of $\xb$ is not a fundamental property arising from the ELBO and challenges the usefulness of variational autoencoders as representation learners.
% %


\paragraph{Applications of VAEs}
Despite the challenges facing the training of VAEs, they have been successfully applied to a number of tasks including image generation \parencite{kingma_autoencoding_2014,rezende_stochastic_2014}, image inpainting \parencite{pathak_context_2016}, image super-resolution \parencite{sonderby_amortised_2017,chira_image_2022}, and speech synthesis \parencite{hsu_unsupervised_2017,hsu_hierarchical_2019}. 
VAEs have also proven themselves useful for semi"=supervised learning \parencite{kingma_semi-supervised_2014,kingma_improved_2016,maaloe_biva_2019}.


\paragraph{Self"=supervised learning}
An alternative approach to representation learning is provided by self"=supervised learning \parencite{mikolov_efficient_2013,devlin_bert_2018,chen_simple_2020,schneider_wav2vec_2019} which is a form of unsupervised learning where the training objective is derived from the data itself. 
While it is fair to say that not all self"=supervised training objectives are as well-motivated as the ELBO is for VAEs, they have shown impressive results within the fields of natural language processing \parencite{devlin_bert_2018,chen_simple_2020}, speech processing \parencite{schneider_wav2vec_2019}, and computer vision \parencite{chen_simple_2020}. 
For a deeper comparison of VAEs and self"=supervised methods for speech processing, we refer the reader to our paper \textcite{borgholt_brief_2022} included in \cref{app:paper-brief}. 
\Cref{chp:paper-review} provides a comprehensive introduction to and review of self"=supervised learning in speech recognition. 

