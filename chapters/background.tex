%!TEX root = ../thesis.tex

% ~8 pages

\chapter[technical background]{Technical background}\label{chp:technical-background}

While the papers in this thesis were written to be self-contained, page limits and the need to maintain a specific focus, forced us to limit the scope of technical background each paper provides. In this chapter, we present a more in-depth overview of the technical background relevant to the papers in this thesis. 
We start by quantifying the concept of uncertainty, and how it relates to information and probability. We then introduce the task of out-of-distribution detection and a few ways to tackle it. 
Then, we present variational autoencoders, and how they can be used to learn representations of speech. Finally, we introduce the concept of self-supervised learning and highlight important differences to variational autoencoders. 


\section{Uncertainty and information}

In society, the concept of uncertainty goes by many names, and its meaning can vary depending on the specific context. However, across most quantitative scientific fields, a consensus definition appears to align with the following \cite{hubbard_how_2014}:
%
\begin{center}
    \textit{Uncertainty is the lack of certainty; a state of limited knowledge where it is impossible to exactly describe the existing state, a future outcome, or more than one possible outcome.}
\end{center}
%
Although such a purely lexical definition of uncertainty might prompt a philosophical inquiry, it highlights an important connection between uncertainty, information and probability. When information is limited, we can describe the state of the world only with uncertainty; we are simply not certain. In that sense, uncertainty is a word we use to refer to the information that is missing, whether we know what is missing or not. 
A natural way to describe an uncertain world state is to use probabilities. To our luck, information theory provides exactly that; a mathematically rigorous method for quantifying information through the language of probability theory. In the following, we shall see that, in this context, uncertainty can be understood as the information entropy of a random variable \cite{mackay_information_2003}. 


\subsection{Entropy for discrete random variables}

First characterized by Claude Shannon in 1948 \cite{shannon_mathematical_1948}, information entropy, $H(X)$, is a measure of the average amount of information contained in a discrete random variable $X$. Shannon's definition follows from three fundamental axioms of information theory. Let $I_X(x)$ be the information carried by a specific outcome $x$ of sampling the discrete random variable $X$ with probability mass function $p_X(x)$. Then, these axioms are as follows:
\begin{enumerate}[label=(\Roman*)]
    \item The more likely an outcome is, the less information it carries; $I_X(x)$ is a monotonically decreasing function in $p_X(x)$.
    \item Outcomes that are certain to happen carry no information; if $p_X(x) = 1$ then $I_X(x) = 0$.
    \item The joint information carried by independent outcomes is the sum of the information carried by each outcome; if $x_i$ and $x_j$ are independent then $I(x_i, x_j) = I(x_i) + I(x_j)$. 
\end{enumerate}

From these axioms, Shannon found that the \emph{information content} $I_X(x)$ of an outcome $x$ with probability $p_X(x)$ can suitably be defined as the negative logarithm of the probability of the outcome,
%
\begin{equation} \label{eq:information-content}
    I_X(x) = -\log_b p_X(x) \enspace .
\end{equation}
%

The \emph{information entropy of a discrete random variable} $X$ is defined as the \emph{expected information content} of an outcome of $X$,
%
\begin{equation} \label{eq:information-entropy}
    H(X) = \mathbb{E}_X\left[I_X(x)\right] = - \sum_{x\in X} p_X(x) \log p_X(x) \enspace .
\end{equation}
%
This means that $H(X)$ measures the amount of information we can expect to gain from observing an outcome of $X$, when we know only its distribution $p_X(x)$. If all possible outcomes are equally likely, then $H(X)$ is maximized, and if only one outcome is possible, then $H(X)$ is minimized.

\todo[inline]{Come up with a suitable example explaining the intuition behind information entropy.}


\subsection{Entropy for continuous random variables}

To generalize the concept of information entropy to continuous random variables, Shannon originally replaced the sum over the probability mass function in \eqref{eq:information-entropy} with an integral over the probability density function, as suggested by the definition $H(X) = \mathbb{E}_X\left[I_X(x)\right]$. This approach leads to the definition of the \emph{information entropy of a continuous random variable} $X$ as, 
%
\begin{equation} \label{eq:differential-entropy}
    H(X) = - \int_{\mathcal{X}} p_X(x) \log p_X(x) \, dx \enspace ,
\end{equation}
%
which is called the \emph{differential entropy} \cite{shannon_mathematical_1948}. Here, $\mathcal{X}$ is the support of $X$. 

However, the differential entropy does not have several of the desirable attributes of the discrete version; it can be negative, it is not invariant under a change of variables, and it is not dimensionally correct%
\footnote{\label{fn:dimensional-analysis-of-differential-entropy}
    Information entropy has the same dimensionality as information content which is typically measured in units of bits, depending on the base of the logarithm. 
    However, the bit is not a base unit of the International System of Units (SI), nor is it an official derived unit.
    A bit is quite simply a number, specifically 0 or 1. If something is said to have a size of 4 bits, it means that it can be described with 4 binary digits, i.e. 4 numbers, each either 0 or 1.

    As such, it might be useful to think of information as a dimensionless quantity rather than a quantity with units.
    Following this interpretation, the discrete information entropy in \cref{eq:information-entropy} is also dimensionless, because a probability mass function is dimensionless. 
    Since probability density functions have dimensionality of the inverse of some quantity, e.g. inverse length, the dimensionality of the continuous differential entropy in \cref{eq:differential-entropy} becomes,
    \begin{equation*}
        \text{dim} \left( H(X) \right) = \text{dim} \left( \int_{\mathcal{X}} p_X(x) \log p_X(x) \, dx \right) = \frac{1}{\text{length}} \log\left(\text{length}\right) \enspace ,
    \end{equation*}
    which is clearly not dimensionless and therefore does not correspond to the discrete information entropy.
    % As a final twist on this story, note that the SI system does actually have a unit for an amount of substance: the mole \cite{internationalbureauofweightsandmeasures_systeme_2019}. 
    % The mole is defined as the amount of substance that contains as many elementary entities as there are atoms in $\SI{12}{g}$ of carbon-12, that is $\SI{1}{mol} = 6.02214076\times10^{23}$.
    % Taking the position that a mole is quite simply a number (not a number of particles) \cite{debievre_atomic_1992,baranski_atomic_2012}, and a bit is a number of elementary (information) entities, we could define a bit as a derived unit from the mole as $\SI{1}{bit} = \SI{1}{mol} / 6.02214076\times10^{23}$.
}. 
For these reasons, the differential entropy may not be a suitable measure of information, or uncertainty, for continuous random variables. 

Instead, \textcite{jaynes_information_1957,jaynes_prior_1968} argued that the information entropy of a continuous distribution should be defined as the limiting density of increasingly dense discrete distributions, $H_{N\rightarrow\infty}(X)$. This argument leads to,
%
\begin{equation} \label{eq:corrected-differential-entropy-jaynes}
    H_{N\rightarrow\infty}(X) = \lim_{N\rightarrow\infty} [H_{N}(X) - \log N] = - \int_\mathcal{X} p(x) \log \left(\frac{p(x)}{q(x)}\right) \enspace ,
\end{equation}
%
where $H_N(X)$ is the limiting density for discrete points, $q(x)$ is a uniform density over the quantization of the continuous space, and we have subtracted a $\log N$ term that would go to infinity in the limit of infinite points.
By doing this, the information entropy becomes positive, dimensionally correct, and invariant under a change of variables%
\footnote{\label{fn:change-of-variables-invariance-of-corrected-differential-entropy}
    Under a change of independent variable from $x$ to $y(x)$, we have that
    \begin{align*}
        \widetilde{w}(y)\,dy = w(x)\,dx \enspace , \quad
        \widetilde{q}(y)\,dy = q(x)\,dx \enspace .
    \end{align*}
    Plugging this into the differential entropy \cref{eq:corrected-differential-entropy-jaynes} we arrive at the original expression but now with $y$ as the independent variable,
    \begin{equation*}
        H_{N\rightarrow\infty}(y(x)) =  = - \int_\mathcal{Y} \widetilde{w}(y)\,\frac{dy}{dx} \log \frac{\widetilde{w}(y)\frac{dy}{dx}}{\widetilde{q}(y)\frac{dy}{dx}} \, dx = - \int_\mathcal{Y} w(y) \log \frac{w(y)}{q(y)} \, dy \enspace .
    \end{equation*}
}.

The form on the right-hand side of \cref{eq:corrected-differential-entropy-jaynes} can be recognized as the negative Kullback-Leibler divergence of $p(x)$ to $q(x)$ \cite{kullback_information_1959},
%
\begin{equation} \label{eq:kullback-leibler-divergence}
    D_{KL}(P\parallel Q) = \int_\mathcal{X} p(x) \log \left(\frac{p(x)}{q(x)}\right) \enspace .
\end{equation}
%
The Kullback-Leibler divergence $D_{KL}(P\parallel Q)$ is often interpreted as the amount of additional information required to represent events from distribution $P$ with density $p(x)$ using a code optimized for distribution $Q$ with density $q(x)$. In other words, it measures how surprised one would be if they used distribution $Q$ to represent events from distribution $P$, i.e. the \emph{relative entropy} of $P$ with respect to $Q$. 
The KL-divergence is also known as the \emph{information gain}. 

Returning to Jaynes' argument and \cref{eq:corrected-differential-entropy-jaynes}, we can interpret this to say that the information entropy of a continuous random variable $X$ should be defined as the expected difference in information entropy between its density and a uniform density. 
Identifying the Kullback-Leibler divergence as a measure of (relative) information entropy provides a natural point of convergence for this section since it is equivalently defined for both discrete and continuous random variables as opposed to the differential entropy. 
In this view, information entropy (relative or absolute) is a quantity that distills the distribution of a random variable into a single number that describes the diversity of the potential outcomes of the random variable. 

% In fact, Kullback initially motivated the divergence as an expected likelihood ratio \cref{kullback_information_1959}.


% \subsection{Two kinds of uncertainty}

% \todo[inline]{Consider not using the terms aleatoric and epistemic uncertainty. Instead use the terms ``uncertainty due to lack of knowledge (reducible)" and ``uncertainty inherent to the data (irreducible)".}

% Uncertainty can be divided into two kinds: aleatoric and epistemic. 

% Aleatoric uncertainty is defined as being inherent to the data, and is irreducible. 

% Epistemic uncertainty is uncertainty due to lack of knowledge, and can be reduced with more data. 


% Two kinds of uncertainty:\\
% - Aleatoric: Uncertainty inherent to the data. Irreducible. "Known unknowns". \\
% - Epistemic: Uncertainty due to lack of knowledge. Can be reduced with more data. "Unknown unknowns".\\



\section{Unsupervised out-of-distribution detection}

Unsupervised out-of-distribution detection (OOD) is the task of identifying data that is unlikely to originate from the distribution of the training data.



% Out-of-distribution detection and anomaly detection are both related concepts in machine learning and deep learning, but they address slightly different problems and have distinct focuses.

% Out-of-Distribution Detection (OOD Detection):
% Out-of-distribution detection involves determining whether a given input or data point belongs to the same distribution as the training data. In other words, it helps identify whether an input is coming from a category or distribution that the model has never encountered during training. This is especially important for safety-critical applications where the model should not make predictions on inputs that are significantly different from what it has seen before. OOD detection aims to prevent the model from making confident predictions on unfamiliar inputs that might lead to incorrect results.
% Anomaly Detection:
% Anomaly detection, on the other hand, focuses on identifying rare and unusual instances within the data, regardless of whether they belong to the same distribution as the training data or not. Anomalies can be defined as data points that deviate significantly from the norm or expected behavior. Anomaly detection is used to find instances that are different from the majority and might indicate potential errors, fraud, faults, or other unusual occurrences.
% In summary, the key differences are:

% Focus:
% OOD Detection: Focuses on detecting inputs that are coming from distributions that differ significantly from the training data distribution.
% Anomaly Detection: Focuses on identifying rare and unusual instances, regardless of whether they come from a different distribution or not.
% Purpose:
% OOD Detection: Primarily used for safety and robustness, ensuring that the model doesn't make confident predictions on unfamiliar data.
% Anomaly Detection: Used for various applications such as fraud detection, fault detection, and identifying outliers in the data.
% Detection Approach:
% OOD Detection: Often involves measuring the uncertainty of the model's predictions. High uncertainty indicates that the input might be out-of-distribution.
% Anomaly Detection: Focuses on detecting data points that are statistically rare or far from the norm.
% It's worth noting that while the concepts are distinct, they can sometimes overlap. An out-of-distribution data point could also be considered an anomaly, but not all anomalies are necessarily out-of-distribution. Anomaly detection methods might use OOD detection techniques as part of their process to flag anomalous instances that are not in line with the expected distribution.




% Nice review:
% https://openaccess.thecvf.com/content/CVPR2023W/VAND/papers/Graham_Denoising_Diffusion_Models_for_Out-of-Distribution_Detection_CVPRW_2023_paper.pdf

% \cite{kabir_neural_2018} Survey of uncertainty quantification in deep learning.

% https://arxiv.org/pdf/2306.05674.pdf




\section{Variational autoencoders}



% Trouble in paradise: factoriing the ELBO to show that we minimize the mutual information between z and x. 
% Use this as transition to self-supervised methods as alternative for representation learning: 
% https://jmtomczak.github.io/blog/13/13_trouble_in_paradise.html


\section{Speech representation learning}



\iffalse


% TODO Delete this probably
Following the derivation of \textcite{jaynes_information_1957}, in the discrete entropy expression in \cref{eq:information-entropy}, we assume that the number of discrete points, $x_i, i=1,2,\dots,N$, in the support of $X$ grows larger such that in the limit $N\rightarrow\infty$, the density of points becomes a continuous function $m(x)$, and we can replace the sum with an integral. Denoting the number of points satisfying $a<x_i<b$ by $N_{a,b}$, we have
%
\begin{equation}
    \lim_{N\rightarrow\infty} \frac{N_{a,b}}{N} = \int_a^b m(x) \, dx \enspace .
\end{equation}
%
If this limit behaves sufficiently well, adjacent differences $(x_{i+1} - x_{i})$ in the neighborhood of any particular value of $x$ will also tend to zero, so that
%
\begin{equation} \label{eq:limiting-density-discrete-to-continuous}
    \lim_{N\rightarrow\infty} \left[ N(x_{i+1} - x_{i}) \right] = \left[ m(x_{i}) \right]^{-1} \enspace .
\end{equation}
%
In the limit, the value of the discrete probability mass function will become a continuous probability density function $w(x)$, according to the limiting form of
%
\begin{equation}
    p_i = w(x_i)\left(x_{i+1} - x_{i}\right) \enspace ,
\end{equation}
%
where $p_i$ is the value of the probability mass function at $x_i$ which via \cref{eq:limiting-density-discrete-to-continuous} becomes,
%
\begin{equation}
    p_i = w(x_i)\left[Nm(x_i)\right]^{-1} \enspace .
\end{equation}
%
Consequently, the discrete entropy expression in \cref{eq:information-entropy} becomes,
\begin{align}
    % H_{N,\text{discrete}}(X) &= - \int_\mathcal{X} w(x)\left[Nm(x)\right]^{-1} \, dx \, \log \left[\frac{w(x)}{Nm(x)}\right] \nonumber \\
    % &= - \int_\mathcal{X} w(x) \, dx \, \log \left[\frac{w(x)}{Nm(x)}\right]
    H_{N,\text{discrete}}(X) &= - \int_\mathcal{X} w(x) \, dx \, \log \left[\frac{w(x)}{Nm(x)}\right] \nonumber \\
\end{align}
%
and taking the limit $N\rightarrow\infty$,
%
\begin{equation}
    \lim_{N\rightarrow\infty} H_{N,\text{discrete}}(X) = H_{N} = \log N - \int_\mathcal{X} w(x) \log \left[\frac{w(x)}{m(x)}\right] \enspace .
\end{equation}
%
Note that since $\lim_{N\rightarrow\infty} \log N = \infty$, this procedure suggests that the entropy in the discrete sense of a continuous random variable should be infinite.




Modelling paradigms
\begin{enumerate}
    \item Variational inference and variational autoencoders
    \item Automatic speech recognition
    \item Self-supervised learning
\end{enumerate}

\cite{kingma_autoencoding_2014}
\cite{rezende_stochastic_2014}

Two approaches to statistics:\\
- Bayesian\\
- Frequentist\\

Two kinds of uncertainty:\\
- Aleatoric: Uncertainty inherent to the data. Irreducible. "Known unknowns".\\
- Epistemic: Uncertainty due to lack of knowledge. Can be reduced with more data. "Unknown unknowns".\\

Approaches to uncertainty in deep learning:\\
- Tuning of supervised classifiers to output well-calibrated probabilities that correspond to the actual likelihood of the prediction being correct (on some validation set).\\
- Learning rich unsupervised representations of the data that can be used to estimate the uncertainty of the prediction (e.g. by measuring the distance to the nearest training example in the latent space).\\
- Learning of a probability distribution over the parameters of the model, which can then be used to compute the uncertainty of the prediction (Bayesian neural networks).\\


\fi 
