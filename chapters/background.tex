%!TEX root = ../thesis.tex

% ~8 pages

\chapter[technical background]{Technical background}\label{chp:technical-background}

While the papers in this thesis were written to be self-contained, page limits and the need to maintain a specific focus, forced us to limit the scope of technical background each paper provides. In this chapter, we present a more in-depth overview of the technical background relevant to the papers in this thesis. 
We start by quantifying the concept of uncertainty, and how it relates to information and probability. We then introduce the task of out-of-distribution detection and a few ways to tackle it. 
Then, we present variational autoencoders, and how they can be used to learn representations of speech. Finally, we introduce the concept of self-supervised learning and highlight important differences to variational autoencoders. 


\section{Uncertainty and information}

In society, the concept of uncertainty goes by many names, and its meaning can vary depending on the specific context. However, across most quantitative scientific fields, a consensus definition appears to align with the following \cite{hubbard_how_2014}:
%
\begin{center}
    \textit{Uncertainty is the lack of certainty; a state of limited knowledge where it is impossible to exactly describe the existing state, a future outcome, or more than one possible outcome.}
\end{center}
%
Although such a purely lexical definition of uncertainty might prompt a philosophical inquiry, it highlights an important connection between uncertainty, information and probability. When information is limited, we can describe the state of the world only with uncertainty; we are simply not certain. In that sense, uncertainty is a word we use to refer to the information that is missing, whether we know what is missing or not. 
A natural way to describe an uncertain world state is to use probabilities. To our luck, information theory provides exactly that; a mathematically rigorous method for quantifying information through the language of probability theory. In the following, we shall see that, in this context, uncertainty can be understood as the information entropy of a random variable \cite{mackay_information_2003}. 


\subsection{Entropy for discrete random variables}

First characterized by Claude Shannon in 1948 \cite{shannon_mathematical_1948}, information entropy, $H(X)$, is a measure of the average amount of information contained in a discrete random variable $X$. Shannon's definition follows from three fundamental axioms of information theory. Let $I_X(x)$ be the information carried by a specific outcome $x$ of sampling the discrete random variable $X$ with probability mass function $p_X(x)$. Then, these axioms are as follows:
\begin{enumerate}[label=(\Roman*)]
    \item The more likely an outcome is, the less information it carries; $I_X(x)$ is a monotonically decreasing function in $p_X(x)$.
    \item Outcomes that are certain to happen carry no information; if $p_X(x) = 1$ then $I_X(x) = 0$.
    \item The joint information carried by independent outcomes is the sum of the information carried by each outcome; if $x_i$ and $x_j$ are independent then $I(x_i, x_j) = I(x_i) + I(x_j)$. 
\end{enumerate}

From these axioms, Shannon found that the \emph{information content} $I_X(x)$ of an outcome $x$ with probability $p_X(x)$ can suitably be defined as the negative logarithm of the probability of the outcome,
%
\begin{equation} \label{eq:information-content}
    I_X(x) = -\log_b p_X(x) \enspace .
\end{equation}
%

The \emph{information entropy of a discrete random variable} $X$ is defined as the \emph{expected information content} of an outcome of $X$,
%
\begin{equation} \label{eq:information-entropy}
    H(X) = \mathbb{E}_X\left[I_X(x)\right] = - \sum_{x\in X} p_X(x) \log p_X(x) \enspace .
\end{equation}
%
This means that $H(X)$ measures the amount of information we can expect to gain from observing an outcome of $X$, when we know only its distribution $p_X(x)$. If all possible outcomes are equally likely, then $H(X)$ is maximized, and if only one outcome is possible, then $H(X)$ is minimized.

\todo[inline]{Come up with a suitable example explaining the intuition behind information entropy.}


\subsection{Entropy for continuous random variables}

To generalize the concept of information entropy to continuous random variables, Shannon originally replaced the sum over the probability mass function in \eqref{eq:information-entropy} with an integral over the probability density function, as suggested by the definition $H(X) = \mathbb{E}_X\left[I_X(x)\right]$. This approach leads to the definition of the \emph{information entropy of a continuous random variable} $X$ as, 
%
\begin{equation} \label{eq:differential-entropy}
    H(X) = - \int_{\mathcal{X}} p_X(x) \log p_X(x) \, dx \enspace ,
\end{equation}
%
which is called the \emph{differential entropy} \cite{shannon_mathematical_1948}. Here, $\mathcal{X}$ is the support of $X$. 

However, the differential entropy does not have several of the desirable attributes of the discrete version; it can be negative, it is not invariant under a change of variables, and it is not dimensionally correct%
\footnote{\label{fn:dimensional-analysis-of-differential-entropy}
    Information entropy has the same dimensionality as information content which is typically measured in units of bits, but depends on the base of the logarithm. 
    However, the bit is not a base unit of the International System of Units (SI), nor is it an official derived unit.
    A bit is quite simply a number, specifically 0 or 1. If something is said to have a size of 4 bits, it means that it can be described with 4 binary digits, i.e. 4 numbers, each either 0 or 1.

    As such, it might be useful to think of information as a dimensionless quantity rather than a quantity with units.
    Following this interpretation, the discrete information entropy in \cref{eq:information-entropy} is also dimensionless, because a probability mass function is dimensionless. 
    Since probability density functions have dimensionality of the inverse of some quantity, e.g. inverse length, the dimensionality of the continuous differential entropy in \cref{eq:differential-entropy} becomes,
    \begin{equation*}
        \text{dim} \left( H(X) \right) = \text{dim} \left( \int_{\mathcal{X}} p_X(x) \log p_X(x) \, dx \right) = \frac{1}{\text{length}} \log\left(\text{length}\right) \enspace ,
    \end{equation*}
    which is clearly not dimensionless and therefore cannot correspond directly to the discrete information entropy.
    % As a final twist on this story, note that the SI system does actually have a unit for an amount of substance: the mole \cite{internationalbureauofweightsandmeasures_systeme_2019}. 
    % The mole is defined as the amount of substance that contains as many elementary entities as there are atoms in $\SI{12}{g}$ of carbon-12, that is $\SI{1}{mol} = 6.02214076\times10^{23}$.
    % Taking the position that a mole is quite simply a number (not a number of particles) \cite{debievre_atomic_1992,baranski_atomic_2012}, and a bit is a number of elementary (information) entities, we could define a bit as a derived unit from the mole as $\SI{1}{bit} = \SI{1}{mol} / 6.02214076\times10^{23}$.
}. 
For these reasons, the differential entropy may not be a suitable measure of information, or uncertainty, for continuous random variables. 

Instead, \textcite{jaynes_information_1957,jaynes_prior_1968} argued that the information entropy of a continuous distribution should be defined as the limiting density of increasingly dense discrete distributions, $H_{N\rightarrow\infty}(X)$. This argument leads to,
%
\begin{equation} \label{eq:corrected-differential-entropy-jaynes}
    H_{N\rightarrow\infty}(X) \equiv \lim_{N\rightarrow\infty} [H_{N}(X) - \log N] = - \int_\mathcal{X} p(x) \log \left(\frac{p(x)}{q(x)}\right) \enspace ,
\end{equation}
%
where $H_N(X)$ is the limiting density for discrete points, $q(x)$ is a uniform density over the quantization of the continuous space, and we have subtracted a $\log N$ term that would go to infinity in the limit of infinite points.
By doing this, the information entropy becomes positive, dimensionally correct, and invariant under a change of variables%
\footnote{\label{fn:change-of-variables-invariance-of-corrected-differential-entropy}
    Under a change of independent variable from $x$ to $y(x)$, we have that
    \begin{align*}
        \widetilde{w}(y)\,dy = w(x)\,dx \enspace , \quad
        \widetilde{q}(y)\,dy = q(x)\,dx \enspace .
    \end{align*}
    Plugging this into the differential entropy \cref{eq:corrected-differential-entropy-jaynes} we arrive at the original expression but now with $y$ as the independent variable,
    \begin{equation*}
        H_{N\rightarrow\infty}(y(x)) =  = - \int_\mathcal{Y} \widetilde{w}(y)\,\frac{dy}{dx} \log \frac{\widetilde{w}(y)\frac{dy}{dx}}{\widetilde{q}(y)\frac{dy}{dx}} \, dx = - \int_\mathcal{Y} w(y) \log \frac{w(y)}{q(y)} \, dy \enspace .
    \end{equation*}
}.

The form on the right-hand side of \cref{eq:corrected-differential-entropy-jaynes} can be recognized as the negative Kullback-Leibler divergence of $p(x)$ to $q(x)$ \cite{kullback_information_1959},
%
\begin{equation} \label{eq:kullback-leibler-divergence}
    D_{KL}(P\parallel Q) = \int_\mathcal{X} p(x) \log \left(\frac{p(x)}{q(x)}\right) \enspace .
\end{equation}
%
The Kullback-Leibler divergence $D_{KL}(P\parallel Q)$, also known as the \emph{information gain}, is often interpreted as the amount of additional information required to represent events from distribution $P$ with density $p(x)$ using a code optimized for distribution $Q$ with density $q(x)$. In other words, it measures how surprised one would be if they used distribution $Q$ to represent events from distribution $P$, i.e. the \emph{relative entropy} of $P$ with respect to $Q$.

Returning to Jaynes' argument and \cref{eq:corrected-differential-entropy-jaynes}, we can interpret this to say that the information entropy of a continuous random variable $X$ should be defined as the expected difference in information entropy between its density and a uniform density. 
Identifying the Kullback-Leibler divergence as a measure of this relative information entropy provides a natural point of convergence for this section since it is equivalently defined for both discrete and continuous random variables, contrary to the differential entropy. 
In this view, information entropy is a quantity that distills the distribution of a random variable into a single number that describes the diversity of the potential outcomes of the random variable. 
% In the following, we will see that this interpretation of information entropy as a measure of diversity is useful for understanding the concept of uncertainty in machine learning.

% In fact, Kullback initially motivated the divergence as an expected likelihood ratio \cref{kullback_information_1959}.


\section{Out-of-distribution detection}
% 
% OUTLINE: C.f. https://openaccess.thecvf.com/content/CVPR2023W/VAND/papers/Graham_Denoising_Diffusion_Models_for_Out-of-Distribution_Detection_CVPRW_2023_paper.pdf
% - Formalize the problem. https://arxiv.org/pdf/2302.10326.pdf
%   - Assume we have a dataset $\mathcal{D}_{\text{train}}$ of in-distribution data and we would like to build a model that can be used to assess whether a certain data point $\xb$ is sampled from $\mathcal{D}_{\text{train}}$ or not. Given a test point $\xb$, the model outputs a score $s(\xb)$ that indicates whether $\xb$ is in-distribution or out-of-distribution. 
%   - In the context of neural networks, this problem dates back to at least 1994 \cite{bishop_novelty_1994}. 
% - Categorisation into supervised and unsupervised. 
%   - Supervised: Whether p(y|x) should be trusted for a given x. 
%   - Unsupervised: Whether x should be trusted and used at all. 
% - Supervised OODD:
%   - Requires some examples of (synthetic) OODD data, which in some sense makes it inherently flawed. We can never really be sure that we have good enough coverage of OODD data.
%   - One limitation of model-dependent OoD techniques is that they may discard information about p(x) in learning the task-specific model p(y|x). That information may be useful for OoD detection.
%   - Likelihood Ratios for Out-of-Distribution Detection \cite{ren_likelihood_2019} provides a number of baselines for supervised OODD:
%       - 1 Maximum class probability of the predictive distribution p(y|x) \cite{hendrycks_baseline_2017}
%       - 2 Entropy of predictive distribution p(y|x) \cite{ren_likelihood_2019}
%       - 3 ODIN \cite{liang_enhancing_2018} (Uses synthetic OOD data by adding noise to in-distribution)
%       - 4 Mahalanobis distance to class conditional Gaussian distribution \cite{lee_training_2018}
%       - 5 DeepEnsemble \cite{lakshminarayanan_simple_2017} The classifier-based ensemble method that uses the average of the predictions from multiple independently trained models with random initialization of network parameters and random shuffling of training inputs
%       - 6 The log-odds of a binary classifier trained to distinguish between in-distribution inputs from all classes as one class and randomly perturbed in-distribution inputs as the other. \cite{ren_likelihood_2019}
%       - 7 The maximum class probability over K in-distribution classes of a (K + 1)-class classifier where the additional class is perturbed in-distribution. \cite{ren_likelihood_2019}
%       - 8 The maximum class probability of a K-class classifier for in-distribution classes but the predicted class distribution is explicitly trained to output uniform distribution on perturbed in-distribution inputs. \cite{ren_likelihood_2019}. This is similar to using simulated OOD inputs from GAN \cite{lee_training_2018} or using auxiliary datasets of outliers \cite{hendrycks_deep_2019} for calibration purpose.
%   - X Deep semi-supervised anomaly detection \cite{ruff_deep_2020}.
%   - Variational information bottleneck (VIB) (Alemi et al., 2018b) performs divergence estimation in latent space to detect OoD, but is technically a model-dependent technique because the latent code is trained jointly with the downstream classification task. \cite{alemi_deep_2017}
% - Unsupervised ODDD
%   - No OODD data required. Isolates uncertainty related to the input data.
%   - Unsupervised can be categorised into generative and reconstructive.
%   - Generative
%       - 9 Compute WAIC (\cite{watanabe_algebraic_2009, watanabe_asymptotic_2010}) using ensemble of generative models \cite{choi_waic_2019}
%       - Likelihood Ratios for Out-of-Distribution Detection \cite{ren_likelihood_2019}
%       - Input complexity and out-of-distribution detection with likelihood-based generative models \cite{serra_input_2020}
%       - Likelihood regret for variational autoencoders \cite{xiao_likelihood_2020}
%       - Typicality test for generative OODD (Detecting out-of-distribution inputs to deep generative models using typicality) \cite{nalisnick_detecting_2019}
%       - Density of states estimation for out of distribution detection \cite{morningstar_density_2021}
%   - Reconstructive
%       - A review of novelty detection \cite{pimentel_review_2014}
%       - Outlier detection using autoencoders \cite{lyudchik_outlier_2016} and ensembles of autoencoders \cite{chen_outlier_2017}
%       - Improving reconstruction autoencoder out-of-distribution detection with mahalanobis distance \cite{denouden_improving_2018}
%       - Denosing diffusion models for out-of-distribution detection \cite{graham_denoising_2023}
%       - Diffusion inpainting \cite{liu_unsupervised_2023a}

% , MC Dropout (Gal & Ghahramani, 2016) and  model a calibrated predictive distribution for a classification task.

In this section we will introduce out-of-distribution detection, relate it to the concept of uncertainty and to the previous section on information entropy.
To supplement the related work sections of the papers in \cref{chp:paper-hierarchical,chp:paper-modelagnostic}, we will also provide a brief overview of some of the related methods that have been proposed in the literature. 

Out-of-distribution detection is the task of identifying data that is unlikely to originate from the distribution of the training data. 
In the general case, we assume that we have a dataset $\mathcal{D}_{\text{training}}$ of in-distribution data and we would like to build a model that can be used to assess whether a certain data point $\xb$ is sampled from $\mathcal{D}_{\text{training}}$ or not. 
Out-of-distribution detection is closely related to the concept of uncertainty. Specifically, since such data is unknown to the model, it is, by definition, a source of epistemic uncertainty. Only in cases where strong prior knowledge of the data is incorporated into the model, can we expect the model to be able to make predictions on out-of-distribution data.

One way to categorize the many approaches to out-of-distribution detection is based on whether the method estimates a conditional probability distribution $p(y|\xb)$ over some target variable given the input, or a probability distribution $p(\xb)$ over the input itself. We might refer to the former as \emph{supervised} out-of-distribution detection, and the latter as \emph{unsupervised} out-of-distribution detection \cite{graham_denoising_2023,liu_unsupervised_2023a}. Supervised out-of-distribution detection tries to assess whether the model's prediction $p(y|\xb)$ should be trusted for a given input $\xb$, whereas unsupervised out-of-distribution detection seeks to assess whether the input $\xb$ should be trusted and used at all.

\subsection{Supervised out-of-distribution detection}

In the supervised setting the supervision can come from different sources that result in different levels of informedness. 
If one assumes access to representative out-of-distribution data, then one can train a model to distinguish between in-distribution and out-of-distribution data. 
The out-of-distribution data can either be synthetic or real.
Such approaches have achieved high performance \cite{lee_training_2018,hendrycks_deep_2019,ruff_deep_2020} but require access to out-of-distribution data, which in some sense makes it inherently flawed; it is often not possible to guarantee good enough coverage of out-of-distribution data to cover all possibilities. 
\todo[inline]{Describe an example method or two...}

Less informed approaches only assume access to in-distribution data when learning the task-specific model $p(y|\xb)$. 
One particularly simple approach requiring only in-distribution data uses the maximum class probability of the predictive distribution $p(y|\xb)$ to evaluate whether the example is in-distribution or not \cite{hendrycks_baseline_2017}. 
Other simple methods presented by \textcite{ren_likelihood_2019} include using the entropy of $p(y|\xb)$, the maximum class probability with an additional class trained to predict perturbed in-distribution data, the maximum class probability where the predicted class distribution is explicitly trained to output uniform distribution on perturbed in-distribution inputs, and the log-odds of a binary classifier trained to distinguish between original and perturbed in-distribution data. 

In ODIN \cite{liang_enhancing_2018}, the authors propose to add input-dependent noise to the inputs of a pre-trained classifier and use the calibrated maximum class probability of the predictive distribution $p(y|\xb)$ to evaluate whether the example is in-distribution or not. This method does not require retraining the classifier and generally improves on the simpler methods mentioned above. 
In \textcite{lee_training_2018}, the authors fit a multivariate Gaussian distribution to the activations of the penultimate layer of a pre-trained classifier and use the Mahalanobis distance to this distribution to evaluate whether the example is in-distribution or not. \todo[inline]{Continue here ...}

%       - 2 Entropy of predictive distribution p(y|x) \cite{ren_likelihood_2019}
%       - 6 The log-odds of a binary classifier trained to distinguish between in-distribution inputs from all classes as one class and randomly perturbed in-distribution inputs as the other. \cite{ren_likelihood_2019}
%       - 7 The maximum class probability over K in-distribution classes of a (K + 1)-class classifier where the additional class is perturbed in-distribution. \cite{ren_likelihood_2019}
%       - 8 The maximum class probability of a K-class classifier for in-distribution classes but the predicted class distribution is explicitly trained to output uniform distribution on perturbed in-distribution inputs. \cite{ren_likelihood_2019}. This is similar to using simulated OOD inputs from GAN \cite{lee_training_2018} or using auxiliary datasets of outliers \cite{hendrycks_deep_2019} for calibration purpose.


%       - 3 ODIN \cite{liang_enhancing_2018} (Uses synthetic OOD data by adding noise to in-distribution)
%       - 4 Mahalanobis distance to class conditional Gaussian distribution \cite{lee_training_2018}
%       - 5 DeepEnsemble \cite{lakshminarayanan_simple_2017} The classifier-based ensemble method that uses the average of the predictions from multiple independently trained models with random initialization of network parameters and random shuffling of training inputs

%   - Variational information bottleneck (VIB) (Alemi et al., 2018b) performs divergence estimation in latent space to detect OoD, but is technically a model-dependent technique because the latent code is trained jointly with the downstream classification task. \cite{alemi_deep_2017}


\cite{hendrycks_baseline_2017, lakshminarayanan_simple_2017, lliang_enhancing_2018, lee_simple_2018, huang_importance_2021, wang_vim_2022}




 either synthetic out-of-distribution data or from real out-of-distribution data. Synthetic out-of-distribution data is data that is generated by some process that is known to be out-of-distribution. For example, if we are training a model to classify images of cats and dogs, we could generate synthetic out-of-distribution data by adding noise to the images. Real out-of-distribution data is data that is known to be out-of-distribution, but is not generated synthetically. For example, if we are training a model to classify images of cats and dogs, we could use images of cars as real out-of-distribution data.

 requires some examples of out-of-distribution data, 

 which in some sense makes it inherently flawed. As we discussed earlier, for many modern applications of machine learning, the data is high dimensional and complex which makes sufficiently sampling the space of out-of-distribution data intractable and makes it impossible to guarantee robust out-of-distribution detection. Another weakness of supervised out-of-distribution detection is that in learning the task-specific model $p(y|\xb)$ a model may discard information about $p(\xb)$ which could be useful for out-of-distribution detection.


 \subsection{Unsupervised out-of-distribution detection}

% Assume we have a dataset $\mathcal{D}_{\text{training}}$ of in-distribution data and we would like to build a model that can be used to assess whether a certain data point $\xb$ is sampled from $\mathcal{D}_{\text{training}}$ or not. Given a test point $\xb$, the model outputs a score $s(\xb)$ that indicates whether $\xb$ is in-distribution or out-of-distribution. 
In the context of neural networks, this problem dates back to at least 1994 \cite{bishop_novelty_1994}. 


Continual novelty detection \cite{aljundi_continual_2022}


% Out-of-distribution detection and anomaly detection are both related concepts in machine learning and deep learning, but they address slightly different problems and have distinct focuses.

% Out-of-Distribution Detection (OOD Detection):
% Out-of-distribution detection involves determining whether a given input or data point belongs to the same distribution as the training data. In other words, it helps identify whether an input is coming from a category or distribution that the model has never encountered during training. This is especially important for safety-critical applications where the model should not make predictions on inputs that are significantly different from what it has seen before. OOD detection aims to prevent the model from making confident predictions on unfamiliar inputs that might lead to incorrect results.
% Anomaly Detection:
% Anomaly detection, on the other hand, focuses on identifying rare and unusual instances within the data, regardless of whether they belong to the same distribution as the training data or not. Anomalies can be defined as data points that deviate significantly from the norm or expected behavior. Anomaly detection is used to find instances that are different from the majority and might indicate potential errors, fraud, faults, or other unusual occurrences.
% In summary, the key differences are:

% Focus:
% OOD Detection: Focuses on detecting inputs that are coming from distributions that differ significantly from the training data distribution.
% Anomaly Detection: Focuses on identifying rare and unusual instances, regardless of whether they come from a different distribution or not.
% Purpose:
% OOD Detection: Primarily used for safety and robustness, ensuring that the model doesn't make confident predictions on unfamiliar data.
% Anomaly Detection: Used for various applications such as fraud detection, fault detection, and identifying outliers in the data.
% Detection Approach:
% OOD Detection: Often involves measuring the uncertainty of the model's predictions. High uncertainty indicates that the input might be out-of-distribution.
% Anomaly Detection: Focuses on detecting data points that are statistically rare or far from the norm.
% It's worth noting that while the concepts are distinct, they can sometimes overlap. An out-of-distribution data point could also be considered an anomaly, but not all anomalies are necessarily out-of-distribution. Anomaly detection methods might use OOD detection techniques as part of their process to flag anomalous instances that are not in line with the expected distribution.




% Nice review:
% https://openaccess.thecvf.com/content/CVPR2023W/VAND/papers/Graham_Denoising_Diffusion_Models_for_Out-of-Distribution_Detection_CVPRW_2023_paper.pdf

% \cite{kabir_neural_2018} Survey of uncertainty quantification in deep learning.

% https://arxiv.org/pdf/2306.05674.pdf




\section{Variational autoencoders}



% Trouble in paradise: factoriing the ELBO to show that we minimize the mutual information between z and x. 
% Use this as transition to self-supervised methods as alternative for representation learning: 
% https://jmtomczak.github.io/blog/13/13_trouble_in_paradise.html


\section{Speech representation learning}



\iffalse


% TODO Delete this probably
Following the derivation of \textcite{jaynes_information_1957}, in the discrete entropy expression in \cref{eq:information-entropy}, we assume that the number of discrete points, $x_i, i=1,2,\dots,N$, in the support of $X$ grows larger such that in the limit $N\rightarrow\infty$, the density of points becomes a continuous function $m(x)$, and we can replace the sum with an integral. Denoting the number of points satisfying $a<x_i<b$ by $N_{a,b}$, we have
%
\begin{equation}
    \lim_{N\rightarrow\infty} \frac{N_{a,b}}{N} = \int_a^b m(x) \, dx \enspace .
\end{equation}
%
If this limit behaves sufficiently well, adjacent differences $(x_{i+1} - x_{i})$ in the neighborhood of any particular value of $x$ will also tend to zero, so that
%
\begin{equation} \label{eq:limiting-density-discrete-to-continuous}
    \lim_{N\rightarrow\infty} \left[ N(x_{i+1} - x_{i}) \right] = \left[ m(x_{i}) \right]^{-1} \enspace .
\end{equation}
%
In the limit, the value of the discrete probability mass function will become a continuous probability density function $w(x)$, according to the limiting form of
%
\begin{equation}
    p_i = w(x_i)\left(x_{i+1} - x_{i}\right) \enspace ,
\end{equation}
%
where $p_i$ is the value of the probability mass function at $x_i$ which via \cref{eq:limiting-density-discrete-to-continuous} becomes,
%
\begin{equation}
    p_i = w(x_i)\left[Nm(x_i)\right]^{-1} \enspace .
\end{equation}
%
Consequently, the discrete entropy expression in \cref{eq:information-entropy} becomes,
\begin{align}
    % H_{N,\text{discrete}}(X) &= - \int_\mathcal{X} w(x)\left[Nm(x)\right]^{-1} \, dx \, \log \left[\frac{w(x)}{Nm(x)}\right] \nonumber \\
    % &= - \int_\mathcal{X} w(x) \, dx \, \log \left[\frac{w(x)}{Nm(x)}\right]
    H_{N,\text{discrete}}(X) &= - \int_\mathcal{X} w(x) \, dx \, \log \left[\frac{w(x)}{Nm(x)}\right] \nonumber \\
\end{align}
%
and taking the limit $N\rightarrow\infty$,
%
\begin{equation}
    \lim_{N\rightarrow\infty} H_{N,\text{discrete}}(X) = H_{N} = \log N - \int_\mathcal{X} w(x) \log \left[\frac{w(x)}{m(x)}\right] \enspace .
\end{equation}
%
Note that since $\lim_{N\rightarrow\infty} \log N = \infty$, this procedure suggests that the entropy in the discrete sense of a continuous random variable should be infinite.




Modelling paradigms
\begin{enumerate}
    \item Variational inference and variational autoencoders
    \item Automatic speech recognition
    \item Self-supervised learning
\end{enumerate}

\cite{kingma_autoencoding_2014}
\cite{rezende_stochastic_2014}

Two approaches to statistics:\\
- Bayesian\\
- Frequentist\\

Two kinds of uncertainty:\\
- Aleatoric: Uncertainty inherent to the data. Irreducible. "Known unknowns".\\
- Epistemic: Uncertainty due to lack of knowledge. Can be reduced with more data. "Unknown unknowns".\\

Approaches to uncertainty in deep learning:\\
- Tuning of supervised classifiers to output well-calibrated probabilities that correspond to the actual likelihood of the prediction being correct (on some validation set).\\
- Learning rich unsupervised representations of the data that can be used to estimate the uncertainty of the prediction (e.g. by measuring the distance to the nearest training example in the latent space).\\
- Learning of a probability distribution over the parameters of the model, which can then be used to compute the uncertainty of the prediction (Bayesian neural networks).\\


\fi 
