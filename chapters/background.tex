%!TEX root = ../thesis.tex

% ~8 pages

\chapter[technical background]{Technical background}\label{chp:technical-background}

While the papers in this thesis were written to be self-contained, page limits and the need to maintain a specific focus, forced us to limit the scope of technical background each paper provides. In this chapter, we present a more in-depth overview of the technical background relevant to the papers in this thesis. 
We start by quantifying the concept of uncertainty, and how it relates to information and probability. We then introduce the task of out-of-distribution detection and a few ways to tackle it. 
Then, we present variational autoencoders, and how they can be used to learn representations of speech. Finally, we introduce the concept of self-supervised learning and highlight important differences to variational autoencoders. 


\section{Uncertainty and information}

In society, the concept of uncertainty goes by many names, and its meaning can vary depending on the specific context. However, across most quantitative scientific fields, a consensus definition appears to align with the following \cite{hubbard_how_2014}:
%
\begin{center}
    \textit{Uncertainty is the lack of certainty; a state of limited knowledge where it is impossible to exactly describe the existing state, a future outcome, or more than one possible outcome.}
\end{center}
%
Although such a purely lexical definition of uncertainty might prompt a philosophical inquiry, it highlights an important connection between uncertainty, information and probability. When information is limited, we can describe the state of the world only with uncertainty; we are simply not certain. In that sense, uncertainty is a word we use to refer to the information that is missing, whether we know what is missing or not. 
A natural way to describe an uncertain world state is to use probabilities. To our luck, information theory provides exactly that; a mathematically rigorous method for quantifying information through the language of probability theory. In the following, we shall see that, in this context, uncertainty can be understood as the information entropy of a random variable \cite{mackay_information_2003}. 


\subsection{Entropy for discrete random variables}

First characterized by Claude Shannon in 1948 \cite{shannon_mathematical_1948}, information entropy, $H(X)$, is a measure of the average amount of information contained in a discrete random variable $X$. Shannon's definition follows from three fundamental axioms of information theory. Let $I_X(x)$ be the information carried by a specific outcome $x$ of sampling the discrete random variable $X$ with probability mass function $p_X(x)$. Then, these axioms are as follows:
\begin{enumerate}[label=(\Roman*)]
    \item The more likely an outcome is, the less information it carries; $I_X(x)$ is a monotonically decreasing function in $p_X(x)$.
    \item Outcomes that are certain to happen carry no information; if $p_X(x) = 1$ then $I_X(x) = 0$.
    \item The joint information carried by independent outcomes is the sum of the information carried by each outcome; if $x_i$ and $x_j$ are independent then $I(x_i, x_j) = I(x_i) + I(x_j)$. 
\end{enumerate}

From these axioms, Shannon found that the \emph{information content} $I_X(x)$ of an outcome $x$ with probability $p_X(x)$ can suitably be defined as the negative logarithm of the probability of the outcome,
%
\begin{equation} \label{eq:information-content}
    I_X(x) = -\log_b p_X(x) \enspace .
\end{equation}
%

The \emph{information entropy of a discrete random variable} $X$ is defined as the \emph{expected information content} of an outcome of $X$,
%
\begin{equation} \label{eq:information-entropy}
    H(X) = \mathbb{E}_X\left[I_X(x)\right] = - \sum_{x\in X} p_X(x) \log p_X(x) \enspace .
\end{equation}
%
This means that $H(X)$ measures the amount of information we can expect to gain from observing an outcome of $X$, when we know only its distribution $p_X(x)$. If all possible outcomes are equally likely, then $H(X)$ is maximized, and if only one outcome is possible, then $H(X)$ is minimized.

\todo[inline]{Come up with a suitable example explaining the intuition behind information entropy.}


\subsection{Entropy for continuous random variables}

To generalize the concept of information entropy to continuous random variables, Shannon originally replaced the sum over the probability mass function in \eqref{eq:information-entropy} with an integral over the probability density function, as suggested by the definition $H(X) = \mathbb{E}_X\left[I_X(x)\right]$. This approach leads to the definition of the \emph{information entropy of a continuous random variable} $X$ as, 
%
\begin{equation} \label{eq:differential-entropy}
    H(X) = - \int_{\mathcal{X}} p_X(x) \log p_X(x) \, dx \enspace ,
\end{equation}
%
which is called the \emph{differential entropy} \cite{shannon_mathematical_1948}. Here, $\mathcal{X}$ is the support of $X$. 

However, the differential entropy does not have several of the desirable attributes of the discrete version; it can be negative, it is not invariant under a change of variables, and it is not dimensionally correct%
\footnote{\label{fn:dimensional-analysis-of-differential-entropy}
    Information entropy has the same dimensionality as information content which is typically measured in units of bits, but depends on the base of the logarithm. 
    However, the bit is not a base unit of the International System of Units (SI), nor is it an official derived unit.
    A bit is quite simply a number, specifically 0 or 1. If something is said to have a size of 4 bits, it means that it can be described with 4 binary digits, i.e. 4 numbers, each either 0 or 1.

    As such, it might be useful to think of information as a dimensionless quantity rather than a quantity with units.
    Following this interpretation, the discrete information entropy in \cref{eq:information-entropy} is also dimensionless, because a probability mass function is dimensionless. 
    Since probability density functions have dimensionality of the inverse of some quantity, e.g. inverse length, the dimensionality of the continuous differential entropy in \cref{eq:differential-entropy} becomes,
    \begin{equation*}
        \text{dim} \left( H(X) \right) = \text{dim} \left( \int_{\mathcal{X}} p_X(x) \log p_X(x) \, dx \right) = \frac{1}{\text{length}} \log\left(\text{length}\right) \enspace ,
    \end{equation*}
    which is clearly not dimensionless and therefore cannot correspond directly to the discrete information entropy.
    % As a final twist on this story, note that the SI system does actually have a unit for an amount of substance: the mole \cite{internationalbureauofweightsandmeasures_systeme_2019}. 
    % The mole is defined as the amount of substance that contains as many elementary entities as there are atoms in $\SI{12}{g}$ of carbon-12, that is $\SI{1}{mol} = 6.02214076\times10^{23}$.
    % Taking the position that a mole is quite simply a number (not a number of particles) \cite{debievre_atomic_1992,baranski_atomic_2012}, and a bit is a number of elementary (information) entities, we could define a bit as a derived unit from the mole as $\SI{1}{bit} = \SI{1}{mol} / 6.02214076\times10^{23}$.
}. 
For these reasons, the differential entropy may not be a suitable measure of information, or uncertainty, for continuous random variables. 

Instead, \textcite{jaynes_information_1957,jaynes_prior_1968} argued that the information entropy of a continuous distribution should be defined as the limiting density of increasingly dense discrete distributions, $H_{N\rightarrow\infty}(X)$. This argument leads to,
%
\begin{equation} \label{eq:corrected-differential-entropy-jaynes}
    H_{N\rightarrow\infty}(X) \equiv \lim_{N\rightarrow\infty} [H_{N}(X) - \log N] = - \int_\mathcal{X} p(x) \log \left(\frac{p(x)}{q(x)}\right) \enspace ,
\end{equation}
%
where $H_N(X)$ is the limiting density for discrete points, $q(x)$ is a uniform density over the quantization of the continuous space, and we have subtracted a $\log N$ term that would go to infinity in the limit of infinite points.
By doing this, the information entropy becomes positive, dimensionally correct, and invariant under a change of variables%
\footnote{\label{fn:change-of-variables-invariance-of-corrected-differential-entropy}
    Under a change of independent variable from $x$ to $y(x)$, we have that
    \begin{align*}
        \widetilde{w}(y)\,dy = w(x)\,dx \enspace , \quad
        \widetilde{q}(y)\,dy = q(x)\,dx \enspace .
    \end{align*}
    Plugging this into the differential entropy \cref{eq:corrected-differential-entropy-jaynes} we arrive at the original expression but now with $y$ as the independent variable,
    \begin{equation*}
        H_{N\rightarrow\infty}(y(x)) =  = - \int_\mathcal{Y} \widetilde{w}(y)\,\frac{dy}{dx} \log \frac{\widetilde{w}(y)\frac{dy}{dx}}{\widetilde{q}(y)\frac{dy}{dx}} \, dx = - \int_\mathcal{Y} w(y) \log \frac{w(y)}{q(y)} \, dy \enspace .
    \end{equation*}
}.

The form on the right-hand side of \cref{eq:corrected-differential-entropy-jaynes} can be recognized as the negative Kullback-Leibler divergence of $p(x)$ to $q(x)$ \cite{kullback_information_1959},
%
\begin{equation} \label{eq:kullback-leibler-divergence}
    D_{KL}(P\parallel Q) = \int_\mathcal{X} p(x) \log \left(\frac{p(x)}{q(x)}\right) \enspace .
\end{equation}
%
The Kullback-Leibler divergence $D_{KL}(P\parallel Q)$, also known as the \emph{information gain}, is often interpreted as the amount of additional information required to represent events from distribution $P$ with density $p(x)$ using a code optimized for distribution $Q$ with density $q(x)$. In other words, it measures how surprised one would be if they used distribution $Q$ to represent events from distribution $P$, i.e. the \emph{relative entropy} of $P$ with respect to $Q$.

Returning to Jaynes' argument and \cref{eq:corrected-differential-entropy-jaynes}, we can interpret this to say that the information entropy of a continuous random variable $X$ should be defined as the expected difference in information entropy between its density and a uniform density. 
Identifying the Kullback-Leibler divergence as a measure of this relative information entropy provides a natural point of convergence for this section since it is equivalently defined for both discrete and continuous random variables, contrary to the differential entropy. 
In this view, information entropy is a quantity that distills the distribution of a random variable into a single number that describes the diversity of the potential outcomes of the random variable. 
% In the following, we will see that this interpretation of information entropy as a measure of diversity is useful for understanding the concept of uncertainty in machine learning.

% In fact, Kullback initially motivated the divergence as an expected likelihood ratio \cref{kullback_information_1959}.


\section{Out-of-distribution detection} \label{sec:out-of-distribution-detection}
% 
% OUTLINE: C.f. https://openaccess.thecvf.com/content/CVPR2023W/VAND/papers/Graham_Denoising_Diffusion_Models_for_Out-of-Distribution_Detection_CVPRW_2023_paper.pdf
% - Formalize the problem. https://arxiv.org/pdf/2302.10326.pdf
%   - Assume we have a dataset $\mathcal{D}_{\text{train}}$ of in-distribution data and we would like to build a model that can be used to assess whether a certain data point $\xb$ is sampled from $\mathcal{D}_{\text{train}}$ or not. Given a test point $\xb$, the model outputs a score $s(\xb)$ that indicates whether $\xb$ is in-distribution or out-of-distribution. 
%   - In the context of neural networks, this problem dates back to at least 1994 \cite{bishop_novelty_1994}. 
%
% - Categorisation into supervised and unsupervised. 
%   - Supervised: Whether p(y|x) should be trusted for a given x. 
%   - Unsupervised: Whether x should be trusted and used at all. 
%
In this section we will introduce out-of-distribution (OOD) detection, relate it to the concept of uncertainty and to the previous section on information entropy. 
To supplement the short related work sections of the papers in \cref{chp:paper-hierarchical,chp:paper-modelagnostic}, we will then provide a concise review of the literature on out-of-distribution detection. 

\subsection{An overview of the task}
%
OOD detection is the task of identifying data that is unlikely to originate from the distribution of the training data and, in the context of neural networks, dates back several decades \cite{bishop_novelty_1994, chang_figure_1993}. 
In the literature, the task is also referred to as open-set \cite{bendale_open_2016, cardoso_weightless_2017, panaredabusto_open_2017}, outlier-rejection \cite{xu_deep_2014, mor_confidence_2018}, or selective prediction \cite{geifman_selective_2017}. A closely related field is that of network-based uncertainty estimation [14, 10, 36, 20].
In the general case, we assume that we have a domain of in-distribution data, $\mathcal{D}_{\text{in}}$, and we would like to build a model that can be used to assess whether a certain data point $\xb$ is sampled from that domain or not. 
Since such OOD data is unknown to the model, it is, by definition, a source of epistemic uncertainty. This makes OOD detection closely related to uncertainty quantification. Only in cases where strong prior knowledge of the data is incorporated into a model, can we expect it to be able to make reliable predictions on OOD data.

Recently, several approaches for deep neural networks have been developed that formally address the rejection of samples $\xb\notin\mathcal{D}_{\text{in}}$. 
One way to categorize the different approaches is based on whether the method estimates a conditional probability distribution $p(y|\xb)$ over some target variable given the input, or a probability distribution $p(\xb)$ over the input itself. We might refer to the former as \emph{supervised} OOD detection, and the latter as \emph{unsupervised} OOD detection \cite{graham_denoising_2023,liu_unsupervised_2023a}, although it is important to note that this distinction relates only to the availability of some target value, $y$ - not whether OOD data is available for supervision\footnote{
    In our overview, we distinguish between supervised and unsupervised out-of-distribution by letting methods be classified as supervised that use any kind of target value $y$, whether it relates to the original task, available OOD data, or both. This is the same distinction made by \textcite{graham_denoising_2023} and \textcite{liu_unsupervised_2023a}. 
    However, it is important to note that in other works, the distinction is made based on whether the model is trained on OOD data or not \cite{hendrycks_baseline_2017,liu_energy-based_2020}. This difference in nomenclature is currently unresolved in the literature. 
}. 
Supervised OOD detection tries to assess whether the model's prediction $p(y|\xb)$ should be trusted for a given input $\xb$, whereas unsupervised OOD detection judges whether the input $\xb$ should be trusted and used at all. 

Central to any OOD detection method is the ability to assign a score $s(\xb) \in \mathbb{R}$ to a given input $\xb$ that indicates the degree to which the input is likely to be OOD. After obtaining such a scoring method, we typically use a validation set to tune a threshold $\tau$ such that $\xb$ is considered OOD if $s(\xb) > \tau$. The threshold is typically chosen such that performance on the validation set is above some standard, for instance by imposing constraints on the recall and precision. 

There are many ways to obtain such a scoring method. 
Unsupervised methods often derive the score 
from the \emph{likelihood} assigned to the input \cite{bishop_novelty_1994,choi_waic_2019,kirichenko_why_2020,ren_likelihood_2019,serra_input_2020,xiao_likelihood_2020,morningstar_density_2021,nalisnick_detecting_2019,bergamin_modelagnostic_2022,maaloe_biva_2019,havtorn_hierarchical_2021}, 
from a \emph{reconstruction} of the input $\xb$ \cite{sakurada_anomaly_2014,xia_learning_2015,lyudchik_outlier_2016,zhou_anomaly_2017,chen_outlier_2017,schlegl_unsupervised_2017,zong_deep_2018,li_madgan_2019,graham_denoising_2023,liu_unsupervised_2023a}, or 
from a latent \emph{representation} of the input \cite{denouden_improving_2018,hendrycks_using_2019,ahmadian_likelihoodfree_2021,bergman_classificationbased_2020,tack_csi_2020,sehwag_ssd_2021,xiao_we_2021}.  % https://arxiv.org/pdf/2302.10326.pdf
Supervised methods often derive the score 
from the \emph{probabilities} given by the predictive distribution $p(y|\xb)$ \cite{hendrycks_baseline_2017,hendrycks_scaling_2022} or 
from the \emph{logits} of $p(y|\xb)$ \cite{hendrycks_2022_scaling,liu_energy-based_2020,}. 
Other methods use a latent \emph{representation} of the input \cite{lee_simple_2018,li_anomaly_2019,ndiour_outofdistribution_2020,cook_outlier_2020,zaeemzadeh_outofdistribution_2021}, similar to unsupervised methods. 
In the following sections, we provide a more in-depth overview of the different approaches to OOD detection following the above categorization into supervised and unsupervised methods. For the supervised methods we further distinguish between whether methods use real OOD data, synthetic OOD data, or no OOD data at all. 

\subsection{Supervised out-of-distribution detection}
% OUTLINE:
% - Supervised OODD:
%   - Requires some examples of (synthetic) OODD data, which in some sense makes it inherently flawed. We can never really be sure that we have good enough coverage of OODD data.
%   - One limitation of model-dependent OoD techniques is that they may discard information about p(x) in learning the task-specific model p(y|x). That information may be useful for OoD detection.
%   - Likelihood Ratios for Out-of-Distribution Detection \cite{ren_likelihood_2019} provides a number of baselines for supervised OODD:
%       - 1 Maximum class probability of the predictive distribution p(y|x) \cite{hendrycks_baseline_2017}
%       - 2 Entropy of predictive distribution p(y|x) \cite{ren_likelihood_2019}
%       - 3 ODIN \cite{liang_enhancing_2018} (Uses synthetic OOD data by adding noise to in-distribution)
%       - 4 Mahalanobis distance to class conditional Gaussian distribution \cite{lee_simple_2018}
%       - 5 DeepEnsemble \cite{lakshminarayanan_simple_2017} The classifier-based ensemble method that uses the average of the predictions from multiple independently trained models with random initialization of network parameters and random shuffling of training inputs
%       - 6 The log-odds of a binary classifier trained to distinguish between in-distribution inputs from all classes as one class and randomly perturbed in-distribution inputs as the other. \cite{ren_likelihood_2019}
%       - 7 The maximum class probability over K in-distribution classes of a (K + 1)-class classifier where the additional class is perturbed in-distribution. \cite{ren_likelihood_2019}
%       - 8 The maximum class probability of a K-class classifier for in-distribution classes but the predicted class distribution is explicitly trained to output uniform distribution on perturbed in-distribution inputs. \cite{ren_likelihood_2019}. This is similar to using simulated OOD inputs from GAN \cite{lee_training_2018} or using auxiliary datasets of outliers \cite{hendrycks_deep_2019} for calibration purpose.
%   - X Deep semi-supervised anomaly detection \cite{ruff_deep_2020}.
%   - Variational information bottleneck (VIB) (Alemi et al., 2018b) performs divergence estimation in latent space to detect OoD, but is technically a model-dependent technique because the latent code is trained jointly with the downstream classification task. \cite{alemi_deep_2017}
%
% - ViM: Out-Of-Distribution with Virtual-logit Matchinghttps://arxiv.org/pdf/2203.10807.pdf in cludes a short overview of many modern methods
%
In the supervised setting the supervision can come from different sources depending on how much we know, or assume to know, about the OOD data. In the least informed setting, the supervision is from the target variable $y$ only. In the most informed setting we also assume access to representative OOD data. 

% Actual OOD data
\paragraph{Methods using real OOD data}
Methods that use representative OOD data have achieved high performance since they can directly learn to distinguish between in-distribution and OOD data. 
\textcite{hendrycks_deep_2019} augments the original training objective with a task-dependent outlier exposure loss that aims to make the output logits different for outlier data. 
In a similar vein, \textcite{dhamija_reducing_2018} propose losses designed to maximize the entropy of $p(y|\xb)$ and decrease feature magnitudes for OOD data sampled from other datasets. 
\textcite{ruff_deep_2020} use semi-supervised learning and learn representations of in-distribution data that concentrate close to a centroid in latent space while labelled outliers are pushed away from the centroid. 
Other methods including MCD \cite{yu_unsupervised_2019}, NGC \cite{wu_ngc_2021}, and UDG \cite{yang_semantically_2021} utilize external, unlabeled, noisy data to improve OOD detection performance without requiring cleanly labelled OOD examples.
As we discussed earlier, for many modern applications of machine learning, input data is often high dimensional and complex making sufficiently sampling the space of out-of-distribution data intractable. 
Therefore, methods that rely on samples of OOD data are inherently limited by its availability and how well it covers the large variation in OOD data. 

% Synthetic OOD data
\paragraph{Methods using synthetic OOD data}
A number of methods do not require access to actual OOD data but synthesize it instead. 
Several methods synthesize this OOD data by adding noise to in-distribution data \cite{liang_enhancing_2018, lee_simple_2018, ren_likelihood_2019}. 
For instance, \textcite{ren_likelihood_2019} propose a number of baselines including training a binary classifier to distinguish between original and perturbed in-distribution data. They also propose adding an OOD class to softmax classifiers and training it to predict perturbed in-distribution data, or alternatively, training the predicted class distribution to output uniform distribution for perturbed in-distribution inputs. 
With ODIN \cite{liang_enhancing_2018}, the authors propose to calibrate $p(y|\xb)$ with temperature scaling \cite{guo_calibration_2017} and add gradient-based, input-dependent perturbations to the inputs to use the calibrated maximum class probability as the OOD score. This method does not require retraining the classifier and generally improves on the baselines of \textcite{ren_likelihood_2019}. 
\textcite{vyas_outofdistribution_2018} train an ensemble of classifiers on different subsets of the training data, with the left out data taken as OOD, and propose novel loss over $p(y|\xb)$ that seeks to maintain a set margin between its average entropy for the OOD and in-distribution examples. 
Another approach generates OOD inputs using a GAN \cite{lee_training_2018}. 
Similar to actual OOD data, the usefulness of synthetic OOD data is also fundamentally limited by the intractability of sampling the complete space of OOD data.

% No OOD data, probs
\paragraph{Methods not using OOD data}
The least informed supervised OOD detection methods do not require instantiations of any kind of OOD data, real or synthetic. 
A baseline approach uses the maximum class probability of $p(y|\xb)$ directly by noting that it tends to be larger for correctly classified examples \cite{hendrycks_baseline_2017}. Another baseline method proposes that a high entropy of $p(y|\xb)$ indicates an OOD input \cite{ren_likelihood_2019} 
Other methods that derive the score from the classifier probabilities include \textcite{lakshminarayanan_simple_2017} who propose to use an ensemble of independently trained classifiers to discriminate between in-distribution and OOD data by evaluating the agreement between the classifiers, \textcite{devries_learning_2018} who augment the network with a confidence estimation branch that learns to estimate the confidence of the classifier separately from the probability, and \textcite{huang_importance_2021} who compute the gradient of the KL-divergence of the predictive distribution $p(y|\xb)$ to a uniform distribution noting that the magnitude of gradients is higher for in-distribution data than for OOD data. 
The Variational Information Bottleneck \cite{alemi_deep_2017} jointly learns a probabilistic latent representation, $\zb$ and $p(y|\xb)$, using a generalized variational autoencoder \cite{kingma_autoencoding_2014} that tries to maximize the mutual information between $\zb$ and $y$.

\textcite{hsu_generalized_2020} proposes a generalized version of ODIN that removes the need for simulating OOD data.
The authors note that most current methods make a closed world assumption and implicitly condition on the in-domain $\mathcal{D}_{\text{in}}$ in the form of the predictive distribution $p(y|\xb, \mathcal{D}_{\text{in}})$. With this observation, the authors decompose the $p(y|\xb, \mathcal{D}_{\text{in}})$ into a joint class-domain probability and a domain probability,
%
\begin{equation} \label{eq: joint class-domain and domain probability}
    p(y|\xb,\mathcal{D}_{\text{in}}) = \frac{p(y, \mathcal{D}_{\text{in}}|\xb)}{p(\mathcal{D}_{\text{in}}|\xb)} \enspace .
\end{equation}
%
Without data from the out-domain, it is not possible to directly learn either $p(y, \mathcal{D}_{\text{in}}|\xb)$ or $p(\mathcal{D}_{\text{in}}|\xb)$. Instead, the authors use this observation to impose the inductive bias of predicting logits as a fraction between two carefully designed network branches, imitating the form of \cref{eq: joint class-domain and domain probability}. 

% No OOD data, logits
A number of works have noted that the maximum softmax probability is not generally a reliable score for OOD detection \cite{hendrycks_scaling_2022,liu_energy-based_2020}. 
\textcite{liu_energy-based_2020} make an interesting argument as to why based on the energy $E(\xb; f)$ of a softmax classifier $f(\xb)$ \cite{lecun_tutorial_2006},
%
\begin{equation} \label{eq:softmax-classifier-energy}
    E(\xb; f) = - \log \sum_{i=1}^K \exp \left( f_i(\xb) \right) \enspace .
\end{equation}
%
Specifically, the authors relate the maximum softmax probability,
%
\begin{align} \label{eq:softmax-classifier-maximum-probability}
    \max_y p(y|\xb;f) 
    = \max_y \frac{\exp f_y(\xb)}{\sum_{i=1}^K \exp f_i(\xb)} 
    = \frac{\exp f^\text{max}(\xb)}{\sum_{i=1}^K \exp f_i(\xb)} \enspace ,
\end{align}
%
to the energy by noting that,
%
\begin{align}
    \log \max_y p(y|\xb;f) = E(\xb; f(\xb) - f^\text{max}(\xb)) = E(\xb; f) - f^\text{max}(\xb) \enspace .
\end{align}
%
This shows that log of the softmax confidence score is equivalent to the special case of the energy score, where all the logits are shifted by their maximum logit value. 
The authors empirically show that $E(\xb;f)$ is a more reliable score for OOD detection than the maximum softmax probability and note that the energy $E(\xb; f)$ tends to be larger for OOD data than for in-distribution data - contrary to the maximum softmax probability.
They conclude that this shift results in $\max_y p(y|\xb;f)$ being a biased score for OOD detection and propose instead to use the energy directly as the OOD score. This energy-score is improved in ReAct by feature clipping \cite{sun_react_2021}. 

% No OOD, feature
The final category of supervised methods derive the score from a latent representation of the input. 
Some methods define OOD classes. \textcite{huang_mos_2021} groups the classes of the target variable $y$ and defines an OOD class for each group. Each training example is then the correct target for one group and an OOD example for all other groups; a kind of hierarchical version of the OOD class of simpler baselines based on noise augmentation \cite{ren_likelihood_2019}. 
Similarly, to represent a virtual OOD class, \textcite{wang_vim_2022} generates an additional logit by first computing the residual of the input's latent space representation against a principal feature space and then converting it to a valid logit by matching its mean over training samples to the average maximum logits. 

Other methods note that the difficulty of detecting OOD data might be attributed to the curse of dimensionality in the learned feature spaces and propose to use dimensionality reduction techniques. In \textcite{ndiour_outofdistribution_2020}, apply dimensionality reduction on learned, high-dimensional features to capture the true feature subspace and compute the norm of the difference between the original feature and the pre-image of its low-dimensional manifold embedding. 
\textcite{zaeemzadeh_outofdistribution_2021} forces the ID samples to embed into a union of 1-dimensional subspaces during training and computes the minimum angular distance from the feature to the class-wise subspaces. 
NuSA \cite{cook_outlier_2020} uses projects features onto the column space of the classification weight matrix and computes the ratio of the norm the projected and original features.
\textcite{lee_simple_2018} fit a multivariate Gaussian distribution to the activations of the penultimate layer of a pre-trained classifier and use the Mahalanobis distance to this distribution to evaluate whether inputs are OOD. This method can also be seen as ameliorating the curse of dimensionality by clustering the high-dimensional feature space.

% Summary and weaknesses of supervised OOD detection approaches
% Methods that use real OOD data achieve high performance but are limited by the availability of OOD data and how well it covers the large variation in OOD data.
% A general weakness of all supervised out-of-distribution detection is that in learning the task-specific model $p(y|\xb)$ a model may discard information about $p(\xb)$ which could be useful for out-of-distribution detection.

% This makes them inherently flawed since we can never really be sure that we have good enough coverage of OOD data.
% Furthermore, the methods that use synthetic OOD data are limited by the intractability of sampling the complete space of OOD data. 


%       - 2 Entropy of predictive distribution p(y|x) \cite{ren_likelihood_2019}
%       - 6 The log-odds of a binary classifier trained to distinguish between in-distribution inputs from all classes as one class and randomly perturbed in-distribution inputs as the other. \cite{ren_likelihood_2019}
%       - 7 The maximum class probability over K in-distribution classes of a (K + 1)-class classifier where the additional class is perturbed in-distribution. \cite{ren_likelihood_2019}
%       - 8 The maximum class probability of a K-class classifier for in-distribution classes but the predicted class distribution is explicitly trained to output uniform distribution on perturbed in-distribution inputs. \cite{ren_likelihood_2019}. This is similar to using simulated OOD inputs from GAN \cite{lee_training_2018} or using auxiliary datasets of outliers \cite{hendrycks_deep_2019} for calibration purpose.

%       - 3 ODIN \cite{liang_enhancing_2018} (Uses synthetic OOD data by adding noise to in-distribution)
%       - 4 Mahalanobis distance to class conditional Gaussian distribution \cite{lee_training_2018}
%       - 5 DeepEnsemble \cite{lakshminarayanan_simple_2017} The classifier-based ensemble method that uses the average of the predictions from multiple independently trained models with random initialization of network parameters and random shuffling of training inputs

%   - Variational information bottleneck (VIB) (Alemi et al., 2018b) performs divergence estimation in latent space to detect OoD, but is technically a model-dependent technique because the latent code is trained jointly with the downstream classification task. \cite{alemi_deep_2017}

% either synthetic out-of-distribution data or from real out-of-distribution data. Synthetic out-of-distribution data is data that is generated by some process that is known to be out-of-distribution. For example, if we are training a model to classify images of cats and dogs, we could generate synthetic out-of-distribution data by adding noise to the images. Real out-of-distribution data is data that is known to be out-of-distribution, but is not generated synthetically. For example, if we are training a model to classify images of cats and dogs, we could use images of cars as real out-of-distribution data.


\subsection{Unsupervised out-of-distribution detection}
% OUTLINE:
% - Unsupervised ODDD
%   - No OODD data required. Isolates uncertainty related to the input data.
%   - Unsupervised can be categorised into generative and reconstructive.
%   - Generative
%       - 9 Compute WAIC (\cite{watanabe_algebraic_2009, watanabe_asymptotic_2010}) using ensemble of generative models \cite{choi_waic_2019}
%       - Likelihood Ratios for Out-of-Distribution Detection \cite{ren_likelihood_2019}
%       - Input complexity and out-of-distribution detection with likelihood-based generative models \cite{serra_input_2020}
%       - Likelihood regret for variational autoencoders \cite{xiao_likelihood_2020}
%       - Typicality test for generative OODD (Detecting out-of-distribution inputs to deep generative models using typicality) \cite{nalisnick_detecting_2019}
%       - Density of states estimation for out of distribution detection \cite{morningstar_density_2021}
%   - Reconstructive
%       - A review of novelty detection \cite{pimentel_review_2014}
%       - Outlier detection using autoencoders \cite{lyudchik_outlier_2016} and ensembles of autoencoders \cite{chen_outlier_2017}
%       - Improving reconstruction autoencoder out-of-distribution detection with mahalanobis distance \cite{denouden_improving_2018}
%       - Denosing diffusion models for out-of-distribution detection \cite{graham_denoising_2023}
%       - Diffusion inpainting \cite{liu_unsupervised_2023a}

% , MC Dropout (Gal & Ghahramani, 2016) and  model a calibrated predictive distribution for a classification task.

% Assume we have a dataset $\mathcal{D}_{\text{training}}$ of in-distribution data and we would like to build a model that can be used to assess whether a certain data point $\xb$ is sampled from $\mathcal{D}_{\text{training}}$ or not. Given a test point $\xb$, the model outputs a score $s(\xb)$ that indicates whether $\xb$ is in-distribution or out-of-distribution. 

Unsupervised methods often derive the score ...

\paragraph{Likelihood-based}
from the likelihood assigned to the input \cite{bishop_novelty_1994,choi_waic_2019,kirichenko_why_2020,ren_likelihood_2019,serra_input_2020,xiao_likelihood_2020,morningstar_density_2021,nalisnick_detecting_2019,bergamin_modelagnostic_2022,maaloe_biva_2019,havtorn_hierarchical_2021}, 

\paragraph{Reconstruction-based}
from a reconstruction of the input $\xb$ \cite{sakurada_anomaly_2014,xia_learning_2015,lyudchik_outlier_2016,zhou_anomaly_2017,chen_outlier_2017,schlegl_unsupervised_2017,zong_deep_2018,li_madgan_2019,graham_denoising_2023,liu_unsupervised_2023a}, or 

\paragraph{Representation-based}
from a latent representation of the input \cite{denouden_improving_2018, hendrycks_using_2019, ahmadian_likelihoodfree_2021,bergman_classificationbased_2020, tack_csi_2020, sehwag_ssd_2021, xiao_we_2021}.  % https://arxiv.org/pdf/2302.10326.pdf



% Out-of-distribution detection and anomaly detection are both related concepts in machine learning and deep learning, but they address slightly different problems and have distinct focuses.

% Out-of-Distribution Detection (OOD Detection):
% Out-of-distribution detection involves determining whether a given input or data point belongs to the same distribution as the training data. In other words, it helps identify whether an input is coming from a category or distribution that the model has never encountered during training. This is especially important for safety-critical applications where the model should not make predictions on inputs that are significantly different from what it has seen before. OOD detection aims to prevent the model from making confident predictions on unfamiliar inputs that might lead to incorrect results.
% Anomaly Detection:
% Anomaly detection, on the other hand, focuses on identifying rare and unusual instances within the data, regardless of whether they belong to the same distribution as the training data or not. Anomalies can be defined as data points that deviate significantly from the norm or expected behavior. Anomaly detection is used to find instances that are different from the majority and might indicate potential errors, fraud, faults, or other unusual occurrences.
% In summary, the key differences are:

% Focus:
% OOD Detection: Focuses on detecting inputs that are coming from distributions that differ significantly from the training data distribution.
% Anomaly Detection: Focuses on identifying rare and unusual instances, regardless of whether they come from a different distribution or not.
% Purpose:
% OOD Detection: Primarily used for safety and robustness, ensuring that the model doesn't make confident predictions on unfamiliar data.
% Anomaly Detection: Used for various applications such as fraud detection, fault detection, and identifying outliers in the data.
% Detection Approach:
% OOD Detection: Often involves measuring the uncertainty of the model's predictions. High uncertainty indicates that the input might be out-of-distribution.
% Anomaly Detection: Focuses on detecting data points that are statistically rare or far from the norm.
% It's worth noting that while the concepts are distinct, they can sometimes overlap. An out-of-distribution data point could also be considered an anomaly, but not all anomalies are necessarily out-of-distribution. Anomaly detection methods might use OOD detection techniques as part of their process to flag anomalous instances that are not in line with the expected distribution.




% Nice review:
% https://openaccess.thecvf.com/content/CVPR2023W/VAND/papers/Graham_Denoising_Diffusion_Models_for_Out-of-Distribution_Detection_CVPRW_2023_paper.pdf

% \cite{kabir_neural_2018} Survey of uncertainty quantification in deep learning.

% https://arxiv.org/pdf/2306.05674.pdf




\section{Variational autoencoders}


VAE \cite{kingma_autoencoding_2014,rezende_stochastic_2014}

Derive ELBO

Importance sampling \cite{burda_importance_2016}

Semi-supervised learning \cite{kingma_semi-supervised_2014,maaloe_semi-supervised_2017,siddharth_learning_2017}

Trouble in paradise \cite{tomczak_trouble_2022}

% Trouble in paradise: factoriing the ELBO to show that we minimize the mutual information between z and x. 
% Use this as transition to self-supervised methods as alternative for representation learning: 
% https://jmtomczak.github.io/blog/13/13_trouble_in_paradise.html


\section{Speech representation learning}



\iffalse


% TODO Delete this probably
Following the derivation of \textcite{jaynes_information_1957}, in the discrete entropy expression in \cref{eq:information-entropy}, we assume that the number of discrete points, $x_i, i=1,2,\dots,N$, in the support of $X$ grows larger such that in the limit $N\rightarrow\infty$, the density of points becomes a continuous function $m(x)$, and we can replace the sum with an integral. Denoting the number of points satisfying $a<x_i<b$ by $N_{a,b}$, we have
%
\begin{equation}
    \lim_{N\rightarrow\infty} \frac{N_{a,b}}{N} = \int_a^b m(x) \, dx \enspace .
\end{equation}
%
If this limit behaves sufficiently well, adjacent differences $(x_{i+1} - x_{i})$ in the neighborhood of any particular value of $x$ will also tend to zero, so that
%
\begin{equation} \label{eq:limiting-density-discrete-to-continuous}
    \lim_{N\rightarrow\infty} \left[ N(x_{i+1} - x_{i}) \right] = \left[ m(x_{i}) \right]^{-1} \enspace .
\end{equation}
%
In the limit, the value of the discrete probability mass function will become a continuous probability density function $w(x)$, according to the limiting form of
%
\begin{equation}
    p_i = w(x_i)\left(x_{i+1} - x_{i}\right) \enspace ,
\end{equation}
%
where $p_i$ is the value of the probability mass function at $x_i$ which via \cref{eq:limiting-density-discrete-to-continuous} becomes,
%
\begin{equation}
    p_i = w(x_i)\left[Nm(x_i)\right]^{-1} \enspace .
\end{equation}
%
Consequently, the discrete entropy expression in \cref{eq:information-entropy} becomes,
\begin{align}
    % H_{N,\text{discrete}}(X) &= - \int_\mathcal{X} w(x)\left[Nm(x)\right]^{-1} \, dx \, \log \left[\frac{w(x)}{Nm(x)}\right] \nonumber \\
    % &= - \int_\mathcal{X} w(x) \, dx \, \log \left[\frac{w(x)}{Nm(x)}\right]
    H_{N,\text{discrete}}(X) &= - \int_\mathcal{X} w(x) \, dx \, \log \left[\frac{w(x)}{Nm(x)}\right] \nonumber \\
\end{align}
%
and taking the limit $N\rightarrow\infty$,
%
\begin{equation}
    \lim_{N\rightarrow\infty} H_{N,\text{discrete}}(X) = H_{N} = \log N - \int_\mathcal{X} w(x) \log \left[\frac{w(x)}{m(x)}\right] \enspace .
\end{equation}
%
Note that since $\lim_{N\rightarrow\infty} \log N = \infty$, this procedure suggests that the entropy in the discrete sense of a continuous random variable should be infinite.




Modelling paradigms
\begin{enumerate}
    \item Variational inference and variational autoencoders
    \item Automatic speech recognition
    \item Self-supervised learning
\end{enumerate}

\cite{kingma_autoencoding_2014}
\cite{rezende_stochastic_2014}

Two approaches to statistics:\\
- Bayesian\\
- Frequentist\\

Two kinds of uncertainty:\\
- Aleatoric: Uncertainty inherent to the data. Irreducible. "Known unknowns".\\
- Epistemic: Uncertainty due to lack of knowledge. Can be reduced with more data. "Unknown unknowns".\\

Approaches to uncertainty in deep learning:\\
- Tuning of supervised classifiers to output well-calibrated probabilities that correspond to the actual likelihood of the prediction being correct (on some validation set).\\
- Learning rich unsupervised representations of the data that can be used to estimate the uncertainty of the prediction (e.g. by measuring the distance to the nearest training example in the latent space).\\
- Learning of a probability distribution over the parameters of the model, which can then be used to compute the uncertainty of the prediction (Bayesian neural networks).\\


\fi 
