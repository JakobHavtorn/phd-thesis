%!TEX root = ../thesis.tex

% ~8 pages

\chapter[technical background]{Technical Background}\label{chp:technical-background}

\section{Uncertainty and information}

% https://en.wikipedia.org/wiki/Uncertainty#cite_note-3
% https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8371683

In society, the concept of uncertainty goes by many names, and its meaning can vary depending on the specific context. However, across most quantitative scientific fields, a consensus definition appears to align with the following \cite{hubbard_how_2014}:
%
\begin{center}
    \textit{Uncertainty is the lack of certainty; a state of limited knowledge where it is impossible to exactly describe the existing state, a future outcome, or more than one possible outcome.}
\end{center}
%
While such a purely lexical definition of uncertainty might trigger a philosophical inquisition, it highlights an important connection between uncertainty, information and probability. When information is not complete, we can describe the state of the world only with uncertainty; we are simply not certain. In that sense, uncertainty is a word we use to refer to information that is missing. A natural way to describe a state of the world with uncertainty is to use probabilities. To our luck, information theory provides exactly that; a mathematically rigorous method for quantifying information through the language of probabilities. In this context, uncertainty can be understood as the information entropy of a random variable \cite{mackay_information_2003}. 


\subsection{Entropy for discrete random variables}
First characterized by Claude Shannon in 1948 \cite{shannon_mathematical_1948}, information entropy, $H(X)$, is a measure of the average amount of information contained in a discrete random variable $X$. Shannon's definition follows from three fundamental axioms of information theory. Let $I_X(x)$ be the information carried by a specific outcome $x$ of sampling the discrete random variable $X$ with probability mass function $p_X(x)$. Then, these axioms are as follows:
\begin{enumerate}[label=(\Roman*)]
    \item The more likely an outcome is, the less information it carries; $I_X(x)$ is a monotonically decreasing function in $p_X(x)$.
    \item Outcomes that are certain to happen carry no information; if $p_X(x) = 1$ then $I_X(x) = 0$.
    \item The joint information carried by independent outcomes is the sum of the information carried by each outcome; if $x_i$ and $x_j$ are independent then $I(x_i, x_j) = I(x_i) + I(x_j)$. 
\end{enumerate}

From these axioms, Shannon found that the \emph{information content} $I_X(x)$ of an outcome $x$ with probability $p_X(x)$ can suitably be defined as the negative logarithm of the probability of the outcome,
%
\begin{equation} \label{eq:information-content}
    I_X(x) = -\log_b p_X(x) \enspace .
\end{equation}
%
% Information content is measured in units of bits when the logarithm base $b$ is $2$, or nats when $b=e$.

The \emph{information entropy} of a random variable $X$ is defined as the \emph{expected information content} of an outcome of $X$,
%
\begin{equation} \label{eq:information-entropy}
    H(X) = \mathbb{E}_X\left[I_X(x)\right] = - \sum_{x\in X} p_X(x) \log p_X(x) \enspace .
\end{equation}
%
This means that $H(X)$ measures the amount of information we can expect to gain from observing an outcome of $X$, when we know only its distribution $p_X(x)$. 

\lesstodo[inline]{Come up with a suitable example explaining the intuition behind information entropy.}


\subsection{Entropy for continuous random variables}
To generalize the concept of information entropy to continuous random variables, Shannon originally replaced the sum over the probability mass function in \eqref{eq:information-entropy} with an integral over the probability density function, as suggested by the definition $H(X) = \mathbb{E}_X\left[I_X(x)\right]$. This leads to the definition of the information entropy of a continuous random variable $X$ as the \emph{differential entropy} \cite{shannon_mathematical_1948}, 
%
\begin{equation} \label{eq:differential-entropy}
    H(X) = \mathbb{E}_X\left[I_X(x)\right] = - \int_{\mathcal{X}} p_X(x) \log p_X(x) \, dx \enspace ,
\end{equation}
%
where $\mathcal{X}$ is the support of $X$. 

However, the differential entropy does not have several of the desired attributes of the discrete version; it can be negative, it is not invariant under a change of variables, and it is not dimensionally correct%
\footnote{
    Information entropy has the same dimensionality as information content which is typically measured in units of bits, depending on the base of the logarithm. 
    However, the bit is not a base unit of the International System of Units (SI), nor is it an official derived unit.
    A bit is quite simply a number, specifically 0 or 1. If something is said to have a size of 4 bits, it means that it can be described with 4 binary digits, i.e. 4 numbers that are either 0 or 1.

    As such, it might be useful to think of information as a dimensionless quantity rather than a quantity with units.
    Following this interpretation, the dimensionality of the discrete information entropy in \cref{eq:information-entropy} is also dimensionless, since a probability mass function is dimensionless. 
    For probability density functions, which have dimensionality of the inverse of some quantity, e.g. inverse length, the dimensionality of differential entropy in \cref{eq:differential-entropy} becomes,
    \begin{equation}
        \text{dim} \left( H(X) \right) = \text{dim} \left( \int_{\mathcal{X}} p_X(x) \log p_X(x) \, dx \right) = \frac{1}{\text{length}} \log\left(\text{length}\right) \enspace .
    \end{equation}
    This is clearly not dimensionless and therefore does not correspond to the discrete information entropy.
    
    % As a final twist on this story, note that the SI system does actually have a unit for an amount of substance: the mole \cite{internationalbureauofweightsandmeasures_systeme_2019}. 
    % The mole is defined as the amount of substance that contains as many elementary entities as there are atoms in $\SI{12}{g}$ of carbon-12, that is $\SI{1}{mol} = 6.02214076\times10^{23}$.
    % Taking the position that a mole is quite simply a number (not a number of particles) \cite{debievre_atomic_1992,baranski_atomic_2012}, and a bit is a number of elementary (information) entities, we could define a bit as a derived unit from the mole as $\SI{1}{bit} = \SI{1}{mol} / 6.02214076\times10^{23}$.
}. 
For these reasons, the differential entropy may not be a suitable measure of information, or uncertainty, for continuous random variables. 

Instead, it can be argued that the information entropy of a continuous distribution should be defined via the limiting density for discrete points \cite{jaynes_information_1957,jaynes_prior_1968}. Following the derivation of \textcite{jaynes_information_1957}, in the discrete entropy expression in \cref{eq:information-entropy}, we assume that the number of discrete points $x_i, i=1,2,\dots,N$, in the support of $X$ grows larger such that in the limit $N\rightarrow\infty$, the density of points becomes a continuous function $m(x)$ and we can replace the sum with an integral. Denoting the number of points satisfying $a<x_i<b$ by $N_{a,b}$, we have
%
\begin{equation}
    \lim_{N\rightarrow\infty} \frac{N_{a,b}}{N} = \int_a^b m(x) \, dx \enspace .
\end{equation}
%
If this limit behaves sufficiently well, adjacent differences $(x_{i+1} - x_{i})$ in the neighbourhood of any particular value of $x$ will also tend to zero, so that
%
\begin{equation} \label{eq:limiting-density-discrete-to-continuous}
    \lim_{N\rightarrow\infty} \left[ N(x_{i+1} - x_{i}) \right] = \left[ m(x_{i}) \right]^{-1} \enspace .
\end{equation}
%
In the limit, the value of the discrete probability mass function will become a continuous probability density function $w(x)$, according to the limiting form of
%
\begin{equation}
    p_i = w(x_i)\left(x_{i+1} - x_{i}\right) \enspace ,
\end{equation}
%
where $p_i$ is the value of the probability mass function at $x_i$ which via \cref{eq:limiting-density-discrete-to-continuous} becomes,
%
\begin{equation}
    p_i = w(x_i)\left[Nm(x_i)\right]^{-1} \enspace .
\end{equation}
%
Consequently, the discrete entropy expression in \cref{eq:information-entropy} becomes,
%
\begin{align}
    H_{N,\text{discrete}}(X) &= - \sum_{i} p_i \log p_i \nonumber \\
    &= - \sum_{i\in\mathcal{X}} w(x_i)\left[Nm(x_i)\right]^{-1} \log \left( w(x_i)\left[Nm(x_i)\right]^{-1} \right) \nonumber \\
    &= - \sum_{i\in\mathcal{X}} w(x_i)\left[Nm(x_i)\right]^{-1} (\log w(x_i) - \log N - \log m(x_i)) \nonumber  \\
    &= 1
\end{align}

%
\begin{align}
    % H_{N,\text{discrete}}(X) &= - \int_\mathcal{X} w(x)\left[Nm(x)\right]^{-1} \, dx \, \log \left[\frac{w(x)}{Nm(x)}\right] \nonumber \\
    % &= - \int_\mathcal{X} w(x) \, dx \, \log \left[\frac{w(x)}{Nm(x)}\right]
    H_{N,\text{discrete}}(X) &= - \int_\mathcal{X} w(x) \, dx \, \log \left[\frac{w(x)}{Nm(x)}\right] \nonumber \\
\end{align}
%
and taking the limit $N\rightarrow\infty$,
%
\begin{equation}
    \lim_{N\rightarrow\infty} H_{N,\text{discrete}}(X) = H_{N} = \log N - \int_\mathcal{X} w(x) \log \left[\frac{w(x)}{m(x)}\right] \enspace .
\end{equation}
%
Note that since $\lim_{N\rightarrow\infty} \log N = \infty$, this procedure suggests that the entropy in the discrete sense of a continuous random variable should be infinite.


% I apologize for any confusion. Let's clarify step 6, focusing on the transition from the discrete entropy expression to the continuous entropy expression. I'll provide a more detailed explanation.

% Step 6: Expression for Discrete Entropy
% - The discrete entropy $H_{N,\text{discrete}}(X)$ using the probability mass function $p_i$ is given by:
%   \[ H_{N,\text{discrete}}(X) = - \sum_{i} p_i \log p_i. \]
% - We transition from discrete to continuous terms, keeping in mind that $p_i = w(x_i)[Nm(x_i)]^{-1}$.
% - So, the discrete entropy expression becomes:
%   \[ H_{N,\text{discrete}}(X) = - \sum_{i} w(x_i)\left[Nm(x_i)\right]^{-1} \log \left( w(x_i)\left[Nm(x_i)\right]^{-1} \right). \]

% Now, let's simplify the above expression by breaking it down step by step:

% 1. **Simplify the term inside the logarithm:**
%    \[ w(x_i)\left[Nm(x_i)\right]^{-1} = \frac{w(x_i)}{Nm(x_i)}. \]

% 2. **Apply the logarithm property $\log(ab) = \log a + \log b$:**
%    \[ \log \left( \frac{w(x_i)}{Nm(x_i)} \right) = \log w(x_i) - \log (Nm(x_i)). \]

% 3. **Distribute the negative sign from the entropy definition:**
%    \[ -w(x_i)\left[Nm(x_i)\right]^{-1} \log \left( \frac{w(x_i)}{Nm(x_i)} \right) = -w(x_i) \log w(x_i) + w(x_i) \log (Nm(x_i)). \]

% 4. **Combine the logarithms using properties of logarithms:**
%    \[ w(x_i) \log (Nm(x_i)) = \log \left( [Nm(x_i)]^{w(x_i)} \right) = \log \left( N^{w(x_i)} \cdot [m(x_i)]^{w(x_i)} \right). \]

% 5. **Apply the property $\log(a^b) = b \log a$:**
%    \[ \log \left( N^{w(x_i)} \cdot [m(x_i)]^{w(x_i)} \right) = w(x_i) \log N + w(x_i) \log m(x_i). \]

% 6. **Substitute this result back into the expression:**
%    \[ -w(x_i) \log w(x_i) + w(x_i) \log (Nm(x_i)) = -w(x_i) \log w(x_i) + w(x_i) \log N + w(x_i) \log m(x_i). \]

% 7. **Use the sum notation to represent the terms:**
%    \[ H_{N,\text{discrete}}(X) = - \sum_{i} \left( -w(x_i) \log w(x_i) + w(x_i) \log N + w(x_i) \log m(x_i) \right). \]

% 8. **Combine the terms inside the sum:**
%    \[ H_{N,\text{discrete}}(X) = \sum_{i} w(x_i) \log w(x_i) - \sum_{i} w(x_i) \log N - \sum_{i} w(x_i) \log m(x_i). \]

% 9. **Recognize that the first term is the entropy of a discrete distribution:**
%    \[ \sum_{i} w(x_i) \log w(x_i) = -\sum_{i} w(x_i) \log \frac{1}{w(x_i)} = -H(X). \]

% 10. **Recognize that the second term is $\log N$ multiplied by the sum of probabilities, which is 1:**
%    \[ \sum_{i} w(x_i) \log N = \log N \sum_{i} w(x_i) = \log N \cdot 1 = \log N. \]

% 11. **Substitute back into the expression:**
%    \[ H_{N,\text{discrete}}(X) = -H(X) - \log N - \sum_{i} w(x_i) \log m(x_i). \]

% This should give you a more detailed understanding of how the transition from the discrete entropy expression to the continuous entropy expression is made while taking into account the continuous limit.






% \cite{kabir_neural_2018} Survey of uncertainty quantification in deep learning.


% Two kinds of uncertainty:\\
% - Aleatoric: Uncertainty inherent to the data. Irreducible. "Known unknowns".\\
% - Epistemic: Uncertainty due to lack of knowledge. Can be reduced with more data. "Unknown unknowns".\\




% https://arxiv.org/pdf/2306.05674.pdf

% We compare our framework with several major related lines of work. First, our work focuses on the quantification of epistemic uncertainty, which refers to the errors coming from the inadequacy of the model or data noises. This is different from aleatoric uncertainty, which refers to the intrinsic stochasticity of the problem [87, 91, 112, 59, 34], or predictive uncertainty which captures the sum of epistemic and aleatoric uncertainties (but not their dissection) [89, 92, 12, 3, 22]. Regarding epistemic uncertainty, a related line of study is deep ensemble that aggregates predictions from multiple independent training replications [76, 70, 38, 8, 89]. This approach, as we will make clear later, can reduce and potentially quantify procedural variability, but a naive use would require demanding retraining effort and does not address data variability. Another line is the Bayesian UQ approach on neural networks [41, 2]. This regards network weights as parameters subject to common priors such as Gaussian. Because of the computation difficulties in exact inference, an array of studies investigate efficient approximate inference approaches to estimate the posteriors [40, 46, 16, 31, 30, 90, 74, 53]. While powerful, these approaches nonetheless possess inference error that could be hard to quantify, and ultimately finding rigorous guarantees on the performance of these approximate posteriors remains open to our best knowledge.

% 87: 
% 91: 
% 112:
% 59: 
% 34: 

% 89: 
% 92: 
% 12:
% 3: 
% 22: 

% 76: 
% 70: 
% 38: 
% 8: 
% 89: 



\section{Probabilistic latent variable models}




\section{Speech representation learning}




\iffalse


Modelling paradigms
\begin{enumerate}
    \item Variational inference and variational autoencoders
    \item Automatic speech recognition
    \item Self-supervised learning
\end{enumerate}

\cite{kingma_autoencoding_2014}
\cite{rezende_stochastic_2014}

Two approaches to statistics:\\
- Bayesian\\
- Frequentist\\

Two kinds of uncertainty:\\
- Aleatoric: Uncertainty inherent to the data. Irreducible. "Known unknowns".\\
- Epistemic: Uncertainty due to lack of knowledge. Can be reduced with more data. "Unknown unknowns".\\

Approaches to uncertainty in deep learning:\\
- Tuning of supervised classifiers to output well-calibrated probabilities that correspond to the actual likelihood of the prediction being correct (on some validation set).\\
- Learning rich unsupervised representations of the data that can be used to estimate the uncertainty of the prediction (e.g. by measuring the distance to the nearest training example in the latent space).\\
- Learning of a probability distribution over the parameters of the model, which can then be used to compute the uncertainty of the prediction (Bayesian neural networks).\\


In the field of machine learning, uncertainty is often represented by a probability distribution. The most common approach is to use Bayesian methods, where uncertainty is captured by posterior distributions over model parameters and predictions. Bayesian neural networks, for instance, offer a powerful framework to model uncertainty in deep learning architectures. By incorporating prior beliefs and updating them based on observed data, these networks can produce probabilistic predictions that provide a measure of uncertainty. This is particularly useful in applications where high-confidence predictions are required, and the consequences of errors can be significant.

Recently, another promising approach to uncertainty estimation in machine learning is the use of Monte Carlo Dropout \cite{gal_dropout_2016}. Monte Carlo Dropout leverages the idea of dropout regularization, originally employed during training to prevent overfitting. To form a prediction with associated uncertainty, Monte Carlo Dropout proposes to make multiple forward passes through the network while sampling different dropout masks for each one. This leads to obtaining a distribution of outputs for each input sample and the variance among these sampled predictions serves as a measure of uncertainty. Monte Carlo Dropout has shown remarkable success in various tasks such as image classification, object detection, and natural language processing. It is computationally efficient and can be easily incorporated into existing deep learning architectures, making it an attractive choice for uncertainty estimation.

Furthermore, there has been a surge of interest in ensemble methods for uncertainty estimation. Ensembles combine the predictions of multiple models to obtain a more robust and calibrated uncertainty measure. Bagging and boosting techniques, which have been widely used in the field of statistics, have found their way into the realm of machine learning for uncertainty estimation as well. By training multiple models with different initializations or using diverse learning algorithms, ensembles can capture different sources of uncertainty, thereby providing a comprehensive assessment of the overall uncertainty in the predictions.

Despite the progress made in uncertainty estimation for machine learning models, challenges still remain. One key concern is the interpretability of uncertainty measures. While probabilistic outputs are more informative than point estimates, effectively communicating uncertainty to end-users and decision-makers is not trivial. Developing visualization techniques and intuitive explanations for uncertainty is an ongoing area of research. Additionally, quantifying uncertainty in high-dimensional data or complex models with massive amounts of parameters requires careful consideration and efficient computational strategies.


\fi 
