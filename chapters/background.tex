%!TEX root = ../thesis.tex

% ~8 pages

\chapter[technical background]{Technical background}\label{chp:technical-background}

While the papers in this thesis were written to be self-contained, page limits and the need to maintain a specific focus, forced us to limit the scope of technical background each paper provides. In this chapter, we present a more in-depth overview of the technical background relevant to the papers in this thesis. 
We start by quantifying the concept of uncertainty, and how it relates to information and probability. We then introduce the task of out-of-distribution detection and a few ways to tackle it. 
Then, we present variational autoencoders, and how they can be used to learn representations of speech. Finally, we introduce the concept of self-supervised learning and highlight important differences to variational autoencoders. 


\section{Uncertainty and information}

In society, the concept of uncertainty goes by many names, and its meaning can vary depending on the specific context. However, across most quantitative scientific fields, a consensus definition appears to align with the following \cite{hubbard_how_2014}:
%
\begin{center}
    \textit{Uncertainty is the lack of certainty; a state of limited knowledge where it is impossible to exactly describe the existing state, a future outcome, or more than one possible outcome.}
\end{center}
%
Although such a purely lexical definition of uncertainty might prompt a philosophical inquiry, it highlights an important connection between uncertainty, information and probability. When information is limited, we can describe the state of the world only with uncertainty; we are simply not certain. In that sense, uncertainty is a word we use to refer to the information that is missing, whether we know what is missing or not. 
A natural way to describe an uncertain world state is to use probabilities. To our luck, information theory provides exactly that; a mathematically rigorous method for quantifying information through the language of probability theory. In the following, we shall see that, in this context, uncertainty can be understood as the information entropy of a random variable \cite{mackay_information_2003}. 


\subsection{Entropy for discrete random variables}

First characterized by Claude Shannon in 1948 \cite{shannon_mathematical_1948}, information entropy, $H(X)$, is a measure of the average amount of information contained in a discrete random variable $X$. Shannon's definition follows from three fundamental axioms of information theory. Let $I_X(x)$ be the information carried by a specific outcome $x$ of sampling the discrete random variable $X$ with probability mass function $p_X(x)$. Then, these axioms are as follows:
\begin{enumerate}[label=(\Roman*)]
    \item The more likely an outcome is, the less information it carries; $I_X(x)$ is a monotonically decreasing function in $p_X(x)$.
    \item Outcomes that are certain to happen carry no information; if $p_X(x) = 1$ then $I_X(x) = 0$.
    \item The joint information carried by independent outcomes is the sum of the information carried by each outcome; if $x_i$ and $x_j$ are independent then $I(x_i, x_j) = I(x_i) + I(x_j)$. 
\end{enumerate}

From these axioms, Shannon found that the \emph{information content} $I_X(x)$ of an outcome $x$ with probability $p_X(x)$ can suitably be defined as the negative logarithm of the probability of the outcome,
%
\begin{equation} \label{eq:information-content}
    I_X(x) = -\log_b p_X(x) \enspace .
\end{equation}
%

The \emph{information entropy of a discrete random variable} $X$ is defined as the \emph{expected information content} of an outcome of $X$,
%
\begin{equation} \label{eq:information-entropy}
    H(X) = \mathbb{E}_X\left[I_X(x)\right] = - \sum_{x\in X} p_X(x) \log p_X(x) \enspace .
\end{equation}
%
This means that $H(X)$ measures the amount of information we can expect to gain from observing an outcome of $X$, when we know only its distribution $p_X(x)$. If all possible outcomes are equally likely, then $H(X)$ is maximized, and if only one outcome is possible, then $H(X)$ is minimized.

\todo[inline]{Come up with a suitable example explaining the intuition behind information entropy.}


\subsection{Entropy for continuous random variables}

To generalize the concept of information entropy to continuous random variables, Shannon originally replaced the sum over the probability mass function in \eqref{eq:information-entropy} with an integral over the probability density function, as suggested by the definition $H(X) = \mathbb{E}_X\left[I_X(x)\right]$. This approach leads to the definition of the \emph{information entropy of a continuous random variable} $X$ as, 
%
\begin{equation} \label{eq:differential-entropy}
    H(X) = - \int_{\mathcal{X}} p_X(x) \log p_X(x) \, dx \enspace ,
\end{equation}
%
which is called the \emph{differential entropy} \cite{shannon_mathematical_1948}. Here, $\mathcal{X}$ is the support of $X$. 

However, the differential entropy does not have several of the desirable attributes of the discrete version; it can be negative, it is not invariant under a change of variables, and it is not dimensionally correct%
\footnote{\label{fn:dimensional-analysis-of-differential-entropy}
    Information entropy has the same dimensionality as information content which is typically measured in units of bits, but depends on the base of the logarithm. 
    However, the bit is not a base unit of the International System of Units (SI), nor is it an official derived unit.
    A bit is quite simply a number, specifically 0 or 1. If something is said to have a size of 4 bits, it means that it can be described with 4 binary digits, i.e. 4 numbers, each either 0 or 1.

    As such, it might be useful to think of information as a dimensionless quantity rather than a quantity with units.
    Following this interpretation, the discrete information entropy in \cref{eq:information-entropy} is also dimensionless, because a probability mass function is dimensionless. 
    Since probability density functions have dimensionality of the inverse of some quantity, e.g. inverse length, the dimensionality of the continuous differential entropy in \cref{eq:differential-entropy} becomes,
    \begin{equation*}
        \text{dim} \left( H(X) \right) = \text{dim} \left( \int_{\mathcal{X}} p_X(x) \log p_X(x) \, dx \right) = \frac{1}{\text{length}} \log\left(\text{length}\right) \enspace ,
    \end{equation*}
    which is clearly not dimensionless and therefore cannot correspond directly to the discrete information entropy.
    % As a final twist on this story, note that the SI system does actually have a unit for an amount of substance: the mole \cite{internationalbureauofweightsandmeasures_systeme_2019}. 
    % The mole is defined as the amount of substance that contains as many elementary entities as there are atoms in $\SI{12}{g}$ of carbon-12, that is $\SI{1}{mol} = 6.02214076\times10^{23}$.
    % Taking the position that a mole is quite simply a number (not a number of particles) \cite{debievre_atomic_1992,baranski_atomic_2012}, and a bit is a number of elementary (information) entities, we could define a bit as a derived unit from the mole as $\SI{1}{bit} = \SI{1}{mol} / 6.02214076\times10^{23}$.
}. 
For these reasons, the differential entropy may not be a suitable measure of information, or uncertainty, for continuous random variables. 

Instead, \textcite{jaynes_information_1957,jaynes_prior_1968} argued that the information entropy of a continuous distribution should be defined as the limiting density of increasingly dense discrete distributions, $H_{N\rightarrow\infty}(X)$. This argument leads to,
%
\begin{equation} \label{eq:corrected-differential-entropy-jaynes}
    H_{N\rightarrow\infty}(X) \equiv \lim_{N\rightarrow\infty} [H_{N}(X) - \log N] = - \int_\mathcal{X} p(x) \log \left(\frac{p(x)}{q(x)}\right) \enspace ,
\end{equation}
%
where $H_N(X)$ is the limiting density for discrete points, $q(x)$ is a uniform density over the quantization of the continuous space, and we have subtracted a $\log N$ term that would go to infinity in the limit of infinite points.
By doing this, the information entropy becomes positive, dimensionally correct, and invariant under a change of variables%
\footnote{\label{fn:change-of-variables-invariance-of-corrected-differential-entropy}
    Under a change of independent variable from $x$ to $y(x)$, we have that
    \begin{align*}
        \widetilde{w}(y)\,dy = w(x)\,dx \enspace , \quad
        \widetilde{q}(y)\,dy = q(x)\,dx \enspace .
    \end{align*}
    Plugging this into the differential entropy \cref{eq:corrected-differential-entropy-jaynes} we arrive at the original expression but now with $y$ as the independent variable,
    \begin{equation*}
        H_{N\rightarrow\infty}(y(x)) =  = - \int_\mathcal{Y} \widetilde{w}(y)\,\frac{dy}{dx} \log \frac{\widetilde{w}(y)\frac{dy}{dx}}{\widetilde{q}(y)\frac{dy}{dx}} \, dx = - \int_\mathcal{Y} w(y) \log \frac{w(y)}{q(y)} \, dy \enspace .
    \end{equation*}
}.

The form on the right-hand side of \cref{eq:corrected-differential-entropy-jaynes} can be recognized as the negative Kullback-Leibler divergence of $p(x)$ to $q(x)$ \cite{kullback_information_1959},
%
\begin{equation} \label{eq:kullback-leibler-divergence}
    D_{KL}(P\parallel Q) = \int_\mathcal{X} p(x) \log \left(\frac{p(x)}{q(x)}\right) \enspace .
\end{equation}
%
The Kullback-Leibler divergence $D_{KL}(P\parallel Q)$, also known as the \emph{information gain}, is often interpreted as the amount of additional information required to represent events from distribution $P$ with density $p(x)$ using a code optimized for distribution $Q$ with density $q(x)$. In other words, it measures how surprised one would be if they used distribution $Q$ to represent events from distribution $P$, i.e. the \emph{relative entropy} of $P$ with respect to $Q$.

Returning to Jaynes' argument and \cref{eq:corrected-differential-entropy-jaynes}, we can interpret this to say that the information entropy of a continuous random variable $X$ should be defined as the expected difference in information entropy between its density and a uniform density. 
Identifying the Kullback-Leibler divergence as a measure of this relative information entropy provides a natural point of convergence for this section since it is equivalently defined for both discrete and continuous random variables, contrary to the differential entropy. 
In this view, information entropy is a quantity that distills the distribution of a random variable into a single number that describes the diversity of the potential outcomes of the random variable. 
% In the following, we will see that this interpretation of information entropy as a measure of diversity is useful for understanding the concept of uncertainty in machine learning.

% In fact, Kullback initially motivated the divergence as an expected likelihood ratio \cref{kullback_information_1959}.


\section{Out-of-distribution detection} \label{sec:out-of-distribution-detection}
% 
% OUTLINE: C.f. https://openaccess.thecvf.com/content/CVPR2023W/VAND/papers/Graham_Denoising_Diffusion_Models_for_Out-of-Distribution_Detection_CVPRW_2023_paper.pdf
% - Formalize the problem. https://arxiv.org/pdf/2302.10326.pdf
%   - Assume we have a dataset $\mathcal{D}_{\text{train}}$ of in-distribution data and we would like to build a model that can be used to assess whether a certain data point $\xb$ is sampled from $\mathcal{D}_{\text{train}}$ or not. Given a test point $\xb$, the model outputs a score $s(\xb)$ that indicates whether $\xb$ is in-distribution or out-of-distribution. 
%   - In the context of neural networks, this problem dates back to at least 1994 \cite{bishop_novelty_1994}. 
%
% - Categorisation into supervised and unsupervised. 
%   - Supervised: Whether p(y|x) should be trusted for a given x. 
%   - Unsupervised: Whether x should be trusted and used at all. 
%
In this section we will introduce out-of-distribution (OOD) detection, relate it to the concept of uncertainty and to the previous section on information entropy. 
To supplement the short related work sections of the papers in \cref{chp:paper-hierarchical,chp:paper-modelagnostic}, we will then provide a concise review of the literature on out-of-distribution detection. A 

\subsection{An overview of the task}
%
OOD detection is the task of identifying test data that is unlikely to originate from the distribution of the training data and, in the context of neural networks, dates back several decades \cite{bishop_novelty_1994, chang_figure_1993}.
In the general case, we assume that we have a domain of in-distribution data, $\mathcal{D}_{\text{in}}$, and we would like to build a model that can be used to assess whether a test data point $\xb$ originates from that domain or not. 
Since such OOD data is unknown to the model, it is, by definition, a source of epistemic uncertainty. This makes OOD detection closely related to uncertainty quantification. Only in cases where strong prior knowledge of the data is incorporated into a model, can we expect it to be able to make reliable predictions on OOD data.

% In the literature, the task is also referred to as open set recognition \cite{bendale_open_2016, cardoso_weightless_2017, panaredabusto_open_2017}, outlier detection \cite{xu_deep_2014, mor_confidence_2018}, or selective prediction \cite{geifman_selective_2017} . 
OOD detection bears many similarities with anomaly detection, novelty detection, open set recognition, and outlier detection \cite{yang_generalized_2022}. 
Some of these differences are subtle, and the terminology is not always used consistently in the literature. To provide some clarity, we will briefly outline the taxonomy of \textcite{yang_generalized_2022}: 
Outlier detection most clearly differs from the other tasks by directly processesing all observations and aiming to select outliers from a single contaminated dataset. 
The remaining tasks differ in whether they detect both covariate and semantic shift and require the simultaneous classification of in-distribution classes.%, and whether the in-distribution data contains more than one class. 
Anomaly detection deals with multiclass data and detects both covariate and semantic shift without requiring simultaneous classification of in-distribution classes whereas novelty detection, open set recognition and OOD detection are usually concerned with semantic shifts. 
Although a vague difference, novelty detection usually defined in terms of a class of normal data, while OOD detection centers around the distribution of the training data. Different from novelty detection, OOD detection methods sometimes draw on examples of OOD data or require the simultaneous classification of in-distribution classes, although this is not always the case. 
OOD detection benchmarks almost always take OOD data to be from external datasets different from the training dataset which distinguishes it from open set recognition that usually benchmarks on a single dataset split into ID and OOD data. 
% Another closely related field is that of network-based uncertainty estimation \cite{neal_bayesian_1995, hernandez-lobato_probabilistic_2015, gawlikowski_survey_2023}. %[14, 10, 36, 20]. 
Although we share the sentiment of \textcite{yang_generalized_2022}, who propose to unify the tasks as ``generalized out-of-distribution detection'', this thesis will follow the nomenclature used in the works most related to ours and refer to the task as out-of-distribution detection.

Recently, several approaches for deep neural networks have been developed that formally address the rejection of samples $\xb\notin\mathcal{D}_{\text{in}}$. 
One way to categorize the different approaches is based on whether the underlying model is a classifier that estimates a conditional probability distribution $p(y|\xb)$ over some target variable given the input, or a model that learns a probability density $p(\xb)$ over the input itself. We might refer to the former as \emph{supervised} OOD detection, and the latter as \emph{unsupervised} OOD detection \cite{graham_denoising_2023,liu_unsupervised_2023a}. It is important to note that this distinction relates only to the availability of some target value, $y$ - not whether OOD data is available for supervision\footnote{
    In our overview, we distinguish between supervised and unsupervised out-of-distribution by letting methods be classified as supervised that use any kind of target value $y$, whether it relates to the original task, available OOD data, or both. This is the same distinction made by \textcite{graham_denoising_2023} and \textcite{liu_unsupervised_2023a}. 
    However, it is important to note that in other works, the distinction is made based on whether the model is trained on OOD data or not \cite{hendrycks_baseline_2017,liu_energy-based_2020}. This difference in nomenclature is currently unresolved in the literature. 
}. 
Supervised OOD detection tries to assess whether the model's prediction $\hat{y}$ via $p(y|\xb)$ should be trusted for a given input $\xb$, whereas unsupervised OOD detection judges whether the input $\xb$ should be trusted and used at all. 

Central to any OOD detection method is the ability to assign a score $s(\xb) \in \mathbb{R}$ to a given input $\xb$ that indicates the degree to which the input is likely to be OOD. After defining a score, we typically use a validation set to tune a threshold $\tau$ such that $\xb$ is considered OOD if $s(\xb) > \tau$. The threshold is typically chosen such that performance on the validation set is above some standard, for instance by imposing constraints on the recall and precision. Many works also evaluate the performance of OOD detection methods using the area under the receiver operating characteristic (AUROC) curve which does not require a threshold to be chosen.

The many different ways of defining a score $s(\xb)$ can be used to further categorize OOD detection methods.
Unsupervised methods often derive the score 
from the \emph{likelihood} assigned to the input \cite{bishop_novelty_1994,choi_waic_2019,kirichenko_why_2020,ren_likelihood_2019,serra_input_2020,xiao_likelihood_2020,morningstar_density_2021,nalisnick_detecting_2019,bergamin_modelagnostic_2022,maaloe_biva_2019,havtorn_hierarchical_2021}, 
from a \emph{reconstruction} of the input $\xb$ \cite{sakurada_anomaly_2014,xia_learning_2015,lyudchik_outlier_2016,zhou_anomaly_2017,chen_outlier_2017,schlegl_unsupervised_2017,zong_deep_2018,li_madgan_2019,graham_denoising_2023,liu_unsupervised_2023a}, or 
from a hidden \emph{representation} of the input \cite{denouden_improving_2018,hendrycks_using_2019,ahmadian_likelihoodfree_2021,bergman_classificationbased_2020,tack_csi_2020,sehwag_ssd_2021,xiao_we_2021}.  % https://arxiv.org/pdf/2302.10326.pdf
Supervised methods often derive the score 
from the \emph{probabilities} given by the predictive distribution $p(y|\xb)$ \cite{hendrycks_baseline_2017,hendrycks_scaling_2022} or 
from the \emph{logits} of $p(y|\xb)$ \cite{hendrycks_scaling_2022,liu_energy-based_2020}. 
Other methods use a latent \emph{representation} of the input \cite{lee_simple_2018,li_anomaly_2019,ndiour_outofdistribution_2020,cook_outlier_2020,zaeemzadeh_outofdistribution_2021}, similar to unsupervised methods. 
In the following sections, we provide a more in-depth overview of the different approaches to OOD detection following the above categorization into supervised and unsupervised methods. For the supervised methods we further distinguish between whether methods use real OOD data, synthetic OOD data, or no OOD data at all. 

\subsection{Supervised out-of-distribution detection}
% OUTLINE:
% - Supervised OODD:
%   - Requires some examples of (synthetic) OODD data, which in some sense makes it inherently flawed. We can never really be sure that we have good enough coverage of OODD data.
%   - One limitation of model-dependent OoD techniques is that they may discard information about p(x) in learning the task-specific model p(y|x). That information may be useful for OoD detection.
%   - Likelihood Ratios for Out-of-Distribution Detection \cite{ren_likelihood_2019} provides a number of baselines for supervised OODD:
%       - 1 Maximum class probability of the predictive distribution p(y|x) \cite{hendrycks_baseline_2017}
%       - 2 Entropy of predictive distribution p(y|x) \cite{ren_likelihood_2019}
%       - 3 ODIN \cite{liang_enhancing_2018} (Uses synthetic OOD data by adding noise to in-distribution)
%       - 4 Mahalanobis distance to class conditional Gaussian distribution \cite{lee_simple_2018}
%       - 5 DeepEnsemble \cite{lakshminarayanan_simple_2017} The classifier-based ensemble method that uses the average of the predictions from multiple independently trained models with random initialization of network parameters and random shuffling of training inputs
%       - 6 The log-odds of a binary classifier trained to distinguish between in-distribution inputs from all classes as one class and randomly perturbed in-distribution inputs as the other. \cite{ren_likelihood_2019}
%       - 7 The maximum class probability over K in-distribution classes of a (K + 1)-class classifier where the additional class is perturbed in-distribution. \cite{ren_likelihood_2019}
%       - 8 The maximum class probability of a K-class classifier for in-distribution classes but the predicted class distribution is explicitly trained to output uniform distribution on perturbed in-distribution inputs. \cite{ren_likelihood_2019}. This is similar to using simulated OOD inputs from generative adversarial networks \cite{lee_training_2018} or using auxiliary datasets of outliers \cite{hendrycks_deep_2019} for calibration purpose.
%   - X Deep semi-supervised anomaly detection \cite{ruff_deep_2020}.
%   - Variational information bottleneck (VIB) (Alemi et al., 2018b) performs divergence estimation in latent space to detect OoD, but is technically a model-dependent technique because the latent code is trained jointly with the downstream classification task. \cite{alemi_deep_2017}
%
% - ViM: Out-Of-Distribution with Virtual-logit Matchinghttps://arxiv.org/pdf/2203.10807.pdf in cludes a short overview of many modern methods
%
In the supervised setting the supervision can come from different sources depending on how much we know, or assume to know, about the OOD data. In the least informed setting, the supervision is from the target variable $y$ only. In the most informed setting we also assume access to representative OOD data. 

% Actual OOD data
\paragraph{Methods using real OOD data}
Methods that use representative OOD data have achieved high performance since they can directly learn to distinguish between in-distribution and OOD data. 
\textcite{hendrycks_deep_2019} augments the original training objective with a task-dependent outlier exposure loss that aims to make the output logits different for outlier data. 
In a similar vein, \textcite{dhamija_reducing_2018} propose losses designed to maximize the entropy of $p(y|\xb)$ and decrease feature magnitudes for OOD data sampled from other datasets. 
\textcite{ruff_deep_2020} use semi-supervised learning and learn representations of in-distribution data that concentrate close to a centroid in latent space while labelled outliers are pushed away from the centroid. 
Other methods including MCD \cite{yu_unsupervised_2019}, NGC \cite{wu_ngc_2021}, and UDG \cite{yang_semantically_2021} utilize external, unlabeled, noisy data to improve OOD detection performance without requiring cleanly labelled OOD examples.
As we discussed earlier, for many modern applications of machine learning, input data is often high dimensional and complex making sufficiently sampling the space of out-of-distribution data intractable. 
Therefore, methods that rely on samples of OOD data are inherently limited by its availability and how well it covers the large variation in OOD data. 

% Synthetic OOD data
\paragraph{Methods using synthetic OOD data}
A number of methods do not require access to actual OOD data but synthesize it instead. 
Several methods synthesize this OOD data by adding noise to in-distribution data \cite{liang_enhancing_2018, lee_simple_2018, ren_likelihood_2019}. 
For instance, \textcite{ren_likelihood_2019} propose a number of baselines including training a binary classifier to distinguish between original and perturbed in-distribution data. They also propose adding an OOD class to softmax classifiers and training it to predict perturbed in-distribution data, or alternatively, training the predicted class distribution to output uniform distribution for perturbed in-distribution inputs. 
With ODIN \cite{liang_enhancing_2018}, the authors propose to calibrate $p(y|\xb)$ with temperature scaling \cite{guo_calibration_2017} and add gradient-based, input-dependent perturbations to the inputs to use the calibrated maximum class probability as the OOD score. This method does not require retraining the classifier and generally improves on the baselines of \textcite{ren_likelihood_2019}. 
\textcite{vyas_outofdistribution_2018} train an ensemble of classifiers on different subsets of the training data, with the left out data taken as OOD, and propose novel loss over $p(y|\xb)$ that seeks to maintain a set margin between its average entropy for the OOD and in-distribution examples. 
Another approach generates OOD inputs using a generative adversarial network \cite{lee_training_2018}. 
Similar to actual OOD data, the usefulness of synthetic OOD data is also fundamentally limited by the intractability of sampling the complete space of OOD data.

% No OOD data, probs
\paragraph{Methods not using OOD data}
The least informed supervised OOD detection methods do not require instantiations of any kind of OOD data, real or synthetic. 
A baseline approach uses the maximum class probability of $p(y|\xb)$ directly by noting that it tends to be larger for correctly classified examples \cite{hendrycks_baseline_2017}. Another baseline method proposes that a high entropy of $p(y|\xb)$ indicates an OOD input \cite{ren_likelihood_2019} 
Other methods that derive the score from the classifier probabilities include \textcite{lakshminarayanan_simple_2017} who propose to use an ensemble of independently trained classifiers to discriminate between in-distribution and OOD data by evaluating the agreement between the classifiers, \textcite{devries_learning_2018} who augment the network with a confidence estimation branch that learns to estimate the confidence of the classifier separately from the probability, and \textcite{huang_importance_2021} who compute the gradient of the KL-divergence of the predictive distribution $p(y|\xb)$ to a uniform distribution noting that the magnitude of gradients is higher for in-distribution data than for OOD data. 
The Variational Information Bottleneck \cite{alemi_deep_2017} jointly learns a probabilistic latent representation, $\zb$ and $p(y|\xb)$, using a generalized variational autoencoder \cite{kingma_autoencoding_2014} that tries to maximize the mutual information between $\zb$ and $y$.

\textcite{hsu_generalized_2020} proposes a generalized version of ODIN that removes the need for simulating OOD data.
The authors note that most current methods make a closed world assumption and implicitly condition on the in-domain $\mathcal{D}_{\text{in}}$ in the form of the predictive distribution $p(y|\xb, \mathcal{D}_{\text{in}})$. With this observation, the authors decompose the $p(y|\xb, \mathcal{D}_{\text{in}})$ into a joint class-domain probability and a domain probability,
%
\begin{equation} \label{eq: joint class-domain and domain probability}
    p(y|\xb,\mathcal{D}_{\text{in}}) = \frac{p(y, \mathcal{D}_{\text{in}}|\xb)}{p(\mathcal{D}_{\text{in}}|\xb)} \enspace .
\end{equation}
%
Without data from the out-domain, it is not possible to directly learn either $p(y, \mathcal{D}_{\text{in}}|\xb)$ or $p(\mathcal{D}_{\text{in}}|\xb)$. Instead, the authors use this observation to impose the inductive bias of predicting logits as a fraction between two carefully designed network branches, imitating the form of \cref{eq: joint class-domain and domain probability}. 

% No OOD data, logits
A number of works have noted that the maximum softmax probability is not generally a reliable score for OOD detection \cite{hendrycks_scaling_2022,liu_energy-based_2020}. 
\textcite{liu_energy-based_2020} make an interesting argument as to why based on the energy $E(\xb; f)$ of a softmax classifier $f(\xb)$ \cite{lecun_tutorial_2006},
%
\begin{equation} \label{eq:softmax-classifier-energy}
    E(\xb; f) = - \log \sum_{i=1}^K \exp \left( f_i(\xb) \right) \enspace .
\end{equation}
%
Specifically, the authors relate the maximum softmax probability,
%
\begin{align} \label{eq:softmax-classifier-maximum-probability}
    \max_y p(y|\xb;f) 
    = \max_y \frac{\exp f_y(\xb)}{\sum_{i=1}^K \exp f_i(\xb)} 
    = \frac{\exp f^\text{max}(\xb)}{\sum_{i=1}^K \exp f_i(\xb)} \enspace ,
\end{align}
%
to the energy by noting that,
%
\begin{align}
    \log \max_y p(y|\xb;f) = E(\xb; f(\xb) - f^\text{max}(\xb)) = E(\xb; f) - f^\text{max}(\xb) \enspace .
\end{align}
%
This shows that log of the softmax confidence score is equivalent to the special case of the energy score, where all the logits are shifted by their maximum logit value. 
The authors empirically show that $E(\xb;f)$ is a more reliable score for OOD detection than the maximum softmax probability and note that the energy $E(\xb; f)$ tends to be larger for OOD data than for in-distribution data - contrary to the maximum softmax probability.
They conclude that this shift results in $\max_y p(y|\xb;f)$ being a biased score for OOD detection and propose instead to use the energy directly as the OOD score. This energy-score is improved in ReAct by feature clipping \cite{sun_react_2021}. 

% No OOD, feature
The final category of supervised methods derive the score from a latent representation of the input. 
Some methods define OOD classes. \textcite{huang_mos_2021} groups the classes of the target variable $y$ and defines an OOD class for each group. Each training example is then the correct target for one group and an OOD example for all other groups; a kind of hierarchical version of the OOD class of simpler baselines based on noise augmentation \cite{ren_likelihood_2019}. 
Similarly, to represent a virtual OOD class, \textcite{wang_vim_2022} generates an additional logit by first computing the residual of the input's latent space representation against a principal feature space and then converting it to a valid logit by matching its mean over training samples to the average maximum logits. 

Other methods note that the difficulty of detecting OOD data might be attributed to the curse of dimensionality in the learned feature spaces and propose to use dimensionality reduction techniques. In \textcite{ndiour_outofdistribution_2020}, apply dimensionality reduction on learned, high-dimensional features to capture the true feature subspace and compute the norm of the difference between the original feature and the pre-image of its low-dimensional manifold embedding. 
\textcite{zaeemzadeh_outofdistribution_2021} forces the ID samples to embed into a union of 1-dimensional subspaces during training and computes the minimum angular distance from the feature to the class-wise subspaces. 
NuSA \cite{cook_outlier_2020} uses projects features onto the column space of the classification weight matrix and computes the ratio of the norm the projected and original features.
\textcite{lee_simple_2018} fit a multivariate Gaussian distribution to the activations of the penultimate layer of a pre-trained classifier and use the Mahalanobis distance to this distribution to evaluate whether inputs are OOD. This method can also be seen as ameliorating the curse of dimensionality by clustering the high-dimensional feature space. 
Finally, Bayesian neural networks have also been proposed for OOD detection although their performance is not yet competitive with other methods \cite{henning_are_2021,dangelo_outofdistribution_2022,nguyen_out_2022}. 

% Summary and weaknesses of supervised OOD detection approaches
A general weakness of all supervised out-of-distribution detection is that in learning the task-specific model $p(y|\xb)$ a model may discard information about $p(\xb)$ which could be useful for out-of-distribution detection.
Methods that use real OOD data achieve high performance but are limited by the availability of OOD data and how well it covers the large variation in OOD data.

% This makes them inherently flawed since we can never really be sure that we have good enough coverage of OOD data.
% Furthermore, the methods that use synthetic OOD data are limited by the intractability of sampling the complete space of OOD data. 


%       - 2 Entropy of predictive distribution p(y|x) \cite{ren_likelihood_2019}
%       - 6 The log-odds of a binary classifier trained to distinguish between in-distribution inputs from all classes as one class and randomly perturbed in-distribution inputs as the other. \cite{ren_likelihood_2019}
%       - 7 The maximum class probability over K in-distribution classes of a (K + 1)-class classifier where the additional class is perturbed in-distribution. \cite{ren_likelihood_2019}
%       - 8 The maximum class probability of a K-class classifier for in-distribution classes but the predicted class distribution is explicitly trained to output uniform distribution on perturbed in-distribution inputs. \cite{ren_likelihood_2019}. This is similar to using simulated OOD inputs from generative adversarial networks \cite{lee_training_2018} or using auxiliary datasets of outliers \cite{hendrycks_deep_2019} for calibration purpose.

%       - 3 ODIN \cite{liang_enhancing_2018} (Uses synthetic OOD data by adding noise to in-distribution)
%       - 4 Mahalanobis distance to class conditional Gaussian distribution \cite{lee_training_2018}
%       - 5 DeepEnsemble \cite{lakshminarayanan_simple_2017} The classifier-based ensemble method that uses the average of the predictions from multiple independently trained models with random initialization of network parameters and random shuffling of training inputs

%   - Variational information bottleneck (VIB) (Alemi et al., 2018b) performs divergence estimation in latent space to detect OoD, but is technically a model-dependent technique because the latent code is trained jointly with the downstream classification task. \cite{alemi_deep_2017}

% either synthetic out-of-distribution data or from real out-of-distribution data. Synthetic out-of-distribution data is data that is generated by some process that is known to be out-of-distribution. For example, if we are training a model to classify images of cats and dogs, we could generate synthetic out-of-distribution data by adding noise to the images. Real out-of-distribution data is data that is known to be out-of-distribution, but is not generated synthetically. For example, if we are training a model to classify images of cats and dogs, we could use images of cars as real out-of-distribution data.


\subsection{Unsupervised out-of-distribution detection}
% OUTLINE:
% - Unsupervised ODDD
%   - No OODD data required. Isolates uncertainty related to the input data.
%   - Unsupervised can be categorised into generative and reconstructive.
%   - Generative
%       - 9 Compute WAIC (\cite{watanabe_algebraic_2009, watanabe_asymptotic_2010}) using ensemble of generative models \cite{choi_waic_2019}
%       - Likelihood Ratios for Out-of-Distribution Detection \cite{ren_likelihood_2019}
%       - Input complexity and out-of-distribution detection with likelihood-based generative models \cite{serra_input_2020}
%       - Likelihood regret for variational autoencoders \cite{xiao_likelihood_2020}
%       - Typicality test for generative OODD (Detecting out-of-distribution inputs to deep generative models using typicality) \cite{nalisnick_detecting_2019}
%       - Density of states estimation for out of distribution detection \cite{morningstar_density_2021}
%   - Reconstructive
%       - A review of novelty detection \cite{pimentel_review_2014}
%       - Outlier detection using autoencoders \cite{lyudchik_outlier_2016} and ensembles of autoencoders \cite{chen_outlier_2017}
%       - Improving reconstruction autoencoder out-of-distribution detection with mahalanobis distance \cite{denouden_improving_2018}
%       - Denosing diffusion models for out-of-distribution detection \cite{graham_denoising_2023}
%       - Diffusion inpainting \cite{liu_unsupervised_2023a}

% , MC Dropout (Gal & Ghahramani, 2016) and  model a calibrated predictive distribution for a classification task.

For high-dimensional data, the most successful unsupervised models for OOD detection are deep autoencoders \cite{hinton_reducing_2006}, self-supervised methods \cite{mikolov_efficient_2013,devlin_bert_2018,chen_simple_2020}, and deep generative models \cite{kingma_autoencoding_2014,rezende_stochastic_2014,dinh_nice_2015,rezende_variational_2015,ho_denoising_2020,hinton_fast_2006,oord_conditional_2016,goodfellow_generative_2014}. 
Other important density estimation methods that have been applied to OOD detection include kernel density estimation \cite{parzen_estimation_1962}, nearest neighbor methods \cite{cover_nearest_1967}, support vector machines \cite{cortes_supportvector_1995,scholkopf_estimating_2001}, and Gaussian mixture models \cite{dempster_maximum_1977}. 
However, these methods are not well suited for high-dimensional data such as images, text or audio, and have had little direct impact on the recent work on OOD detection. 

% In the following we will review the literature on unsupervised OOD detection using deep generative models and categorize the different approaches based on whether they derive their OOD score from the likelihood, a reconstruction, or a latent representation of the input.

\paragraph{Likelihood-based}
% \cite{
%     X bishop_novelty_1994,
%     X choi_waic_2019,
%     X kirichenko_why_2020,
%     X ren_likelihood_2019,
%     X serra_input_2020,
%     X xiao_likelihood_2020,
%     X morningstar_density_2021,
%     X nalisnick_detecting_2019,
%     X bergamin_modelagnostic_2022,
%     X maaloe_biva_2019,
%     X havtorn_hierarchical_2021
% },
%
% Among the deep generative models that compute exact likelihoods are flow-based models, autoregressive models, and diffusion models, whereas variational autoencoders compute a lower bound on the likelihood, energy-based models usually require approximate methods, and generative adversarial networks do not compute an explicit likelihood at all.
\textcite{bishop_novelty_1994} first proposed to use the likelihood assigned to data by a generative model as a measure for detecting anomalous data. Simply put, since the likelihood measures ``how probable the data is'', OOD data is expected to give lower likelihoods than in-distribution data. Although, this method originally gave encouraging results, the advent of deep generative models and their application to high-dimensional data has lead many to observe likelihoods for OOD data that are higher than for in-distribution data \cite{choi_waic_2019,nalisnick_detecting_2019,kirichenko_why_2020}. Such results have sparked interest in trying to explain this phenomenon and many works have proposed new scores for OOD detection derived from the likelihood. 

In \textcite{ren_likelihood_2019}, the authors propose to use the likelihood ratio between a model trained on in-distribution data and a background model trained on perturbed in-distribution data as the OOD score. 
\textcite{serra_input_2020} argue that the failure of DGMs is due to the high-influence that the input complexity has on the likelihood. Therefore, they propose to use a general lossless image compression algorithm as a background model. 
\textcite{choi_waic_2019} propose to use the Watanabe information criterion (WAIC) computed from the likelihood \cite{watanabe_algebraic_2009, watanabe_asymptotic_2010}. 
For variational autoencoders, other work proposes to refit the encoder on a test data example, hypothesizing that the likelihood of OOD data will improve more than for in-distribution data, and use this ``likelihood regret" as the OOD score \cite{xiao_likelihood_2020}. 
\textcite{maaloe_biva_2019} provide initial results that a loosened variational bound on the likelihood, using only encoded representations from the top-most latent variables in a hierarchical variationa autoencoder, can improve OOD detection performance. In \textcite{havtorn_hierarchical_2021}, we show that variational autoencoders are surprisingly good at reconstructing OOD data and propose an improved score based on the likelihood ratio of such loosened bounds \textcite{maaloe_biva_2019}.

A different approach is to use the typicality set hypothesis \cite{nalisnick_detecting_2019}. The typicality set is the subset of the model full support in data space, where the model samples from, that does not overlap with regions of maximal likelihood. \textcite{nalisnick_detecting_2019} propose to use the typicality set as a test statistic for OOD detection while \textcite{morningstar_density_2021} propose to use the related idea of density of states of the model. 
In \textcite{bergamin_modelagnostic_2022} we use Fisher's method \cite{fisher_statistical_1925} to combine Rao's score test statistic \cite{rao_large_1948} with the typicality set test statistic hence including information from both the gradient and the likelihood.


\paragraph{Reconstruction-based}
% \cite{
%     X sakurada_anomaly_2014                 encode and decode data with autoencoders. Dimensionality reduction
%     X lyudchik_outlier_2016                 encode and decode data with autoencoders.
%     X xia_learning_2015                     encode and decode data with autoencoders. We observe that when data are reconstructed from low-dimensional representations, the inliers and the outliers can be well separated according to their reconstruction errors. Based on this basic observation, we gradually inject discriminative information in the learning process of an autoencoder to make the inliers and the outliers more separable.
%     X zhou_anomaly_2017                     encode and decode data with autoencoders. inspired by Robust Principal Component Analysis, and we split the input data X into two parts, , where  can be effectively reconstructed by a deep autoencoder and  contains the outliers and noise in the original data X
%     X zong_deep_2018                        encode and decode data with autoencoders. jointly learn a deep autoencoder and a Gaussian Mixture Model on the learned hidden representations and link their method to neural variational inference \cite{mnih_neural_2014}.
%     X chen_outlier_2017                     encode and decode data with an ensemble of autoencoders.
%     X schlegl_unsupervised_2017             perform generative adversarial networks inversion for a data point, and evaluate its reconstruction error and discriminator confidence under the inverted latent variable.
%     X li_madgan_2019                        perform generative adversarial networks inversion for a data point, and evaluate its reconstruction error and discriminator confidence under the inverted latent variable.
%     X graham_denoising_2023                 leverages diffusion models to reconstruct images at varied diffusion steps, while we mask and inpaint an image repeatedly with fixed steps.
%     X liu_unsupervised_2023a                LMD lifts an image off its original manifold by corrupting it, and maps it towards the in-domain manifold with a diffusion model. For an out-of-domain image, the mapped image would have a large distance away from its original manifold, and LMD would identify it as OOD accordingly.
% }
%
A number of methods derive the OOD score from a reconstruction error. 
Among the first methods are \textcite{sakurada_anomaly_2014, lyudchik_outlier_2016} who note that dimensionality reduction helps separate inliers and outliers and propose to use deep autoencoders to reconstruct the input and evaluate the reconstruction error. \textcite{xia_learning_2015} take a similar approach but also propose to inject discriminative information in the learning process. 
Drawing inspiration from Robust Principal Component Analysis, \textcite{zhou_anomaly_2017} propose to first split the input data into a dense low-rank factor and a sparse factor, assuming that outliers are caught in the sparse factor, and then use a deep autoencoder to reconstruct the dense factor. 
\textcite{zong_deep_2018} jointly learn a deep autoencoder and a Gaussian Mixture Model on the learned hidden representations and draw parallels of their method to neural variational inference \cite{mnih_neural_2014}. 
Similarly to the ensemble-based methods for supervised OOD detection, \textcite{chen_outlier_2017} propose to use an ensemble of autoencoders to reconstruct the input and use the median reconstruction error as the OOD score. 

Although generative adversarial networks do not have the ability to encode a given data point, methods have been proposed to invert the generator to find a latent representation that can be used to reconstruct the input and use the reconstruction error as well as a discriminator score for OOD detection \cite{schlegl_unsupervised_2017,li_madgan_2019}. 

Diffusion models have also been used for OOD detection via a reconstruction-based score. \textcite{graham_denoising_2023} add varying amounts of diffusion noise to an input image and show that reconstructions of OOD inputs from appropriate noise levels fall back onto the in-domain manifold resulting in high reconstruction error. 
\textcite{liu_unsupervised_2023a} lift an image off its original manifold by sampling a number of masks, and then maps it towards the in-domain manifold with a diffusion model, using the median reconstruction error as the OOD score.


% In \textcite{zhou_anomaly_2017} 

\paragraph{Representation-based}
% https://arxiv.org/pdf/2302.10326.pdf
% \cite{
%     X denouden_improving_2018,              leverages the latent variables of an autoencoder, and evaluates the Mahalanobis distance in the latent space along with the data reconstruction error
%     X ahmadian_likelihoodfree_2021,         extracts low-level features from the encoder of an invertible generative model.
%     X hendrycks_using_2019,                 learn a representation over the in-domain data through self-supervised training
%     X bergman_classificationbased_2020,     learn a representation over the in-domain data through self-supervised training
%     X tack_csi_2020,                        learn a representation over the in-domain data through self-supervised training
%     X sehwag_ssd_2021,                      learn a representation over the in-domain data through self-supervised training
%     X xiao_we_2021                          further shows that one can instead use a strong pretrained feature extractor (self-supervised)  while maintaining comparable performance.
% }
%
\textcite{denouden_improving_2018} suggest that reconstruction-based approaches fail to capture particular anomalies that lie far from known inlier samples in latent space but near the latent dimension manifold defined by the parameters of the model. They propose to measure the Mahalanobis distance between the global Gaussian distribution of training set in latent space and an encoded test input. 
\textcite{xiao_we_2021} instead propose to use an existing, strong foundation model, pre-trained with a self-supervised objective, to extract features from the input, and then fit a Gaussian Mixture Model to the features using the minimal Mahalanobis distance to the mixture components as the OOD score.

Several other works also use self-supervised representations of the in-domain data for OOD detection \cite{hendrycks_using_2019,bergman_classificationbased_2020}. 
\textcite{tack_csi_2020} propose to use a contrastive objective to learn representations of the in-domain data contrasted with data augmented in-samples and use a softmax classifier trained on the representations to compute the OOD score. \textcite{sehwag_ssd_2021} present a similar approach but use the feature space Mahalanobis distance similar to \textcite{denouden_improving_2018}. 
\textcite{ahmadian_likelihoodfree_2021} propose to use the latent representation of an invertible generative model to compute the OOD score.


% \subsubsection{Model agnostic: Related works}
% Since \textcite{nalisnick_deep_2019} and \textcite{hendrycks_deep_2019}, different test statistics or methodologies for OOD detection using DGMs were proposed. Most of the recent solutions were highly influenced by three major lines of work: \emph{typicality set}, \emph{likelihood ratio} test statistics, and \emph{model misestimation}. 

% The typicality set hypothesis was introduced by \textcite{nalisnick_detecting_2019} as a possible explanation for the DGMs assigning higher likelihood to OOD data. The typicality set is the subset of the model full support where the model samples from and this does not intersect with the region of higher likelihood. While the typicality test was introduced for batch-OOD detection, \textcite{morningstar_density_2021} shows that it also works well in the single-sample case. This is also confirmed by our own experiments. 

% The likelihood ratio test statistic method by \textcite{ren_likelihood_2019} assumes that every input is composed by a background component and a semantic component. For OOD detection, only the semantic component matters. In addition to a model trained on the in-distribution data, they proposed to train a background model on perturbed inputs data and then for each test example consider as OOD score the likelihood ratio between the two models. \textcite{schirrmeister_understanding_2020}, instead, trained the background model on a more general distribution of images by considering 80 million general tiny images. Similarly to these approaches, \textcite{serra_input_2020} argued that the failure of DGMs is due to the high-influence that the input complexity has on the likelihood. Therefore, they proposed to use a general lossless image compression algorithm as a background model.
% All these methods, however, require additional knowledge of the OOD data for either choosing an image augmentation procedure to perturb the input data or for choosing a specific compressor. 

% Another line of works blame the models themselves and not the test statistics. \textcite{zhang_understanding_2021} argued that model misestimation is the main cause of higher likelihood assigned to OOD data. This can be due to both the model architecture and the maximum likelihood objective. \textcite{kirichenko_why_2020} and \textcite{schirrmeister_understanding_2020} showed that normalizing flows can achieve better OOD performance despite achieving a worse likelihood if one changes some model design choices.
% %
% Other works in the literature focused on deriving specific test statistics that work only for a specific model, for example for VAEs \parencite{xiao_likelihood_2020, maaloe_biva_2019, havtorn_hierarchical_2021}, or for normalizing flows \parencite{kirichenko_why_2020, ahmadian_likelihoodfree_2021}.

% %All the solutions we presented can be described in terms of the following two categories: \emph{model-agnostic} vs \emph{model-specific} test statistics and statistics \emph{with OOD assumptions} vs statistics \emph{with no OOD assumptions}. In this work, we are mostly interested in model-agnostic scores with no OOD assumptions, i.e.\@ scores that can be directly computed with any models and that do not need any explicit assumption on the OOD data. 

% As mentioned in the introduction, we frame the OOD detection problem in terms of statistical tests problem. Recently, \textcite{haroush_statistical_2021} showed that adopting hypothesis testing at the layer and channel level of a neural network can be used for OOD detection in the discriminative setting. They used both Fisher's method and Simes' method to combine class-conditional $p$-values computed for each convolutional and dense layer of a deep neural network. We focus on the unsupervised setting using DGMs and use hypothesis testing on statistics that can be computed on all differentiable DGM. As already explained in section \cref{sec_modelagnostic:combination_p_values}, \textcite{morningstar_density_2021} considered the combination of different statistics for OOD detection. The main difference with their approach is that we propose statistics that can be applied to any differentiable generative model and combine them by using Fisher's method, which takes advantage of using only one-sided independent statistics. Concurrently, \textcite{choi_robust_2021} derived the score statistic by starting from the likelihood ratio statistic and applying a Laplace approximation. They computed the score statistic only for certain layers of the model and for a specific example, the OOD score is given by the infinity norm of these different layer scores after a ReLU operation. Our procedure differs both in the derivation of the score statistic and its usage since we compute the score statistic for the entire model.


% \subsubsection{Hierarchical: Related works}

% So far, no reliable direct likelihood-based method has been found for fully unsupervised deep generative model OOD detection.
% A major line of work considers developing new scores that are more reliable than the likelihood.
% This includes the \textit{typicality} test presented by \textcite{nalisnick_detecting_2019} which is an OOD detection test based on the typicality of a batch of potentially OOD examples.
% This approach however requires a batch of examples from the same class (OOD or not) which limits its practical applicability.
% In \textcite{ren_likelihood_2019}, the \textit{likelihood ratio} between a primary model and a background model was shown to be an effective score for OOD detection.
% However, to train the background model, the in-distribution data is perturbed via a data augmentation technique that is designed with knowledge about the confounding factors between the in-distribution data and the OOD data. Furthermore, it is tuned towards high performance on a known OOD dataset.
% \textcite{serra_input_2020} take a similar approach and attribute the failure to detect OOD data to the high influence of the input complexity on the likelihood and choose a generic lossless compression algorithm as the background model.
% Although this method gives good results, no single best choice of compression algorithm exists for all types of OOD data, and any particular choice encodes prior knowledge about the data into the detection method.
% Both these methods can be seen as correcting for low-level features of the OOD data being assigned high model likelihood by using a second model focused exclusively on these features.

% Similar to these methods, the majority of the approaches to OOD detection make assumptions about the nature of the OOD data.
% The assumptions encompass using labels on the in-distribution data \cite{hendrycks_baseline_2017, liang_enhancing_2018, alemi_uncertainty_2018, lee_simple_2018, lakshminarayanan_simple_2017}, examples of OOD data \cite{hendrycks_deep_2019}, augmenting in-distribution data to mimic it \cite{ren_likelihood_2019}, or assuming a certain data type \cite{serra_input_2020}.
% Any of these assumptions encode implicit biases into the model about the attributes of OOD data which, in turn, might impair performance on truly unknown data examples (unknown unknowns).

% % Introduce the contrast to results with VAEs
% While some of these methods achieve very good results on OOD detection with autoregressive models \cite{oord_pixel_2016, salimans_pixelcnn_2017} and invertible flow-based models \cite{kingma_glow_2018}, it was recently shown that they can be much less effective for VAEs \cite{xiao_likelihood_2020} highlighting the need for a more reliable OOD score for VAEs.
% Although VAEs have the same failure cases as autoregressive and flow-based models, the caveat is that the difference in the likelihood is generally not as big and reconstructions of OOD can be surprisingly good \cite{xiao_likelihood_2020}.
% \textcite{xiao_likelihood_2020} alleviate this by refitting the inference network, as previously proposed by \textcite{cremer_inference_2018, mattei_refit_2018}, to a potentially OOD example and measuring the so-called \textit{likelihood regret}.
% However, refitting the inference network can be computationally expensive, especially for the large hierarchical VAEs that are used to model complex data \cite{maaloe_biva_2019, vahdat_nvae_2020, child_very_2021}. Furthermore, this scales poorly to large amounts of potentially OOD examples as the optimization is done per example.

% A few methods have approached OOD detection in a completely unsupervised fashion \cite{maaloe_biva_2019, choi_waic_2019, xiao_likelihood_2020}.
% The work of \textcite{maaloe_biva_2019} is the most related to ours. They introduce BIVA, a deep hierarchy of stochastic latent variables with a top-down and bottom-up inference model and achieve state-of-the-art likelihood scores. 
% They also provide early results indicative that a looser likelihood bound may have value in OOD detection.
% In this paper, we provide an explanation of those results, and significantly improve upon them.



% Out-of-distribution detection and anomaly detection are both related concepts in machine learning and deep learning, but they address slightly different problems and have distinct focuses.

% Out-of-Distribution Detection (OOD Detection):
% Out-of-distribution detection involves determining whether a given input or data point belongs to the same distribution as the training data. In other words, it helps identify whether an input is coming from a category or distribution that the model has never encountered during training. This is especially important for safety-critical applications where the model should not make predictions on inputs that are significantly different from what it has seen before. OOD detection aims to prevent the model from making confident predictions on unfamiliar inputs that might lead to incorrect results.
% Anomaly Detection:
% Anomaly detection, on the other hand, focuses on identifying rare and unusual instances within the data, regardless of whether they belong to the same distribution as the training data or not. Anomalies can be defined as data points that deviate significantly from the norm or expected behavior. Anomaly detection is used to find instances that are different from the majority and might indicate potential errors, fraud, faults, or other unusual occurrences.
% In summary, the key differences are:

% Focus:
% OOD Detection: Focuses on detecting inputs that are coming from distributions that differ significantly from the training data distribution.
% Anomaly Detection: Focuses on identifying rare and unusual instances, regardless of whether they come from a different distribution or not.
% Purpose:
% OOD Detection: Primarily used for safety and robustness, ensuring that the model doesn't make confident predictions on unfamiliar data.
% Anomaly Detection: Used for various applications such as fraud detection, fault detection, and identifying outliers in the data.
% Detection Approach:
% OOD Detection: Often involves measuring the uncertainty of the model's predictions. High uncertainty indicates that the input might be out-of-distribution.
% Anomaly Detection: Focuses on detecting data points that are statistically rare or far from the norm.
% It's worth noting that while the concepts are distinct, they can sometimes overlap. An out-of-distribution data point could also be considered an anomaly, but not all anomalies are necessarily out-of-distribution. Anomaly detection methods might use OOD detection techniques as part of their process to flag anomalous instances that are not in line with the expected distribution.




% Nice review:
% https://openaccess.thecvf.com/content/CVPR2023W/VAND/papers/Graham_Denoising_Diffusion_Models_for_Out-of-Distribution_Detection_CVPRW_2023_paper.pdf

% \cite{kabir_neural_2018} Survey of uncertainty quantification in deep learning.

% https://arxiv.org/pdf/2306.05674.pdf




\section{Variational autoencoders}



Deep generative models are a class of models that learn a probabilistic representation of the input data and can be used to generate new samples from the same distribution. 
There exist at least six main classes of deep generative models: variational autoencoders \cite{kingma_autoencoding_2014,rezende_stochastic_2014,ranganath_hierarchical_2016,vahdat_nvae_2020,child_very_2021}, normalizing flow models \cite{dinh_nice_2015,rezende_variational_2015,dinh_density_2017,kingma_glow_2018,grathwohl_ffjord_2018}, diffusion models \cite{sohl-dickstein_deep_2015,song_generative_2019, ho_denoising_2020, vahdat_scorebased_2021}, energy-based models \cite{lecun_tutorial_2006,hinton_fast_2006,salakhutdinov_efficient_2010,du_implicit_2019}, autoregressive models \cite{oord_conditional_2016,oord_wavenet_2016,radford_improving_2018}, and generative adversarial networks \cite{goodfellow_generative_2014,arjovsky_wasserstein_2017,brock_large_2019,karras_stylebased_2019}.
Besides data generation, common properties of deep generative models include the ability to compute an exact or approximate likelihood of input $p(\xb)$, reconstruct an input $\xb$ from its latent representation $\zb$, and represent an input $\xb$ in a latent space. These properties enable various methods for their use in OOD detection. 



VAE \cite{kingma_autoencoding_2014,rezende_stochastic_2014}

Derive ELBO

Importance sampling \cite{burda_importance_2016}

Semi-supervised learning \cite{kingma_semi-supervised_2014,maaloe_semi-supervised_2017,siddharth_learning_2017}

Trouble in paradise \cite{tomczak_trouble_2022}

% Trouble in paradise: factoriing the ELBO to show that we minimize the mutual information between z and x. 
% Use this as transition to self-supervised methods as alternative for representation learning: 
% https://jmtomczak.github.io/blog/13/13_trouble_in_paradise.html


\section{Speech representation learning}



\iffalse


% TODO Delete this probably
Following the derivation of \textcite{jaynes_information_1957}, in the discrete entropy expression in \cref{eq:information-entropy}, we assume that the number of discrete points, $x_i, i=1,2,\dots,N$, in the support of $X$ grows larger such that in the limit $N\rightarrow\infty$, the density of points becomes a continuous function $m(x)$, and we can replace the sum with an integral. Denoting the number of points satisfying $a<x_i<b$ by $N_{a,b}$, we have
%
\begin{equation}
    \lim_{N\rightarrow\infty} \frac{N_{a,b}}{N} = \int_a^b m(x) \, dx \enspace .
\end{equation}
%
If this limit behaves sufficiently well, adjacent differences $(x_{i+1} - x_{i})$ in the neighborhood of any particular value of $x$ will also tend to zero, so that
%
\begin{equation} \label{eq:limiting-density-discrete-to-continuous}
    \lim_{N\rightarrow\infty} \left[ N(x_{i+1} - x_{i}) \right] = \left[ m(x_{i}) \right]^{-1} \enspace .
\end{equation}
%
In the limit, the value of the discrete probability mass function will become a continuous probability density function $w(x)$, according to the limiting form of
%
\begin{equation}
    p_i = w(x_i)\left(x_{i+1} - x_{i}\right) \enspace ,
\end{equation}
%
where $p_i$ is the value of the probability mass function at $x_i$ which via \cref{eq:limiting-density-discrete-to-continuous} becomes,
%
\begin{equation}
    p_i = w(x_i)\left[Nm(x_i)\right]^{-1} \enspace .
\end{equation}
%
Consequently, the discrete entropy expression in \cref{eq:information-entropy} becomes,
\begin{align}
    % H_{N,\text{discrete}}(X) &= - \int_\mathcal{X} w(x)\left[Nm(x)\right]^{-1} \, dx \, \log \left[\frac{w(x)}{Nm(x)}\right] \nonumber \\
    % &= - \int_\mathcal{X} w(x) \, dx \, \log \left[\frac{w(x)}{Nm(x)}\right]
    H_{N,\text{discrete}}(X) &= - \int_\mathcal{X} w(x) \, dx \, \log \left[\frac{w(x)}{Nm(x)}\right] \nonumber \\
\end{align}
%
and taking the limit $N\rightarrow\infty$,
%
\begin{equation}
    \lim_{N\rightarrow\infty} H_{N,\text{discrete}}(X) = H_{N} = \log N - \int_\mathcal{X} w(x) \log \left[\frac{w(x)}{m(x)}\right] \enspace .
\end{equation}
%
Note that since $\lim_{N\rightarrow\infty} \log N = \infty$, this procedure suggests that the entropy in the discrete sense of a continuous random variable should be infinite.




Modelling paradigms
\begin{enumerate}
    \item Variational inference and variational autoencoders
    \item Automatic speech recognition
    \item Self-supervised learning
\end{enumerate}

\cite{kingma_autoencoding_2014}
\cite{rezende_stochastic_2014}

Two approaches to statistics:\\
- Bayesian\\
- Frequentist\\

Two kinds of uncertainty:\\
- Aleatoric: Uncertainty inherent to the data. Irreducible. "Known unknowns".\\
- Epistemic: Uncertainty due to lack of knowledge. Can be reduced with more data. "Unknown unknowns".\\

Approaches to uncertainty in deep learning:\\
- Tuning of supervised classifiers to output well-calibrated probabilities that correspond to the actual likelihood of the prediction being correct (on some validation set).\\
- Learning rich unsupervised representations of the data that can be used to estimate the uncertainty of the prediction (e.g. by measuring the distance to the nearest training example in the latent space).\\
- Learning of a probability distribution over the parameters of the model, which can then be used to compute the uncertainty of the prediction (Bayesian neural networks).\\


\fi 
