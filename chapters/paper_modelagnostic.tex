%!TEX root = ../thesis.tex

\chapter[model-agnostic out-of-distribution detection using combined statistical tests]{Model-Agnostic Out-of-Distribution Detection Using Combined Statistical Tests}
\label{chp:paper-modelagnostic}
\ifthenelse{\equal{\skippapers}{true}}{}{

\section*{Abstract}
We present simple methods for out-of-distribution detection using a trained generative model. These techniques, based on classical statistical tests, are model-agnostic in the sense that they can be applied to any differentiable generative model. The idea is to combine a classical parametric test (Rao's score test) with the recently introduced typicality test. These two test statistics are both theoretically well-founded and exploit different sources of information based on the likelihood for the typicality test and its gradient for the score test. We show that combining them using Fisher's method overall leads to a more accurate out-of-distribution test. We also discuss the benefits of casting out-of-distribution detection as a statistical testing problem, noting in particular that false positive rate control can be valuable for practical out-of-distribution detection. Despite their simplicity and generality, these methods can be competitive with model-specific out-of-distribution detection algorithms without any assumptions on the out-distribution. %out-of-distribution data.\looseness=-1

\section{Introduction}

The ability to recognize when data are anomalous, i.e. if they originate from a distribution different from that of the training data, is a necessary property for machine learning models for safe and reliable applications in the real world. Historically, \textcite{bishop_novelty_1994} proposed to use a one-sided threshold on the log-likelihoods of a learned model as a decision rule to identify outliers in a dataset. However, recently, \textcite{nalisnick_deep_2019,hendrycks_deep_2019} showed that state-of-the-art deep generative models (DGMs) failed in this task, assigning higher a likelihood to out-of-distribution (OOD) data than in-distribution data. Most of the recent works focused on proposing new test statistics to alleviate the problem of using the plain likelihood, see \cref{sec_modelagnostic:related_works} for details.

We believe that OOD detection should be formulated as statistical hypothesis testing \parencite{nalisnick_detecting_2019, ahmadian_likelihoodfree_2021, haroush_statistical_2021}. Since the power of a single test depends on the out-distribution \parencite{zhang_understanding_2021}, we propose to approach this problem by using a combination of multiple statistical tests. %While this does not solve completely the problem, because the power of the combined tests will also depend on the out-distribution, we think that we can get better performance than using a single-statistics especially in situations where one statistic fails. 
While the power of the combined test also depends on the out-distribution, we hypothesize that the combined test empirically will perform better, especially in situations where one of the statistics fails.
%
%Furthermore, the use of the statistical testing framework, allows us to correct for the multiple comparisons problem when identifying outliers in a dataset by controlling the number of type I errors through the false discovery rate (FDR).
Furthermore, the use of the statistical testing framework has several advantages. Since we obtain a $p$-value, it is more natural deciding on a threshold as this corresponds to the significance level. In addition to that, it also allows us to correct for the multiple comparisons problem when identifying outliers in a dataset by controlling the number of Type I errors through the false discovery rate (FDR).

In summary, our contributions are the following:
\begin{itemize}
    \item We illustrate the benefits of combining multiple statistical tests to perform OOD detection with DGMs using well-established methods. This allows for a proper decision procedure to control the FDR in a real outlier detection setting.
    \item We revisit some proposed detection scores and highlight their alternative formulation as classical significance tests.
    \item Empirically we show the complementarity of the typicality and the score statistics and that their combination leads to a robust score for anomaly detection. %We evaluate our proposed method with two PixelCNN++ \textcite{salimans_pixelcnn_2017}, two Glow models \textcite{kingma_glow_2018} and one Hierarchical-VAE (HVAE)\textcite{kingma_autoencoding_2014}.
\end{itemize}
%

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  SECTION 2   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Using statistical tests for out-of-distribution detection}

We consider some data of interest that live in a space $\mathcal{X}$. Assume that we have a curated dataset $x_1,\ldots,x_m$, i.e.\@ there are no outliers, and we are interested in understanding if some new data $\tilde{x}_1,\ldots,\tilde{x}_n$ are collectively anomalies. In other words, we wonder whether or not $\tilde{x}_1,\ldots,\tilde{x}_n$ are likely to come from the same distribution that generated our curated dataset. We present in this section two different approaches for doing out-of-distribution detection using statistical tests: one based on classical parametric tests and one based on maximum mean discrepancy. A convenient property of the tests we consider is that they are all one-sided, which means we can expect them to be larger when the data are more likely to be OOD. This allows us to compute $p$-values by simply using the empirical CDF, which is hyperparameter-free.

Note that in this problem formulation, the case $n=1$ corresponds to the situation where we need to decide if a \emph{single} data point is out-of-distribution. This hardest setting will be of particular interest, and this is also the main focus of recent work, see \cref{sec_modelagnostic:related_works}.

\subsection{Parametric tests for out-of-distribution detection}\label{sec_modelagnostic:parametric}
The typical approach is to consider a parametric family $(p_\theta)_{\theta \in  \Theta}$ of probability densities over $\mathcal{X}$ and learn a suitable $\theta_0 \in  \Theta$ using any inference technique, for example maximum likelihood, and the clean data $x_1,\ldots,x_m$. Depending on the input domain, $(p_\theta)_{\theta \in  \Theta}$ could be composed of DGMs (in that case, $\theta$ would be neural network weights) or Gaussian mixture models (in that case, $\theta$ would be composed of means, covariances, and proportions). The question we wish to answer may then be phrased: \emph{is $p_{\theta_0}$ an appropriate model for $\tilde{x}_1,\ldots,\tilde{x}_n$?}
%We consider some data of interest that live in a space $\mathcal{X}$. We are given a parametric family $(p_\theta)_{\theta \in  \Theta}$ of probability densities over $\mathcal{X}$. For instance $(p_\theta)_{\theta \in  \Theta}$ could be composed of DGMs (in that case, $\theta$ would be neural network weights) or Gaussian mixture models (in that case, $\theta$ would be composed of means, covariances, and proportions).
%
%Upon analysis of an initial, in-distribution training dataset, we learn a suitable $\theta_0 \in  \Theta $ using any inference technique, for example maximum likelihood. We are now given additional data points $\tilde{x}_1,\ldots,\tilde{x}_n$ and wonder whether or not they are likely to come from the same distribution that generated our training dataset. The case $n=1$ corresponds to the case where we need to decide if a \emph{single} data point is out-of-distribution; this hardest setting will be of particular interest.
%The question we wish to answer may then be phrased: \emph{is $p_{\theta_0}$ an appropriate model for $\tilde{x}_1,\ldots,\tilde{x}_n$?}

We choose to formalize this problem as a \emph{parametric test} whose alternative hypothesis is that $\tilde{x}$ is \emph{out-of-distribution}. More specifically, if we assume that $\tilde{x}_1,\ldots,\tilde{x}_n \sim_{\textup{i.i.d.}} p_{\tilde{\theta}}$ for some unknown $\tilde{\theta} \in \Theta$, we wish to test $\mathcal{H}_0 : \tilde{\theta} = \theta_0$ against $\mathcal{H} : \tilde{\theta} \neq \theta_0$, where the alternative hypothesis $\mathcal{H}$ is that the test points are OOD.
%we wish to test $\mathcal{H}_\textup{id} : \tilde{\theta} = \theta_0$ against $\mathcal{H}_\textup{ood} : \tilde{\theta} \neq \theta_0$.

Many tests have been proposed for this purpose. The three most famous are the \emph{likelihood ratio test} of \textcite{neyman_use_1928}, Rao's \citeyearpar{rao_large_1948} \emph{score test},
%(also called \emph{Lagrange multiplier test}, mostly by econometricians),
and \emph{the Wald test} \parencite{wald_tests_1943}. These three classics are nicely reviewed by \textcite{buse_likelihood_1982} or by \textcite{rao_score_2005}, who called them the ``Holy Trinity''. A recent and interesting one is the \emph{gradient test} of \textcite{terrell_gradient_2002}, which is reviewed in great detail in Lemonte's \citeyearpar{lemonte_gradient_2016} monograph.

Let us review the statistics of these four tests:
\begin{itemize}
    \item likelihood ratio statistic is $S_{LR} = 2 ( \ell ( \hat{\theta}) -\ell ( \theta_0))$,
    \item Wald statistic is $S_{W} = (\hat{\theta} - \theta_0)^T I(\hat{\theta}) (\hat{\theta} - \theta_0)$,
    \item score statistic is $S_{S} = \nabla \ell ( \theta_0)^T I(\theta_0)^{-1}  \nabla \ell ( \theta_0)$,
    \item gradient statistic is $S_{G} = \nabla \ell ( \theta_0)^T (\hat{\theta} - \theta_0)$,
\end{itemize}
where $ \ell ( \theta) = \log p_\theta (\tilde{x}_1,\ldots,\tilde{x}_n)$ is the likelihood function, 
$I({\theta}) = \mathbb{E}_{p_\theta}[\nabla \ell ( \theta)\nabla \ell ( \theta)^T]$ is the Fisher information matrix (FIM), and $ \hat{\theta} \in \arg\max_{\theta \in \Theta} \ell ( \theta)$.

The likelihood ratio statistic, the Wald statistic and the gradient statistic all require to fit a model on the additional data points $\tilde{x}_1,\ldots,\tilde{x}_n$ in order to compute either $\ell(\hat{\theta})$ or $\hat{\theta}$. 
%
%In our setting, if we want to use one of those statistics as an OOD score for a single example, we should fit a DGM on that single data point. \textcite{xiao_likelihood_2020} try to do that in the context of VAEs by re-fitting only the inference network, or encoder, of a VAE to every single additional example. This is the typical thing to do when dealing with out-of-sample data in VAEs, as argued by \textcite{mattei2018refit, cremer2018inference}. 
In our setting, if we want to use one of those statistics as an OOD score for a single example, we should fit a DGM on that single data point. \textcite{xiao_likelihood_2020} did this for a variational autoencoder (VAE, \citealp{kingma_autoencoding_2014,rezende_stochastic_2014}) by only re-fitting inference network (or encoder) to the additional example, which is a typical approach to dealing with out-of-sample data in VAEs, as argued by \textcite{cremer_inference_2018} and \textcite{mattei_refit_2018}.
However, much of the recent works in the literature \parencite{ren_likelihood_2019, schirrmeister_understanding_2020, serra_input_2020} mainly focus on deriving different versions of what they call a likelihood ratio statistic.

%In our setting, if we want to use one of those statistics as an OOD score for a single example, we should fit a DGM on that single data point, which barely sounds serious. Most of the recent works in the literature mainly focus on deriving different versions of what they call a likelihood ratio statistic. Among those that trained a likelihood-based model on $\tilde{x}_1,\ldots,\tilde{x}_n$, we note that the work by \textcite{xiao_likelihood_2020} is the one that is closer to the real definition of the likelihood ratio statistic given by \textcite{neyman_use_1928}. Indeed, they propose to compute $\ell(\hat{\theta})$ by refitting the inference network of a VAE to every single additional example as proposed in \textcite{mattei2018refit, cremer2018inference}. 
%While a similar approach could potentially be used to compute also the gradient statistic and the Wald statistic, this will be possible only for a VAE model. 
%
We tried to derive a general way to compute both the Wald statistic and the gradient statistic, by computing $ \hat{\theta}$ with a few steps of a gradient-based optimization algorithm initialized at $ \theta_0$, but this resulted in a very unstable update leading to computational issues (results not shown). Therefore, in this work we focus on studying the relevance of the score statistic for performing out-of-distribution detection since it is the only statistic that does not require fitting an additional model to the OOD data.


\subsection{Maximum mean discrepancy for out-of-distribution detection}\label{sec_modelagnostic:MMD}
Another way of approaching out-of-distribution detection from a testing perspective is through a \emph{two-sample test}. Denoting $p_\textup{data}$ the true training data distribution, the goal is to test $\mathcal{H}_0: \tilde{x}_1,\ldots,\tilde{x}_n \sim p_\textup{data}$ against $\mathcal{H}: \tilde{x}_1,\ldots,\tilde{x}_n \not\sim p_\textup{data}$, where the alternative hypothesis $\mathcal{H}$ again is that the test points are OOD.

A popular way of building statistics for two-sample tests is to use a measure of distance between $p_\textup{data}$ and the distribution of $\tilde{x}_1,\ldots,\tilde{x}_n$. The key idea here will be to use the trained generative model to build this measure of distance. To this end, we will use the \emph{maximum mean discrepancy (MMD)} of \textcite{gretton_kernel_2012}, which is a kernel-based measure of distance. Then, $p_\theta$ will be used to specify an appropriate kernel.

More specifically, given a kernel whose feature map is $\Phi : \mathcal{X} \to \mathcal{H}$, the MMD between two distributions $P$ and $Q$ over $\mathcal{X}$ is defined as
\begin{equation}
    \textup{MMD}_\Phi(P,Q) = \| E_{X\sim P}[\Phi(X)] - E_{Y\sim Q}[\Phi(Y)] \|_\mathcal{H}. \hspace{-.1em}
\end{equation}
In our context, the test statistics will be of the form
\begin{equation}
    \label{eq_modelagnostic:mmd}
    \textup{MMD}_\Phi\left(\frac{1}{m}\sum_{i=1}^{m} x_i,\frac{1}{n}\sum_{i=1}^n \tilde{x}_i\right) = \left\|\frac{1}{m}\sum_{i=1}^{m} \Phi(x_i) -  \frac{1}{n}\sum_{i=1}^{n} \Phi(\tilde{x}_i)\right\|_\mathcal{H},
\end{equation}
where $\Phi$ is a kernel feature map built using the generative model and $x_1,\ldots,x_m$ is the training data, i.e.\@ samples from  $p_\textup{data}$. When $\mathcal{H}$ is a simple finite-dimensional Hilbert space and $\Phi$ can be computed easily, then \cref{eq_modelagnostic:mmd} can be computed by going through the data and computing the means in an online fashion.% While the definition seems to imply that we can only perform group-sample OOD detection, it is straightforward to notice that any test statistic of the form of \cref{eq_modelagnostic:mmd} can be used also for single-test OOD detection when $n=1$.%by just substituting  $\frac{1}{n}\sum_{i=1}^{n} \Phi(\tilde{x}_i)$ with simply $\Phi(\tilde{x})$.

%\subsection{How to choose $\Phi$?}
As always with kernel methods, a key question is how to choose the kernel, or its feature map $\Phi$.
Here, we want to use the trained generative model $p_\theta$ to build our kernel feature map $\Phi$.

\paragraph{The Fisher kernel} An important example of kernel based on a generative model is the \emph{Fisher kernel} of \textcite{jaakkola_exploiting_1999}. The embedding of this kernel is the Fisher score 
\begin{equation}
    \label{eq_modelagnostic:Fisher_score}
    \Phi_\textup{Fisher} (x) = I(\theta)^{-\frac{1}{2}} \nabla \log p_\theta (x),
\end{equation}

and the corresponding reproducing kernel Hilbert space norm is just the $\ell_2$ norm: $|| \cdot || _\mathcal{H} = || \cdot ||_2$. In the case of the Fisher kernel, this means that \cref{eq_modelagnostic:mmd} becomes:
\begin{multline}
    \label{eq_modelagnostic:mmd_fisher}
    \textup{MMD}_{\Phi_\textup{Fisher}}\left(\frac{1}{m} \sum_{i=1}^{m} x_i,\frac{1}{n}\sum_{i=1}^n \tilde{x}_i\right) = \\ \left\|\frac{I(\theta)^{-\frac{1}{2}}}{m} \sum_{i=1}^{m}  \nabla \log p_\theta (x_i) - \frac{I(\theta)^{-\frac{1}{2}}}{n}  \sum_{i=1}^{n}  \nabla \log p_\theta (\tilde{x}_i)\right\|_2.
\end{multline}
We will see later that MMD with a Fisher kernel is closely related to the score statistic. In \cref{appendix_modelagnostic:mahalanobis}, we additionally show that another popular OOD metric known as the \emph{Mahalanobis score} \parencite{lee_simple_2018} can be interpreted as a MMD statistic with a certain Fisher kernel.




\paragraph{The typicality kernel} A very simple approach of embedding the data using $p_\theta$ is to choose $\Phi_\textup{Typical}(x) = \log p_\theta (x)$. Then, MMD is exactly equivalent to the \emph{typicality test statistic} of \textcite{nalisnick_detecting_2019}, although this connection was not explicitly stated  by \textcite{nalisnick_detecting_2019}. Because of this, we call the kernel $k(x,y) = \log p_\theta (x) \cdot \log p_\theta (y)$ the \emph{typicality kernel}.  %We could also call this "new" kernel the "likelihood kernel".
%While $\Phi_\textup{Typical}$ is not as well motivated as the Fisher kernel, we found that it generally gives good results. 
While $\Phi_\textup{Typical}$ is not as well motivated as a kernel as $\Phi_\textup{Fisher}$, the concepts of typicality and typical set can be used to explain unintuitive behaviours of probability distributions in high-dimensional space as highlighted by \textcite{nalisnick_deep_2019}. We also found that using this kernel generally gives good results for OOD tasks. An interesting analysis that we'd not consider in this paper would be to study the properties of this kernel.



In general, neither of these two kernels are characteristic, meaning that our MMD can be zero even if the distributions are not identical. This could be solved by combining them with a characteristic kernel, as in \textcite{liu_learning_2020}, at the price of including a new hyperparameter.


%By looking at the definition of the two considered test statistics we can see that these are both one-sided. We can expect them to be larger when the data are more likely to be OOD. This allows us to compute $p$-values by simply using the empirical CDF, which is hyperparameter-free. 

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  SECTION 3   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Combining different test statistics}
\label{sec_modelagnostic:combination_p_values}
For single-sample OOD detection, \textcite{zhang_understanding_2021} proved that there is not a single statistic that is constantly better compared to all the possible alternatives of interest. For this reason, we believe that using a combination of different test statistics should lead to an overall better OOD detection in settings where a single statistic might fail. Assume we compute $k$ different test statistics $T_1,\dots, T_k$, each testing $\mathcal{H}_0$ against $\mathcal{H}$ as defined in \cref{sec_modelagnostic:parametric,sec_modelagnostic:MMD}. The goal is to combine these different tests into a single statistical test that ideally will perform better than the initial single tests. However, different tests can have different magnitudes, and they can differ also in the direction of out-of-distribution detection, i.e.\@ for some statistics having a higher value is associated with being OOD, while for other smaller values are OOD. This makes a combination non-trivial.

\textcite{morningstar_density_2021} proposed the density of states estimator (DoSE) to overcome this problem. They only focused on the single-sample detection task, i.e.\@ $n=1$ following our problem formulation. Their idea is to fit different nonparametric density estimators, such as a kernel-density estimator (KDE) or a one-class support vector machines (SVM), for each different statistic $T_1,\dots, T_k$ by using the values computed on the training set examples. For a single test example, $\tilde{x}_1$, they first compute $T_1,\dots, T_k$ and then combine those statistics by summing the different KDEs log-density. %This approach can be used when combining both one-sided and two-sided test statistic. 
While this approach can be used for any type of statistic, and thus is more general, it uses less prior information. Indeed, if we use only statistics that are truly one-sided, then we assume that a method that leverages the true nature of the statistics should work better. In addition to that, fitting a KDE introduces an additional hyperparameter.

In our work, instead, we propose a different approach and leverage the fact that we use only one-sided test statistics. This setting is a well-studied problem in the literature both for independent \parencite{fisher_statistical_1925, folks_asymptotic_1971} and dependent one-sided test statistics \parencite{brown_400_1975, wilson_harmonic_2019}. All these approaches rely on the computation of $p$-values of each statistic for the test set $\tilde{x}_1,\ldots,\tilde{x}_n$. This corresponds to computing
$p_j = \textup{Pr}(T_j > t_j \mid \mathcal{H}_0)$, i.e.\@ the probability that the $j$'th test is bigger than the observed value under the null hypothesis $\mathcal{H}_0$, where we assume that each $T_j$ has a continuous distribution. Using $p$-values also solves the problem of the statistics having different scales. Indeed, $p$-values transform the different test statistics into the unit interval.

\paragraph{Computation of $p$-values} 
We want to approximate the distribution of the $p$-values $p_1, \ldots, p_k$ of $\tilde{x}_1,\ldots,\tilde{x}_n$ under the null hypothesis $\mathcal{H}_0$. When $\mathcal{H}_0$ is true, then $p_j$ is uniformly distributed on the interval $[0,1]$. To succeed in this, we should be able to compute $p_j = \textup{Pr}(T_j > t_j \mid \mathcal{H}_0)$, therefore we need to estimate the distribution of each statistic $T_j$ under $\mathcal{H}_0$. %As done by \textcite{nalisnick_detecting_2019}, we assume the existence of a validation set $\mathbf{X}'$ that was not used to train our generative model. Instead of bootstrapping $K$ new datasets $\{X^{'}_k\}_{k=1}^{K}$ of size $M$, as they did in their work, we directly evaluate each test statistic $T_j$ on every single validation example. Asymptotically, this is equivalent to creating $K$ new datasets of size $M=1$ when $K \rightarrow \infty$. 
As done by \textcite{nalisnick_detecting_2019}, we assume the existence of a validation set $\mathbf{X}'$ that was not used to train our generative model. From $\mathbf{X}'$ we bootstrap $S$ new datasets $\{\mathbf{X}^{'}_s\}_{s=1}^{S}$ of size $M'$ by using bootstrap resampling. When $n$ is small, for example $n=1$ or $n=2$, where $n=1$ corresponds to single-sample OOD detection, and the validation set is big, a convenient alternative to bootstrapping is to directly evaluate each test statistic $T_j$ on every single validation example. Asymptotically, this is equivalent to creating $S$ new datasets of size $M'=1$ when $S \rightarrow \infty$. In case of $n=2$, i.e.\@ two-samples OOD detection, and a big validation set we can simply bootstrap without resampling.
We then use these values to estimate the empirical distribution function (eCDF) of the considered statistic $T_j$ under $\mathcal{H}_0$. To obtain the $p$-values of test examples $\tilde{x}_1,\ldots, \tilde{x}_n$ for the test statistic $T_j = t_j$, we simply compute $p_j = 1 - \textup{Pr}(T_j < t_j \mid \mathcal{H}_0)$ using the eCDF.

\paragraph{Combining test statistics by combining $p$-values}
Fisher's \citeyearpar{fisher_statistical_1925} method is a procedure to combine different $p$-values $p_1,\dots, p_k$. This method assumes that all the considered test statistics are independent, and \textcite{folks_asymptotic_1971} proved that it is asymptotically optimal among all methods of combining independent tests.
%
Given $T_1,\dots, T_k$ and corresponding $p$-values $p_1,\dots, p_k$, Fisher's method combines the $p$-values into a test statistic $X^2$ defined as
\begin{equation}
    \label{eq_modelagnostic:Fisher_method}
    X^2 \sim -2 \sum_{j=1}^{k} \ln (p_j).
\end{equation}
In case all null-hypotheses are accepted, the resulting test statistic $X^2$ follows a chi-squared distribution with $2k$ degrees of freedom. 
%
In the \cref{appendix_modelagnostic:harmonic}, we also consider the Harmonic mean $p$-value \parencite{wilson_harmonic_2019} as a way to combine $p$-values from different statistics. This method usually works best when the statistics are not independent.

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  SECTION 4   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{From test statistics to practical out-of-distribution scores}
\label{sec_modelagnostic:section4}

Several of the test statistics that we consider make use of the inverse of the Fisher information matrix $I(\theta)$. The true Fisher information matrix requires an identifiable model to be invertible \parencite{watanabe_algebraic_2009} and computing its inverse is $\mathcal{O}(m^3)$, where $m$ is the number of model parameters. For DGMs, the Fisher information matrix might not be invertible due to the fact that DGMs typically do not satisfy the identifiability condition. Also, the inversion may be computationally impractical, since state-of-the-art DGMs involve very high-dimensional parameter spaces $\Theta$. For the same reason, storing $I(\theta)$ can also be challenging.

We replace it by using a proxy matrix that has to be easy to compute and invert. A first idea is to simply replace $I(\theta)$ by the identity matrix. A more refined way is to look for a diagonal approximation. In \cref{appendix_modelagnostic:cheap}, we describe cheap ways of computing such approximations. In particular, we will study two cases: the case where $I(\theta)$ is replaced by the identity matrix and the case where $I(\theta)$ is replaced by a diagonal matrix estimated using the training data.

A possible third option would be to estimate the diagonal of $I(\theta)$ using samples from the model. However, for autoregressive models as the PixelCNN, sampling is a sequential procedure, and therefore it is computationally expensive to generate many samples when the input-space is high-dimensional. For this reason, we do not consider it in this work. More complex and precise approximations of the FIM exists, such as the Kronecker-factored Approximate Curvature (K-FAC, \citealp{martens_optimizing_2015}), but these are not defined for all types of layers used by state-of-the-art models.

\paragraph{On the difficulty of computing per-example gradients} Both the diagonal approximation of the FIM and the computation of the MMD with Fisher kernel of \cref{eq_modelagnostic:mmd_fisher} require the gradient computation for all training and test examples. This is known as a costly procedure. For example, if we have to compute the gradient for $N$ examples using a simple fully connected network with $l$ layers of size $p$, the naive procedure of using a batch-size of dimension 1 is $\mathcal{O}(Nlp^2)$ \parencite{goodfellow_efficient_2015}. While more efficient per-example gradient computations were proposed \parencite{goodfellow_efficient_2015, rochette_efficient_2019}, these techniques can only be applied on simple fully connected or convolutional networks. While for this paper we relied on the naive solution of looping through every example one at the time, a more efficient solution is provided by the BackPACK library \parencite{dangel_backpack_2020} which allows to compute the gradient with respect each sample in a mini-batch. 

\subsection{Relationship between MMD with Fisher kernel and the score statistic and gradient norm} 
Depending on the choice of the Fisher information approximation, we can notice that there is a strong connection between the MMD using a Fisher kernel, the score statistic and the gradient norm in terms of expected OOD performance. Let us start by looking at the case where we approximate $I(\theta)$ with a diagonal matrix estimated using the training data. At the maximum likelihood estimate, we have that $\mathbb{E}[ \nabla \log p_\theta (x)] = 0$, i.e.\@ the first term inside the norm is $0$. Therefore, we expect that the differences between the OOD scores computed by using \cref{eq_modelagnostic:mmd_fisher} will be preserved if we only consider $\left\|I(\theta)^{-1/2} \nabla \log p_\theta (\tilde{x}_1,\dots,\tilde{x}_n)\right\|_2$,
%\begin{equation}
%\left\|I(\theta)^{-1/2} \nabla \log p_\theta (\tilde{x}_1,\dots,\tilde{x}_n)\right\|_2 =  \sqrt{\nabla \log p_\theta (\tilde{x}_1,\dots,\tilde{x}_n)^T I(\theta)^{-1}  \nabla \log p_\theta (\tilde{x}_1,\dots,\tilde{x}_n)},
%\end{equation}
which corresponds to the square root of the score statistic. Since taking the square root still preserves the difference between values, we can expect that the MMD using a Fisher kernel will perform closely to the score statistic. The same reasoning also holds in case we replace the FIM with an identity matrix. In this specific case, instead, we will get that $\left\|{I}(\theta)^{-1/2} \nabla \log p_\theta (\tilde{x}_1,\dots,\tilde{x}_n)\right\|_2 = \left\|\nabla \log p_\theta (\tilde{x}_1,\dots,\tilde{x}_n)\right\|_2 $, which corresponds to considering the gradient norm. 

Computationally speaking, considering the score statistic instead of the MMD Fisher lets us avoid going through the entire training set to compute the average gradient (first term in \cref{eq_modelagnostic:mmd_fisher}) while carrying the same information. Therefore, in this paper, we will mainly focus on the combination of the typicality test and the score statistic.

\subsection{Why does it make sense to combine the score statistic and the typicality test?}

Let us discuss our choice of combining the score statistic and the typicality test. We will try to look in which situations one of the test fails and the other works and vice versa. Both examples assume that the in-distribution data follows an $\mathcal{N}(0,I_D)$ distribution, and that the correct model has been learned by fitting $(\mathcal{N}(\theta,I_D))_{\theta \in \mathbb{R}^D}$ via maximum-likelihood. Even in this simple setting with no model misspecification, we will see that the two statistics that we consider may have very different strengths.

In this simple Gaussian case, the score statistic can be computed exactly and will be $||\tilde{x}_1 + \ldots + \tilde{x}_n ||_2^2 $. On the other hand, the typicality statistic will be $| (||\tilde{x}_1||_2^2 + \ldots + ||\tilde{x}_n||_2^2)/(2\cdot n) -  D/ 2| $. One interesting regime is the very high-dimensional one ($D \to \infty$). Indeed, by the law of large numbers, these random statistics become deterministic quantities.


\paragraph{Typicality fails, the score succeeds}

Assume that we have two independent OOD data samples that follow a product of truncated normal distributions, with density proportional to $$\mathcal{N}(x | 0,I_D) \cdot  \mathbf{1}\{x_1>0,\ldots,x_D>0\}.$$

We denote by $T_\text{score}^\text{ood},T_\text{score}^\text{id}$ and $T_\text{typicality}^\text{ood},T_\text{typicality}^\text{id}$ the statistics obtained when confronted with either OOD data from the truncated normal, or the in-distribution data. While these statistics are random in general, they will become deterministic when $D \to \infty$, by virtue of the law of large numbers.

For the typicality statistic, these two OOD samples will be indistinguishable from Gaussian ones. Indeed, when $D \to \infty$, both $T_\text{typicality}^\text{ood}$ and $T_\text{typicality}^\text{id}$ will be $\mathcal{O}(D)$.
%
On the other hand, for the score, one can show that
\begin{equation}
T_\text{score}^\text{ood} - T_\text{score}^\text{id} \sim 2 D \mu_\text{TN}^2,
\end{equation}
where $\mu_\text{TN}>0$ is the mean of the truncated normal distribution.


\paragraph{Typicality succeeds, the score fails}

Let us now consider as the OOD distribution a Dirac distribution with mean $0$. Suppose that we see a single sample from this distribution. In this case, the score statistic will be $0$, and will therefore not detect that the point is actually OOD. However, when $D$ is large, the typicality test will be able to declare that this point is anomalous, as shown by \textcite{nalisnick_detecting_2019}.

Therefore, we have that the typicality test and the score statistic are complementary and measure a different type of information. In \cref{appendix_modelagnostic:correlation}, we empirically show that they are not correlated, by plotting the two measures against each other and by computing the correlation matrix.



%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  SECTION 5: RELATED WORKS   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related works}
\label{sec_modelagnostic:related_works}

Since \textcite{nalisnick_deep_2019} and \textcite{hendrycks_deep_2019}, different test statistics or methodologies for OOD detection using DGMs were proposed. Most of the recent solutions were highly influenced by three major lines of work: \emph{typicality set}, \emph{likelihood ratio} test statistics, and \emph{model misestimation}. 

The typicality set hypothesis was introduced by \textcite{nalisnick_detecting_2019} as a possible explanation for the DGMs assigning higher likelihood to OOD data. The typicality set is the subset of the model full support where the model samples from and this does not intersect with the region of higher likelihood. While the typicality test was introduced for batch OOD detection, \textcite{morningstar_density_2021} shows that it also works well in the single-sample case. This is also confirmed by our own experiments. 

The likelihood ratio test statistic method by \textcite{ren_likelihood_2019} assumes that every input is composed by a background component and a semantic component. For OOD detection, only the semantic component matters. In addition to a model trained on the in-distribution data, they proposed to train a background model on perturbed inputs data and then for each test example consider as OOD score the likelihood ratio between the two models. \textcite{schirrmeister_understanding_2020}, instead, trained the background model on a more general distribution of images by considering 80 million general tiny images. Similarly to these approaches, \textcite{serra_input_2020} argued that the failure of DGMs is due to the high-influence that the input complexity has on the likelihood. Therefore, they proposed to use a general lossless image compression algorithm as a background model.
All these methods, however, require additional knowledge of the OOD data for either choosing an image augmentation procedure to perturb the input data or for choosing a specific compressor. 

Another line of works blame the models themselves and not the test statistics. \textcite{zhang_understanding_2021} argued that model misestimation is the main cause of higher likelihood assigned to OOD data. This can be due to both the model architecture and the maximum likelihood objective. \textcite{kirichenko_why_2020} and \textcite{schirrmeister_understanding_2020} showed that normalizing flows can achieve better OOD performance despite achieving a worse likelihood if one changes some model design choices.
%
Other works in the literature focused on deriving specific test statistics that work only for a specific model, for example for VAEs \parencite{xiao_likelihood_2020, maaloe_biva_2019, havtorn_hierarchical_2021}, or for normalizing flows \parencite{kirichenko_why_2020, ahmadian_likelihoodfree_2021}.

%All the solutions we presented can be described in terms of the following two categories: \emph{model-agnostic} vs \emph{model-specific} test statistics and statistics \emph{with OOD assumptions} vs statistics \emph{with no OOD assumptions}. In this work, we are mostly interested in model-agnostic scores with no OOD assumptions, i.e.\@ scores that can be directly computed with any models and that do not need any explicit assumption on the OOD data. 

As mentioned in the introduction, we frame the OOD detection problem in terms of a statistical test problem. Recently, \textcite{haroush_statistical_2021} showed that adopting hypothesis testing at the layer and channel level of a neural network can be used for OOD detection in the discriminative setting. They used both Fisher's method and Simes' method to combine class-conditional $p$-values computed for each convolutional and dense layer of a deep neural network. We focus on the unsupervised setting using DGMs and use hypothesis testing on statistics that can be computed on all differentiable DGM. As already explained in section \cref{sec_modelagnostic:combination_p_values}, \textcite{morningstar_density_2021} considered the combination of different statistics for OOD detection. The main difference with their approach is that we propose statistics that can be applied to any differentiable generative model and combine them by using Fisher's method, which takes advantage of using only one-sided independent statistics. Concurrently, \textcite{choi_robust_2021} derived the score statistic by starting from the likelihood ratio statistic and applying a Laplace approximation. They computed the score statistic only for certain layers of the model and for a specific example, the OOD score is given by the infinity norm of these different layer scores after a ReLU operation. Our procedure differs both in the derivation of the score statistic and its usage since we compute the score statistic for the entire model.

\begin{sidewaysfigure}[tbp]
    \includegraphics[width=\textwidth]{graphics/paper_modelagnostic/pvalues_comb_no_drop_correct_font.pdf}
    \caption[Distribution of the p-values of the typicality test, score statistic, Fisher's method and a combination of all.]{ \emph{First plot}: $p$-values of the typicality test on the two test sets. We can see that under $\mathcal{H}_0$, they should be uniformly distributed. \emph{Second plot}: $p$-values of the score statistic. \emph{Third plot}: values obtained by the Fisher's method. In red, we plot the density function of a $\chi^2$-distribution with 4 degrees. This shows that the statistics are independent. \emph{Fourth plot}: $p$-values obtained of the combination. These plots refer to a PixelCNN++ trained on FashionMNIST without dropout.}
    \label{fig_modelagnostic:comb_p_values}
\end{sidewaysfigure} 

\begin{table*}[tb]
    %\renewcommand{\thetable}{4}
    \caption{AUROC$\uparrow$ for single-sample OOD detection. For Fisher's method we mean the combination of the typicality test and the test statistic. These are also combined using DoSE.}
    \resizebox{\textwidth}{!}{
        \scriptsize
        \begin{tabular}{ccccccc}
            \toprule
            &\multicolumn{6}{c}{\textsc{FashionMNIST (in) / MNIST (out)}}\\
            \cmidrule{2-7}
            & \multicolumn{4}{c}{\textcolor{blue}{\textsc{single statistics}}} & \multicolumn{2}{c}{\textcolor{red}{\textsc{combination}}}\\
            \cmidrule(r){2-5}\cmidrule(l){6-7}
            \textsc{models}  & \textcolor{blue}{\textsc{$\log p(x)$}} & \textcolor{blue}{\textsc{$\|\nabla \log p(x)\|_2$}} & \textcolor{blue}{\textsc{Typicality}} & \textcolor{blue}{\textsc{Score Stat}} & \textcolor{red}{\textsc{Fisher's method}} & \textcolor{red}{\textsc{DoSE$_{\textup{KDE}}$}} \\
            \midrule
            \textsc{PixelCNN++} (dropout)  &  0.0762 & 0.8709 & 0.8314 & \textbf{0.8822} & \textbf{0.9369}  & 0.8822 \\
            \textsc{PixelCNN++} (no dropout)  & 0.1048 &  \textbf{0.9532} &  0.7575 &  0.9381 & \textbf{0.9536} &  0.9382 \\
            \textsc{Glow} (RMSProp) &  0.1970 & 0.8904  & 0.4807 &  \textbf{0.9114} & 0.8598  &  \textbf{0.8901}\\
            \textsc{Glow} (Adam)  & 0.1223 & 0.7705 & 0.6987 &  \textbf{0.8745} & \textbf{0.8839}  & 0.8752 \\
            \textsc{HVAE}  & 0.2620 & 0.8714 &0.4884 & \textbf{0.9578} & 0.9383 & \textbf{0.9498} \\
            \bottomrule
            & & & & & & \\
            \toprule
            &\multicolumn{6}{c}{\textsc{CIFAR10 (in) / SVHN (out)}}\\
            \cmidrule{2-7}
            & \multicolumn{4}{c}{\textcolor{blue}{\textsc{single statistics}}} & \multicolumn{2}{c}{\textcolor{red}{\textsc{combination}}}\\
            \cmidrule(r){2-5}\cmidrule(l){6-7}
            \textsc{models}  & \textcolor{blue}{\textsc{$\log p(x)$}} & \textcolor{blue}{\textsc{$\|\nabla \log p(x)\|_2$}} & \textcolor{blue}{\textsc{Typicality}} & \textcolor{blue}{\textsc{Score Stat}} & \textcolor{red}{\textsc{Fisher's method}} & \textcolor{red}{\textsc{DoSE$_{\textup{KDE}}$}} \\
            \midrule
            \textsc{PixelCNN++} (model1)  & 0.1553 & \textbf{0.8006} &  0.6457 & 0.6407 & \textbf{0.6826} & 0.6571 \\
            \textsc{PixelCNN++} (model2) & 0.1567 & \textbf{0.7923} & 0.6498 &  0.7067 & \textbf{0.7300} & 0.7243 \\
            \textsc{Glow} (RMSProp)  &  0.0630 & 0.8585 & \textbf{0.8651} &  0.7940 &  \textbf{0.8683} & 0.8510 \\
            \textsc{Glow} (Adam)   &  0.0627 & 0.7844 &  \textbf{0.8624} &  0.7655 &  \textbf{0.8613} &  0.8588 \\
            \textsc{HVAE}  & 0.0636 & 0.8067  & \textbf{0.8679} & 0.7335 & \textbf{0.8603} & 0.8179 \\
            \bottomrule
            & & & & & & \\
            \toprule
            &\multicolumn{6}{c}{\textsc{CIFAR10 (in) / CIFAR100 (out)}}\\
            \cmidrule{2-7}
            & \multicolumn{4}{c}{\textcolor{blue}{\textsc{single statistics}}} & \multicolumn{2}{c}{\textcolor{red}{\textsc{combination}}}\\
            \cmidrule(r){2-5}\cmidrule(l){6-7}
            \textsc{models}  & \textcolor{blue}{\textsc{$\log p(x)$}} & \textcolor{blue}{\textsc{$\|\nabla \log p(x)\|_2$}} & \textcolor{blue}{\textsc{Typicality}} & \textcolor{blue}{\textsc{Score Stat}} & \textcolor{red}{\textsc{Fisher's method}} & \textcolor{red}{\textsc{DoSE$_{\textup{KDE}}$}} \\
            \midrule
            \textsc{PixelCNN++} (model1)  & 0.5153 & 0.5306 & \textbf{0.5458} & 0.5362 & \textbf{0.5563}  & 0.5477\\
            \textsc{PixelCNN++} (model2) & 0.5150 &  0.5230 & \textbf{0.5455} & 0.5325  & \textbf{0.5543} & 0.5453 \\
            \textsc{Glow} (RMSProp)  &  0.5206 & 0.5547 & 0.5507 & 0.\textbf{5801} & \textbf{0.5844} &  \textbf{0.5842} \\
            \textsc{Glow} (Adam)   & 0.5206 & 0.5593 & 0.5508  & \textbf{0.5692} & \textbf{0.5775} &  \textbf{0.5767} \\
            \textsc{HVAE}  & 0.5340 & 0.5280 &  0.5493 &  \textbf{0.5798} &  0.5879 &  \textbf{0.5941}\\
            \bottomrule 
        \end{tabular}
        \label{tab_modelagnostic:single_sample_results}
    }
    \vspace*{-\baselineskip}
\end{table*}





%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  SECTION 5: EXP SETUP   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental Setup}

To evaluate the performance of the combination of the typicality test and the score statistic in detecting OOD data, we follow the experiments of \textcite{nalisnick_deep_2019, hendrycks_deep_2019} and considered the OOD detection task on three image dataset pairs that have been proven challenging for DGMs, i.e.\@ FashionMNIST \parencite{xiao_fashionmnist_2017} vs MNIST \parencite{lecun_mnist_1998}, CIFAR10 \parencite{krizhevsky_learning_2009} vs SVHN \parencite{netzer_reading_2011}, and CIFAR10 vs CIFAR100. \textcite{winkens_contrastive_2020} divide these tasks into \textit{far}-OOD tasks, where the in-distribution and out-distribution are different such as in the case of CIFAR10 against SVHN, and \textit{near}-OOD where the two distributions are pretty similar, such as CIFAR10 and CIFAR100. \textit{Near}-OOD tasks are usually most challenging.

For each task, we trained three different state-of-the-art DGMs, a PixelCNN++ \parencite{salimans_pixelcnn_2017}, a Glow model \parencite{kingma_glow_2018}, and a hierarchical variational autoencoder \parencite{kingma_autoencoding_2014,rezende_stochastic_2014} with bottom-up inference (HVAE, \citealp{burda_importance_2016}). These are DGMs parametrized by neural networks that make different assumptions in the modelling choice of the target distribution. In addition to that, for PixelCNN++ and Glow we have a tractable likelihood while for HVAE we can only estimate a lower bound. A more in-depth description of these methods and additional results testing MNIST against FashionMNIST and SVHN against CIFAR10 can be found in \cref{appendix_modelagnostic:mnist_svhn}. We also extensively analyzed, focusing mostly in the influence of the preprocessing, the results on CIFAR10 vs CelebA \parencite{liu_deep_2015} in \cref{appendix_modelagnostic:celeba}. In \cref{appendix_modelagnostic:gmm_ppca}, we also considered a Gaussian Mixture Model and a Probabilistic PCA as simple generative models.

\paragraph{Models} To analyze the effect of model architecture choices and optimization choice, we also consider different versions of the same model that reaches a similar log-likelihood. We consider 5 different models for each dataset pair. On FashionMNIST, we consider two Glow models, one trained using Adam and one using RMSProp and two PixelCNN++, trained with and without dropout. For CIFAR10, we consider two different PixelCNN++, one trained by us (model1) and one using a checkpoint given by the repository we used\footnote{ \href{https://github.com/pclucas14/pixel-cnn-pp}{\texttt{https://github.com/pclucas14/pixel-cnn-pp}}} (model2), and two Glow models (Adam and RMSProp). For both datasets, instead, we consider only one HVAE. 


\paragraph{Baselines} We are mostly interested in testing our methods with other model-agnostic test statistics in the literature. Apart from using the plain likelihood as an OOD score, the only test statistic we are aware of that can be applied to any generative model without requiring any background model or OOD assumptions is the typicality test statistic of \textcite{nalisnick_detecting_2019}. We also considered the gradient norm, which in general seem to work well but fails in the case of SVHN vs CIFAR10 (see \cref{appendix_modelagnostic:mnist_svhn}). In addition to that, we compare our methods to a model-agnostic version of DoSE by \textcite{morningstar_density_2021}, where we used KDEs to combine the score statistic and the typicality test statistic. 


\paragraph{Evaluation} We compare our methods with the baselines by computing the area under the receiver operating characteristic curve (AUROC) as done in previous works \parencite{hendrycks_deep_2019, ren_likelihood_2019, morningstar_density_2021}. We also evaluate our methods in terms of False Discovery Rate (FDR) control \textcite{benjamini_controlling_1995}, i.e.\@ the proportion of false positive among the rejected hypothesis. Note that both quantities need to know the true label (OOD or in-distribution) to be computed.


%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  SECTION 5: RESULTS   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}

\paragraph{One-sample OOD}
We first evaluate our proposed method in the single-sample OOD detection task. Results are summarized in \cref{tab_modelagnostic:single_sample_results}. We start by considering the OOD task on FashionMNIST against MNIST. Looking at the single statistics, we notice that the score statistic is the one that works the best and the combination of the typicality test and the score statistic usually improve the AUROC than the two standalone statistics. In addition to that, it is better than the combination of the two statistics by using a KDE. DoSE seems to perform better on Glow trained with RMSProp, where the typicality is failing. 

On natural images, instead, we have a different trend. The typicality test is better than the score statistic overall. The gradient norm surprisingly performs well in the two dataset pairs, but it fails badly when the model is trained on SVHN (see \cref{appendix_modelagnostic:mnist_svhn}). Regarding the combination of the two statistics, the Fisher's method is always better than DoSE, but in this setting, it improves over the best of the single statistics three out of five times.
%
In the \textit{near}-OOD task, we have that both our method and DoSE using our suggested statistics perform closely. We want to highlight that for this challenging task we get results that are comparable with those reported in \textcite{morningstar_density_2021}, but by using two model-agnostic statistics instead of three model-specific ones.
%
It can be noticed that the way we train our models has a strong influence on both the typicality test and the score statistic, although the models get the same test log-likelihood. In \cref{appendix_modelagnostic:checkpoints}, we also show that this can happen between different checkpoints of the same model.

In \cref{fig_modelagnostic:comb_p_values}, we show that the $p$-values distributions for both the typicality and the score statistic are uniformly distributed under the null-hypothesis and that the combination under the null follows a $\chi^2$ distribution with 4 degrees of freedom. This also supports the fact that the typicality test and the score statistic are independent.

\paragraph{Two-sample OOD}
As \textcite{nalisnick_detecting_2019}, we consider how these test statistics change when performing two-sample OOD detection. Results are summarized in \cref{tab_modelagnostic:two_sample_results}. As shown by \textcite{nalisnick_detecting_2019}, the typicality improves, but also the score statistic gets better if we consider more samples. Combining those leads to an improvement of performance in terms of AUROC with almost all the models. When training on FashionMNIST, the model can almost perfectly distinguish between the in-distribution test set and the OOD test set. While the performance improves for the two \textit{far}-OOD task, we have that the improvement is slightly less evident in the \textit{near}-OOD task of CIFAR10 vs CIFAR100.

\begin{table}[t]
\scriptsize
\centering
\caption{AUROC$\uparrow$ for two-sample OOD detection using the usual considered model.}\normalsize
    %\renewcommand{\thetable}{4}
    \resizebox{0.90\textwidth}{!}{
        \scriptsize
        \begin{tabular}{ccccc}
            \toprule
            &\multicolumn{4}{c}{\textsc{FashionMNIST (in) / MNIST (out)}}\\
            \cmidrule{2-5}
            \textsc{models}  & \textcolor{blue}{\textsc{Typicality}} & \textcolor{blue}{\textsc{Score Stat}} & \textcolor{red}{\textsc{Fisher's method}} & \textcolor{red}{\textsc{DoSE$_{\textup{KDE}}$}} \\
            \midrule
            \textsc{PCNN++} (drop.) &  0.9514 & 0.9828 & \textbf{0.9934} & \textbf{0.9912} \\
            \textsc{PCNN++} (no drop)  & 0.9081 &   0.9853 & \textbf{0.9916} &  \textbf{0.9921} \\
            \textsc{Glow} (RMSProp) & 0.6190 &  \textbf{0.9588} &  0.9187 & 0.7201  \\
            \textsc{Glow} (Adam)  & 0.8525 &  \textbf{0.9716} & \textbf{0.9708} & \textbf{0.9736} \\
            \textsc{HVAE}  & 0.6634 & \textbf{0.9881} & \textbf{0.9837} &  \textbf{0.9889}\\
            \bottomrule
            & & & &\\
            \toprule
            &\multicolumn{4}{c}{\textsc{CIFAR10 (in) / SVHN (out)}}\\
            \cmidrule{2-5}
            \textsc{models}  & \textcolor{blue}{\textsc{Typicality}} & \textcolor{blue}{\textsc{Score Stat}} & \textcolor{red}{\textsc{Fisher's method}} & \textcolor{red}{\textsc{DoSE$_{\textup{KDE}}$}} \\
            \midrule
            \textsc{PCNN++} (m1) & 0.7675 & 0.6555 & \textbf{0.7800} &  0.7046\\
            \textsc{PCNN++} (m2) & 0.7720 &   0.7235 & \textbf{0.8227} & 0.7850\\
            \textsc{Glow} (RMSProp)  & 0.9497 &  0.8624 & \textbf{0.9536} & 0.9379\\
            \textsc{Glow} (Adam) & 0.9480 &  0.8370 & \textbf{0.9519} & 0.9329\\
            \textsc{HVAE} &  \textbf{0.9623} &  0.7754 &  0.9560 & 0.9133\\
            \bottomrule
            & & & &\\
            \toprule
            &\multicolumn{4}{c}{\textsc{CIFAR10 (in) / CIFAR100 (out)}}\\
            \cmidrule{2-5}
            \textsc{models}  & \textcolor{blue}{\textsc{Typicality}} & \textcolor{blue}{\textsc{Score Stat}} & \textcolor{red}{\textsc{Fisher's method}} & \textcolor{red}{\textsc{DoSE$_{\textup{KDE}}$}} \\
            \midrule
            \textsc{PCNN++} (m1) &  0.5433 & 0.5450 &  \textbf{0.5540} &  \textbf{0.5508}\\
            \textsc{PCNN++} (m2) &  0.5435 &  0.5370 &  \textbf{0.5533} & 0.5470 \\
            \textsc{Glow} (RMSProp)  & 0.5550 &  \textbf{0.6211}  & 0.6165 &  \textbf{0.6233}\\
            \textsc{Glow} (Adam) & 0.5558  &   0.6073 & 0.6083 &   \textbf{0.6117}\\
            \textsc{HVAE} & 0.5594  & 0.6188  &  \textbf{0.6218} & \textbf{0.6273} \\
            \bottomrule
        \end{tabular}
        \label{tab_modelagnostic:two_sample_results}
    }
    \vspace*{-\baselineskip}
\end{table}


\subsection{Practical OOD detection with FDR control}
One of the advantages of framing the problem as multiple testing is that we have a well-defined procedure to decide on which hypotheses to reject while controlling the False Discovery Rate (FDR, \citealp{benjamini_controlling_1995}). Imagine we are interested in finding the outliers from the dataset given by the combination of the two test-sets, but we do not want to discard too many inliers, then we can use the Benjamini-Hochberg (BH) procedure \parencite{benjamini_controlling_1995} to decide a threshold and reject all hypothesis below that threshold. For a specific significance level $\alpha$, the procedure guarantees that the FDR stays below that level. Therefore, we can guarantee that the rate of inliers that are classified as outliers is less than the chosen $\alpha$.

%As we have mentioned before, when combining $p$-values of $k$ different statistics using the Fisher's method if the null hypothesis is true, then these new scores are $\chi^2_{2k}$. This allows us to easily compute the $p$-values associate to those new values and then apply the Benjamini-Hochberg procedure. Alternatively, one can directly apply the procedure to the $p$-values of a single test-statistic.

We leverage the fact that when the null hypothesis is true and the $p$-values are independent, then the scores obtained by combining $k$ different statistics are $\chi^2_{2k}$ distributed to compute the $p$-values. Alternatively, the procedure can be also applied to the $p$-values of a single test-statistic.
%
Usually, it is better to use an FDR control when it is actually possible to make few false discoveries, i.e.\@ when we have a strong statistic. Therefore, we expect the procedure to work well when the AUROC is good, for examples on models trained on FashionMNIST.

As can be seen in \cref{fig_modelagnostic:type1}, we have that the Type I ratio line stays below the identity line, meaning that the BH correction is working. When deciding for a specific threshold $\alpha$, we usually have to trade off between Type I and Type II error and in most cases the threshold to choose depends on the application domain. Ideally, we would like to have a low Type I and a low Type II error rate, meaning that we are not considering a lot of in-distribution examples as OOD and at the same time considering a lot of outliers as in-distribution. \cref{fig_modelagnostic:type1} shows that we can achieve this for low values of $\alpha$. When training on CIFAR, instead, we are able to control the FDR only from a certain significance level (see \cref{appendix_modelagnostic:BH_cifar}). This is expected given that the AUROC is not as good as when testing on MNIST.


%%%%%%%%%%%%%%%%%%%%%%% FIGURE
\begin{figure}[tb]
    \centering
    \includegraphics[scale=0.6]{graphics/paper_modelagnostic/fashion_hvae_type1_correct_font.pdf}
    \caption[Type I and Type II errors versus the significance level $\alpha$ on the combination values.]{ Type I (probability of an inlier to be classified as outlier) and Type II (probability of an outlier to be considered as inlier) errors versus the significance level $\alpha$ on the combination values. By using Benjamini-Hochberg correction, we get that the Type I error stays below identity line.}
    \label{fig_modelagnostic:type1}
\end{figure}


%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  SECTION 5: CONCLUSION   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion and Conclusions}
In this paper we studied the task of out-of-distribution detection using deep generative models and a combination of multiple statistical tests. We tested our method using different state-of-the art DGMs on classic image benchmark for OOD detection. We found that combining the two statistic leads to a more robust score that in some cases is close to state-of-the-art model-specific scores that require more assumptions. We also noticed that both the model design choice and the optimization choices have an influence on the score we are computing. 

When considering only one-sided independent statistics, we showed that the Fisher's method tends to work better than combine them by summing the log-density of a KDE. We also noticed that the score statistic tends to perform a bit worse when the number of parameters of the models increases, i.e.\@ in the context of natural images. One possible reason can be that in this setting the diagonal approximation is not good, and therefore one could consider different approximations, such as K-FAC. 

DGMs have recently been used for handling missing data (see e.g.\@ \citealp{mattei_miwae_2019, ma_eddi_2019, nazabal_handling_2020, ipsen_not-miwae_2021}). An interesting future direction would be to extend these OOD detection methods to handle missing values.

The methods  presented in this paper can also easily be applied when using model-specific one-sided statistics. In addition to obtain a more accurate score if one want to combine the test statistics, this also allows one to use well-defined procedure to control the FDR when choosing a which example to mark as outliers. Having this control, is necessary when we want to apply these methods in real settings. 


\section*{Acknowledgements}
Federico Bergamin and Pierre-Alexandre Mattei contributed equally to this paper, which is indicated by the asterisk (*) in the author list. The work was supported by the Innovation Fund Denmark (0175-00014B and 0153-00167B), the Independent Research Fund Denmark (9131-00082B) and the Novo Nordisk Foundation (NNF20OC0062606 and NNF20OC0065611). Furthermore, it was supported by the French government, through the 3IA Côte d'Azur Investments in the Future project managed by the National Research Agency (ANR) with the reference number ANR-19-P3IA-0002.


}