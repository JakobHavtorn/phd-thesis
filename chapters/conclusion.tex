%!TEX root = ../thesis.tex

\chapter[conclusions and outlook]{Conclusions and outlook}\label{chp:conclusion}
% ~3 pages

\textbf{\Cref{chp:introduction}} introduced the motivational cases of automated medical coding and stroke recognition and used them to exemplify the importance of out"=of"=distribution detection, and, by extension, representation learning. In the context of these cases, we discussed possible machine learning system designs for decision support and considered potential sources of uncertainty and ideal model behavior. 
The cases form a reference point for the thesis as a whole, connecting its contributions within out"=of"=distribution detection and representation learning back to practical applications. 
In this conclusion, we will review the studies presented in previous chapters in the context of the discussion of \cref{chp:discussion} and the recent progress in the field, and point to interesting directions of future research. 
As \textbf{\cref{chp:main-contributions}} already details the contributions made by this thesis, we will not reiterate them in detail in this conclusion. 

% \textbf{\cref{chp:introduction}} laid out how progress-driving technological development has always come with new challenges and risks of error - whether via misuse, misunderstanding or inherent limitations. We emphasized that machine learning comes with these same types of risk, in some cases amplifying their impacts, and argued that uncertainty estimation is a key component in ensuring that systems are safe and reliable in practice. 

% The chapter also provided some background for the research project by introducing the health tech company Corti, with which with project has been defined and carried out, and the motivational cases of recognition of stroke cases in emergency calls (\cref{subsec:motivation-stroke-recognition}) and automated medical coding (\cref{subsec:motivation-medical-coding}). Using these cases, we exemplified how uncertainty might arise in practice and how quantifying it might improve the usefulness of machine learning systems built for the tasks.
% Finally, we drew connections between machine learning reliability (\cref{sec:machine-learning-reliability}) and model calibration (\cref{subsec:model-calibration}) and defined the aleatoric, epistemic, and predictive types of uncertainty (\cref{subsec:understanding-uncertainty}). 

% Along with the technical background provided in
% This introduction formed the basis of 


\vspace{1em}
\textbf{\Cref{chp:technical-background}} provided in-depth technical background that could only be covered briefly by the individual studies. 
We first introduced uncertainty as a concept in the context of information and probability theory (\cref{sec:uncertainty-information-theory}). We then defined the task of out-of-distribution detection and reviewed existing work on the problem (\cref{sec:out-of-distribution-detection}). Finally, we provided technical background for variational autoencoders (\cref{sec:variational-autoencoders}).

\vspace{1em}
\textbf{\Cref{chp:paper-hierarchical}} showed how hierarchical variational autoencoders can fail at likelihood"-based OOD detection due to an overemphasis on low-level features that generalize between different data distributions. 
In other words, the latent representations of high-dimensional data from different distributions overlap, especially for latent variables low in the hierarchy.
By exploiting that VAEs tend to learn more abstract features at latent variables high in the hierarchy, we were able to define a likelihood-ratio score that focused more on features unlikely to be shared between datasets and performed much better for OOD detection. 
In line with our discussion in \cref{sec_discussion:representation-learning-with-vaes}, these findings show that VAEs can indeed learn useful latent representations, although good performance, at least for OOD detection, might require selecting an appropriate subset of latent representations to use. 
Similar variations among features learned at different layers have also been identified for self"=supervised models and speech representations \parencite{pasad_layerwise_2021}. 
To obtain good downstream task performance, this suggests that, besides seeking to improve representations overall, future research effort should also be directed towards finding good methods for selecting relevant subsets of latent variables in a given hierarchical VAE for a certain task.
% - VAEs can learn useful representations, but it can be hard to impose the appropriate constraints that enable this.
% - VAEs can learn representations that are useful for OOD detection, but good performance might require selecting an appropriate subset of feature dimensions to use for discriminating.
% Our work shows that a promising direction of future research is into how to select which dimensions are the most discriminatory.

% In this work we hypothesize that the likelihood estimate of variational autoencoders is a poor score for out-of-distribution due to an overemphasis on low-level features that generalize between distributions. 
% We further hypothesize that a well-formed hierarchy of latent variables provides a tool that can be used to select which features to emphasize for out-of-distribution detection and, hence, a way to improve the performance of variational autoencoders on this task. 
% We proceed to provide empirical and theoretical evidence that low-level features do indeed dominate the likelihood score and propose a new method for out-of-distribution detection using hierarchical variational autoencoders based on a likelihood-ratio score that requires data to be in-distribution across all feature-levels. 
% The proposed method is computationally efficient, fully unsupervised, and performs well on several out-of-distribution detection benchmarks. 

\vspace{1em}
\textbf{\Cref{chp:paper-modelagnostic}} took a different approach to OOD detection than \cref{chp:paper-hierarchical} and focused on developing a model-agnostic method. 
We showed that by phrasing OOD detection as a statistical testing problem and combining different tests, orthogonal properties of the individual tests could be leveraged to improve the OOD detection performance over any single test.
%
The formulation of OOD detection as a statistical test also allows for better guarantees for such systems in practice. For instance, as also discussed in \cref{chp:paper-modelagnostic}, the statistical framework enables false positive rate control which is a valuable property in many practical applications especially if they involve high-risk actions such as in medical decision support. 


% In this follow-up work to \cref{chp:paper-hierarchical}, we note that the set of methods proposed for out-of-distribution detection using generative models is quite large and that many are tailored for specific model types, which suggests that it is possible to develop a model-agnostic approach. We hypothesize that by phrasing the task as a statistical testing problem and combining different tests, the method's efficacy can be improved and weaknesses inherent to any particular test can be alleviated. 
% From this hypothesis, we combine a classical parametric test with the recently introduced typicality test to develop a method applicable to any differentiable  generative model with explicit likelihood, and show that this leads to a more accurate out-of-distribution test. 
% Finally, we discuss the benefits of casting out-of-distribution detection as a statistical testing problem, for instance enabling false positive rate control. This property is valuable in many practical applications, especially in high-risk settings such as medical decision-making.



\vspace{1em}
\textbf{\Cref{chp:paper-brief}} provided an overview of unsupervised neural speech representation learning. Such approaches have recently matched supervised methods on many tasks and represent a significant advance in low-resource settings, such as speech recognition for minority languages. 
We found that for the purpose of learning good representations in an unsupervised manner, self-supervised learning seems to have better inductive biases, or at least pose a more forgiving learning problem, than do VAEs. As discussed in \cref{chp:paper-brief,sec_discussion:representation-learning-with-vaes}, this likely relates more to inductive biases imposed by implicit constraints in the optimization problem and architecture than to the underlying formalism. 
For instance, in discussion about the weaknesses of VAE-based approaches (\cref{sec_discussion:representation-learning-with-vaes}) we concluded that their challenges could not be directly attributed to the maximum marginal likelihood objective. Indeed, the masked pre-training objective widely used for successful self"=supervised methods has also been identified to correspond to a maximum marginal likelihood objective \parencite{moreno-munoz_masked_2023}. 

This also leaves potential for future work to improve the ability of VAEs to learn useful representations. As discussed in \cref{chp:paper-brief,sec_discussion:representation-learning-with-vaes}, promising approaches include adopting masked objectives for VAE training, improving architectural designs to impose better inductive biases, and incorporating advances in gradient estimators more widely \parencite{rainforth_tighter_2019,roeder_sticking_2017,tucker_doubly_2019,bauer_generalized_2021}, especially works on large VAE models \parencite{maaloe_biva_2019,vahdat_nvae_2020,child_very_2021}. 

% As we discussed in \cref{chp:discussion}, there are many interesting avenues of future research for improving the ability of VAEs to learn useful representations, including better gradient estimates and masked objectives.

% In this chapter, we present a comprehensive overview of unsupervised neural representation learning for speech. Previous research is categorized into self"-supervised methods and probabilistic latent variable models and described in a common notation. This description assists in developing a model taxonomy that shapes a discussion of the models' representational power, the associated learning strategies, and the methods used to evaluate them. The discussion points to interesting avenues of future research. 

% Chapter 6 presented an overview of unsupervised neural speech representation learning. As emphasized in the overview, the wav2vec 2.0 framework [13], and masked pre-training in general, represent a breakthrough in low-resource speech recognition. The challenge associated with obtaining training data for conversational speech recognition, as discussed in section 1.2.1, has become much smaller. Even when no labeled in-domain data is available, this framework offers a viable solution [112].

% The general idea for masked pre-training may seem simple; reconstruct masked parts of the input or another representation, given context. However, this methodology aligns very well with the general definition of semantics presented in section 1.4: "semantic properties of a lexical item are fully reflected in appropriate aspects of the relations it contracts with actual and potential contexts" ([63], p. 1). Thus, from a philosophical point-of-view, it may be difficult to see how the field should move on from here. Of course, context is more than just the neighboring words in a speech segment. The next wave of representation learning for speech incorporates multiple modalities, such as video [244, 245] or text [18]. Furthermore, how to construct targets for pre-training these models and how that affects the learned features is an avenue that warrants more research. This is of particular interest in the speech domain, where the input codes for speaker identity and emotional state, as well as the semantic content of the utterance.

\vspace{1em}
\textbf{\Cref{chp:paper-benchmarking}} conducted a comprehensive evaluation of stochastic and deterministic generative models, focusing on their model likelihood. 
The chapter also introduced a novel hierarchical VAE type model for speech, by drawing inspiration from the Clockwork VAE \parencite{saxena_clockwork_2021}. 
Despite the limitations of VAEs for representation learning as compared to self"=supervised methods (\cref{chp:discussion}), for sequence data, hierarchical VAE models that operate on multiple temporal scales provide a natural framework for encoding distinct feature categories. 
For instance, pronunciation features might be learned at lower layers, speaker identity at upper layers, and semantic features in intermediate layers. Despite successful attempts at isolating speaker identity from content in some existing work \parencite{hsu_unsupervised_2017}, developing VAE models with the capacity to learn a deep hierarchy of features for speech persists. The model presented in \cref{chp:paper-benchmarking} is an attempt at this challenge.
% Despite the current limitations, VAEs possess a unique ability to learn a distribution over the training data, requiring them to encode diverse aspects of the dataset. This inclusivity, while potentially redundant for certain tasks, proves advantageous for others that benefit from a wide array of features. 

\vspace{1em}
\textbf{\Cref{chp:paper-automated}} examined existing works on automated medical coding and found that in many cases training was suboptimal and evaluation standards were biased. We performed a revised comparison of the selected models and provided updated conclusions on the relative performance of models, and the impact of rare codes and long discharge note documents. 

The practical impact of the work of \cref{chp:paper-automated}, and the work in the field of medical coding in general, depend on a number of factors. 
Much of the field has focused on the MIMIC datasets which originate from the emergency department and ICU of a single hospital. These datasets have enabled much of the progress in the field, but their singular data source also reduces the generality of the derived results and risks biasing the directions of research deemed most impactful \parencite{tengReviewDeepNeural2022, venkateshAutomatingOverburdenedClinical2023,johnsonMIMICIIIFreelyAccessible2016,johnsonMIMICIVFreelyAccessible2023}. 
Furthermore, the complexity and multi-label nature of medical coding has lead to a high prevalence of label errors in MIMIC-III \parencite{searleExperimentalEvaluationDevelopment2020}. While difficult to remove entirely, having multiple diverse datasets would also alleviate the risk that the biases of such errors lead to misguided conclusions. Besides gathering more data, improved evaluation methods, such as human-in-the-loop, could be useful to increase the reliability of results. 

A common weakness of medical coding models is that performance varies widely between classes. The long-tailed distribution of code-frequency is particularly challenging and leads to underperformance on rare codes. Practical applications could still benefit from such models though. By limiting model predictions to a subset of codes for which they perform well, practitioners could focus on harder to code cases. This selection of cases in practical applications of medical coding could also benefit from research into selective prediction \parencite{geifman_selective_2017}. 
Although pre"=training is an obvious approach to improve performance when little data is available, compared to the effect of pre"=training in other domains \parencite{mohamed_selfsupervised_2022, linPretrainedTransformersText2021,baevski_wav2vec_2020,devlin_bert_2018,dosovitskiy_image_2021}, the improvements observed by using pre"=trained models for medical coding have been limited \parencite{jiDoesMagicBERT2021,gaoLimitationsTransformersClinical2021,michalopoulosICDBigBirdContextualEmbedding2022,pascualBERTbasedAutomaticICD2021,zhangBERTXMLLargeScale2020}. This suggests an untapped potential for future research into more targeted pre"=training for medical coding. 

While our work in \cref{chp:paper-automated} focused on unimodal medical coding models, it is highly likely that future state-of-the-art models will augment discharge summaries with multi-modal inputs such as medical code descriptions, synonyms, and hierarchies \parencite{kimReadAttendCode2021, mullenbachExplainablePredictionMedical2018, vuLabelAttentionModel2020, caoHyperCoreHyperbolicCograph2020, baoMedicalCodePrediction2021, yuanCodeSynonymsMatter2022,caoHyperCoreHyperbolicCograph2020, xieEHRCodingMultiscale2019}. 
This is particularly useful since coding standards are updated regularly. 
The ICD standard for instance sees revisions and new codes added yearly, with local adoption following national guidelines \parencite{centerfordiseasecontrolandpreventioncdc_international_2023}. 
Leveraging such modalities will enable adapting models to updated coding standards without having to gather large amounts of new data. 


% The use of supportive systems for medical coding in practice relies on 

% - Errors in the gold-standard data: \textcite{searleExperimentalEvaluationDevelopment2020} investigated the quality of the human annotations in MIMIC-III and concluded that 35\% of the common codes were under-coded. Such errors and subjectivity in manual medical coding make model training and evaluation challenging and suggests that additional evaluation methods using, e.g., a human-in-the-loop, could be useful to increase the reliability of results. 

% Future directions
% - Avoiding predicting several mutually exclusive classes, which is a general problem for multi-label classification.

% - research should focus on improving performance on rare codes while, in the shorter term, developing methods to detect codes that are too challenging for automated coding and, therefore, should be coded manually

% - Fully leveraging pre"=training (while PLM-ICD outperforms the other models in this paper, the improvements are limited compared to the effect of pre"=training in other domains \parencite{mohamed_selfsupervised_2022, linPretrainedTransformersText2021,baevski_wav2vec_2020,devlin_bert_2018,dosovitskiy_image_2021}) 
% Notably, there have been several unsuccessful attempts at using pre"=trained transformers for medical coding \parencite{jiDoesMagicBERT2021,gaoLimitationsTransformersClinical2021,michalopoulosICDBigBirdContextualEmbedding2022,pascualBERTbasedAutomaticICD2021,zhangBERTXMLLargeScale2020}

% - Generalization to other hospitals. discharge summaries from outpatient care are often easier to code than summaries from inpatient care as they are shorter with fewer codes per document \parencite{zhangBERTXMLLargeScale2020, liuEffectiveConvolutionalAttention2021, tsengAdministrativeCostsAssociated2018}





\vspace{1em}
\textbf{\Cref{chp:paper-retrospective}} studied how machine learning might be used to improve decision-making at emergency services in relation to stroke detection. 
We saw that a model was able to improve significantly on the stroke recognition ability of call-takers alone and that the features it used were sensible and related to symptoms and descriptions of stroke. 

In \cref{sec: discussion-stroke-recognition-uncertainty}, we discussed how calibrating the predictive uncertainty of the stroke model is likely to be necessary to ensure sustained use of such a model in practice. In \cref{fig_discussion:retrospective-paper-f1-performance-vs-predicted-probability}, we noted that, as expected, model performance measured by F1-score increased with increasing model certainty which underlines the likely usefulness of uncertainty estimates in better matching practitioner expectations of the predictive performance. 
Nonetheless, basic metrics of model performance might still be obstacles for its practical usefulness. Specifically, the rarity of stroke cases lead to relatively high false positive rate and low precision, likely to induce alarm fatigue among its users. Similar effects are likely to have influenced the practical impact of a similar system for cardiac arrest detection which also showed significant improvements in a retrospective study \parencite{cite14}. Although the model later matched the retrospective results in a prospective study, it did not ultimately result in improved call-taker performance \parencite{cite15}. 

Nevertheless, the strong retrospective performance of the stroke recognition model indicates that there is significant potential for augmenting the medical interview to allow better recognizing stroke cases. 
Possible improvements to the system could include using the audio signal to detect speech-related symptoms such mumbling or slurring, or integrating with electronic health-records to cross-reference with patient history. 
Even so, directly predicting the diagnosis from the conversation is not the only path towards practical impact. By suggesting informative questions to the medical professional, a system could also help guide the course of conversation to avoid missing important details and keep an overview, and to in turn improve the performance of the model. 
Ultimately, machine learning has proven capable of contributing meaningfully to medical conversations aiming to improve patient outcomes. Future work seems poised to make a significant positive impact on the healthcare industry over the years to come. 


% Explore the utilization of audio data 
% in conjunction with EHR information to gain a comprehensive understanding of the patient's condition.


% Suggesting Questions:

% Integrate the ML system into the interview framework to dynamically suggest relevant questions based on the ongoing conversation. This approach leverages the model's retrospective success, guiding clinicians to inquire about specific symptoms or risk factors that may contribute to more accurate stroke identification.

% Fact/Sanity-Checking with Electronic Health Record (EHR):

% Establish a seamless connection between the ML model and the patient's EHR to perform real-time fact-checking during the interview. This ensures the accuracy of the information provided by the patient, enhancing the reliability of the diagnostic process.

% Improved Understanding of the Patient Using Audio and EHR:

% Explore the utilization of audio data in conjunction with EHR information to gain a comprehensive understanding of the patient's condition. By analyzing speech patterns, tone, and content, the ML model can contribute valuable insights to the diagnostic process. Additionally, cross-referencing this audio data with EHR details enhances the model's ability to discern subtle indicators of stroke risk or occurrence.


% - Suggesting questions
% - Fact/sanity-checking with electronic health-record
% - For improved understanding of the patient: Using audio directly, using electronic health-record, 



% easily see its retrospective performance transferred to practical impact, but its 

% Directly predicting the diagnosis from the conversation may not be the best path 

% There are many promising paths forward. 






