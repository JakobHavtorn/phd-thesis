%!TEX root = ../thesis.tex

\chapter[discussion]{Discussion}\label{chp:discussion}
% ~5 pages
%
% OUTLINE:
% - paper-hierarchical:
%   - Suitability of VAEs for representation learning (minimization of mutual information and sensitivity to implicit prior such as architecture)
% - paper-benchmarking
%   - Inferiority of probabilistic methods compared to self"=supervised learning. 
% - 

During the three years since the start of this project, the rapid development of machine learning that started about a decade ago with the ImageNet competation and the work of \textcite{krizhevsky_imagenet_2012} all but slowed down. 
In this section we will briefly revisit selected chapters of the thesis and provide some additional thoughts and discussion. 


\section{Speech representation learning}
%
In this thesis we have studied two different approaches to learning speech representations. Variational autoencoders in \cref{chp:paper-hierarchical,chp:paper-modelagnostic,chp:paper-benchmarking} and self"=supervised methods in \cref{chp:paper-review}. 
We found that the probabilistic formulation of variational autoencoders provide benefits for their application to uncertainty quantification but that they are generally inferior to self"=supervised methods when it comes to performance on downstream tasks such as phoneme recognition (see e.g. \cref{tab: phoneme recognition (PER), tab:unsupervisedASR}). 

VAEs are good at uncertainty quantification (let's at least say that they are).

SSL methods are very good at representation learning as reflected by their dominance on downstream task performance in for text and speech.

It seems like we cannot have a model that has both a principled probabilistic representation of uncertainty and the ability to learn rich representations that are useful for downstream applications. 




\subsection{Improved representation learning with variational autoencoders}

One way to improve the representations learned in VAEs is via semi-supervised learning. 
Here a small number of curated labels are used to inform and enrich the patterns that can be learned from a large unlabelled data set. 
VAEs are strong for semi-supervised learning but there are other strong alternatives too \cite{jiang_speech_2021}. Additionally, semi-supervised learning is more constraining than is self-supervised methods that rely only on pretraining; a single foundation model can be fine-tuned at relatively low cost for a large number of downstream tasks, while semi-supervised models must learn all relevant downstream tasks while simultaneously learning from the unlabelled data, which is expensive. 



\subsection{Uncertainty estimation with self-supervised methods}
%


\textcite{pasad_layerwise_2021} found that layers 6-7 hold the most information about phonetic content and word identity and meaning for \texttt{wav2vec2-base}. 
For \texttt{wav2vec2-large} the phonetic content is highest in layers 11 and 18-19 with a drop in between, while word identity and meaning are highest between layers 12 and 18. 

(layers )



Masking in variational autoencoders

Masked pretrained as maximum likelihood





\section{\Cref{chp:paper-hierarchical} revisited: \dots}

\lesstodo[inline]{Discuss sensitivity to ``implicit" prior such as architecture (and probably optimization method and other). Include reference to \parencite{huszar_is_2017} discussing the usefulness of using a maximum likelihood objective for representation learning in generative models.}
\lesstodo[inline]{Discuss whether VAEs are even suitable for representation learning due to them minimizing a mutual information term in the ELBO (derive this form).}
\lesstodo[inline]{Discuss some recent work e.g. \cref{morningstar_density_2021}}


\section{\Cref{chp:paper-benchmarking} revisited: Bested?}

\lesstodo[inline]{Discuss whether variational autoencoders are viable for learning good representations for speech for downstream tasks when alternatives such as SSL method exist.}
\lesstodo[inline]{Out-of-distribution detection on speech?}


\section{\Cref{chp:paper-review} revisited: \dots} \label{sec:discussion-paper-review}

\lesstodo[inline]{Are self"=supervised speech representations useful for unsupervised uncertainty estimation? \parencite{nava_stateconsistency_2021, nava_uncertaintyaware_2021} Within robotics but not really related.}
% Since the main focus within self"=supervised learning has been on improving downstream task performance, very limited work, if any, has investigated self"=supervised representations in terms of uncertainty estimation. 
% However, in the context of medical applications where data can be abundant but labels are sparse, unsupervised uncertainty estimation is a very interesting direction for future work.
\lesstodo[inline]{OOD data: Generalization or detection (https://arxiv.org/pdf/2110.11334.pdf)?}


\section{\Cref{chp:paper-retrospective} revisited: Calibration}
The work in \textcite{wenstrup_retrospective_2023} focuses on the predictive performance of the ensemble model and an analysis of feature importance, but does not explicitly consider uncertainty estimation. 
As we discussed in \cref{subsec:model-calibration}, such a model can be calibrated to predict probabilities that are aligned with the empirical probability of the model being correct on some validation set.

We compute the calibration curve for the stroke recognition ensemble by sorting the predicted probabilities on test set into a number of bins spanning the range from zero to one. For each bin, we compute the fraction of examples for which the model predicted correctly and then plot these pairs of mean predicted probabilities and correctness fractions. A perfectly calibrated model would have the same fraction of correct predictions in any given bin, as that bin's mean value. 
In \cref{fig_discussion:retrospective-paper-calibration-curve-of-uncalibrated-model}, we plot the calibration curve for the uncalibrated stroke recognition ensemble and the individual models along with a histogram of its predicted probabilities. The miscalibration issue that we previously discussed is clearly visible as a strong overconfidence. 

\begin{figure}
    \begin{subfigure}[c]{0.48\columnwidth}
        \centering
        \includegraphics[width=1\columnwidth]{paper_retrospective_calibration_plots/calibration_curve_ensemble_and_all_models_uncalibrated.pdf}
        % \caption{}
        % \label{fig_discussion:calibration_curve_ensemble_and_all_models_uncalibrated}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[c]{0.48\columnwidth}
        \centering
        \includegraphics[width=1\columnwidth]{paper_retrospective_calibration_plots/histogram_ensemble_and_single_model.pdf}
        % \caption{}
        % \label{fig_discussion:histogram_ensemble_and_single_model}
    \end{subfigure}
    \caption[Calibration curve for the uncalibrated stroke recognition model and empirical distribution of predicted probabilities.]{%
        Calibration curve for the uncalibrated stroke recognition ensemble model (left) and the histogram of predicted probabilities (right). 
        We use the ensemble that achieved the median F1-score which is the one also reported in \cref{tab_retrospective:table3-occlusion-analysis} and \cref{fig_retrospective:figure1-roc-curve}.}
    \label{fig_discussion:retrospective-paper-calibration-curve-of-uncalibrated-model}
\end{figure}

It is interesting to note that the ensemble model is better calibrated than the individual models that form it. Since an ensemble output probability is computed as the harmonic mean of the five individual model probabilities, it is easy to see that the ensembles output probability can never exceed the maximum probability predicted among the individual models. This property tends to make ensemble probabilities less extreme and, since the individual models are overconfident, this results in better calibration (see also the histogram in \cref{fig_discussion:retrospective-paper-calibration-curve-of-uncalibrated-model}). 

To calibrate the ensemble model, we can use methods such as Platt-scaling \parencite{platt_probabilistic_1999} or isotonic regression \parencite{zadrozny_transforming_2002}. 
In both cases, we fit a simple regression model (logistic or isotonic) to the predicted probabilities and the target labels on some validation set. 
In \cref{fig_discussion:retrospective-paper-calibration-curve-sigmoid-isotonic} we have done so for the ensemble model using our validation set for fitting and visualizing the results on the test set. On the left, we plot the resulting calibration curves and on the right we show the logistic and isotonic fits. We see that both methods result in quite good calibrations\footnote{Test set Brier scores: Uncalibrated = $0.003500$, logistic = $0.001807$, isotonic = $0.001774$. Relative improvement in Brier score compared to uncalibrated (Brier skill score): Logistic = $0.4830$, isotonic = $0.4924$.} and that the predicted probabilities are shifted towards smaller values. 

\begin{figure}
    \begin{subfigure}[c]{0.48\columnwidth}
        \centering
        \includegraphics[width=1\columnwidth]{paper_retrospective_calibration_plots/calibration_curves_ensemble.pdf}
        % \caption{}
        % \label{fig_discussion:calibration_curve_ensemble_and_all_models_uncalibrated}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[c]{0.48\columnwidth}
        \centering
        \includegraphics[width=1\columnwidth]{paper_retrospective_calibration_plots/calibration_fits_ensemble.pdf}
        % \caption{}
        % \label{fig_discussion:histogram_ensemble_and_single_model}
    \end{subfigure}
    \caption[Calibration fits and curves for the stroke recognition model using Platt-scaling and isotonic regression for calibration.]{ Calibration curves using sigmoid and isotonic calibration fits for the stroke recognition ensemble model (left) and the calibration fits (right). We use the ensemble that achieved the median F1-score which is the one also reported in \cref{tab_retrospective:table3-occlusion-analysis} and \cref{fig_retrospective:figure1-roc-curve}.}
    \label{fig_discussion:retrospective-paper-calibration-curve-sigmoid-isotonic}
\end{figure}

While correct calibration provides some interpretability to the predictions, the actual values of the probabilities are still tied of the model's performance. For example, with the logistic calibration, we can note that the predicted probabilities have been shifted so far towards smaller values that the highest possible predicted probability is about 50\%. This means strokes can be predicted at best with a 50\% chance of being correct. 
If this model were to be deployed in a randomized controlled trial, the calibrated probabilities might be incorporated in the design of the user interface such that a user can discern between confident and uncertain model predictions. However, it is easy to imagine how this generally low confidence might lead the user to ignore the model. This highlights a problem with uncertainties

% Brier scores:  {'uncalibrated': 0.0034955788687128925, 'logistic': 0.0018072483306197486, 'isotonic': 0.0017744177396100578, 'average': 0.0021955477869598783}
% BSS uncalibrated reference:  {'logistic': 0.48299025755205005, 'isotonic': 0.4923822902432705}
% BSS mean reference:  {'logistic': 0.17685766561145932, 'isotonic': 0.1918109229282361}


% \lesstodo[inline]{Discuss the calibration of the stroke model and report calibration curve.}
% \lesstodo[inline]{Report the results of calibrating the stroke model using e.g. Platt scaling or Isotonic scaling.} % https://scikit-learn.org/stable/modules/calibration.html

% \lesstodo[inline]{Maybe mention MultiQT paper \parencite{havtorn_multiqt_2020}.}
