\chapter[supplementary material: a retrospective study on machine learning-assisted stroke recognition for medical helpline calls]{Supplementary Material for: A Retrospective Study on Machine Learning-Assisted Stroke Recognition for Medical Helpline Calls}
\label{app:supplementary-retrospective}
\ifthenelse{\equal{\skipappendices}{true}}{}{

\section{Data flow digram}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{paper_retrospective/data_flowchart.pdf}
    \caption[Overview of data flow from the initial data sources to the final stroke dataset.]{Overview of data flow from the initial data sources to the final stroke dataset.}
    \label{fig_retrospective:data_flowchart}
\end{figure}


\section{Machine learning pipeline}

\subsection{Model training}

We used stochastic gradient descent on mini-batches of data to train the stroke classification model. We used the Adam (adaptive moment estimation) optimisation algorithm and ensured an equal number of stroke positives and negatives in each batch by stratifying the class labels during sampling. We saved the model parameters after each epoch if the maximum F1-score (across all possible thresholds) improved in the validation dataset. We used the latest saved parameters as the final result of the run.

\subsection{Hyperparameters}\label{sec_retrospective:hyperparameters}

The selection of hyperparameters followed a simple two-stage process using validation data (table A1). First, a manual search was conducted by running different model configurations with varying numbers of epochs, updates per epoch, batch sizes, vectoriser types, and hyperparameters. Subsequently, a structured grid search was performed to further tune a subset of these hyperparameters.

\begin{table}[h]
    \centering
    \caption{Overview of hyperparameters used for training the text classification models.}
    \label{tab_retrospective:tableA1-hyperparameters}
    \resizebox*{0.98\textwidth}{!}{%
    \begin{tabular}{l|l|l}
        \toprule

        Name & Chosen value & Grid search range \\

        \midrule

        \multicolumn{3}{c}{\emph{Determined from initial heuristic, manual hyperparameter search on validation fold}}\\

        \midrule

        Epochs & 30 & - \\
        Parameter updates per epoch & 500 & - \\
        Batch size & 128 & - \\
        Batch sampling & Label stratified (balanced) & - \\
        Type of text vectoriser & Count vectorisation & - \\
        Size of bag-of-words vector & 10,000 & - \\
        Optimisation algorithm & Adam $(\beta_1=0.9, \beta_2=0.999)$ & - \\
        Learning rate schedule & Cosine annealing from start & - \\

        \midrule

        \multicolumn{3}{c}{\emph{Determined from grid search on validation fold}} \\ 

        \midrule

        Learning rate start & 0.0003 & \{0.003, 0.0003\} \\
        Learning rate end & 0.0003 & \{0.0003, 0.00003, 0.000003\} \\
        Model input dropout & 0.50 & \{0.25, 0.50\} \\
        Model dropout & 0.25 & \{0.25, 0.50\} \\
        Model configuration & [256, 128, 64, 32, 16] & \{[64, 32, 16], [256, 128, 64, 32, 16]\} \\

        \bottomrule
    \end{tabular}%
    }
\end{table}


\subsubsection{Bag-of-words selection}

Each transcript was transformed into a fixed-size bag-of-words vector to serve as input for the classification model. These vectors encode the occurrence of words and character n-grams within a fixed vocabulary. We selected vocabulary by first computing the χ2-statistics for all word uni-and bi-grams and character three-, four-, and fivegrams that occurred in more than ten training calls. We then retained the M highest-scoring word n-grams and $M$ highest-scoring character n-grams, yielding $2M$ input features, where $M$ represents a tuned hyperparameter. By complementing word n-grams with character n-grams, the model can use out-of-vocabulary words not included in the word n-grams and robustly represent words misspelt by the speech recogniser. The feature vector was input into the classification model (\cref{fig_retrospective:model_sketch}).

As part of our manual hyperparameter search, we trained the models using vectorisers of different sizes. We discovered that using 5,000-word n-grams and 5,000-character n-grams (M=5,000) struck a good trade-off between size, feature quality, and model performance, yielding 10,000 bag-of-words features.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\textwidth]{paper_retrospective/model_sketch.pdf}
    \caption[Overview of machine learning pipeline for stroke recognition.]{Overview of machine learning pipeline. Panel A presents a schematic overview of the machine learning pipeline. The individual models are broken down in panels B and C. The 2D convolutional layers have parameters [input channels, output channels]-[kernel width, kernel height]-[stride width, stride height]. The long short-term memory (LSTM) blocks have parameters hidden units-number of layers. The rectified linear unit (ReLU), sigmoid, and softmax layers have parameters [input features, output features]. Joining arrows indicate the concatenation of two vector sequences along the feature dimension. The full set of model hyperparameters is listed in \cref{sec_retrospective:hyperparameters}.}
    \label{fig_retrospective:model_sketch}
\end{figure}

\subsection{Ensembling details}

A common way to combine individual classification models into an ensemble is to use a voting scheme, such as majority voting, where a combined prediction is made based on the consensus among the individual models. However, this approach makes the ensemble not have a continuous output score. This is problematic for two reasons.
%
\begin{itemize}
    \item The lack of a continuous output score prevents the evaluation of the model's performance across a continuous range of thresholds required for plotting the receiver operating characteristic and precisionrecall curves.
    \item The lack of a continuous output score deteriorates the quality of assessing the effect of different words on ensemble performances (see \cref{sec_retrospective:model_explainability}).
\end{itemize}
%
Therefore, we used a different ensemble method, which is briefly described in the main text. Herein, we provide a mathematically rigorous definition of the proposed method.

Let $z^{(n,d)}$ be the logit output of model $n$ for transcript $d$, $t^{(n)}$ be the tuned logit threshold of model $n$, and $N$ be the number of models in the ensemble. The output score $p^{(n)}$ is then given by 
%
\begin{equation}
    p^{(d)} = \frac{1}{N} \sum_{n=1}^N \sigma(z^{(n,d)} - t^{(n)}) \enspace ,
\end{equation}
%
where $\sigma(\cdot)$ is the sigmoid function (or standard logistic function). The final ensembel predction $s^{(d)}$ is then simply 
%
\begin{equation}
    s^{(d)} = I_{p^{(d)} > 0.5}(p^{(d)}) \enspace ,
\end{equation}
%
where $I_{\cdot}(\cdot)$ is the indicator function that returns 1 if the subscript condition is satisfied and 0 otherwise. 


\section{Significance testing and confidence intervals}

We used standard methods for significance testing and computing the confidence intervals. We used approximate methods owing to data size and to maintain computational feasibility \cite{dwass_modified_1957,eden_validity_1933}

We performed \emph{paired approximate permutation} tests by pairing each observation from the first sample to a random observation from the other sample (without replacement), while keeping each observation within its original sample. This allowed us to test the significance of the observed pairings on the chosen statistics, i.e. whether a significant difference was observed in the test statistics depending on whether the call taker or model made the prediction. We used this approach to test whether
%
\begin{itemize}
    \item the model performed better than the call takers on the 2021 test set,
    \item including 112 training data improved the model's performance on the 2021 test set.
\end{itemize}
%
We performed \emph{independent approximate permutation} tests by randomly assigning observations to either of the two samples (without replacement) while maintaining any differences in sample size. This approach allowed us to test the significance of the observed sample assignments, i.e. whether there was a significant difference in test statistics depending on whether the predictions were assigned to the model or call taker. We used this approach to test whether.
%
\begin{itemize}
    \item the model performed better on the 2021 test set with diagnostic categories than on the test set without diagnostic categories,
    \item the model performed better on men than women on the 2021 test set,
    \item the model performed better on the 65+ group than on the 18-64 group on the 2021 test set,
    \item the call taker performed better on men than women on the 2021 test set,
    \item the call taker performed better on the 65+ group than on the 18-64 group on the 2021 test set.
\end{itemize}
%
The p-values were not exact because we used approximate permutation tests. However, owing to the large dataset size and substantial number of observations, the estimated p-values had tight confidence intervals. We reported the upper bound of the 99\% confidence interval on the p-value computed as the usual binomial distribution confidence interval.
%
\begin{equation}
    \text{CI}(p) = \hat{p} \pm z \sqrt{\frac{\hat{p}(1-\hat{p})}{N}} \enspace ,
\end{equation}
%
where $N$ is the number of resamplings.

We computed \emph{bootstrapped confidence intervals} for the statistics by resampling (with replacement) the predictions made by model or call taker on a relevant subgroup and recomputing the relevant statistics for each bootstrap sample. This process established a bootstrap distribution of the statistic that was then used to estimate the standard error and compute the confidence intervals. We computed confidence intervals using the bootstrap distribution percentiles. This method yielded reliable results because our dataset was large, and the bootstrap distribution was symmetrical and centred on the observed statistic. No observed bootstrap distributions differed significantly from normal distributions (according to Anderson-Darling and Shapiro-Wilk tests). Therefore, confidence intervals computed alternatively as studentised bootstrap intervals (t-intervals) did not differ substantially from percentile confidence intervals. This further validated our tests.

We used $N=15,000$ resamplings for permutation tests and $N=8,192$ for confidence intervals.


\section{Software}

We used Python version 3.8.10. PyTorch version 1.12.1 + cu113 was used to train the neural network models. We used SciKit-Learn (version 1.2.2) to perform bag-of-words vectorisation. We used NumPy version 1.23.5, Pandas version 1.5.3, Matplotlib version 3.7.1, and SciPy version 1.10.1 to perform data analysis, plotting, and testing.


\section{Additional results: Model performance across demographics}

When 1-1-2 data were not used for training (\cref{tab_retrospective:tableA2}), the model performed significantly better in men than in women on the test set in terms of all metrics (p < 0.0001) and significantly better on the 65+ group than on the 1864 group in terms of all metrics (p < 0.0001). As noted in the main text, both these statements were also true when 1-1-2 data were used for the training (\cref{tab_retrospective:table2}) and that including 1-1-2 data significantly improved overall performance (p < 0.0001).

Compared to the test set (\cref{tab_retrospective:table2}), model performance on the 2021 calls without diagnostic category was significantly worse in all demographic subgroups (\cref{tab_retrospective:tableA3}) in terms of all metrics (p < 0.0001), except for positive predictive value (PPV) for male (where p = 0.0056 [significant]) and for the false positive rate (FPR) on females and individuals aged 65+ years (where p = 0.213 and p = 0.362, respectively [insignificant]).

% \todo[inline]{Insert table A2 here}
\begin{table}[h]
    \centering
    \caption{Model performance based on sex and age [mean (95\% CI)] when 1-1-2 training data is not used for training.}
    \label{tab_retrospective:tableA2}
    % \resizebox*{0.98\textwidth}{!}{%
    \begin{tabular}{l|cc|cc}
        \toprule

         & \multicolumn{2}{c|}{Model} & \multicolumn{2}{c}{Model} \\
        \midrule
         & Female & Male & 18-64 years & 65+ years \\

        \midrule

        F1-score [\%] $\uparrow$ & \makecell{28.7 \\ (27.8-29.6)} & \makecell{36.2 \\ (35.3-37.2)} & \makecell{20.5 \\ (19.5-21.5)} & \makecell{39.4 \\ (38.6-40.3)} \\
        \midrule
        Sensitivity [\%] $\uparrow$ & \makecell{58.2 \\ (56.7-59.9)} & \makecell{62.1 \\ (60.7-63.6)} & \makecell{53.0 \\ (50.9-55.0)} & \makecell{63.0 \\ (61.9-64.3)} \\
        \midrule
        PPV [\%] $\uparrow$ & \makecell{19.0 \\ (18.3-19.7)} & \makecell{25.6 \\ (24.8-26.4)} & \makecell{12.7 \\ (12.0-13.4)} & \makecell{28.7 \\ (27.9-29.5)} \\
        \midrule
        \makecell[l]{FOR [\%] $\downarrow$ \\ (1 - NPV)}  & \makecell{0.077 \\ (0.073-0.080)} & \makecell{0.102 \\ (0.097-0.106)} & \makecell{0.034 \\ (0.032-0.036)} & \makecell{0.318 \\ (0.305-0.331)} \\
        \midrule
        \makecell[l]{FPR [\%] $\downarrow$ \\ (1 - specificity)} & \makecell{0.45 \\ (0.446-0.463)} & \makecell{0.483 \\ (0.473-0.494)} & \makecell{0.264 \\ (0.258-0.270)} & \makecell{1.335 \\ (1.308-1.362)} \\

        \bottomrule
    \end{tabular}%
    % }
\end{table}


% \todo[inline]{Insert table A3 here}
\begin{table}[h]
    \centering
    \caption{Model performance based on sex and age [mean (95\% CI)] on the 2021 data without diagnostic category.}
    \label{tab_retrospective:tableA3}
    % \resizebox*{0.98\textwidth}{!}{%
    \begin{tabular}{l|cc|cc}
        \toprule

         & \multicolumn{2}{c|}{Model} & \multicolumn{2}{c}{Model} \\
        \midrule
         & Female & Male & 18-64 years & 65+ years \\

        \midrule

        F1-score [\%] $\uparrow$                                 & \makecell{30.4 \\ (29.3-31.5)} & \makecell{35.2 \\ (34.0-36.4)} & \makecell{24.3 \\ (23.0-25.7)} & \makecell{36.6 \\ (35.6-37.6)} \\
        \midrule
        Sensitivity [\%] $\uparrow$                              & \makecell{44.5 \\ (42.9-46.0)} & \makecell{52.8 \\ (51.1-54.4)} & \makecell{45.6 \\ (43.4-47.9)} & \makecell{49.2 \\ (47.9-50.5)} \\
        \midrule
        PPV [\%] $\uparrow$                                      & \makecell{23.1 \\ (22.2-24.1)} & \makecell{26.4 \\ (25.3-27.4)} & \makecell{16.6 \\ (15.5-17.5)} & \makecell{29.1 \\ (28.2-30.0)} \\
        \midrule
        \makecell[l]{FOR [\%] $\downarrow$ \\ (1 - NPV)}         & \makecell{0.152 \\ (0.146-0.159)} & \makecell{0.155 \\ (0.147-0.162)} & \makecell{0.058 \\ (0.055-0.062)} & \makecell{0.372 \\ (0.359-0.386)} \\
        \midrule
        \makecell[l]{FPR [\%] $\downarrow$ \\ (1 - specificity)} & \makecell{0.404 \\ (0.394-0.414)} & \makecell{0.480 \\ (0.467-0.494)} & \makecell{0.246 \\ (0.239-0.254)} & \makecell{0.873 \\ (0.852-0.894)} \\

        \bottomrule
    \end{tabular}%
    % }
\end{table}


\section{Additional results: Model with patient age and sex as explicit inputs}

To assess the importance of patient age and sex for accurate model stroke recognition we have performed an experiment to test whether explicitly adding the age and sex of the patient as inputs to the model improves performance.

Specifically, we encode patient sex as two binary numbers which we concatenate to the bag-of-words input representation. The first has the value of 1 if the patient is female, and the second has the value of 1 if the patient is male. Neither is 1 if the patient's sex is unknown/undisclosed. Similarly, we include patient age as a 15-dimensional one-hot vector where the first value represents patients with an age below 25 and the last value represents patients with an age of 90 or above. Values in between represent 5-year intervals.

We trained the model similarly to how we performed our ablation experiments. That is, we used the hyperparameters that gave the best performance on the validation set to train 11 differently seeded versions of the model and then report mean metrics and associated CIs on the MH-1813 test data. The results are as shown in \cref{tab_retrospective:tableA4} below.

\begin{table}[h]
    \centering
    \caption{Overall performance on MH-1813 test data for the model that also takes patient age and sex as direct inputs. We also list the original performance of call-takers and the model (w/o sex and age) from the main manuscript for ease of comparison [mean (95\% CI)].}
    \label{tab_retrospective:tableA4}
    \resizebox*{0.98\textwidth}{!}{%
    \begin{tabular}{l|ccccc}
        \toprule
        & F1-score [\%] $\uparrow$ & Sensitivity [\%] $\uparrow$ & PPV [\%] $\uparrow$ & \makecell[c]{FOR [\%] $\downarrow$ \\ (1 - NPV)} & \makecell[c]{FPR [\%] $\downarrow$ \\ (1 - specificity)} \\
        \midrule
        & \multicolumn{5}{c}{\textit{Overall}} \\
        \midrule
        
        Call-takers                             & \makecell[c]{25.8 \\ (23.7-27.9)} & \makecell[c]{52.7 \\ (49.2-56.4)} & \makecell[c]{17.1 \\ (15.5-18.6)} & \makecell[c]{0.105 \\ (0.094-0.116)} & \makecell[c]{0.565 \\ (0.539-0.590)} \\
        \midrule
        \makecell[l]{Model \\ w/o sex and age}  & \makecell[c]{35.7 \\ (35.0-36.4)} & \makecell[c]{63.0 \\ (62.0-64.1)} & \makecell[c]{24.9 \\ (24.3-25.5)} & \makecell[c]{0.082 \\ (0.079-0.085)} & \makecell[c]{0.419 \\ (0.413-0.426)} \\
        \midrule
        \makecell[l]{Model \\ w/ sex and age}   & \makecell[c]{35.8 \\ (35.1-36.5)} & \makecell[c]{64.1 \\ (63.1-65.1)} & \makecell[c]{24.9 \\ (24.3-25.4)} & \makecell[c]{0.080 \\ (0.077-0.082)} & \makecell[c]{0.427 \\ (0.421-0.434)} \\
 
        \bottomrule
    \end{tabular}%
    }
\end{table}

We note that the performance difference between the original model and the models with age and sex inputs is statistically insignificant (p > 0.05). We hypothesize this might be because information regarding the age and sex of the patient is, in many cases, already present in the transcript. Another reason might be that the patient's age and sex are less useful discriminators than other indicative factors. This latter hypothesis is supported by our occlusion analysis and \cref{tab_retrospective:table3} where no highly ranked words directly refer to the patient's age or sex.

It is important to note that in practice the information about patient sex and age is extracted from the patient's CPR number which is typically entered by the patient themselves while queuing for MH-1813. However, at call lines in countries without similar systems in place, such information may not be readily available and cannot be incorporated as an explicit input feature. The same is the case for the 1-1-2 call line in Denmark.


\section{Additional results: Model without MH-1813 training data}
%
The ablation study in the main manuscript that examines the importance of the two different source domains (1-1-2 and MH-1813) includes only two of the three possible combinations of data sources, specifically: training on both 1-1-2 and MH-1813 data, and training only on MH-1813 excluding 1-1-2 data. For this reason, we have conducted an experiment that includes the remaining combination: training only on 1-1-2 excluding MH-1813 data.

Since the manuscript focuses on the MH-1813 line, the main purpose of experimenting with including and excluding the 1-1-2 data was to examine whether using the out-of-domain 1-1-2 data could improve stroke recognition performance of the machine learning framework on the MH-1813 data. This has interest since many prehospital call centres operate both a high acuity emergency line (like 1-1-2) and a low-acuity medical helpline (like MH-1813) which makes high-acuity data available for modelling and a potentially valuable data source. The ablation experiment performed here, on the other hand, has a different purpose. It aims to show whether a model to assist the MH-1813 helpline could be developed also in the hypothetical case of only having access to out-of-domain, high-acuity training data from 1-1-2. This prospect may be interesting for some call centres that, for instance, have only recently started operating a medical helpline, and so, do not have in-domain training data available.

We trained the models and provided the results of this experiment in \cref{tab_retrospective:tableA5} below using the same methods as in the main manuscript. We tested the significance of these results using the same statistical significance testing methods as used and described in the main manuscript.

We see that the performance of the ensemble model trained only with 1-1-2 data compared to training only with MH-1813 data was worse in terms of sensitivity and FOR (p < 0.0001), but on-par in terms of F1-score and PPV, and better in terms of FPR (p < 0.0001). Compared to training with all data, training only on 1-1-2 was worse on all metrics (p < 0.0001) except FPR, where it was better (p < 0.0001). We note that the model still performed better than the call-takers in terms of F1-score, PPV, and FPR (p < 0.0001) and was on-par in terms of sensitivity and FOR.

In summary, training on only 1-1-2 data was only somewhat worse than training on only MH-1813 data, and still outperformed call-takers to some degree. This indicates that the domain shift between different call lines, even with different acuity levels, is small enough that naive domain transfer of models works fairly well.

\begin{table}[h]
    \centering
    \caption{Overall performance on MH-1813 test data, performance without 1-1-2 training data, and performance without 1813 training data [mean (95\% CI)].}
    \label{tab_retrospective:tableA5}
    \resizebox*{0.98\textwidth}{!}{%
    \begin{tabular}{l|ccccc}
        \toprule
        & F1-score [\%] $\uparrow$ & Sensitivity [\%] $\uparrow$ & PPV [\%] $\uparrow$ & \makecell[c]{FOR [\%] $\downarrow$ \\ (1 - NPV)} & \makecell[c]{FPR [\%] $\downarrow$ \\ (1 - specificity)} \\
        \midrule
        & \multicolumn{5}{c}{\textit{Overall}} \\
        \midrule
        
        Call-takers  & \makecell[c]{25.8 \\ (23.7-27.9)} & \makecell[c]{52,7 \\ (49,2-56.4)} & \makecell[c]{17.1 \\ (15.5-18.6)} & \makecell[c]{0.105 \\ (0.094-0.116)} & \makecell[c]{0.565 \\ (0.539-0.590)} \\
        \midrule
        Model        & \makecell[c]{35.7 \\ (35.0-36.4)} & \makecell[c]{63.0 \\ (62.0-64.1)} & \makecell[c]{24.9 \\ (24.3-25.5)} & \makecell[c]{0.082 \\ (0.079-0.085)} & \makecell[c]{0.419 \\ (0.413-0.426)} \\
        \midrule
        & \multicolumn{5}{c}{\textit{Without 1-1-2 training data (only MH-1813 data)}} \\
        \midrule
        
        Model        & \makecell[c]{32.4 \\ (31.8-33.1)} & \makecell[c]{60.4 \\ (59.3-61.4)} & \makecell[c]{22.2 \\ (21.6-22.7)} & \makecell[c]{0.088 \\ (0.085-0.091)} & \makecell[c]{0.467 \\ (0.460-0.474)} \\

        \midrule
        & \multicolumn{5}{c}{\textit{Without MH-1813 training data (only 1-1-2 data)}} \\
        \midrule

        Model        & \makecell[c]{31.4 \\ (30.7-32.1)} & \makecell[c]{50.4 \\ (49.3-51.4)} & \makecell[c]{22.8 \\ (22.2-23.4)} & \makecell[c]{0.110 \\ (0.106-0.113)} & \makecell[c]{0.375 \\ (0.369-0.381)} \\

        \bottomrule
    \end{tabular}%
    }
\end{table}


\section{Additional results: Detailed model explainability tables}

% % \todo[inline]{Insert table A4 here}
\begin{table}[h]
    \centering
    \caption{Mean impact for words with the largest positive rank score in calls predicted as stroke.}
    \label{tab_retrospective:tableA6}
    \resizebox*{\textwidth}{!}{%
    \begin{tabular}{l|llccc}
        \toprule
        \multicolumn{6}{c}{Stroke predictions, $D=1,897$} \\
        \midrule
         & Word, $w$ (\textit{Danish}) & Translation (\textit{English}) & Rank, $r^{(w)}$ & Occurences, $D^{(w)}$ & \makecell{Impact, $i^{(d,w)}$ \\ mean $\pm$ std.} \\
        \midrule    
        1. & Ambulance & Ambulance & $1.000$ & $1,$680 & $0.52 \pm 0.51$ \\
        2. & Blodprop & Blood clot & $0.599$ & $89$5 & $0.51 \pm 0.58$ \\
        3. & Venstre & Left & $0.381$ & $1,$108 & $0.38 \pm 0.4$ \\
        4. & Højre & Right & $0.326$ & $1,$050 & $0.31 \pm 0.42$ \\
        5. & Dobbeltsyn & Double vision & $0.247$ & $84$ & $1.01 \pm 1.26$ \\
        6. & Ordene & The words & $0.217$ & $34$4 & $0.6 \pm 0.45$ \\
        7. & Pludselig & Suddenly & $0.142$ & $78$3 & $0.29 \pm 0.28$ \\
        8. & Arm & Arm & $0.140$ & $70$9 & $0.3 \pm 0.3$ \\
        9. & Side & Side & $0.125$ & $1,$139 & $0.23 \pm 0.21$ \\
        10. & Apopleksi & Stroke & $0.102$ & $11$7 & $0.33 \pm 0.82$ \\
        11. & Dobbelt & Double & $0.102$ & $11$3 & $0.54 \pm 0.72$ \\
        12. & Styre & Control & $0.092$ & $13$4 & $0.63 \pm 0.46$ \\
        13. & Opkald & Call & $0.067$ & $39$ & $0.18 \pm 1.22$ \\
        14. & Følelsesløs & Numb & $0.065$ & $94$ & $0.53 \pm 0.58$ \\
        15. & Minutter & Minutes & $0.064$ & $76$3 & $0.22 \pm 0.16$ \\
        16. & Talebesvær & Difficulties speaking & $0.063$ & $44$ & $0.87 \pm 0.72$ \\
        17. & Hjerneblødning & Haemorrhagic stroke & $0.060$ & $13$3 & $0.4 \pm 0.49$ \\
        18. & Hånd & Hand & $0.057$ & $29$7 & $0.28 \pm 0.31$ \\
        19. & Ambulancen & The ambulance & $0.055$ & $52$1 & $0.21 \pm 0.23$ \\
        20. & Snøvler & Slurred speech & $0.052$ & $58$ & $0.71 \pm 0.54$ \\
        21. & Blodpropper & Blood clots & $0.051$ & $22$4 & $0.27 \pm 0.36$ \\
        22. & Hurtigt & Fast & $0.048$ & $66$3 & $0.18 \pm 0.18$ \\
        23. & Udtrykke & Express & $0.044$ & $44$ & $0.59 \pm 0.74$ \\
        24. & Blodfortyndende & Blood thinner & $0.044$ & $25$9 & $0.32 \pm 0.22$ \\
        25. & Usammenhængende & Incoherent & $0.043$ & $15$ & $1.14 \pm 1.13$ \\
        26. & Skæv & Lopsided & $0.039$ & $21$1 & $0.29 \pm 0.28$ \\
        27. & Nedsat & Reduced & $0.038$ & $52$8 & $0.14 \pm 0.21$ \\
        28. & Hænger & Hangs & $0.036$ & $62$8 & $0.15 \pm 0.17$ \\
        29. & Forbigående & Transient & $0.035$ & $48$ & $0.52 \pm 0.62$ \\
        30. & Vrøvler & Not making sense & $0.033$ & $14$ & $1.13 \pm 0.89$ \\
        \bottomrule
    \end{tabular}%
    }
\end{table}

% \todo[inline]{Insert table A5 here}
\begin{table}[h]
    \centering
    \caption{Mean impact for words with the largest negative rank score in calls predicted as non-stroke.}
    \label{tab_retrospective:tableA7}
    \resizebox*{\textwidth}{!}{%
    \begin{tabular}{l|llcrr}
        \toprule
        \multicolumn{6}{c}{Non-stroke predictions, $D=342,133$} \\
        \midrule
         & Word, $w$ (\textit{Danish}) & Translation (\textit{English}) & Rank, $r^{(w)}$ & Occurences, $D^{(w)}$ & \makecell{Impact, $i^{(d,w)}$ \\ mean $\pm$ std.} \\
        \midrule    
        1. & Stivkrampe & Tetanus & $-1.000$ & $4378$ & $-19.40 \pm 10.61$ \\
        2. & Gravid & Pregnant & $-0.901$ & $8749$ & $-12.08 \pm 8.64$ \\
        3. & Skåret & Cut & $-0.772$ & $7592$ & $-11.98 \pm 8.61$ \\
        4. & Forbinding & Bandage & $-0.569$ & $4561$ & $-12.87 \pm 10.08$ \\
        5. & Amager & Amager (a location) & $-0.566$ & $23776$ & $-5.60 \pm 4.43$ \\
        6. & Klokken & O'clock & $-0.535$ & $94436$ & $-2.22 \pm 2.69$ \\
        7. & Skadestuen & The emergency room & $-0.486$ & $42809$ & $-3.72 \pm 3.23$ \\
        8. & Politiet & The police & $-0.413$ & $2903$ & $-10.73 \pm 13.77$ \\
        9. & Hævet & Swollen & $-0.388$ & $60559$ & $-2.84 \pm 2.38$ \\
        10. & Håndkøb & over the counter (otc) & $-0.372$ & $4641$ & $-11.64 \pm 6.00$ \\
        11. & Halsen & The neck & $-0.366$ & $30151$ & $-3.33 \pm 3.86$ \\
        12. & Feber & Fever & $-0.361$ & $112586$ & $-1.94 \pm 1.76$ \\
        13. & Recept & Prescription & $-0.334$ & $5450$ & $-9.87 \pm 5.82$ \\
        14. & Centimetre & Centimetre & $-0.311$ & $12026$ & $-6.01 \pm 4.39$ \\
        15. & Knæet & The knee & $-0.300$ & $8875$ & $-6.12 \pm 5.91$ \\
        16. & Apoteket & The pharmacy & $-0.267$ & $10085$ & $-6.05 \pm 4.49$ \\
        17. & Maven & The stomach & $-0.267$ & $42105$ & $-2.36 \pm 2.82$ \\
        18. & Psykiatrisk & Psychiatric & $-0.263$ & $3688$ & $-8.99 \pm 8.49$ \\
        19. & Lungebetændelse & Pneumonia & $-0.231$ & $7597$ & $-5.79 \pm 5.62$ \\
        20. & Mavesmerter & Stomach pain & $-0.209$ & $10551$ & $-5.12 \pm 4.02$ \\
        21. & Afføring & Stool & $-0.199$ & $19155$ & $-3.40 \pm 3.27$ \\
        22. & Ribbenene & The ribs & $-0.195$ & $3928$ & $-8.26 \pm 6.18$ \\
        23. & Bløde & Bleed & $-0.194$ & $10501$ & $-4.88 \pm 3.97$ \\
        24. & Bløder & Bleeding & $-0.193$ & $24313$ & $-2.90 \pm 2.93$ \\
        25. & Ribben & Ribs & $-0.189$ & $2941$ & $-8.96 \pm 7.56$ \\
        26. & Brækket & Broken & $-0.183$ & $19415$ & $-3.49 \pm 2.83$ \\
        27. & Betændelse & Inflammation & $-0.181$ & $10050$ & $-5.27 \pm 3.30$ \\
        28. & Forkølet & Common cold & $-0.161$ & $8127$ & $-5.31 \pm 3.75$ \\
        29. & Morgen & Morning or morrow & $-0.160$ & $78558$ & $-1.23 \pm 1.70$ \\
        30. & Hævelse & Swelling & $-0.159$ & $17762$ & $-3.71 \pm 2.32$ \\

        \bottomrule
    \end{tabular}%
    }
\end{table}


\section{Additional results: Fine-tuning of Danish BERT model for stroke recognition}
%
During the preliminary experimental phase, we fine-tuned a BERT model pre-trained on Danish text from CommonCrawl, Wikipedia, OpenSubtitles, and other Danish online forums (available at \url{https://github.com/certainlyio/nordic_bert}). 
The model was fine-tuned for 10,000 updates using linear learning rate warm-up (1,000 updates) and decay (9,000 updates). 
The maximum learning rate was set to $5\times 10^{-5}$ and an accumulated batch size of 128. 
The maximum sequence length of the pre-trained model was 512 input tokens. 
To accommodate longer input sequences, which was necessary for our dataset, we concatenated several copies of the original positional embedding matrix. 
The results of the final model are presented in \cref{tab_retrospective:tableA8}. 

We see that the fine-tuned BERT model performs slightly worse across F1-score, sensitivity, PPV and FOR (p < 0.0001), but better in terms of FPR (p < 0.0001), compared the MLP model presented in the main manuscript. As described in the discussion section, we hypothesize that the number of stroke positives was too small for these advanced models to learn more complex patterns than the MLP ensemble. In addition, the BERT model would likely benefit from pre-training on text data from the target domain, or a domain close to it, rather than various online fora.

\begin{table}[h]
    \centering
    \caption{Overall performance on MH-1813 test data for the model that also takes patient age and sex as direct inputs. We also list the original performance of call-takers and the model (w/o sex and age) from the main manuscript for ease of comparison [mean (95\% CI)].}
    \label{tab_retrospective:tableA8}
    \resizebox*{0.98\textwidth}{!}{%
    \begin{tabular}{l|ccccc}
        \toprule
        & F1-score [\%] $\uparrow$ & Sensitivity [\%] $\uparrow$ & PPV [\%] $\uparrow$ & \makecell[c]{FOR [\%] $\downarrow$ \\ (1 - NPV)} & \makecell[c]{FPR [\%] $\downarrow$ \\ (1 - specificity)} \\
        \midrule
        & \multicolumn{5}{c}{\textit{Overall}} \\
        \midrule
        
        Call-takers                             & \makecell[c]{25.8 \\ (23.7-27.9)} & \makecell[c]{52.7 \\ (49.2-56.4)} & \makecell[c]{17.1 \\ (15.5-18.6)} & \makecell[c]{0.105 \\ (0.094-0.116)} & \makecell[c]{0.565 \\ (0.539-0.590)} \\
        \midrule
        \makecell[l]{MLP}                       & \makecell[c]{35.7 \\ (35.0-36.4)} & \makecell[c]{63.0 \\ (62.0-64.1)} & \makecell[c]{24.9 \\ (24.3-25.5)} & \makecell[c]{0.082 \\ (0.079-0.085)} & \makecell[c]{0.419 \\ (0.413-0.426)} \\
        \midrule
        \makecell[l]{BERT \\ (fine-tuned)}      & \makecell[c]{33.8 \\ (31.5-36.2)} & \makecell[c]{57.5 \\ (53.9-60.9)} & \makecell[c]{23.9 \\ (21.9-25.9)} & \makecell[c]{0.094 \\ (0.084-0.104)} & \makecell[c]{0.403 \\ (0.381-0.424)} \\
 
        \bottomrule
    \end{tabular}%
    }
\end{table}


\section{Simulation of a prospective study on 2021 data}

\subsection{Method}

The machine learning frameworks can be deployed in different forms in clinical practice. To assess the potential outcomes of deploying this framework in a future prospective study, we performed an experiment using the 2021 test data to simulate different scenarios. Each scenario included two main variables.
%
\begin{enumerate}[label=\Roman*.]
    \item When is the model prediction presented to the call taker?
    \item How does prediction influence the diagnostic code the call taker assigns to the call?
\end{enumerate}
%
There are two primary options as to when the model prediction is presented (I):
%
\begin{enumerate}
    \item Notify the call taker of potential false positive or negative stroke cases after the call ends.
    \item Notify the call taker of potential false positive or negative stroke cases during the call.
\end{enumerate}
%
Option 1 is identical to the method used in the main study. In option 2, predictions are made during the call based only on partial transcriptions. We implemented option 2 in such a manner that the model predicted every time 50 new words were transcribed and added to the transcript. A stroke positive was triggered only when three consecutive positive predictions were made (i.e. without intermediate negative stroke predictions). In other words, the sigmoid activation of the model had to remain above 0.5 for three consecutive predictions, for example, after 150, 200, and 250 words were transcribed.

As we can only assume how call takers are influenced by model predictions (II), precisely evaluating the hypothetical performance of call takers when supported by a machine learning framework is impossible. Furthermore, option 2 may influence the conversation, further complicating matters. Therefore, we report the results combining the call taker and the model under the following two assumptions:
%
\begin{enumerate}[label=\Alph*.]
    \item Call takers change any stroke prediction from negative to positive if the model predicts a positive (call takers mirror model positives).
    \item Call takers change any stroke prediction from positive to negative if the model predicts a negative (call takers mirror model negatives).
\end{enumerate}
%
By definition, method A tended to increase sensitivity and decrease PPV, whereas method B tended to decrease sensitivity and increase PPV.

We also report the results of the model itself (C). This method corresponds to call takers mirroring the model predictions exactly. This is not feasible in practice, although technically possible, because the conversation and instructions given to patients may conflict with the actions taken by the call taker after hanging up. Method 1.C is identical to the method employed in the main text, and we have copied the same results here for easier comparison.


\subsection{Results}

As expected (\cref{tab_retrospective:tableA9}), method 2.C (raw model predictions during calls) yielded slightly worse results than 1.C (raw model predictions after calls). Compared with method C, method A (call takers mirror model positives) led to increased sensitivity and decreased PPV, whereas method B (call takers mirror model negatives) led to decreased sensitivity and increased PPV, as expected. The numerical changes compared with method C are quite large because, in our simulation, the call taker is assumed to strictly follow methods A or B without divergence.

% \todo[inline]{Insert table A6 here}
\begin{table}[h]
    \centering
    \caption{Overall performance of model, call takers and simulated combinations of model and call takers on MH-1813 test data.}
    \label{tab_retrospective:tableA9}
    \resizebox*{0.98\textwidth}{!}{%
    \begin{tabular}{c|c|cc|cc|cc}
        \toprule

        Predictor & Call taker & \multicolumn{2}{c|}{Model} & \multicolumn{4}{c}{Call taker supported by the model (simulated)} \\
        \midrule
        When & - & After call & During call & After call & During call & After call & During call \\
        \midrule
        Method & - & 1.C & 2.C & 1.A & 1.B & 2.A & 2.B \\
        \midrule

        \makecell[l]{F1-score [\%] $\uparrow$}                   & \makecell[c]{25.8 \\ (23.7-27.9)} & \makecell[c]{35.7 \\ (35.0-36.4)} & \makecell[c]{33.1 \\ (32.4-33.7)} & \makecell[c]{28.9 \\ (28.3-29.5)} & \makecell[c]{33.3 \\ (32.5-34.1)} & \makecell[c]{27.6 \\ (27.0-28.1)} & \makecell[c]{32.7 \\ (31.8-33.5)} \\
        \midrule
        \makecell[l]{Sensitivity [\%] $\uparrow$}                & \makecell[c]{52.7 \\ (49.2-56.4)} & \makecell[c]{63.0 \\ (62.0-64.1)} & \makecell[c]{58.7 \\ (57.7-59.8)} & \makecell[c]{72.4 \\ (71.5-73.3)} & \makecell[c]{43.4 \\ (42.3-44.5)} & \makecell[c]{72.3 \\ (71.4-73.3)} & \makecell[c]{39.1 \\ (38.1-40.1)} \\
        \midrule
        \makecell[l]{PPV [\%] $\uparrow$}                        & \makecell[c]{17.1 \\ (15.5-18.6)} & \makecell[c]{24.9 \\ (24.3-25.5)} & \makecell[c]{23.0 \\ (22.5-23.6)} & \makecell[c]{18.0 \\ (17.6-18.4)} & \makecell[c]{27.0 \\ (26.3-27.8)} & \makecell[c]{17.0 \\ (16.7-17.4)} & \makecell[c]{28.1 \\ (27.3-28.9)} \\
        \midrule
        \makecell[l]{FOR [\%] $\downarrow$ \\ (1 - NPV)}         & \makecell[c]{0.105 \\ (0.094-0.116)} & \makecell[c]{0.082 \\ (0.079-0.085)} & \makecell[c]{0.091 \\ (0.088-0.094)} & \makecell[c]{0.061 \\ (0.059-0.064)} & \makecell[c]{0.125 \\ (0.121-0.129)} & \makecell[c]{0.061 \\ (0.059-0.064)} & \makecell[c]{0.134 \\ (0.131-0.138)} \\
        \midrule
        \makecell[l]{FPR [\%] $\downarrow$ \\ (1 - specificity)} & \makecell[c]{0.565 \\ (0.539-0.590)} & \makecell[c]{0.419 \\ (0.413-0.426)} & \makecell[c]{0.432 \\ (0.426-0.439)} & \makecell[c]{0.726 \\ (0.717-0.735)} & \makecell[c]{0.258 \\ (0.253-0.263)} & \makecell[c]{0.776 \\ (0.767-0.786)} & \makecell[c]{0.221 \\ (0.216-0.226)} \\

        \bottomrule
    \end{tabular}%
    }
\end{table}

Method 1.A (call takers mirror model positives after a call) yields a better F1-score, sensitivity, PPV, and FOR than call takers alone, although at the cost of a slightly higher FPR. This stands in contrast to methods 1.B, 2.A, and 2.B where either the sensitivity or the PPV is worse for the combined system than for call takers alone. Regardless, the F1-score (harmonic mean of sensitivity and PPV) is higher for all methods of combining call takers and model (1.A through 2.C).

These findings highlight that the implementation strategy selected for practice can substantially affect performance. Therefore, it may be possible to implement the system in a way that improves stroke recognition in practice.


\section{Research in context}

\subsection{Evidence before this study}

We searched the PubMed database for articles published in any language up to 9 May 2023 using the following terms: stroke AND (artificial intelligence OR machine learning OR deep learning) AND (EMS OR emergency medical services OR dispatch OR telephone). We identified 88 articles, none of which reported the results of machine learning-based stroke recognition during telephone calls.

One study assessed the potential impact of using speech classification software for stroke recognition by extrapolating results from a similar solution for recognising cardiac arrest. Several authors of this study co-authored the article; however, it did not report the results of an actual novel machine learning framework. The remaining articles primarily reported the use of machine learning in imaging diagnostics, stroke recognition using movement-tracking mobile devices, and the development of stroke recognition tools using other non-audio data.

\subsection{Added value of this study}

This study is the first to investigate the use of a machine learning framework for stroke recognition in medical helpline calls. The study's results can be replicated in other call lines and for other acute illnesses. A machine learning framework has been previously described for outof-hospital cardiac arrest; however, our results illustrate the feasibility of employing machine learning for detecting stroke - a more complex acute condition.

\subsection{Implications of all the available evidence}

Implementing the framework described in this study could lead to improved recognition of patients with stroke during initial contact with health services. Improving recognition would result in more patients being eligible for advanced stroke treatment and better overall outcomes owing to faster referral to a stroke unit.

}
